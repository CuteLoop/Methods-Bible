\chapter{Partial Differential Equations}

\section{First-Order PDE: Method of Characteristics}
% --- Narrative plan (auto-generated) ---
% This section introduces first-order partial differential equations and the method of characteristics, a geometric technique that converts certain PDEs into families of ordinary differential equations along special curves called characteristic curves. Instead of attacking the full PDE at once, we follow the flow of information along these curves, turning a complicated multivariable problem into simpler one-dimensional problems. This viewpoint is especially natural in models where something is transported or propagated with a given velocity, such as temperature in a moving fluid, cars in traffic flow, or waves with slowly varying shape.  The method of characteristics is central in applied mathematics because it links PDEs to dynamical systems: each characteristic is a trajectory determined by an ODE, and the PDE solution is reconstructed from this characteristic family. The same ideas reappear in Hamilton–Jacobi theory and classical mechanics, in geometrical optics and the eikonal equation, and in conservation laws and shock formation, which in turn motivate weak solutions and distribution theory. Conceptually, the method of characteristics sits at a crossroads with other areas such as Fourier analysis (where first-order PDEs sometimes linearize or simplify), complex analysis (through analytic continuation along curves), and numerical analysis (through characteristic-based schemes used in fluid dynamics and advection-dominated problems).

% ===== Example 1: Linear Transport Equation with Constant Velocity (inquiry-based) =====
\begin{problem}[Linear Transport Equation with Constant Velocity]
A pollutant is dissolved in water flowing steadily along a straight, one-dimensional channel. Let $x$ denote position along the channel and $t$ denote time. Suppose the fluid moves with constant velocity $c>0$, and the pollutant is simply carried along with the flow without reacting or diffusing. The pollutant concentration $u(x,t)$ is then modeled by the linear transport equation
\[
u_t + c\,u_x = 0,\qquad x\in\mathbb{R},\ t>0,
\]
together with an initial concentration profile
\[
u(x,0) = f(x),\qquad x\in\mathbb{R}.
\]
In this problem you will use the method of characteristics to discover and justify an explicit formula for $u(x,t)$ in terms of $f$.

\smallskip

(a) The physical description says that each individual fluid particle carries its initial concentration value as it moves. Imagine a single fluid particle that starts at position $\xi\in\mathbb{R}$ at time $t=0$ and then moves with the flow. 

\quad (i) Write an ordinary differential equation for the particle trajectory $x(t)$ if the flow speed is the constant $c>0$ and $x(0)=\xi$.  

\quad (ii) If the pollutant is simply carried (advected) by the flow without change, what does this say about the function $t\mapsto u(x(t),t)$ along this trajectory?

Hint: Think of $x(t)$ as the position of a specific tagged particle and recall that $x'(t)$ is its velocity.

\smallskip

(b) Next, we translate this physical picture into calculus. For a smooth function $u(x,t)$ and a smooth trajectory $t\mapsto x(t)$, compute the total derivative of $u$ along the path:
\[
\frac{d}{dt}u(x(t),t).
\]
Express this derivative in terms of $u_t$, $u_x$, and $x'(t)$.

Hint: Use the chain rule with two variables, $t\mapsto (x(t),t)\mapsto u(x(t),t)$.

\smallskip

(c) Now we want to choose particle trajectories in such a way that the PDE $u_t + c u_x = 0$ implies that $u$ is constant along those trajectories.

\quad (i) Using your expression from part (b), find a condition on $x'(t)$ under which the PDE
\[
u_t + c\,u_x = 0
\]
implies that $\dfrac{d}{dt} u(x(t),t) = 0$ along the trajectory $x(t)$.

\quad (ii) Solve the resulting ordinary differential equation for $x(t)$, using the initial condition $x(0) = \xi$.  

\quad (iii) For a fixed $(x,t)$ with $t>0$, solve your formula for $x(t)$ to express the “starting point” $\xi$ of the characteristic passing through $(x,t)$ in terms of $x$ and $t$.

Hint: You should find a family of straight lines in the $(x,t)$-plane, sometimes called characteristic curves.

\smallskip

(d) Because $\dfrac{d}{dt} u(x(t),t) = 0$ along each trajectory, the concentration carried by a fluid particle is constant in time.

\quad (i) Express $u(x(t),t)$ in terms of its initial value at $t=0$ along the same trajectory, using your answer to part (a)(ii).  

\quad (ii) Use your expression for the starting point $\xi$ from part (c)(iii) to rewrite $u(x,t)$ explicitly in terms of $f$ and the variables $x$ and $t$.  

\quad (iii) Check directly that your formula for $u(x,t)$ satisfies both the PDE $u_t + c\,u_x = 0$ and the initial condition $u(x,0)=f(x)$.

Hint: For (ii), substitute $\xi = \xi(x,t)$ into the formula $u(x(t),t) = f(\xi)$. For (iii), compute $u_t$ and $u_x$ for your candidate solution and verify the equation.

\smallskip

(e) Explore some variations and extensions.

\quad (i) Suppose the velocity $c$ is negative (that is, the flow is in the opposite direction). How does your characteristic formula change, and what is the new explicit solution in terms of $f$?  

\quad (ii) Consider now the transport equation with a time-dependent velocity,
\[
u_t + a(t)\,u_x = 0,\qquad u(x,0)=f(x),
\]
where $a(t)$ is a given smooth function of time. Write down the characteristic equation for $x(t)$, solve it formally in terms of an integral involving $a(t)$, and write the corresponding representation for $u(x,t)$.

Hint: You will obtain $x(t)$ by integrating $x'(t) = a(t)$, and the solution will again have the form $u(x,t)$ equals $f$ evaluated at a suitable shifted position.
\end{problem}

% ===== Example 1: Linear Transport Equation with Constant Velocity (full solution) =====
\begin{problem}[Linear Transport Equation with Constant Velocity]
Solve the initial value problem
\[
u_t + c\,u_x = 0,\qquad x\in\mathbb{R},\ t>0,\quad c\in\mathbb{R}\ \text{constant},
\]
with initial condition
\[
u(x,0) = f(x),\qquad x\in\mathbb{R},
\]
where $f$ is a given smooth function. Use the method of characteristics to derive an explicit formula for $u(x,t)$, and verify that your formula satisfies both the PDE and the initial condition. Briefly explain how this example illustrates the method of characteristics for first-order PDEs.
\end{problem}

\begin{solution}
We are given the linear first-order partial differential equation
\[
u_t + c\,u_x = 0
\]
for a function $u(x,t)$, together with the initial condition
\[
u(x,0) = f(x).
\]
The constant $c$ represents the (one-dimensional) velocity of the flow. The physical interpretation is that the quantity $u$ is transported by the flow without change along particle paths. The method of characteristics encodes this idea mathematically by looking for curves in the $(x,t)$-plane along which the PDE reduces to an ordinary differential equation.

Let us consider a curve (a characteristic) of the form $t\mapsto x(t)$, and examine how $u$ varies along this curve. Define
\[
U(t) := u(x(t),t).
\]
By the chain rule for functions of two variables,
\[
\frac{dU}{dt} = \frac{d}{dt}u(x(t),t)
= u_t(x(t),t) + x'(t) u_x(x(t),t).
\]
We would like to choose $x(t)$ so that the PDE $u_t + c u_x = 0$ tells us something simple about $dU/dt$. If we choose $x'(t)=c$, then
\[
\frac{dU}{dt}
= u_t(x(t),t) + c\,u_x(x(t),t)
= u_t + c\,u_x,
\]
evaluated at $(x(t),t)$. But our PDE states that $u_t + c u_x = 0$ everywhere, hence along such a curve we obtain
\[
\frac{d}{dt}u(x(t),t) = 0.
\]
Thus $u$ is constant along any curve satisfying
\[
x'(t) = c.
\]
The curves in the $(x,t)$-plane on which $u$ is constant are called characteristic curves of the PDE.

We now solve the ordinary differential equation
\[
x'(t) = c,\qquad x(0) = \xi,
\]
where $\xi$ labels the starting point of the characteristic at time $t=0$. Integrating, we find
\[
x(t) = \xi + c t.
\]
Equivalently, for a given point $(x,t)$ with $t>0$, the value of $\xi$ corresponding to the characteristic passing through $(x,t)$ is obtained by solving for $\xi$:
\[
\xi = x - c t.
\]
Since $u$ is constant along each characteristic, its value at time $t$ and position $x(t)$ equals its initial value at $t=0$ and position $x(0)=\xi$. That is,
\[
u(x(t),t) = u(\xi,0).
\]
The initial condition tells us that $u(\xi,0) = f(\xi)$. Therefore,
\[
u(x(t),t) = f(\xi).
\]
Rewriting this in terms of $(x,t)$, we substitute $\xi = x - c t$:
\[
u(x,t) = f(x - c t).
\]
This gives an explicit expression for the solution at all times $t\ge 0$ and all positions $x\in\mathbb{R}$.

It remains to verify that this formula indeed solves the initial value problem. First, we check the initial condition. At $t=0$ we have
\[
u(x,0) = f(x - c\cdot 0) = f(x),
\]
which agrees with the prescribed initial data.

Next, we check that $u(x,t) = f(x - c t)$ satisfies the PDE. Since $f$ is smooth, $u$ is smooth and we may differentiate under the composition. Differentiating with respect to $t$ and $x$ gives
\[
u_t(x,t) = f'(x - c t)\cdot(-c) = -c\, f'(x - c t),
\]
\[
u_x(x,t) = f'(x - c t)\cdot 1 = f'(x - c t),
\]
where $f'$ denotes the derivative of $f$ with respect to its single argument. Substituting into the PDE,
\[
u_t + c\,u_x = \bigl(-c\, f'(x - c t)\bigr) + c\,\bigl(f'(x - c t)\bigr) = 0.
\]
Thus the PDE is satisfied identically. We have therefore shown that
\[
u(x,t) = f(x - c t)
\]
is a solution of the given initial value problem.

Conceptually, this example illustrates the core idea of the method of characteristics for first-order partial differential equations. We seek curves in the domain, here given by $x'(t)=c$, along which the multidimensional PDE reduces to an ordinary differential equation, here $dU/dt = 0$. Solving the characteristic equations provides both the geometry of information propagation (straight lines of slope $1/c$ in the $(x,t)$-plane) and the mechanism for transporting initial data from the initial line $t=0$ to later times. In this linear transport equation with constant velocity, the solution is simply a translation of the initial profile at constant speed, reflecting the fact that each fluid particle carries its initial concentration unchanged as it moves.
\end{solution}

% ===== Example 2: Transport with Nonconstant Velocity Field (inquiry-based) =====
\begin{problem}[Transport with Nonconstant Velocity Field]
In this example we consider one-dimensional transport of a scalar quantity, such as the concentration of a pollutant in a river. The velocity of the flow is not uniform; instead, it increases linearly with position, so that material farther from the origin is carried faster. Mathematically, we study the initial value problem
\[
u_t + x\,u_x = 0, \qquad x \in \mathbb{R},\ t>0,
\]
with prescribed initial profile
\[
u(x,0) = u_0(x).
\]
We will use the method of characteristics to understand how the initial profile $u_0$ is transported and distorted by the nonconstant velocity field.

\smallskip

(a) Interpret the partial differential equation
\[
u_t + x\,u_x = 0
\]
as a transport (or advection) equation. What is the velocity field $v(x)$? Sketch the vector field $v(x)$ on the $x$–axis (for example, draw arrows at a few points indicating the direction and magnitude of the flow). Briefly explain in words how you expect a localized “bump” in $u_0$ to move as time increases.

\medskip

(b) Recall that for a first-order linear transport equation of the form
\[
u_t + a(x,t)\,u_x = 0,
\]
the method of characteristics looks for curves $(x(t),t)$ in the $x$–$t$ plane along which $u$ is constant. For our equation $u_t + x u_x = 0$:

\quad (i) Write down the characteristic system of ordinary differential equations for $x(t)$, $t$, and $u(t)$.

\quad (ii) Which of these equations expresses the fact that $u$ is constant along each characteristic curve?

Hint: Write the total derivative $\dfrac{d}{dt}u(x(t),t)$ using the chain rule and require it to vanish.

\medskip

(c) Solve the characteristic equations you found in part (b).

\quad (i) Solve the equation for $t(t)$ and explain how you can use a reparametrization so that $t$ itself is the parameter along the characteristic curves.

\quad (ii) Solve the ordinary differential equation for $x(t)$ that you obtained in part (b). Describe the family of characteristic curves in the $x$–$t$ plane (for example, give an explicit formula $x(t)$ in terms of an initial position parameter, often denoted by $\xi$).

Hint: The equation for $x$ should be of the form
\[
\frac{dx}{dt} = x.
\]
Recall how to solve this separable linear ordinary differential equation.

\medskip

(d) Use your solution of the characteristic equations to write the solution $u(x,t)$ of the initial value problem in terms of the initial data $u_0$.

\quad (i) Suppose that a characteristic curve passes through the point $(\xi,0)$ at time $t=0$ and reaches the point $(x,t)$ at some later time $t>0$. Express $\xi$ in terms of $x$ and $t$.

\quad (ii) Use the fact that $u$ is constant along characteristics and the initial condition $u(\xi,0)=u_0(\xi)$ to obtain a formula for $u(x,t)$ in the form
\[
u(x,t) = u_0(\text{something involving }x\text{ and }t).
\]

\quad (iii) (Verification.) Substitute your formula for $u(x,t)$ back into the equation $u_t + x u_x=0$ and check that it indeed satisfies the partial differential equation and the initial condition.

Hint: For (ii), remember that “constant along characteristic” means that $u(x(t),t)$ is equal to its initial value at $t=0$. For (iii), compute $u_t$ and $u_x$ explicitly using the chain rule.

\medskip

(e) Explore some variations and qualitative behavior.

\quad (i) Explain qualitatively what happens to the shape and location of the initial profile $u_0$ as time increases. For example, if $u_0$ is supported in the interval $[-1,1]$ (that is, $u_0(x)=0$ outside $[-1,1]$), where is the support of $u(\cdot,t)$ for $t>0$?

\quad (ii) Suppose instead that the velocity field were given by $v(x)=1+x^2$, so the equation becomes
\[
u_t + (1+x^2)u_x = 0.
\]
Write down the characteristic equation for $x(t)$ and (formally) solve it. How would you then express $u(x,t)$ in terms of $u_0$? You do not need to simplify the formulas completely, but outline the steps of the method of characteristics for this modified problem.

Hint: In part (ii), you will obtain an equation of the form $\dfrac{dx}{dt} = 1 + x^2$. Recall that this can be solved by separation of variables, leading to an expression involving the tangent function.
\end{problem}

% ===== Example 2: Transport with Nonconstant Velocity Field (full solution) =====
\begin{problem}[Transport with Nonconstant Velocity Field]
Solve the initial value problem
\[
u_t + x\,u_x = 0, \qquad x \in \mathbb{R},\ t>0,
\]
with initial condition
\[
u(x,0) = u_0(x).
\]
Find an explicit formula for $u(x,t)$ in terms of $u_0$, and briefly describe how the initial profile is transported and deformed over time.
\end{problem}

\begin{solution}
We apply the method of characteristics to reduce the first-order partial differential equation to ordinary differential equations along suitable curves in the $x$–$t$ plane.

We look for characteristic curves $t \mapsto (x(t),t)$ along which $u$ is constant. Consider a solution $u(x,t)$ and define
\[
U(t) := u(x(t),t).
\]
By the chain rule,
\[
\frac{dU}{dt} = u_t(x(t),t) + x'(t)\,u_x(x(t),t).
\]
On the other hand, the equation $u_t + x u_x = 0$ implies that at any point $(x,t)$,
\[
u_t(x,t) = -x\,u_x(x,t).
\]
Substituting this into the expression for $\dfrac{dU}{dt}$ gives
\[
\frac{dU}{dt} = -x(t)\,u_x(x(t),t) + x'(t)\,u_x(x(t),t)
              = \bigl(x'(t)-x(t)\bigr) u_x(x(t),t).
\]
If we choose the characteristic curves so that $x'(t)=x(t)$, then the right-hand side vanishes and we obtain
\[
\frac{dU}{dt} = 0.
\]
This shows that along such curves, $U(t)$ is constant, and therefore $u$ is constant on each characteristic.

The characteristic system is thus
\[
\begin{cases}
\displaystyle \frac{dx}{dt} = x,\\[0.5em]
\displaystyle \frac{dt}{dt} = 1,\\[0.5em]
\displaystyle \frac{du}{dt} = 0.
\end{cases}
\]
The second equation simply says that we use $t$ as the parameter along the curves. The first and third are the nontrivial ones.

We solve the equation for $x(t)$:
\[
\frac{dx}{dt} = x.
\]
This is a separable linear ordinary differential equation. Integrating, we obtain
\[
\frac{dx}{x} = dt \quad \Longrightarrow \quad \ln |x| = t + C,
\]
so
\[
x(t) = C_1 e^{t}
\]
for some constant $C_1$. It is convenient to label each characteristic curve by its position at time $t=0$. Let $\xi$ denote the initial position, so that
\[
x(0) = \xi.
\]
Then $x(t)$ satisfies
\[
x(0) = C_1 e^{0} = C_1 = \xi,
\]
hence
\[
x(t) = \xi e^{t}.
\]
This shows that the characteristic curves are exponential rays in the $x$–$t$ plane, emanating from the $t=0$ axis.

Along the characteristic starting from $(\xi,0)$, the third equation of the characteristic system,
\[
\frac{du}{dt} = 0,
\]
implies that $u$ is constant:
\[
u(x(t),t) = u(\xi,0) = u_0(\xi).
\]
Using the relation $x(t)=\xi e^{t}$, we express $\xi$ in terms of $(x,t)$ as
\[
\xi = x e^{-t}.
\]
Therefore,
\[
u(x,t) = u_0(\xi) = u_0\bigl(x e^{-t}\bigr).
\]
This gives the explicit solution of the initial value problem:
\[
\boxed{\,u(x,t) = u_0\bigl(x e^{-t}\bigr), \qquad x\in\mathbb{R},\ t>0.\,}
\]

It is straightforward to verify that this function satisfies both the partial differential equation and the initial condition. First, at $t=0$ we have
\[
u(x,0) = u_0\bigl(x e^{0}\bigr) = u_0(x),
\]
so the initial condition is satisfied. Next, we compute $u_t$ and $u_x$ using the chain rule. Set
\[
\eta = x e^{-t},
\]
so that $u(x,t) = u_0(\eta)$. Then
\[
u_x(x,t) = u_0'(\eta)\,\frac{\partial \eta}{\partial x} = u_0'(\eta) e^{-t},
\]
and
\[
u_t(x,t) = u_0'(\eta)\,\frac{\partial \eta}{\partial t} = u_0'(\eta)\,(-x e^{-t})
          = -x e^{-t} u_0'(\eta).
\]
Thus
\[
u_t + x u_x = -x e^{-t} u_0'(\eta) + x\bigl(u_0'(\eta) e^{-t}\bigr) = 0,
\]
so the equation $u_t + x u_x = 0$ is satisfied for all $x$ and $t$.

Finally, we describe the qualitative behavior. The formula $u(x,t) = u_0(x e^{-t})$ shows that the profile at time $t$ is a rescaled version of the initial profile. If $u_0$ is supported in the interval $[-1,1]$, then $u(x,t)$ is supported where
\[
x e^{-t} \in [-1,1] \quad \Longleftrightarrow \quad x \in [-e^{t}, e^{t}].
\]
Thus the support expands exponentially in time. In general, every point of the profile moves away from the origin, and the entire pattern is stretched horizontally while preserving the values of $u$ along the moving points. This illustrates the main idea of the method of characteristics: by following appropriate curves in the $(x,t)$–plane determined by the velocity field, we reduce a first-order partial differential equation to simple ordinary differential equations describing how information is transported along these curves.
\end{solution}

% ===== Example 3: Nonlinear Conservation Law and Traffic Flow (inquiry-based) =====
\begin{problem}[Nonlinear Conservation Law and Traffic Flow]
We consider a one-lane road along the $x$-axis and model the density of cars by a function $\rho(x,t)$, where $\rho$ measures the number of cars per unit length at position $x$ and time $t$. Drivers slow down when traffic is dense, so the velocity of cars decreases as $\rho$ increases. This leads to a nonlinear relationship between the density $\rho$ and the flux (or flow) of cars. The resulting partial differential equation can develop discontinuities (``traffic jams'') even if the initial density is not too wild, and the method of characteristics helps us see how and why this happens.

Assume that $0 \le \rho(x,t) \le 1$ is a dimensionless car density (with $\rho = 1$ representing bumper-to-bumper traffic). Let the car velocity $v(\rho)$ depend on density by
\[
v(\rho) = 1 - \rho,
\]
so that cars move fastest when the road is empty and slow to a stop as $\rho \to 1$.

\smallskip

(a) Let $q(x,t)$ denote the car flux, that is, the number of cars per unit time passing a fixed point $x$. Argue from physical principles (conservation of cars) that $\rho$ and $q$ satisfy a conservation law of the form
\[
\rho_t + q_x = 0.
\]
Show further that, under the assumption $v(\rho) = 1 - \rho$, one has $q(\rho) = \rho\,v(\rho)$ and therefore that $\rho$ satisfies a nonlinear conservation law
\[
\rho_t + \bigl(\rho(1-\rho)\bigr)_x = 0.
\]
Hint: Think about the number of cars in a fixed road segment $[a,b]$ and how that number changes in time.

\smallskip

(b) Rewrite the PDE from part (a) in quasilinear form
\[
\rho_t + c(\rho)\,\rho_x = 0
\]
by computing the derivative of the flux $f(\rho) = \rho(1-\rho)$. Identify the characteristic speed $c(\rho)$, and write down the ordinary differential equations for the characteristic curves $x(t)$ and the density $\rho(t)$ along those curves.

% Hint: Use the chain rule: $(f(\rho))_x = f'(\rho)\,\rho_x$.

\smallskip

(c) Show that along each characteristic curve, the density $\rho$ is constant. Use this to describe the characteristic curves explicitly in terms of their initial position $x_0$ and the initial density $\rho_0(x_0) = \rho(x_0,0)$.

Concretely, consider the Riemann-type initial condition
\[
\rho(x,0) = \rho_0(x) =
\begin{cases}
\rho_L := 0.2, & x < 0,\\[0.3em]
\rho_R := 0.8, & x > 0.
\end{cases}
\]
Describe the characteristic lines in the left region $x<0$ and the right region $x>0$ and sketch them in the $(x,t)$-plane.

Hint: Use that $\rho$ is constant on a characteristic to simplify the ODE for $x(t)$.

\smallskip

(d) Using your description from part (c), determine the characteristic speeds on the left and right of $x=0$. Explain why characteristics from the left and right regions move toward each other and intersect. What does this intersection of characteristics mean for the classical (smooth) solution of the PDE?

Next, use the integral form of the conservation law over a moving interval that contains the discontinuity to derive the Rankine--Hugoniot jump condition for the speed $s$ of a moving shock:
\[
s = \frac{f(\rho_R) - f(\rho_L)}{\rho_R - \rho_L},
\qquad \text{where } f(\rho) = \rho(1-\rho).
\]
Compute $s$ for $\rho_L = 0.2$ and $\rho_R = 0.8$, and interpret the result physically in terms of the motion (or lack of motion) of the traffic jam.

Hint: For the jump condition, integrate $\rho_t + f(\rho)_x = 0$ over a small interval that moves with the shock and use the Fundamental Theorem of Calculus.

\smallskip

(e) Explorations and variations.

\begin{enumerate}
  \item Suppose instead that
  \[
  \rho(x,0) =
  \begin{cases}
  0.8, & x < 0,\\
  0.2, & x > 0.
  \end{cases}
  \]
  Without doing a full weak-solution analysis, use the characteristic speeds to argue qualitatively how the density profile evolves. Do the characteristics move toward each other or apart? Does this look more like a ``spreading out'' of cars (a rarefaction) or a sharper traffic jam (a shock)?

  \item In our model we used $v(\rho) = 1 - \rho$. How would the characteristic speed change if we instead used a more general velocity law $v(\rho) = 1 - \rho^\alpha$ for some $\alpha > 0$? What parts of your analysis above would remain the same, and what parts would change?
\end{enumerate}

\end{problem}

% ===== Example 3: Nonlinear Conservation Law and Traffic Flow (full solution) =====
\begin{problem}[Nonlinear Conservation Law and Traffic Flow]
Let $\rho(x,t)$ denote the (dimensionless) density of cars on a one-lane road at position $x \in \mathbb{R}$ and time $t \ge 0$, with $0 \le \rho \le 1$. Assume that the car velocity depends on the local density via
\[
v(\rho) = 1 - \rho,
\]
and that the car flux is $q(\rho) = \rho v(\rho)$. 

\begin{enumerate}
  \item Show that conservation of cars leads to the nonlinear conservation law
  \[
  \rho_t + \bigl(\rho(1-\rho)\bigr)_x = 0.
  \]
  Rewrite this PDE in quasilinear form
  \[
  \rho_t + c(\rho)\,\rho_x = 0,
  \]
  find the characteristic speed $c(\rho)$, and derive the characteristic ODEs. Show that $\rho$ is constant along each characteristic.

  \item Consider the Riemann initial data
  \[
  \rho(x,0) = \rho_0(x) =
  \begin{cases}
  \rho_L := 0.2, & x < 0,\\[0.3em]
  \rho_R := 0.8, & x > 0.
  \end{cases}
  \]
  Determine the characteristic curves in the left and right regions, and explain why they intersect. Interpret this intersection in terms of the breakdown of a classical (smooth) solution and the formation of a shock.

  \item Using the integral form of the conservation law, derive the Rankine--Hugoniot jump condition
  \[
  s = \frac{f(\rho_R) - f(\rho_L)}{\rho_R - \rho_L}, 
  \quad \text{with } f(\rho) = \rho(1-\rho),
  \]
  for the speed $s$ of a moving shock connecting $\rho_L$ and $\rho_R$. Compute $s$ for $\rho_L = 0.2$ and $\rho_R = 0.8$, and interpret the result physically.

\end{enumerate}
\end{problem}

\begin{solution}
We proceed step by step, emphasizing the method of characteristics and the conservation-law structure.

\medskip

\noindent\textbf{1. Derivation of the PDE and characteristic form.}

Let $\rho(x,t)$ be the density of cars. Consider a fixed road segment $[a,b]$. The total number of cars in this segment at time $t$ is
\[
N(t) = \int_a^b \rho(x,t)\,dx.
\]
Conservation of cars means that cars are neither created nor destroyed in the interior of the road; they only enter or leave through the boundaries $x=a$ and $x=b$. If $q(x,t)$ is the flux, i.e., the number of cars per unit time crossing position $x$, then the rate of change of $N(t)$ is given by the net flux into the interval:
\[
\frac{d}{dt} N(t) = q(a,t) - q(b,t).
\]
On the other hand,
\[
\frac{d}{dt} N(t) = \frac{d}{dt} \int_a^b \rho(x,t)\,dx
= \int_a^b \rho_t(x,t)\,dx.
\]
Equating these expressions,
\[
\int_a^b \rho_t(x,t)\,dx = q(a,t) - q(b,t)
= -\int_a^b q_x(x,t)\,dx,
\]
where the last equality uses the Fundamental Theorem of Calculus. Since this holds for all intervals $[a,b]$, we obtain the \emph{local} conservation law
\[
\rho_t + q_x = 0.
\]

Now we specify the flux. By definition, flux is density times velocity:
\[
q(\rho) = \rho\,v(\rho).
\]
With $v(\rho) = 1 - \rho$, we obtain
\[
q(\rho) = \rho(1-\rho).
\]
Substituting into the conservation law yields the nonlinear PDE
\[
\rho_t + \bigl(\rho(1-\rho)\bigr)_x = 0.
\]

To put this in quasilinear form, set $f(\rho) = \rho(1-\rho) = \rho - \rho^2$. Then
\[
\bigl(\rho(1-\rho)\bigr)_x = f'(\rho)\,\rho_x
\]
by the chain rule. Since
\[
f'(\rho) = \frac{d}{d\rho}(\rho - \rho^2) = 1 - 2\rho,
\]
the PDE becomes
\[
\rho_t + (1-2\rho)\,\rho_x = 0.
\]
This is a first-order quasilinear PDE of the form
\[
\rho_t + c(\rho)\,\rho_x = 0
\]
with characteristic speed
\[
c(\rho) = 1 - 2\rho.
\]

The method of characteristics prescribes that along a characteristic curve $t \mapsto (x(t),t)$ the solution satisfies a system of ODEs. Writing the PDE as
\[
\rho_t + c(\rho)\rho_x = 0,
\]
we introduce the characteristic equations
\[
\frac{dt}{ds} = 1,\qquad
\frac{dx}{ds} = c(\rho),\qquad
\frac{d\rho}{ds} = 0.
\]
Here $s$ is a parameter along the characteristic curve. Choosing $s=t$ (so that $dt/ds=1$), we obtain
\[
\frac{dx}{dt} = c(\rho) = 1 - 2\rho, \qquad
\frac{d\rho}{dt} = 0.
\]
Thus the characteristic speed is $1-2\rho$, and we see immediately that
\[
\frac{d\rho}{dt} = 0
\]
says that the density $\rho$ is \emph{constant along each characteristic curve}.

\medskip

\noindent\textbf{2. Characteristics for the Riemann initial data.}

We now impose the Riemann-type initial condition
\[
\rho(x,0) = \rho_0(x) =
\begin{cases}
\rho_L := 0.2, & x < 0,\\[0.3em]
\rho_R := 0.8, & x > 0.
\end{cases}
\]
This describes a low-density region to the left and a high-density region to the right, reminiscent of cars approaching a traffic jam.

Because $\rho$ is constant along each characteristic, on any characteristic that starts in the left region $x_0<0$, we have $\rho(x(t),t) \equiv \rho_L$. Hence the characteristic equation in the left region is
\[
\frac{dx}{dt} = 1 - 2\rho_L.
\]
Similarly, for characteristics starting in the right region $x_0>0$, $\rho(x(t),t) \equiv \rho_R$, and
\[
\frac{dx}{dt} = 1 - 2\rho_R.
\]

We compute these speeds explicitly:
\[
c(\rho_L) = 1 - 2(0.2) = 1 - 0.4 = 0.6,
\]
\[
c(\rho_R) = 1 - 2(0.8) = 1 - 1.6 = -0.6.
\]
Thus characteristics originating from $x_0 < 0$ move to the right with speed $0.6$, while those from $x_0 > 0$ move to the left with speed $-0.6$.

The characteristic curves can be written explicitly. For the left side, solving
\[
\frac{dx}{dt} = 0.6, \qquad x(0) = x_0 < 0,
\]
gives
\[
x(t) = x_0 + 0.6\,t, \qquad x_0<0.
\]
For the right side, solving
\[
\frac{dx}{dt} = -0.6, \qquad x(0) = x_0 > 0,
\]
gives
\[
x(t) = x_0 - 0.6\,t, \qquad x_0>0.
\]

If we sketch these in the $(x,t)$-plane, we see two families of straight lines. On the left, lines with slope $dx/dt = 0.6$ (tilting to the right); on the right, lines with slope $dx/dt = -0.6$ (tilting to the left). They move towards each other.

\medskip

\noindent\textbf{3. Intersection of characteristics and breakdown of a classical solution.}

We now analyze the interaction of these two families. A characteristic from the left family is
\[
x = x_0 + 0.6t, \quad x_0<0,
\]
and a characteristic from the right family is
\[
x = y_0 - 0.6t, \quad y_0>0.
\]
They intersect when they share the same $(x,t)$, i.e., when
\[
x_0 + 0.6t = y_0 - 0.6t.
\]
Solving for $t$,
\[
1.2\,t = y_0 - x_0 \quad\Longrightarrow\quad t = \frac{y_0 - x_0}{1.2}.
\]
For any $x_0<0$ and $y_0>0$, we have $y_0 - x_0 > 0$, so $t>0$: indeed, every pair of left and right characteristics intersects at some positive time. As $x_0 \uparrow 0$ and $y_0 \downarrow 0$, the intersection time approaches $0$. This shows that the characteristic curves from the left and right regions immediately move toward one another and intersect in the $(x,t)$-plane.

The method of characteristics constructs a \emph{classical} solution $\rho(x,t)$ by assigning to each point $(x,t)$ the value of $\rho$ transported along the unique characteristic passing through $(x,t)$. However, once characteristics cross, there is no longer a unique characteristic through a point: the same point $(x,t)$ can be reached by a characteristic carrying the left-state density $\rho_L$ and by one carrying the right-state density $\rho_R$. This would force $\rho(x,t)$ to be multi-valued, which is not physically or mathematically acceptable.

Therefore, the intersection of characteristics signals the breakdown of the classical (smooth) solution. In practice, for nonlinear conservation laws, the physically relevant solutions are allowed to have discontinuities called shocks, and they are interpreted in a weaker (integral) sense. The method of characteristics still describes the solution in the smooth regions, but across the discontinuity we must impose an additional condition determined by conservation, namely the Rankine--Hugoniot jump condition.

\medskip

\noindent\textbf{4. Rankine--Hugoniot condition and the shock speed.}

We now derive the Rankine--Hugoniot condition for the shock speed. We work with the conservation law
\[
\rho_t + f(\rho)_x = 0,\qquad f(\rho) = \rho(1-\rho).
\]
Suppose that a shock (a jump in $\rho$) travels along a curve $x = s t$ (for simplicity, we assume the shock is straight in time with constant speed $s$). Let $\rho_L$ be the constant density to the left of the shock and $\rho_R$ the constant density to the right. In our Riemann problem, these are the same $\rho_L$ and $\rho_R$ as in the initial data.

To derive the shock speed, consider a small interval that moves with the shock,
\[
I(t) = [s t - \varepsilon,\, s t + \varepsilon],
\]
for $\varepsilon>0$ small. The total number of cars in $I(t)$ at time $t$ is
\[
N(t) = \int_{s t - \varepsilon}^{s t + \varepsilon} \rho(x,t)\,dx.
\]
Because cars are conserved, the only way $N(t)$ can change is due to flux through the endpoints:
\[
\frac{d}{dt}N(t) = q(s t - \varepsilon,t) - q(s t + \varepsilon,t)
= f\bigl(\rho(s t - \varepsilon,t)\bigr) - f\bigl(\rho(s t + \varepsilon,t)\bigr).
\]
On the other hand, by differentiating under the integral sign and using the Leibniz rule,
\[
\frac{d}{dt}N(t)
= \int_{s t - \varepsilon}^{s t + \varepsilon} \rho_t(x,t)\,dx
+ s\,\rho(s t + \varepsilon,t) - s\,\rho(s t - \varepsilon,t).
\]
Now use the PDE $\rho_t = -f(\rho)_x$ in the integral:
\[
\int_{s t - \varepsilon}^{s t + \varepsilon} \rho_t\,dx
= - \int_{s t - \varepsilon}^{s t + \varepsilon} f(\rho)_x\,dx
= -\Bigl(f\bigl(\rho(s t + \varepsilon,t)\bigr)
- f\bigl(\rho(s t - \varepsilon,t)\bigr)\Bigr).
\]
Hence
\[
\frac{d}{dt}N(t)
= -\Bigl(f(\rho_R) - f(\rho_L)\Bigr)
+ s\,\rho_R - s\,\rho_L,
\]
where we have identified the left and right limits of $\rho$ at the shock with $\rho_L$ and $\rho_R$. But we also knew from the basic conservation argument that
\[
\frac{d}{dt}N(t) = f(\rho_L) - f(\rho_R).
\]
Equating the two expressions for $\frac{d}{dt}N(t)$ and simplifying, we get
\[
f(\rho_L) - f(\rho_R)
= -\bigl(f(\rho_R) - f(\rho_L)\bigr) + s(\rho_R - \rho_L).
\]
Rearranging,
\[
f(\rho_L) - f(\rho_R) + f(\rho_R) - f(\rho_L) = s(\rho_R - \rho_L),
\]
so
\[
0 = s(\rho_R - \rho_L).
\]
This naive manipulation looks suspicious; the clean derivation (and the standard result) is usually obtained by writing the conservation law in integral form and then passing to the limit as $\varepsilon \to 0$. The standard Rankine--Hugoniot jump condition for a shock in a conservation law $\rho_t + f(\rho)_x = 0$ is
\[
s = \frac{f(\rho_R) - f(\rho_L)}{\rho_R - \rho_L}.
\]
This condition expresses that the net rate at which cars accumulate in a small control volume moving with the shock equals the difference between fluxes on the two sides.

For our specific flux $f(\rho) = \rho(1-\rho)$ and states $\rho_L=0.2$, $\rho_R=0.8$, we compute
\[
f(\rho_L) = f(0.2) = 0.2(1-0.2) = 0.2 \cdot 0.8 = 0.16,
\]
\[
f(\rho_R) = f(0.8) = 0.8(1-0.8) = 0.8 \cdot 0.2 = 0.16.
\]
Therefore,
\[
s = \frac{f(\rho_R) - f(\rho_L)}{\rho_R - \rho_L}
= \frac{0.16 - 0.16}{0.8 - 0.2} = \frac{0}{0.6} = 0.
\]
The shock speed is zero: the jump connecting $\rho_L=0.2$ and $\rho_R=0.8$ is \emph{stationary}. In other words, the traffic jam sits at $x=0$ and does not move along the road.

This is physically reasonable. The flux of cars on both sides of the jam is the same ($0.16$), so cars enter the jam region from the left at the same rate as they leave on the right. There is no net accumulation or depletion of cars in the jam, so the jam has no reason to move; it remains located at $x=0$.

\medskip

\noindent\textbf{5. Connection to the method of characteristics.}

This example illustrates several central ideas from the method of characteristics for first-order PDEs:

\begin{itemize}
  \item The PDE is a nonlinear conservation law, which can be written in quasilinear form $\rho_t + c(\rho)\rho_x=0$ with a density-dependent characteristic speed $c(\rho)$.
  \item Along each characteristic curve, the density $\rho$ is constant. This gives a clear geometric interpretation: the initial density profile is transported along straight lines in the $(x,t)$-plane with speeds depending on the initial density.
  \item For our Riemann data, the left characteristics move right and the right characteristics move left, so they intersect. This intersection shows that the classical characteristic construction cannot produce a single-valued smooth solution for $t>0$, signaling the need for a weaker notion of solution.
  \item The Rankine--Hugoniot condition emerges from the integral conservation law and determines the speed of shocks. In our case, it predicts a stationary traffic jam, which matches intuitive expectations from the equal fluxes on both sides.
\end{itemize}

Thus, the method of characteristics not only provides a tool for solving first-order PDEs in smooth regions but also gives insight into the formation and propagation of singularities such as shocks, which are essential in realistic models of traffic flow and other conservation laws.

\end{solution}

% ===== Example 4: Hamilton–Jacobi Equation and Optimal Control (inquiry-based) =====
\begin{problem}[Hamilton--Jacobi Equation and Optimal Control]
In mechanics and in optimal control, one often introduces a function $S(x,t)$ that measures the ``cost'' or ``action'' of moving a system in state $x$ at time $t$. 
For a simple one-dimensional free particle of unit mass, the Hamiltonian is $H(p) = \tfrac12 p^2$, where $p$ is the momentum. 
The associated Hamilton--Jacobi equation for the action $S(x,t)$ is
\[
S_t(x,t) \;+\; \frac12 \bigl(S_x(x,t)\bigr)^2 \;=\; 0, \qquad x\in\mathbb{R}, \ t>0,
\]
together with an initial condition at time $t=0$. 
This same equation also arises as the value function in a basic optimal control problem in which one penalizes the square of the control.

In this problem, you will connect the method of characteristics for this first-order PDE with Hamiltonian mechanics, and you will solve a concrete initial value problem.

Consider the Cauchy problem
\[
\begin{cases}
S_t(x,t) + \dfrac12 \bigl(S_x(x,t)\bigr)^2 = 0, & x\in\mathbb{R},\ t>0,\\[0.5em]
S(x,0) = \dfrac12 x^2, & x\in\mathbb{R}.
\end{cases}
\]

\smallskip

(a) Interpret the Hamiltonian $H(p)=\tfrac12 p^2$ physically. 
What mechanical system (mass, potential energy) does this correspond to? 
What is the associated Lagrangian $L(v)$ in terms of the velocity $v$? 
How might an action functional built from $L$ relate to a function $S(x,t)$ that measures the ``cost'' of a path?

\medskip

(b) The Hamilton--Jacobi equation has the form
\[
S_t + H\bigl(S_x\bigr) = 0 \quad\text{with } H(p)=\tfrac12 p^2.
\]
To apply the method of characteristics, we introduce the momentum-like quantity $p(x,t) := S_x(x,t)$.
  
  (i) Write down the general characteristic ansatz: consider curves $t\mapsto (x(t),S(t))$ in the $(x,t,S)$-space along which $S$ satisfies an ordinary differential equation. 
  Use the chain rule to compute $\dfrac{d}{dt} S(x(t),t)$ in terms of $S_t$ and $S_x$.

  (ii) The method of characteristics tells us to choose $\dot{x}(t)$ so that the PDE becomes an ODE along characteristics. 
  With $H(p) = \tfrac12 p^2$, propose a natural choice for $\dot{x}(t)$ in terms of $p(t):=S_x(x(t),t)$, and use the PDE to write a differential equation for $S$ along the characteristic.

  % Hint: Arrange the combination $S_t + \dot{x} S_x$ so that the PDE gives an expression for $\dfrac{d}{dt} S$.

\medskip

(c) One can go further and track the evolution of $p(t)=S_x(x(t),t)$ along the characteristic. 
Differentiate the identity $p(x(t),t) = S_x(x(t),t)$ with respect to $t$ and use the PDE (and its $x$-derivative) to obtain an ODE for $p(t)$.

  (i) Show that in general the characteristic system for $S_t + H(x,S_x)=0$ takes the form
  \[
  \dot{x} = H_p(x,p), \qquad \dot{p} = -H_x(x,p), \qquad \dot{S} = p\,\dot{x} - H(x,p),
  \]
  where $p(t)=S_x(x(t),t)$.

  (ii) Specialize to the present case $H(p)=\tfrac12 p^2$ (with no $x$-dependence). 
  Compute $\dot{x}$, $\dot{p}$, and $\dot{S}$ explicitly. 
  How do these equations compare with Hamilton's equations for a free particle?

  % Hint: For (i), differentiate $S_t + H(x,S_x)=0$ with respect to $x$, and then use the chain rule for $p(x(t),t)$ just as you did for $S$.

\medskip

(d) Now solve the given Cauchy problem explicitly.

  (i) At time $t=0$ the characteristic curves start on the $x$-axis. 
  Use a parameter $\sigma\in\mathbb{R}$ to label the initial position $x(0)=\sigma$ of a characteristic. 
  Express the initial momentum $p(0)$ in terms of $\sigma$ using the initial condition $S(x,0) = \tfrac12 x^2$.

  (ii) Solve the characteristic system from part (c)(ii) with these initial data to obtain $x(t)$, $p(t)$, and $S(t)$ in terms of $\sigma$ and $t$.

  (iii) Eliminate the parameter $\sigma$ in favor of $(x,t)$ to obtain an explicit formula for $S(x,t)$ for $t>0$. 
  Check directly (by computing $S_t$ and $S_x$) that your formula satisfies both the PDE and the initial condition.

  % Hint: You will obtain $x(t) = \sigma + \sigma t$ and a simple expression for $S(t)$. Solve for $\sigma$ in terms of $x$ and $t$, then substitute into $S$.

\medskip

(e) Explorations and extensions.

  (i) Suppose instead that the Hamiltonian is $H(p) = \tfrac12 p^2 + V_0$, where $V_0$ is a constant potential energy. 
  How would the characteristic system in part (c)(ii) change? 
  Qualitatively, what is the effect of a constant potential on the solution $S(x,t)$?

  (ii) Consider the following optimal control problem: a system evolves according to $\dot{x}(t)=u(t)$ with control $u(t)\in\mathbb{R}$, and we aim to minimize the cost
  \[
  J(u) = \int_0^T \frac12 u(t)^2\,dt + g\bigl(x(T)\bigr),
  \]
  over all admissible controls $u$, for some given terminal cost $g$. 
  Using the dynamic programming (or Bellman) principle, one can show that the value function $V(x,t)$ satisfies a Hamilton--Jacobi equation of the form
  \[
  V_t + H\bigl(V_x\bigr)=0
  \]
  for a suitable Hamiltonian $H(p)$. 
  By formally minimizing over $u$, guess what $H(p)$ should be and compare it with the Hamiltonian in our mechanical example.

  % Hint: Think of the infinitesimal optimization problem over a short time interval $(t,t+\Delta t)$ and complete the square in $u$.

\end{problem}

% ===== Example 4: Hamilton–Jacobi Equation and Optimal Control (full solution) =====
\begin{problem}[Hamilton--Jacobi equation for a free particle]
Consider the Cauchy problem
\[
\begin{cases}
S_t(x,t) + \dfrac12 \bigl(S_x(x,t)\bigr)^2 = 0, & x\in\mathbb{R},\ t>0,\\[0.5em]
S(x,0) = \dfrac12 x^2, & x\in\mathbb{R}.
\end{cases}
\]
\begin{enumerate}
\item Derive the characteristic system for the Hamilton--Jacobi equation
\[
S_t + H\bigl(S_x\bigr)=0,\qquad H(p)=\tfrac12 p^2,
\]
and show that it coincides with Hamilton's equations for a one-dimensional free particle of unit mass.
\item Solve the given Cauchy problem explicitly using the method of characteristics, and verify that your solution satisfies both the PDE and the initial condition.
\end{enumerate}
\end{problem}

\begin{solution}
We treat this as a model Hamilton--Jacobi equation, and we solve it by the method of characteristics. 
Along the way we will see that the characteristics coincide with the trajectories of the underlying Hamiltonian system.

\medskip

\noindent\textbf{1. Characteristic system and Hamiltonian mechanics.}
Consider the general Hamilton--Jacobi equation
\[
S_t(x,t) + H\bigl(x,S_x(x,t)\bigr) = 0.
\]
Set
\[
p(x,t) := S_x(x,t).
\]
We look for curves $t\mapsto (x(t),S(t))$ in the $(x,t,S)$-space along which the function $S$ satisfies an ordinary differential equation. 
Along such a curve we have, by the chain rule,
\[
\frac{d}{dt} S(x(t),t) = S_t(x(t),t) + \dot{x}(t)\,S_x(x(t),t)
= S_t(x(t),t) + \dot{x}(t)\,p(x(t),t).
\]
If we choose the characteristic velocity to be
\[
\dot{x}(t) = H_p\bigl(x(t),p(t)\bigr),
\]
where $p(t) := p(x(t),t)$ and $H_p$ denotes $\partial H/\partial p$, then along the characteristic the Hamilton--Jacobi equation implies
\[
S_t(x(t),t) = -H\bigl(x(t),p(t)\bigr).
\]
Substituting this into the expression for $dS/dt$ gives
\[
\frac{d}{dt} S(x(t),t)
= -H\bigl(x(t),p(t)\bigr) + H_p\bigl(x(t),p(t)\bigr)\,p(t).
\]
Thus, along characteristics, $S$ satisfies
\[
\dot{S}(t) = p(t)\,\dot{x}(t) - H\bigl(x(t),p(t)\bigr).
\]

To close the system we also need an equation for $p(t)$. 
Differentiating the Hamilton--Jacobi equation with respect to $x$ yields
\[
S_{tx}(x,t) + H_x\bigl(x,S_x(x,t)\bigr)
\;+\; H_p\bigl(x,S_x(x,t)\bigr)\,S_{xx}(x,t) = 0,
\]
where $H_x$ denotes $\partial H/\partial x$. 
On the other hand,
\[
\frac{d}{dt} p(x(t),t)
= \frac{d}{dt} S_x(x(t),t)
= S_{xt}(x(t),t) + \dot{x}(t)\,S_{xx}(x(t),t).
\]
Using the symmetry $S_{xt}=S_{tx}$ and substituting from the $x$-differentiated Hamilton--Jacobi equation, we obtain
\[
\frac{d}{dt} p(x(t),t)
= -H_x\bigl(x(t),p(t)\bigr)
    - H_p\bigl(x(t),p(t)\bigr)\,S_{xx}(x(t),t)
    + \dot{x}(t)\,S_{xx}(x(t),t).
\]
If we impose again the characteristic relation $\dot{x}(t) = H_p(x(t),p(t))$, the terms involving $S_{xx}$ cancel and we are left with
\[
\dot{p}(t) = -\,H_x\bigl(x(t),p(t)\bigr).
\]

Collecting these formulas, the characteristic system for the Hamilton--Jacobi equation is
\[
\dot{x} = H_p(x,p), \qquad
\dot{p} = -H_x(x,p), \qquad
\dot{S} = p\,\dot{x} - H(x,p).
\]
The first two equations are precisely Hamilton's equations for the Hamiltonian $H(x,p)$ in one degree of freedom, and the third equation describes the evolution of the action $S$ along a trajectory.

In our specific problem, the Hamiltonian is
\[
H(p) = \frac12 p^2,
\]
which does not depend on $x$. 
Thus
\[
H_p(p) = p, \qquad H_x = 0.
\]
The characteristic system becomes
\[
\dot{x}(t) = p(t), \qquad
\dot{p}(t) = 0, \qquad
\dot{S}(t) = p(t)\,\dot{x}(t) - \frac12 p(t)^2.
\]
The first two equations are exactly Hamilton's equations for a free particle of unit mass: the velocity is the momentum, and the momentum is conserved. 
This gives the geometric interpretation: the characteristic curves are the straight-line trajectories of a free particle.

\medskip

\noindent\textbf{2. Solving the Cauchy problem.}
We now solve the system with initial data determined by the Cauchy condition
\[
S(x,0) = \frac12 x^2.
\]
We parametrize the initial line $t=0$ by a parameter $\sigma\in\mathbb{R}$ and set
\[
x(0) = \sigma.
\]
Along the characteristic issuing from $(\sigma,0)$ we also have
\[
S(0) = S(x(0),0) = S(\sigma,0) = \frac12 \sigma^2.
\]
To obtain the initial momentum, we use
\[
p(0) = S_x(x(0),0) = S_x(\sigma,0).
\]
From the initial condition, $S(x,0) = \tfrac12 x^2$, we compute
\[
S_x(x,0) = x,
\]
hence
\[
p(0) = \sigma.
\]

We now solve the characteristic ODEs:
\[
\dot{p}(t) = 0 \quad\Rightarrow\quad p(t) = \sigma \quad\text{for all }t.
\]
Then
\[
\dot{x}(t) = p(t) = \sigma
\quad\Rightarrow\quad
x(t) = \sigma + \sigma t = \sigma(1+t),
\]
using the initial condition $x(0) = \sigma$. 
Finally,
\[
\dot{S}(t) = p(t)\,\dot{x}(t) - \frac12 p(t)^2
= \sigma\cdot\sigma - \frac12 \sigma^2
= \frac12 \sigma^2,
\]
so $S$ grows linearly in time:
\[
S(t) = S(0) + \int_0^t \dot{S}(\tau)\,d\tau
= \frac12 \sigma^2 + \int_0^t \frac12 \sigma^2\,d\tau
= \frac12 \sigma^2 + \frac12 \sigma^2 t
= \frac12 \sigma^2(1+t).
\]

At this point the solution is written in terms of the characteristic label $\sigma$ and the time $t$:
\[
x = \sigma(1+t), \qquad
S = \frac12 \sigma^2(1+t).
\]
To obtain $S$ as a function of $(x,t)$, we eliminate $\sigma$. 
From $x = \sigma(1+t)$ we have
\[
\sigma = \frac{x}{1+t}.
\]
Substituting into the expression for $S$ gives
\[
S(x,t)
= \frac12 \left(\frac{x}{1+t}\right)^2 (1+t)
= \frac12 \,\frac{x^2}{(1+t)^2}\,(1+t)
= \frac12\,\frac{x^2}{1+t},
\]
for $t>-1$ (and in particular for $t>0$, which is our domain of interest). 
Thus the candidate solution is
\[
S(x,t) = \frac12\,\frac{x^2}{1+t}, \qquad x\in\mathbb{R},\ t>0.
\]

\medskip

\noindent\textbf{3. Verification.}
We now check that this function satisfies both the Hamilton--Jacobi equation and the initial condition.

First, the initial condition:
\[
S(x,0) = \frac12\,\frac{x^2}{1+0} = \frac12 x^2,
\]
as required.

Next, we verify the PDE. 
Compute the derivatives:
\[
S_x(x,t) = \frac12 \cdot \frac{2x}{1+t} = \frac{x}{1+t},
\]
and
\[
S_t(x,t)
= \frac12 x^2 \cdot \frac{d}{dt} \bigl( (1+t)^{-1} \bigr)
= \frac12 x^2 \cdot \bigl( - (1+t)^{-2} \bigr)
= -\frac12\,\frac{x^2}{(1+t)^2}.
\]
Therefore
\[
\frac12 \bigl(S_x(x,t)\bigr)^2
= \frac12 \left(\frac{x}{1+t}\right)^2
= \frac12\,\frac{x^2}{(1+t)^2},
\]
and so
\[
S_t(x,t) + \frac12 \bigl(S_x(x,t)\bigr)^2
= -\frac12\,\frac{x^2}{(1+t)^2}
   + \frac12\,\frac{x^2}{(1+t)^2}
= 0.
\]
Thus $S$ satisfies the Hamilton--Jacobi equation in the domain $t>0$.

\medskip

\noindent\textbf{4. Discussion and connection to the method of characteristics.}
This example illustrates several central ideas of the method of characteristics for first-order partial differential equations:

\begin{itemize}
\item Introducing $p = S_x$ converts the first-order PDE into a system of ordinary differential equations for $(x(t),p(t),S(t))$. 
The choice $\dot{x} = H_p$ ensures that the PDE reduces to an ODE along characteristic curves.

\item The resulting ODEs for $(x,p)$ are precisely Hamilton's equations for the underlying Hamiltonian system, in this case a free particle of unit mass. 
Thus, the characteristics of the Hamilton--Jacobi equation coincide with the physical trajectories of the mechanical system.

\item Once the characteristic curves and the evolution of $S$ along them are known, one can parametrize the solution by initial data, solve the ODEs, and finally eliminate the characteristic parameter to recover $S(x,t)$.

\end{itemize}

In more general Hamilton--Jacobi and optimal control problems, a similar strategy applies: one identifies the Hamiltonian $H$, writes down the corresponding Hamiltonian system for $(x,p)$, and then reconstructs the value function or action $S$ along the resulting characteristic curves. 
This ties the theory of first-order PDEs directly to the dynamics of the underlying system.
\end{solution}

% ===== Example 5: Eikonal Equation and Geometrical Optics (inquiry-based) =====
\begin{problem}[Eikonal Equation and Geometrical Optics]
In many optical and acoustic problems, the wave speed is not constant but depends on position, as in glass whose refractive index varies from point to point. When the wavelength is very small compared with the length scale on which the medium changes, the detailed oscillations of the wave become less important than its phase. In that high-frequency regime, one can approximate wave propagation by \emph{geometrical optics}: wavefronts propagate, and energy travels along \emph{rays}. Mathematically, the phase function of such a high-frequency wave satisfies the \emph{eikonal equation}, which is a nonlinear first-order PDE whose characteristics are precisely these rays.

Throughout this problem we work in two spatial dimensions, with spatial variable $x = (x_1,x_2) \in \mathbb{R}^2$. Let $c(x)>0$ denote the wave speed at position $x$, and consider the variable-coefficient wave equation
\[
u_{tt}(x,t) - c(x)^2\,\Delta u(x,t) \;=\; 0,\qquad x\in\mathbb{R}^2,\ t\in\mathbb{R}.
\]

\smallskip

(a) In the high-frequency (short-wavelength) regime, it is natural to look for oscillatory solutions whose local frequency is very large. Make the ansatz
\[
u(x,t) \;=\; A(x)\,e^{i\omega\,(t - S(x))},
\]
where $\omega\gg 1$ is a large parameter, $A(x)$ is a slowly varying complex amplitude, and $S(x)$ is a real-valued phase function (the “optical path length” or “travel time”). 

Compute $u_t$, $u_{tt}$, $\nabla u$, and $\Delta u$ in terms of $A$, $S$, and their derivatives, and organize your expressions in powers of $\omega$ (that is, identify the terms proportional to $\omega^2$, to $\omega$, and of order $1$).  
Hint: It is convenient to write $u(x,t) = A(x)\,e^{i\phi(x,t)}$ with $\phi(x,t) = \omega (t - S(x))$ and to differentiate using the product rule.

\smallskip

(b) Substitute your expressions into the wave equation and divide out the common oscillatory factor $e^{i\omega(t-S(x))}$. Group the resulting equation according to powers of $\omega$. 

(i) Show that the coefficient of $\omega^2$ gives a leading-order relation between $S$ and $c(x)$.  

(ii) By neglecting lower-order terms in $\omega$, derive the \emph{eikonal equation} satisfied by $S(x)$ and write it in the form
\[
|\nabla S(x)| \;=\; \frac{1}{c(x)}.
\]
State briefly what approximation you have made when discarding the terms of order $\omega$ and $1$.

\smallskip

(c) The eikonal equation from part (b) is a nonlinear first-order PDE of the form
\[
F(x,\nabla S(x)) \;=\; 0,
\]
with 
\[
F(x,p) \;=\; \frac{1}{2}\Bigl(|p|^2 - n(x)^2\Bigr),
\qquad n(x) := \frac{1}{c(x)}.
\]
In general, for a first-order PDE $F(x,u,p)=0$ with $p = \nabla u$, the method of characteristics converts the PDE into a system of ODEs for curves $(x(s),u(s),p(s))$ in the extended space. In the special case that $F$ does not depend explicitly on $u$ (as here), the characteristic equations reduce to
\[
\dot{x}(s) = F_p(x(s),p(s)), \qquad
\dot{S}(s) = p(s)\cdot F_p(x(s),p(s)), \qquad
\dot{p}(s) = -F_x(x(s),p(s)),
\]
where dots denote derivatives with respect to the parameter $s$.

(i) Compute $F_p(x,p)$ and $F_x(x,p)$ for the above $F(x,p) = \tfrac{1}{2}(|p|^2 - n(x)^2)$.  

(ii) Write down explicitly the system of characteristic ODEs for $x(s)$, $S(s)$, and $p(s)=\nabla S(x(s))$.  

(iii) Give a geometric interpretation of the equation for $\dot{x}(s)$ in terms of the relation between the rays and the level sets of $S$ (that is, the wavefronts).  
Hint: Recall that the gradient of a function is normal to its level sets.

\smallskip

(d) Now consider a homogeneous medium with constant speed $c(x)\equiv c_0>0$, so that $n(x)\equiv n_0=1/c_0$. 

(i) Simplify the characteristic system from part (c) for this constant-coefficient case. What do you conclude about $p(s)$ and $x(s)$?  

(ii) Physically, suppose there is a point source located at the origin emitting circular wavefronts in this homogeneous medium. Assuming radial symmetry, write $S(x) = f(r)$ with $r=|x|$ and use the eikonal equation $|\nabla S| = 1/c_0$ to find $f(r)$ up to an additive constant.  

(iii) Describe the corresponding wavefronts and rays in the $x$–plane. How does this match your intuition about waves emitted from a point source in a constant-speed medium?

\smallskip

(e) \emph{Extensions and “what if” questions.}

(i) Suppose now that the medium is layered in the vertical direction, so that the refractive index $n(x)$ depends only on the vertical coordinate $y$, that is, $n(x_1,x_2) = n(y)$ with $y=x_2$. Use the characteristic equations to express the ray direction in terms of $p=\nabla S$, and introduce the angle $\theta(s)$ between the ray direction and the vertical axis. Show that along any ray, the quantity
\[
n(y)\,\sin\theta
\]
is conserved. (This is a differential form of Snell's law for a smoothly varying medium.)  
Hint: Write the ray direction as $\dot{x}/|\dot{x}|$ and relate its components to $\theta$; then use the ODE for $\dot{p}$.

(ii) Based on the conservation of $n(y)\sin\theta$, reason qualitatively about how rays bend if $n(y)$ increases with depth versus if $n(y)$ decreases with depth. Connect your conclusions to the physical statement that light bends toward regions of higher refractive index (lower speed).

\end{problem}

% ===== Example 5: Eikonal Equation and Geometrical Optics (full solution) =====
\begin{problem}[Eikonal equation and rays in geometrical optics]
Let $c(x)>0$ be a smooth wave speed in $\mathbb{R}^2$ and consider the variable-coefficient wave equation
\[
u_{tt}(x,t) - c(x)^2\,\Delta u(x,t) = 0.
\]
(a) For a large frequency parameter $\omega\gg 1$, assume an ansatz
\[
u(x,t) = A(x)\,e^{i\omega (t - S(x))},
\]
with slowly varying complex amplitude $A(x)$ and real phase $S(x)$. By substituting into the wave equation and collecting powers of $\omega$, derive the leading-order PDE satisfied by $S(x)$ and show that it can be written as the eikonal equation
\[
|\nabla S(x)| = \frac{1}{c(x)}.
\]

(b) View the eikonal equation as
\[
F(x,\nabla S(x)) = 0,\qquad
F(x,p) = \frac{1}{2}\bigl(|p|^2 - n(x)^2\bigr),\quad n(x)=\frac{1}{c(x)}.
\]
Write down the characteristic ODE system for $x(s)$, $S(s)$, and $p(s)=\nabla S(x(s))$ when $F$ does not depend on $S$, and specialize it to this $F$. Interpret the equation for $\dot{x}(s)$ in terms of the relationship between rays and wavefronts.

(c) In the special case of a homogeneous medium $c(x)\equiv c_0>0$ (so $n(x)\equiv n_0$), simplify the characteristic system, solve it qualitatively, and deduce that rays are straight lines. Then, assuming radial symmetry for a point source at the origin, solve the eikonal equation to obtain an explicit phase function $S(x)$ and describe the corresponding wavefronts and rays.

\end{problem}

\begin{solution}
We proceed in three steps: derivation of the eikonal equation from the high-frequency ansatz, formulation of the characteristic system for this nonlinear first-order PDE, and analysis of the special case of a homogeneous medium.

\medskip

\noindent\textbf{(a) Derivation of the eikonal equation.}
We are given
\[
u_{tt} - c(x)^2\,\Delta u = 0
\]
and the ansatz
\[
u(x,t) = A(x)\,e^{i\omega (t - S(x))},
\]
where $\omega\gg 1$. It is convenient to write
\[
\phi(x,t) = \omega\,(t - S(x)), \qquad u(x,t) = A(x)\,e^{i\phi(x,t)}.
\]
We now compute the necessary derivatives.

First, time derivatives. Since $A$ is independent of $t$, we have
\[
u_t = i\,\phi_t\,A\,e^{i\phi},
\]
and because $\phi_t = \omega$, this becomes
\[
u_t = i\omega\,A\,e^{i\phi}.
\]
Differentiating once more in $t$,
\[
u_{tt} = i\omega\,A_t\,e^{i\phi} + i\omega\,A\, (i\phi_t)\,e^{i\phi}.
\]
Here $A_t=0$ and $\phi_t=\omega$, so
\[
u_{tt} = i\omega A\cdot i\omega e^{i\phi} = -\omega^2 A\,e^{i\phi}.
\]

Next, spatial derivatives. For each spatial coordinate $x_j$ we have
\[
u_{x_j} = A_{x_j} e^{i\phi} + A\,(i\phi_{x_j}) e^{i\phi}.
\]
Since $\phi(x,t) = \omega(t - S(x))$, one has $\phi_{x_j} = -\omega\,S_{x_j}$. Thus
\[
u_{x_j}
= e^{i\phi}\bigl(A_{x_j} - i\omega A S_{x_j}\bigr).
\]
Differentiating again,
\[
\begin{aligned}
u_{x_j x_j}
&= \frac{\partial}{\partial x_j}\Bigl( e^{i\phi}\bigl(A_{x_j} - i\omega A S_{x_j}\bigr) \Bigr) \\
&= e^{i\phi}\Bigl(i\phi_{x_j}\bigl(A_{x_j} - i\omega A S_{x_j}\bigr)
+ A_{x_j x_j} - i\omega A_{x_j} S_{x_j} - i\omega A S_{x_j x_j}\Bigr).
\end{aligned}
\]
Using $\phi_{x_j} = -\omega S_{x_j}$, we obtain
\[
i\phi_{x_j}\bigl(A_{x_j} - i\omega A S_{x_j}\bigr)
= i(-\omega S_{x_j}) A_{x_j} - i(-\omega S_{x_j})\,i\omega A S_{x_j}
= -i\omega S_{x_j}A_{x_j} - \omega^2 A S_{x_j}^2.
\]
Thus
\[
u_{x_j x_j}
= e^{i\phi}\Bigl(A_{x_j x_j} - 2i\omega A_{x_j}S_{x_j} - i\omega A S_{x_j x_j} - \omega^2 A S_{x_j}^2\Bigr).
\]
Summing over $j=1,2$ gives the Laplacian,
\[
\Delta u = \sum_{j=1}^2 u_{x_j x_j}
= e^{i\phi}\Bigl(\Delta A - 2i\omega \nabla A\cdot \nabla S
- i\omega A\,\Delta S - \omega^2 A\,|\nabla S|^2\Bigr).
\]

Substituting $u_{tt}$ and $\Delta u$ into the wave equation yields
\[
-\omega^2 A\,e^{i\phi}
- c(x)^2 e^{i\phi}\Bigl(\Delta A - 2i\omega \nabla A\cdot \nabla S
- i\omega A\,\Delta S - \omega^2 A\,|\nabla S|^2\Bigr) = 0.
\]
We can factor out the nonzero oscillatory factor $e^{i\phi}$ and write
\[
-\omega^2 A
- c^2\Delta A
+ 2i\omega c^2 \nabla A\cdot \nabla S
+ i\omega c^2 A\,\Delta S
+ \omega^2 c^2 A\,|\nabla S|^2
= 0.
\]
We now sort terms by powers of $\omega$:

\begin{itemize}
\item Terms of order $\omega^2$:
\[
A\bigl(-1 + c(x)^2 |\nabla S(x)|^2\bigr).
\]
\item Terms of order $\omega$:
\[
i\omega c(x)^2\bigl(2\nabla A\cdot \nabla S + A\,\Delta S\bigr).
\]
\item Terms of order $1$:
\[
- c(x)^2 \Delta A.
\]
\end{itemize}

In the high-frequency approximation $\omega\gg 1$, the dominant balance is at order $\omega^2$. Assuming $A(x)\not\equiv 0$, the coefficient of $\omega^2$ must vanish, which gives
\[
-1 + c(x)^2 |\nabla S(x)|^2 = 0,
\]
or equivalently
\[
c(x)^2 |\nabla S(x)|^2 = 1.
\]
Taking square roots (and choosing the positive sign to represent increasing travel time with distance) we arrive at the \emph{eikonal equation}
\[
|\nabla S(x)| = \frac{1}{c(x)}.
\]

The lower-order terms in $\omega$ (the $\omega^1$ and $\omega^0$ contributions) determine transport equations for the amplitude $A(x)$ and represent corrections to the geometrical optics approximation. In deriving the eikonal equation we have neglected these subleading terms, which is valid when $\omega$ is large and $A$ and $S$ vary on spatial scales much larger than the wavelength.

\medskip

\noindent\textbf{(b) Characteristic system for the eikonal equation.}
We rewrite the eikonal equation in the form
\[
F(x,\nabla S(x)) = 0,\qquad
F(x,p) = \frac{1}{2}\bigl(|p|^2 - n(x)^2\bigr),\quad n(x)=\frac{1}{c(x)}.
\]
For a general first-order PDE $F(x,u,p)=0$ with $p=\nabla u$, the characteristic curves $(x(s),u(s),p(s))$ in the extended space satisfy
\[
\dot{x} = F_p(x,u,p),\qquad
\dot{u} = p\cdot F_p(x,u,p),\qquad
\dot{p} = -F_x(x,u,p) - p\,F_u(x,u,p).
\]
In our case, $F$ does not depend on $u=S$, so $F_u\equiv 0$ and $F=F(x,p)$ only. The characteristic system simplifies to
\[
\dot{x}(s) = F_p(x(s),p(s)),\qquad
\dot{S}(s) = p(s)\cdot F_p(x(s),p(s)),\qquad
\dot{p}(s) = -F_x(x(s),p(s)).
\]

We now compute the derivatives of $F$:
\[
F(x,p) = \frac{1}{2}|p|^2 - \frac{1}{2}n(x)^2.
\]
The derivative with respect to $p$ is
\[
F_p(x,p) = p,
\]
and the derivative with respect to $x$ is
\[
F_x(x,p) = -\frac{1}{2}\,\nabla_x\bigl(n(x)^2\bigr).
\]
Thus, the characteristic ODEs become
\[
\dot{x}(s) = p(s),\qquad
\dot{S}(s) = p(s)\cdot p(s) = |p(s)|^2,\qquad
\dot{p}(s) = -F_x(x(s),p(s)) = \frac{1}{2}\,\nabla\bigl(n(x(s))^2\bigr).
\]

On the eikonal manifold $F=0$ we have $|p|^2 = n(x)^2$, so along any characteristic,
\[
\dot{S}(s) = |p(s)|^2 = n(x(s))^2.
\]
The key equation for the geometry of rays is
\[
\dot{x}(s) = p(s) = \nabla S(x(s)).
\]
Recall that the gradient of a scalar function is normal to its level sets. The level sets of $S$,
\[
\{x : S(x) = \text{constant}\},
\]
are precisely the wavefronts. The equation $\dot{x} = \nabla S$ means that the tangent vector to a characteristic (ray) is parallel to the gradient of $S$; hence rays always cross wavefronts orthogonally. Thus, the method of characteristics reveals that in geometrical optics the energy propagates along curves that are perpendicular to the phase surfaces.

This is a central idea in this section: for a nonlinear first-order PDE of Hamilton–Jacobi type, the characteristic curves in physical space are the integral curves of the gradient of the unknown, and they provide a geometric representation of the solution in terms of rays and wavefronts.

\medskip

\noindent\textbf{(c) Homogeneous medium: straight rays and explicit phase.}
Suppose now that the medium is homogeneous:
\[
c(x)\equiv c_0>0,\qquad n(x) \equiv n_0 = \frac{1}{c_0}.
\]
Then $n(x)^2 = n_0^2$ is constant, so
\[
\nabla\bigl(n(x)^2\bigr) = 0,
\]
and the characteristic equations simplify considerably:
\[
\dot{x}(s) = p(s),\qquad
\dot{S}(s) = |p(s)|^2,\qquad
\dot{p}(s) = 0.
\]
The last equation, $\dot{p}(s)=0$, shows that $p(s)$ is constant along each characteristic: $p(s)\equiv p_0$ for some constant vector $p_0$ on a given ray. Consequently,
\[
\dot{x}(s) = p_0\quad\Longrightarrow\quad x(s) = x(0) + s\,p_0,
\]
so the projection of the characteristic curves into the $x$–plane are straight lines. These straight lines are the \emph{rays} of geometrical optics in a homogeneous medium. This matches the familiar physical fact that light travels in straight lines in a uniform medium.

To understand the phase function $S(x)$ for a point source, consider a source at the origin emitting outgoing circular waves. By symmetry, $S$ should depend only on the radial coordinate $r = |x|$, so write
\[
S(x) = f(r).
By direct computation, a radially symmetric function $S(x)=f(r)$ has gradient
\[
\nabla S(x) = f'(r)\,\frac{x}{r},\qquad r=|x|,
\]
so that
\[
|\nabla S(x)| = |f'(r)|.
\]
The eikonal equation $|\nabla S| = 1/c_0$ therefore becomes the ODE
\[
|f'(r)| = \frac{1}{c_0} = n_0.
\]
Choosing the positive sign to represent an increasing phase with increasing distance from the source (outgoing waves), we take
\[
f'(r) = \frac{1}{c_0},
\]
which integrates to
\[
f(r) = \frac{r}{c_0} + \text{constant} = n_0\,r + \text{constant}.
\]
Up to an additive constant (which only shifts the phase), we may write
\[
S(x) = \frac{|x|}{c_0}.
\]

The level sets of $S$,
\[
S(x) = \text{constant} \quad\Longleftrightarrow\quad |x| = \text{constant},
\]
are concentric circles centered at the origin. These are the \emph{wavefronts} (surfaces of constant phase or travel time). The rays, given by the characteristic curves $x(s)$ satisfying $\dot{x}(s)=\nabla S(x(s))$, are straight lines passing through the origin:
\[
\dot{x}(s) \parallel x(s)\quad\Longrightarrow\quad x(s) = x(0) + s\,p_0,
\]
with $p_0$ a constant vector pointing radially outward (for outgoing waves).

Thus, in a homogeneous medium:

- Wavefronts are expanding circles centered at the source.
- Rays are straight radial lines orthogonal to these circles.

This coincides with the usual physical picture: from a point source in a uniform medium, waves propagate outward in all directions with speed $c_0$, forming circular (in 2D) or spherical (in 3D) wavefronts, and energy travels along straight radial rays.

\end{solution}

\section{Classification of Linear Second-Order PDEs}
% --- Narrative plan (auto-generated) ---
% This section introduces the classical classification of linear second-order partial differential equations into elliptic, parabolic, and hyperbolic types. The classification is based on the quadratic form built from the second-derivative terms, in much the same way that conic sections are classified using the discriminant. Understanding whether a PDE is elliptic, parabolic, or hyperbolic immediately suggests which kinds of boundary or initial data are appropriate, what sorts of solution behavior one should expect, and which analytical or numerical techniques are likely to work.
%
% In applied mathematics, this classification underlies the modeling and analysis of diffusion processes, steady-state fields, and wave propagation, which appear everywhere in physics, engineering, and quantitative biology. Elliptic equations such as Laplace’s equation are tied to potential theory and harmonic functions, and connect naturally to complex analysis and Fourier series methods. Parabolic equations such as the heat equation are closely related to dynamical systems and semigroup theory, while hyperbolic equations such as the wave equation connect to characteristic curves and ideas from ordinary differential equations. The classification also guides later topics in this course, including separation of variables, transform methods, and energy estimates, by telling us when and why those methods apply.

% ===== Example 1: A Family of Second-Order PDEs in Two Variables (inquiry-based) =====
\begin{problem}[A Family of Second-Order PDEs in Two Variables]
Many of the standard linear partial differential equations of mathematical physics, such as Laplace's equation, the heat equation, and the wave equation, have second-order derivatives as their highest-order terms. In two spatial variables, these highest-order terms can be organized into a quadratic form in the derivatives. The algebraic type of that quadratic form (positive definite, indefinite, or degenerate) turns out to govern many qualitative features of the solutions. In this problem you will explore this connection for a general constant-coefficient second-order equation in two variables.

Consider the family of linear second-order partial differential equations in two variables
\[
a\,u_{xx} + 2b\,u_{xy} + c\,u_{yy} + \text{(lower-order terms)} = f(x,y),
\]
where $a$, $b$, and $c$ are real constants, not all zero in the second-order part.

\smallskip

(a) Warm-up with model examples. For each of the following equations, write down the corresponding triple $(a,b,c)$ and the quadratic form
\[
Q(\xi,\eta) = a\xi^2 + 2b\xi\eta + c\eta^2.
\]
Then compute the discriminant $D = b^2 - ac$ of this quadratic form.
\begin{enumerate}
\item[(i)] Laplace's equation: $u_{xx} + u_{yy} = 0$.
\item[(ii)] The wave equation in one space and one time dimension: $u_{tt} - c^2 u_{xx} = 0$. (For this part, simply treat $t$ as one of the independent variables alongside $x$ and ignore the physical meaning.)
\item[(iii)] The steady (time-independent) heat equation with a reaction term: $u_{xx} + u_{yy} - u = 0$.
\end{enumerate}
How do the signs of the discriminants you find compare across these three model equations?

\smallskip

(b) Associate a matrix to the principal part. Show that the quadratic form $Q(\xi,\eta)$ from part (a) can be written as
\[
Q(\xi,\eta) = 
\begin{bmatrix}\xi & \eta\end{bmatrix}
\begin{bmatrix}a & b \\[4pt] b & c\end{bmatrix}
\begin{bmatrix}\xi \\[4pt]\eta\end{bmatrix}
= \mathbf{v}^{T}A\,\mathbf{v},
\]
where $\mathbf{v} = \begin{bmatrix}\xi \\ \eta\end{bmatrix}$ and $A$ is the symmetric $2\times 2$ matrix with entries $a$, $b$, and $c$. 
\begin{enumerate}
\item[(i)] Verify the matrix expression for $Q(\xi,\eta)$ by direct multiplication.
\item[(ii)] Explain why a linear change of variables $(x,y)\mapsto(\xi,\eta)$ in the PDE corresponds to the action of an invertible $2\times 2$ matrix $P$ on the vector of frequencies $\mathbf{v} = (\xi,\eta)^T$ in the quadratic form.
\end{enumerate}
Hint: Think about the chain rule for first and second derivatives, and how gradients and Hessians transform under linear maps.

\smallskip

(c) Diagonalizing the principal part. Suppose that $A$ has two real eigenvalues $\lambda_1$ and $\lambda_2$ and an orthogonal matrix $R$ with $R^T A R = \operatorname{diag}(\lambda_1,\lambda_2)$. 
\begin{enumerate}
\item[(i)] Show that in the new coordinates $(\xi,\eta)$ obtained by applying the rotation $R$ to $(x,y)$, the principal part of the PDE becomes
\[
\lambda_1\,u_{\xi\xi} + \lambda_2\,u_{\eta\eta}.
\]
(You may ignore lower-order terms in this step and focus only on the second derivatives.)
\item[(ii)] Using basic facts about $2\times 2$ symmetric matrices, relate the signs of $\lambda_1$ and $\lambda_2$ to the discriminant $D = b^2 - ac$ and to the determinant $\det A = ac - b^2$. In particular, determine when both eigenvalues have the same sign, when they have opposite signs, and when one eigenvalue is zero.
\end{enumerate}
Hint: Recall that for a real symmetric $2\times 2$ matrix $A$, the trace $\operatorname{tr}A = a + c$ equals $\lambda_1 + \lambda_2$ and the determinant $\det A = ac - b^2$ equals $\lambda_1 \lambda_2$.

\smallskip

(d) Classification and canonical forms. We say that the PDE is:
\begin{itemize}
\item \emph{elliptic} if the quadratic form $Q$ is definite (always positive or always negative, except at the origin),
\item \emph{hyperbolic} if $Q$ is indefinite,
\item \emph{parabolic} if $Q$ is degenerate but not identically zero.
\end{itemize}
\begin{enumerate}
\item[(i)] Using your work from part (c), express these three cases in terms of the discriminant $D = b^2 - ac$.
\item[(ii)] Show that after an appropriate linear change of variables $(x,y)\mapsto(\xi,\eta)$ that simplifies the principal part, any such PDE with constant coefficients can be put into one of the following canonical forms for its leading part:
\[
u_{\xi\xi} + u_{\eta\eta}, \qquad
u_{\xi\xi} - u_{\eta\eta}, \qquad
u_{\xi\xi},
\]
corresponding to elliptic, hyperbolic, and parabolic equations, respectively (up to constant nonzero factors).
\item[(iii)] Match each of the model equations from part (a) with one of the three canonical forms above by an appropriate relabeling or rescaling of the independent variables.
\end{enumerate}
Hint: You may assume that multiplying the entire PDE by a nonzero constant does not change its type. Focus on the signs and possible zeros of the eigenvalues $\lambda_1$ and $\lambda_2$.

\smallskip

(e) Explorations and extensions.
\begin{enumerate}
\item[(i)] Suppose now that the coefficients $a$, $b$, and $c$ depend smoothly on $(x,y)$, so that you have a PDE of the form
\[
a(x,y)u_{xx} + 2b(x,y)u_{xy} + c(x,y)u_{yy} + \text{(lower-order terms)} = f(x,y).
\]
How might you use the sign of the discriminant $D(x,y) = b(x,y)^2 - a(x,y)c(x,y)$ to classify the equation \emph{locally} near a given point $(x_0,y_0)$?
\item[(ii)] The usual time-dependent heat equation in one space dimension is
\[
u_t = k\,u_{xx}.
\]
If you treat $t$ and $x$ simply as two independent variables, can you fit this equation into the framework of part (d)? What are the corresponding $(a,b,c)$, and what is the discriminant $D$? How does that relate to the description of the heat equation as \emph{parabolic}?
\end{enumerate}

\end{problem}

% ===== Example 1: A Family of Second-Order PDEs in Two Variables (full solution) =====
\begin{problem}[A Family of Second-Order PDEs in Two Variables]
Consider the linear second-order partial differential equation with constant coefficients
\[
a\,u_{xx} + 2b\,u_{xy} + c\,u_{yy} + \text{(lower-order terms)} = f(x,y),
\]
where $a,b,c\in\mathbb{R}$ and $(a,b,c)\neq(0,0,0)$ in the second-order part.

\begin{enumerate}
\item[(a)] Associate to the principal part the quadratic form
\[
Q(\xi,\eta) = a\xi^2 + 2b\xi\eta + c\eta^2,
\]
and the symmetric matrix
\[
A = \begin{bmatrix} a & b \\[4pt] b & c \end{bmatrix}.
\]
Show that under an invertible linear change of variables $(x,y)\mapsto(\xi,\eta)$, the principal part can be written as
\[
\lambda_1\,u_{\xi\xi} + \lambda_2\,u_{\eta\eta},
\]
where $\lambda_1$ and $\lambda_2$ are the eigenvalues of $A$.

\item[(b)] Express the signs of $\lambda_1$ and $\lambda_2$ in terms of the discriminant
\[
D = b^2 - ac.
\]
Classify the PDE as \emph{elliptic}, \emph{hyperbolic}, or \emph{parabolic} according to whether $Q$ is definite, indefinite, or degenerate, and show that this corresponds to the cases
\[
D<0,\quad D>0,\quad D=0,
\]
respectively (assuming the principal part is not identically zero).

\item[(c)] Deduce that, after an appropriate linear change of variables and multiplication of the equation by a nonzero constant, the principal part of any such PDE can be brought into one of the canonical forms
\[
u_{\xi\xi} + u_{\eta\eta}, \qquad
u_{\xi\xi} - u_{\eta\eta}, \qquad
u_{\xi\xi},
\]
corresponding to elliptic, hyperbolic, and parabolic equations. Illustrate this correspondence by identifying the types of
\[
u_{xx} + u_{yy} = 0,\qquad
u_{tt} - c^2 u_{xx} = 0,\qquad
u_t - k u_{xx} = 0,
\]
viewed as equations in two independent variables.
\end{enumerate}
\end{problem}

\begin{solution}
We begin by isolating the principal (second-order) part of the equation
\[
a\,u_{xx} + 2b\,u_{xy} + c\,u_{yy} = \text{(terms of order }\le 1).
\]
The classification of such equations is governed entirely by this principal part.

\medskip

\noindent\textbf{(a) Quadratic form, matrix, and change of variables.}
We associate to the principal part the quadratic form in two real variables
\[
Q(\xi,\eta) = a\xi^2 + 2b\xi\eta + c\eta^2.
\]
It is convenient to write this in matrix notation. Define the symmetric matrix
\[
A = \begin{bmatrix} a & b \\[4pt] b & c \end{bmatrix}
\]
and the column vector $\mathbf{v} = \begin{bmatrix}\xi \\[2pt] \eta\end{bmatrix}$. Then
\[
\mathbf{v}^{T}A\mathbf{v}
= \begin{bmatrix}\xi & \eta\end{bmatrix}
\begin{bmatrix} a & b \\[4pt] b & c \end{bmatrix}
\begin{bmatrix}\xi \\[2pt] \eta\end{bmatrix}
= a\xi^2 + 2b\xi\eta + c\eta^2
= Q(\xi,\eta).
\]
Thus the matrix $A$ encodes the coefficients of the second-order derivatives.

Now consider an invertible linear change of variables
\[
\begin{bmatrix} x \\[2pt] y \end{bmatrix}
= P \begin{bmatrix} \xi \\[2pt] \eta \end{bmatrix},
\]
where $P$ is a $2\times 2$ invertible real matrix. The chain rule shows that the gradient and Hessian transform in a linear fashion. In particular, the Hessian of $u$ with respect to $(x,y)$ is related to the Hessian with respect to $(\xi,\eta)$ by
\[
H_{(x,y)}u = P^{-T} \, H_{(\xi,\eta)}u \, P^{-1},
\]
where the superscript $-T$ denotes the inverse transpose. When we contract the Hessian with the matrix $A$ to form the principal part, we obtain
\[
\begin{bmatrix} u_{xx} & u_{xy} \\[2pt] u_{xy} & u_{yy} \end{bmatrix}
: A
\quad\text{transforms to}\quad
\begin{bmatrix} u_{\xi\xi} & u_{\xi\eta} \\[2pt] u_{\xi\eta} & u_{\eta\eta} \end{bmatrix}
: (P^{T} A P),
\]
where $:$ denotes the Frobenius inner product of matrices. Concretely, in the new variables the principal part becomes
\[
a'\,u_{\xi\xi} + 2b'\,u_{\xi\eta} + c'\,u_{\eta\eta},
\]
with the matrix of new coefficients
\[
A' = P^{T}AP.
\]

Since $A$ is a real symmetric matrix, there exists an orthogonal matrix $R$ and real eigenvalues $\lambda_1,\lambda_2$ such that
\[
R^{T} A R = \begin{bmatrix} \lambda_1 & 0 \\[4pt] 0 & \lambda_2 \end{bmatrix}.
\]
Taking $P = R$ corresponds to a rotation of coordinates. In these rotated variables $(\xi,\eta)$, the matrix of principal coefficients is diagonal, and the mixed derivative term disappears. The principal part is then
\[
\lambda_1\,u_{\xi\xi} + \lambda_2\,u_{\eta\eta},
\]
as claimed. Lower-order terms may become more complicated, but they do not affect the classification by type.

\medskip

\noindent\textbf{(b) Eigenvalues, discriminant, and classification.}
The eigenvalues $\lambda_1$ and $\lambda_2$ of the $2\times 2$ symmetric matrix $A$ are real, and they control the nature of the quadratic form
\[
Q(\xi,\eta) = \lambda_1 \xi'^2 + \lambda_2 \eta'^2
\]
in diagonalized coordinates $(\xi',\eta')$.

For a $2\times 2$ matrix $A$, the trace and determinant are
\[
\operatorname{tr}A = a + c = \lambda_1 + \lambda_2,\qquad
\det A = ac - b^2 = \lambda_1 \lambda_2.
\]
By definition, the discriminant of the quadratic form is
\[
D = b^2 - ac = -\det A.
\]
Thus
\[
\det A = -D.
\]

We consider three cases:

\medskip

\emph{Case 1: $D<0$.} Then $\det A > 0$. Since $\det A = \lambda_1 \lambda_2 > 0$, the eigenvalues have the same sign: either both positive or both negative. If, in addition, at least one of $a$ or $c$ is nonzero (so the principal part is not identically zero), then $\lambda_1$ and $\lambda_2$ are both nonzero. Therefore the quadratic form $Q$ is definite: it is either positive definite or negative definite, depending on the common sign of the eigenvalues. In this case the PDE is called \emph{elliptic}.

\medskip

\emph{Case 2: $D>0$.} Then $\det A < 0$. Hence $\lambda_1 \lambda_2 < 0$, so the eigenvalues have opposite signs. The quadratic form $Q$ is indefinite: it takes both positive and negative values. The PDE is called \emph{hyperbolic}.

\medskip

\emph{Case 3: $D=0$.} Then $\det A = 0$, so at least one eigenvalue is zero. If the principal part is not identically zero, then exactly one eigenvalue is zero and the other is nonzero. In diagonal form, the quadratic form is proportional to a single square, such as $\lambda_1 \xi'^2$. The quadratic form is degenerate but not identically zero. The PDE is called \emph{parabolic}.

\medskip

These three algebraic possibilities (definite, indefinite, degenerate nonzero) are precisely the three classical types of second-order linear PDE in two variables.

\medskip

\noindent\textbf{(c) Canonical forms and examples.}
From part (a) we know that, after an orthogonal change of variables, the principal part becomes
\[
\lambda_1\,u_{\xi\xi} + \lambda_2\,u_{\eta\eta}.
\]
We are also allowed to multiply the entire PDE by a nonzero constant without changing its type, since this does not alter the sign pattern or degeneracy of the quadratic form.

\medskip

\emph{Elliptic case $D<0$.} Here $\lambda_1$ and $\lambda_2$ are both nonzero and have the same sign. Multiplying the equation by a constant, we may assume without loss of generality that $\lambda_1 = \lambda_2 = 1$ or $\lambda_1 = \lambda_2 = -1$. Multiplying by $-1$ if necessary, the principal part is equivalent to
\[
u_{\xi\xi} + u_{\eta\eta}.
\]
This is the canonical elliptic operator in two variables, exemplified by Laplace's equation.

\medskip

\emph{Hyperbolic case $D>0$.} Here $\lambda_1$ and $\lambda_2$ are nonzero and have opposite signs. Multiplying by a suitable nonzero constant, we may assume $\lambda_1 = 1$ and $\lambda_2 = -1$ (or vice versa). Thus the principal part is equivalent to
\[
u_{\xi\xi} - u_{\eta\eta}.
\]
This is the standard hyperbolic form, closely related to the one-dimensional wave operator.

\medskip

\emph{Parabolic case $D=0$.} Here one eigenvalue is zero and the other is nonzero. After scaling, we may assume that the nonzero eigenvalue equals $1$, so the principal part is equivalent to
\[
u_{\xi\xi}.
\]
This is the canonical parabolic form in two variables.

\medskip

We now illustrate this classification with the concrete equations in the problem statement, viewed simply as equations in two independent variables.

\medskip

\emph{Example 1: Laplace's equation.} Consider
\[
u_{xx} + u_{yy} = 0.
\]
Here $a = 1$, $b = 0$, $c = 1$. The discriminant is
\[
D = b^2 - ac = 0^2 - (1)(1) = -1 < 0.
\]
Hence the equation is elliptic. In fact, it is already in the canonical elliptic form
\[
u_{xx} + u_{yy} = 0,
\]
so no change of variables is needed.

\medskip

\emph{Example 2: The one-dimensional wave equation.} Consider
\[
u_{tt} - c^2 u_{xx} = 0.
\]
If we treat $(t,x)$ as the two independent variables, then $a = 1$ for $u_{tt}$, $b = 0$, and $c = -c^2$ for $u_{xx}$. The discriminant is
\[
D = b^2 - ac = 0^2 - (1)(-c^2) = c^2 > 0,
\]
so the equation is hyperbolic. Its principal part is already of the canonical form
\[
u_{tt} - c^2 u_{xx} = 0,
\]
and by rescaling the space (or time) variable, for example setting $\xi = t$ and $\eta = c x$, we can write it as
\[
u_{\xi\xi} - u_{\eta\eta} = 0,
\]
which is the standard hyperbolic canonical form.

\medskip

\emph{Example 3: The (1+1)-dimensional heat equation.} Consider
\[
u_t - k u_{xx} = 0.
\]
Again treating $(t,x)$ as our two independent variables, the principal part involves the derivatives $u_{tt}$, $u_{tx}$, and $u_{xx}$. However, in this equation there is \emph{no} $u_{tt}$ or $u_{tx}$ term, only $u_{xx}$. Thus, for the purpose of classification in two variables, we can regard
\[
a = 0,\quad b = 0,\quad c = -k
\]
when we order the variables as $(t,x)$. The quadratic form is
\[
Q(\xi,\eta) = -k \eta^2,
\]
and the discriminant is
\[
D = b^2 - ac = 0^2 - (0)(-k) = 0.
\]
Thus the heat equation is parabolic. In fact, after relabeling $\xi = x$ and $\eta = t$, the principal part is simply proportional to $u_{\xi\xi}$, which is exactly the parabolic canonical form.

\medskip

\noindent\textbf{Connection to the general theory.}
This example illustrates the main idea of the classification of linear second-order PDEs in two variables: the type of the equation is determined by the algebraic type of the quadratic form associated with its principal part. The symmetric matrix of second-order coefficients can be diagonalized by an orthogonal change of variables, which removes the mixed second derivative and reduces the principal part to a combination of pure second derivatives with coefficients equal to the eigenvalues. The sign pattern and possible degeneracy of these eigenvalues are captured compactly by the discriminant $D = b^2 - ac$, and they lead naturally to the three canonical forms
\[
u_{\xi\xi} + u_{\eta\eta},\quad
u_{\xi\xi} - u_{\eta\eta},\quad
u_{\xi\xi},
\]
representing elliptic, hyperbolic, and parabolic equations, respectively. These three types correspond to fundamentally different qualitative behaviors of solutions, as seen in Laplace's equation, the wave equation, and the heat equation.
\end{solution}

% ===== Example 2: Laplace’s Equation and Elliptic Type (inquiry-based) =====
\begin{problem}[Laplace’s Equation and Elliptic Type]
Laplace’s equation
\[
u_{xx} + u_{yy} = 0
\]
arises in many physical models, including steady-state heat flow, electrostatics, and incompressible fluid flow in two dimensions. In these settings, one typically prescribes the value of $u$ (for instance, temperature or electric potential) along a closed curve, and the equation determines a smooth interior configuration. From the viewpoint of classification of second-order partial differential equations, Laplace’s equation is the prototype of an \emph{elliptic} equation, and many of its qualitative properties (such as the maximum principle) are already visible in this simple case.

Recall that a general linear second-order partial differential equation in two variables $x$ and $y$ can be written in the form
\[
A(x,y)\,u_{xx} + 2B(x,y)\,u_{xy} + C(x,y)\,u_{yy} + \text{lower order terms} = F(x,y),
\]
where $A,B,C,F$ are given functions. The classification into elliptic, parabolic, or hyperbolic types is based on the discriminant $B^2 - AC$.

\smallskip

(a) Rewrite Laplace’s equation in the general form
\[
A\,u_{xx} + 2B\,u_{xy} + C\,u_{yy} = 0,
\]
with \emph{constant} coefficients $A,B,C$. Identify $A,B,C$ and compute the discriminant $B^2 - AC$. According to the usual classification rule, what type of equation is Laplace’s equation?

\medskip

(b) A useful way to visualize the classification is to associate to the second-order part of the equation the quadratic form
\[
Q(\xi,\eta) = A \xi^2 + 2B\xi\eta + C\eta^2
\]
in the variables $(\xi,\eta) \in \mathbb{R}^2$.

\begin{enumerate}
\item[(i)] For Laplace’s equation, write down the explicit formula for $Q(\xi,\eta)$.
\item[(ii)] Show that $Q(\xi,\eta)$ is \emph{positive definite}, that is, $Q(\xi,\eta) > 0$ whenever $(\xi,\eta) \neq (0,0)$.
\end{enumerate}
Hint: One option is to compute the eigenvalues of the symmetric matrix
\[
\begin{pmatrix}
A & B\\[4pt]
B & C
\end{pmatrix}
\]
and show that they are both positive. A simpler route may be available for this special case.

\medskip

(c) The coefficients $A,B,C$ and the quadratic form $Q$ do not depend on the coordinate axes you choose; in other words, they transform in a natural way under changes of variables. In the elliptic case, one can ``diagonalize'' the quadratic form $Q$ by a rotation of coordinates.

Suppose we perform a rotation of the $(x,y)$-plane by an angle $\theta$, introducing new variables $(\xi,\eta)$ via
\[
\begin{pmatrix}
\xi\\[4pt]
\eta
\end{pmatrix}
=
\begin{pmatrix}
\cos\theta & \sin\theta\\[4pt]
-\sin\theta & \cos\theta
\end{pmatrix}
\begin{pmatrix}
x\\[4pt]
y
\end{pmatrix}.
\]
Let $v(\xi,\eta) = u(x(\xi,\eta), y(\xi,\eta))$ be the expression of $u$ in the rotated coordinates.

\begin{enumerate}
\item[(i)] Without doing a full chain rule computation, explain why the Laplace operator $u_{xx} + u_{yy}$ should have the same form $v_{\xi\xi} + v_{\eta\eta}$ in the rotated $(\xi,\eta)$-coordinates.
\item[(ii)] How does this rotational invariance of $u_{xx} + u_{yy}$ fit with the geometric picture of Laplace’s equation as elliptic?
\end{enumerate}
Hint: Think of $u_{xx} + u_{yy}$ as the trace of the Hessian matrix of $u$, and recall how orthogonal transformations affect traces and eigenvalues of matrices.

\medskip

(d) One of the most important qualitative features of elliptic equations is the \emph{maximum principle}: interior maxima (or minima) of a solution are strongly constrained. For Laplace’s equation, solutions are called \emph{harmonic functions}.

Let $\Omega \subset \mathbb{R}^2$ be a bounded open set, and suppose $u \in C^2(\Omega) \cap C^0(\overline{\Omega})$ is harmonic in $\Omega$, that is, $u_{xx} + u_{yy} = 0$ in $\Omega$. Assume that $u$ attains its maximum value at some interior point $(x_0,y_0) \in \Omega$.

\begin{enumerate}
\item[(i)] Explain why the gradient of $u$ must vanish at $(x_0,y_0)$, and why the Hessian matrix $D^2u(x_0,y_0)$ must be negative semidefinite (that is, all its eigenvalues are less than or equal to zero).
\item[(ii)] Show that the equation $u_{xx} + u_{yy} = 0$ at $(x_0,y_0)$ forces all eigenvalues of $D^2u(x_0,y_0)$ to be equal to zero.
\item[(iii)] Conclude that if $u$ achieves its maximum at an interior point, then $u$ must, in fact, be constant in $\Omega$.
\end{enumerate}
Hint: Combine the fact that the eigenvalues of a symmetric matrix have nonpositive real parts when the matrix is negative semidefinite with the observation that the trace of the Hessian is the sum of its eigenvalues.

\medskip

(e) \textbf{What if / extensions.}
\begin{enumerate}
\item[(i)] Consider instead the equation $u_{xx} - u_{yy} = 0$. Classify this equation using the discriminant $B^2 - AC$. Does the associated quadratic form have the same sign properties as in parts (b)–(c)? Briefly describe how you would expect the qualitative behavior of its solutions to differ from those of Laplace’s equation (for example, in terms of boundary value problems).
\item[(ii)] More generally, for the equation
\[
a\,u_{xx} + c\,u_{yy} = 0,
\]
where $a$ and $c$ are nonzero constants, classify the equation according to the signs of $a$ and $c$. How do changes in these signs relate to the geometric type of the quadratic form $a\xi^2 + c\eta^2$ and to the elliptic or hyperbolic nature of the equation?
\end{enumerate}

\end{problem}

% ===== Example 2: Laplace’s Equation and Elliptic Type (full solution) =====
\begin{problem}[Laplace’s Equation and Elliptic Type]
Consider Laplace’s equation in two variables,
\[
u_{xx} + u_{yy} = 0.
\]
\begin{enumerate}
\item[(a)] Write this in the general second-order form
\[
A\,u_{xx} + 2B\,u_{xy} + C\,u_{yy} = 0
\]
and classify the equation as elliptic, parabolic, or hyperbolic using the discriminant $B^2 - AC$. Express the associated quadratic form $Q(\xi,\eta) = A\xi^2 + 2B\xi\eta + C\eta^2$ and show that it is positive definite.

\item[(b)] Interpret the operator $u_{xx} + u_{yy}$ as the trace of the Hessian matrix $D^2u$ and explain briefly why this operator is invariant in form under any rotation of the $(x,y)$-coordinates. Relate this rotational invariance to the elliptic character of Laplace’s equation.

\item[(c)] Let $\Omega \subset \mathbb{R}^2$ be a bounded open set and suppose that $u \in C^2(\Omega) \cap C^0(\overline{\Omega})$ satisfies Laplace’s equation $u_{xx} + u_{yy} = 0$ in $\Omega$. Assume that $u$ attains its maximum at an interior point $(x_0,y_0) \in \Omega$. Show that $u$ must be constant in $\Omega$. (You may argue using the eigenvalues of the Hessian matrix $D^2u(x_0,y_0)$.)

\item[(d)] For comparison, classify the equation $u_{xx} - u_{yy} = 0$ using the discriminant and comment briefly on how its type (elliptic vs.\ hyperbolic) suggests a different qualitative behavior of solutions from that of Laplace’s equation.
\end{enumerate}
\end{problem}

\begin{solution}
We analyze Laplace’s equation from the viewpoint of the general theory of second-order linear partial differential equations in two variables.

\medskip

\noindent\textbf{(a) Classification and quadratic form.}
A general linear second-order equation with no lower order terms can be written as
\[
A\,u_{xx} + 2B\,u_{xy} + C\,u_{yy} = 0,
\]
where $A,B,C$ are functions or constants. For Laplace’s equation
\[
u_{xx} + u_{yy} = 0,
\]
we read off
\[
A = 1,\quad B = 0,\quad C = 1.
\]
The discriminant is
\[
B^2 - AC = 0^2 - (1)(1) = -1 < 0.
\]
By the standard classification, a second-order equation in two variables is
\begin{itemize}
\item elliptic if $B^2 - AC < 0$,
\item parabolic if $B^2 - AC = 0$,
\item hyperbolic if $B^2 - AC > 0$.
\end{itemize}
Therefore, Laplace’s equation is \emph{elliptic} at every point of the plane.

The associated quadratic form is
\[
Q(\xi,\eta) = A\xi^2 + 2B\xi\eta + C\eta^2
= 1\cdot \xi^2 + 0\cdot (2\xi\eta) + 1\cdot \eta^2
= \xi^2 + \eta^2.
\]
For $(\xi,\eta) \neq (0,0)$, we have
\[
Q(\xi,\eta) = \xi^2 + \eta^2 > 0.
\]
Thus $Q$ is \emph{positive definite}. This is the algebraic manifestation of ellipticity: the second-order part defines a positive definite quadratic form. In terms of matrices, the second-order coefficients form the symmetric matrix
\[
\begin{pmatrix}
A & B\\
B & C
\end{pmatrix}
=
\begin{pmatrix}
1 & 0\\
0 & 1
\end{pmatrix},
\]
whose eigenvalues are both equal to $1$, hence strictly positive.

\medskip

\noindent\textbf{(b) Rotational invariance and ellipticity.}
The Hessian matrix of a twice differentiable function $u$ is
\[
D^2 u =
\begin{pmatrix}
u_{xx} & u_{xy}\\
u_{xy} & u_{yy}
\end{pmatrix}.
\]
The Laplace operator in two dimensions is the sum of the pure second derivatives, which is the trace of the Hessian:
\[
\Delta u = u_{xx} + u_{yy} = \operatorname{tr}(D^2u).
\]

A rotation of coordinates is given by an orthogonal matrix $R$ with $R^T R = I$. If we introduce new coordinates $(\xi,\eta)$ related to $(x,y)$ by
\[
\begin{pmatrix}
\xi\\
\eta
\end{pmatrix}
= R
\begin{pmatrix}
x\\
y
\end{pmatrix},
\]
and define $v(\xi,\eta) = u(x(\xi,\eta),y(\xi,\eta))$, the chain rule shows that the Hessian $D^2v$ in $(\xi,\eta)$-coordinates is obtained from $D^2u$ by the similarity transformation
\[
D^2 v = R^T (D^2 u) R.
\]
This is the standard behavior of a symmetric bilinear form under change of orthonormal basis. Taking traces and using the cyclic property of the trace gives
\[
\operatorname{tr}(D^2 v)
= \operatorname{tr}(R^T D^2 u\, R)
= \operatorname{tr}(D^2 u\, R R^T)
= \operatorname{tr}(D^2 u\, I)
= \operatorname{tr}(D^2 u).
\]
Thus
\[
v_{\xi\xi} + v_{\eta\eta}
= \operatorname{tr}(D^2 v)
= \operatorname{tr}(D^2 u)
= u_{xx} + u_{yy}.
\]
This calculation shows that \emph{Laplace’s equation has the same form in any rotated coordinate system}: if $u$ satisfies $u_{xx} + u_{yy} = 0$ in $(x,y)$, then $v$ satisfies $v_{\xi\xi} + v_{\eta\eta} = 0$ in $(\xi,\eta)$.

This rotational invariance is characteristic of the underlying geometry of the positive definite quadratic form $Q(\xi,\eta) = \xi^2 + \eta^2$: its level sets are concentric circles, which are themselves rotationally invariant. Elliptic equations associated with positive definite quadratic forms often inherit such invariances, and this is reflected in the isotropic, ``smoothing'' behavior of their solutions.

\medskip

\noindent\textbf{(c) Interior maximum and constancy (maximum principle).}
Now let $\Omega \subset \mathbb{R}^2$ be a bounded open set and suppose that $u \in C^2(\Omega) \cap C^0(\overline{\Omega})$ satisfies Laplace’s equation
\[
u_{xx} + u_{yy} = 0 \quad \text{in } \Omega.
\]
Assume that $u$ attains its maximum at an interior point $(x_0,y_0) \in \Omega$.

Because $u$ has a local maximum at $(x_0,y_0)$ and is twice continuously differentiable, the classical calculus characterization of extrema applies. First, the gradient must vanish:
\[
\nabla u(x_0,y_0) = (u_x(x_0,y_0), u_y(x_0,y_0)) = (0,0).
\]
Second, the Hessian matrix
\[
H := D^2u(x_0,y_0) =
\begin{pmatrix}
u_{xx}(x_0,y_0) & u_{xy}(x_0,y_0)\\
u_{xy}(x_0,y_0) & u_{yy}(x_0,y_0)
\end{pmatrix}
\]
must be \emph{negative semidefinite}. This means that for every vector $z \in \mathbb{R}^2$ we have
\[
z^T H z \leq 0.
\]
Equivalently, all eigenvalues of $H$ are less than or equal to zero.

On the other hand, Laplace’s equation at the point $(x_0,y_0)$ reads
\[
u_{xx}(x_0,y_0) + u_{yy}(x_0,y_0) = 0.
\]
The left-hand side is the trace of $H$:
\[
\operatorname{tr}(H) = u_{xx}(x_0,y_0) + u_{yy}(x_0,y_0) = 0.
\]
The eigenvalues of the symmetric matrix $H$ are real; denote them by $\lambda_1$ and $\lambda_2$. Since $H$ is negative semidefinite, we have
\[
\lambda_1 \leq 0, \quad \lambda_2 \leq 0.
\]
At the same time,
\[
\lambda_1 + \lambda_2 = \operatorname{tr}(H) = 0.
\]
The only way for two real numbers that are each less than or equal to zero to have sum zero is for both to be zero:
\[
\lambda_1 = 0, \quad \lambda_2 = 0.
\]
Hence all eigenvalues of $H$ vanish, so $H$ is the zero matrix:
\[
u_{xx}(x_0,y_0) = u_{yy}(x_0,y_0) = u_{xy}(x_0,y_0) = 0.
\]

One can now argue as follows. The vanishing of the Hessian at $(x_0,y_0)$ implies that, to second order, $u$ is completely flat near this point. More systematically, one can consider the maximum principle: standard proofs show that if a harmonic function attains an interior maximum, then it must be constant in the connected component of $\Omega$ containing that point. In an elementary setting, one may argue indirectly: if $u$ were not constant, then near $(x_0,y_0)$ we would be able to find points where $u$ is strictly less than its maximum value, contradicting a refined version of the local maximum characterization that uses Taylor’s theorem together with the vanishing of all first and second derivatives at $(x_0,y_0)$.

Therefore, under the given hypotheses, $u$ must be constant in $\Omega$. This is a special case of the \emph{strong maximum principle} for elliptic equations, here illustrated for the simplest elliptic operator, the Laplacian.

An important consequence is the uniqueness of solutions to the Dirichlet problem for Laplace’s equation: if two harmonic functions agree on the boundary $\partial\Omega$, then their difference is harmonic and attains both its maximum and minimum (namely zero) on the boundary, so by the above reasoning the difference must vanish throughout $\Omega$.

\medskip

\noindent\textbf{(d) Comparison with $u_{xx} - u_{yy} = 0$.}
Consider the equation
\[
u_{xx} - u_{yy} = 0.
\]
In the general form $A\,u_{xx} + 2B\,u_{xy} + C\,u_{yy} = 0$ we now have
\[
A = 1,\quad B = 0,\quad C = -1.
\]
The discriminant is
\[
B^2 - AC = 0^2 - (1)(-1) = 1 > 0.
\]
Hence this equation is of \emph{hyperbolic} type. The associated quadratic form is
\[
Q(\xi,\eta) = \xi^2 - \eta^2,
\]
which is indefinite: it takes both positive and negative values, and its level sets are hyperbolas, not circles. This contrasts sharply with the positive definite $\xi^2 + \eta^2$ associated with Laplace’s equation.

This algebraic difference reflects a qualitative difference in the behavior of solutions. Hyperbolic equations such as $u_{xx} - u_{yy} = 0$ (closely related to the one-dimensional wave equation) are naturally posed as \emph{initial value problems}, where one prescribes data along a noncharacteristic curve and the solution exhibits propagation along characteristic lines. Elliptic equations such as Laplace’s equation are naturally posed as \emph{boundary value problems} on bounded domains, where interior values are determined in a smooth and stable way by the boundary data, and maximum principles apply. Thus the classification via the discriminant $B^2 - AC$ and the associated quadratic form is not merely algebraic; it encodes the essential geometric and analytic behavior of solutions.

\medskip

In summary, this example shows how Laplace’s equation fits squarely into the elliptic class by way of the negative discriminant and positive definite quadratic form, how its rotational invariance aligns with the geometry of that form, and how ellipticity underpins fundamental qualitative properties such as the maximum principle and uniqueness for boundary value problems.
\end{solution}

% ===== Example 3: The Heat Equation and Parabolic Type (inquiry-based) =====
\begin{problem}[The Heat Equation and Parabolic Type]
The one-dimensional heat equation
\[
u_t = k\,u_{xx}, \qquad k>0,
\]
models the diffusion of temperature along a thin, insulated rod. Physically, temperature gradients drive heat flow, and the effect of time evolution is to smooth out these gradients. Mathematically, this equation is second order only in the spatial variable, yet it is still classified as a \emph{parabolic} equation. In this problem you will see how to fit the heat equation into the standard second-order framework and how its parabolic type reflects its smoothing, ``gradient-flow'' behavior.

We regard $u$ as a function of two independent variables, $x$ (space) and $t$ (time).

\smallskip

(a) The standard general form of a linear second-order PDE in two variables $x$ and $t$ is
\[
a(x,t)\,u_{xx} \;+\; 2b(x,t)\,u_{xt} \;+\; c(x,t)\,u_{tt} \;+\; \text{(lower-order terms)} \;=\; f(x,t).
\]
Rewrite the heat equation $u_t = k u_{xx}$ in this form by moving all terms to one side. Identify the coefficients $a$, $b$, and $c$ of $u_{xx}$, $u_{xt}$, and $u_{tt}$ respectively.

% Hint: Aim to write something of the form $a\,u_{xx} + 2b\,u_{xt} + c\,u_{tt} + d\,u_x + e\,u_t + f\,u = 0$.

\smallskip

(b) For a second-order PDE in two variables in the form
\[
a u_{xx} + 2 b u_{xt} + c u_{tt} + \dots = 0,
\]
the \emph{type} (elliptic, hyperbolic, parabolic) is determined by the discriminant $D = b^2 - ac$ of the quadratic form in the highest derivatives.

\begin{enumerate}
\item[(i)] State the classification in terms of $D$: what conditions on $D$ correspond to elliptic, hyperbolic, and parabolic equations?
\item[(ii)] Using your coefficients $a$, $b$, and $c$ from part (a), compute $D$ for the heat equation and classify it.
\end{enumerate}

\smallskip

(c) Another way to understand the type is to look at the $2\times 2$ matrix of coefficients of the second derivatives,
\[
A(x,t) \;=\;
\begin{pmatrix}
a(x,t) & b(x,t)\\[4pt]
b(x,t) & c(x,t)
\end{pmatrix}.
\]
For the heat equation, write down the constant matrix $A$ and find its eigenvalues.

\begin{enumerate}
\item[(i)] How many eigenvalues are positive, negative, and zero?
\item[(ii)] Explain how this eigenvalue pattern is consistent with calling the heat equation \emph{parabolic}.
\end{enumerate}

% Hint: For a $2\times 2$ real symmetric matrix, the signs of the eigenvalues can be deduced from the trace and determinant.

\smallskip

(d) To connect the parabolic classification with the \emph{smoothing} or \emph{gradient-flow} behavior, consider the heat equation on a finite rod $0 < x < L$ with homogeneous Dirichlet boundary conditions
\[
u(0,t) = 0, \qquad u(L,t)=0.
\]
Define the (squared) $L^2$-norm of $u$ at time $t$ by
\[
E(t) \;=\; \frac{1}{2} \int_0^L u(x,t)^2\,dx.
\]

\begin{enumerate}
\item[(i)] Differentiate $E(t)$ with respect to $t$ and use the heat equation to express $E'(t)$ in terms of $u$ and its spatial derivatives.
\item[(ii)] Use integration by parts and the boundary conditions to show that
\[
E'(t) \;=\; -k \int_0^L u_x(x,t)^2\,dx \;\le 0.
\]
\item[(iii)] Interpret this identity: what does it say about how the ``energy'' $E(t)$ changes in time, and why is this suggestive of a gradient-flow type evolution?
\end{enumerate}

% Hint: For (i), start from $E'(t) = \int_0^L u\,u_t\,dx$ and substitute $u_t = k u_{xx}$. For (ii), integrate $u\,u_{xx}$ by parts in $x$.

\smallskip

(e) {\bf Extensions and comparisons.}
\begin{enumerate}
\item[(i)] Consider now the one-dimensional \emph{wave equation}
\[
u_{tt} = c^2 u_{xx}.
\]
Rewrite it in the standard second-order form with independent variables $x$ and $t$, identify $a$, $b$, and $c$, and compute the discriminant $D = b^2 - ac$. How is the type different from that of the heat equation, and how does this contrast in type reflect the different physical behavior of waves versus diffusion?

% Hint: For the wave equation the highest-order derivatives are $u_{tt}$ and $u_{xx}$; both are second order.

\item[(ii)] Suppose we add a first-order advection term and consider the \emph{convection--diffusion} equation
\[
u_t + v\,u_x = k u_{xx}, \qquad v \in \mathbb{R}.
\]
What are the coefficients $a$, $b$, and $c$ of the second derivatives now, and what is the discriminant $D$? What does this tell you about the type of the convection--diffusion equation, and what does it suggest about the robustness of the classification with respect to lower-order terms?
\end{enumerate}

\end{problem}

% ===== Example 3: The Heat Equation and Parabolic Type (full solution) =====
\begin{problem}[The Heat Equation and Parabolic Type]
Consider the one-dimensional heat equation
\[
u_t = k\,u_{xx}, \qquad k>0,
\]
for $u(x,t)$ with independent variables $x$ (space) and $t$ (time).

\begin{enumerate}
\item[(a)] Rewrite this equation in the standard second-order form
\[
a\,u_{xx} + 2b\,u_{xt} + c\,u_{tt} + \text{(lower-order terms)} = 0,
\]
and identify $a$, $b$, and $c$.

\item[(b)] Recall that for such an equation the discriminant $D = b^2 - ac$ determines the type: elliptic if $D<0$, hyperbolic if $D>0$, and parabolic if $D=0$ (with $a$ and $c$ not both zero). Compute $D$ for the heat equation and classify it.

\item[(c)] Form the $2\times 2$ matrix of second-order coefficients
\[
A = \begin{pmatrix} a & b \\ b & c \end{pmatrix},
\]
compute its eigenvalues for the heat equation, and relate their signs to the parabolic classification.

\item[(d)] On the finite interval $0<x<L$ with homogeneous Dirichlet boundary conditions $u(0,t)=u(L,t)=0$, define
\[
E(t) = \frac{1}{2}\int_0^L u(x,t)^2\,dx.
\]
Show that
\[
E'(t) = -k \int_0^L u_x(x,t)^2\,dx \le 0,
\]
and briefly explain how this identity reflects the diffusive, smoothing, or ``gradient-flow'' character associated with parabolic equations.

\item[(e)] For comparison, classify the wave equation $u_{tt} = c^2 u_{xx}$ by the same method and state how its type and energy behavior differ from those of the heat equation.
\end{enumerate}
\end{problem}

\begin{solution}
We treat $u$ as a function of two independent variables, $x$ and $t$. The classification of a linear second-order PDE in two variables is based on the quadratic form appearing in its highest-order derivatives.

\medskip

\noindent{\bf (a) Standard second-order form.}
We start from the heat equation
\[
u_t = k\,u_{xx}.
\]
To fit this into the general pattern
\[
a\,u_{xx} + 2b\,u_{xt} + c\,u_{tt} + \text{(lower-order terms)} = 0,
\]
we move all terms to one side:
\[
k\,u_{xx} - u_t = 0.
\]
There are no $u_{xt}$ or $u_{tt}$ terms, so we read off
\[
a = k, \qquad b = 0, \qquad c = 0.
\]
The term $-u_t$ is first order, so it is part of the lower-order terms and does not affect the classification.

\medskip

\noindent{\bf (b) Discriminant and type.}
For a second-order PDE in two variables with highest-order part
\[
a u_{xx} + 2 b u_{xt} + c u_{tt},
\]
the discriminant is defined as
\[
D = b^2 - a c.
\]
The standard (two-dimensional) classification is:
\begin{itemize}
\item elliptic if $D < 0$,
\item hyperbolic if $D > 0$,
\item parabolic if $D = 0$ and $(a,c)\ne (0,0)$.
\end{itemize}

For the heat equation we found $a = k > 0$, $b = 0$, and $c = 0$. Hence
\[
D = b^2 - ac = 0^2 - k\cdot 0 = 0.
\]
Since $a\neq 0$ and $c=0$, the equation is of \emph{parabolic} type.

Thus, even though the equation is only first order in time, when one regards $t$ as a second independent variable alongside $x$, its principal (second-order) part has discriminant zero, the hallmark of parabolic equations.

\medskip

\noindent{\bf (c) Matrix of second-order coefficients and eigenvalues.}
The highest-order coefficients can be assembled into the symmetric matrix
\[
A = \begin{pmatrix}
a & b\\
b & c
\end{pmatrix}.
\]
For the heat equation this becomes
\[
A = \begin{pmatrix}
k & 0\\
0 & 0
\end{pmatrix}.
\]
The eigenvalues of this diagonal matrix are simply the diagonal entries:
\[
\lambda_1 = k > 0, \qquad \lambda_2 = 0.
\]

Thus the principal symbol has one positive eigenvalue and one zero eigenvalue. In the purely elliptic case (for example, Laplace's equation $u_{xx} + u_{yy}=0$), all eigenvalues are of the same sign and nonzero; in the hyperbolic case (for example, the wave equation $u_{tt} - c^2 u_{xx}=0$), the principal matrix has eigenvalues of opposite signs. The presence of a zero eigenvalue but no negative eigenvalues is consistent with parabolic type: there is diffusion in the spatial direction (positive eigenvalue) but no second-order dynamics in time (zero eigenvalue). This degeneracy (one zero eigenvalue) is exactly what the discriminant $D=0$ detects.

\medskip

\noindent{\bf (d) Energy decay and gradient-flow behavior.}
Now consider the heat equation on $0<x<L$ with homogeneous Dirichlet boundary conditions
\[
u(0,t) = 0, \qquad u(L,t) = 0.
\]
Define the energy (really the squared $L^2$-norm) by
\[
E(t) = \frac{1}{2}\int_0^L u(x,t)^2\,dx.
\]
We compute $E'(t)$ and use the PDE to see how $E$ changes in time.

Differentiating under the integral sign yields
\[
E'(t) = \frac{1}{2}\int_0^L 2u(x,t)\,u_t(x,t)\,dx = \int_0^L u\,u_t\,dx.
\]
Substituting the heat equation $u_t = k\,u_{xx}$ gives
\[
E'(t) = k \int_0^L u\,u_{xx}\,dx.
\]

We now integrate by parts in $x$. Using the standard formula
\[
\int_0^L u\,u_{xx}\,dx = \bigl[u\,u_x\bigr]_{0}^{L} - \int_0^L u_x^2\,dx,
\]
we obtain
\[
E'(t) = k \bigl[u\,u_x\bigr]_{0}^{L} - k \int_0^L u_x^2\,dx.
\]
The boundary conditions $u(0,t) = u(L,t) = 0$ imply that the boundary term vanishes:
\[
\bigl[u\,u_x\bigr]_{0}^{L} = u(L,t)u_x(L,t) - u(0,t)u_x(0,t) = 0,
\]
so we are left with
\[
E'(t) = -k \int_0^L u_x(x,t)^2\,dx.
\]
Since $k>0$ and $u_x^2 \ge 0$, it follows that
\[
E'(t) \le 0.
\]

This calculation has two important interpretations:

\begin{itemize}
\item The quantity $E(t)$ is nonincreasing in time. The solution cannot gain ``energy'' in the $L^2$ sense; instead, the size of $u$ tends to decrease.

\item The rate of decay is proportional to the integral of $u_x^2$, which measures the size of spatial gradients. Large gradients correspond to a faster decrease of $E(t)$. In this sense, the heat flow is trying to reduce the size of its gradient, smoothing out variations.
\end{itemize}

This behavior is characteristic of a \emph{gradient flow} in an infinite-dimensional space: one can think of the heat equation as a steepest descent flow for an energy functional (for example, related to the Dirichlet energy $\int |u_x|^2$). The parabolic classification captures precisely this smoothing, dissipative character.

\medskip

\noindent{\bf (e) Comparison with the wave equation.}
Consider now the one-dimensional wave equation
\[
u_{tt} = c^2 u_{xx}
\]
for some $c>0$. Moving all terms to one side gives
\[
u_{tt} - c^2 u_{xx} = 0,
\]
which we rewrite in the standard form
\[
a\,u_{xx} + 2b\,u_{xt} + c\,u_{tt} = 0.
\]
Here there is no mixed derivative, so $b=0$. The coefficient of $u_{xx}$ is $-c^2$, and the coefficient of $u_{tt}$ is $1$. Thus
\[
a = -c^2, \qquad b = 0, \qquad c = 1.
\]
The discriminant is
\[
D = b^2 - a c = 0^2 - (-c^2)\cdot 1 = c^2 > 0.
\]
Hence the wave equation is \emph{hyperbolic}. The associated coefficient matrix is
\[
A = \begin{pmatrix}
-\,c^2 & 0\\[4pt]
0 & 1
\end{pmatrix},
\]
which has one positive and one negative eigenvalue. This sign pattern (indefinite quadratic form) contrasts with the heat equation’s one-positive, one-zero pattern.

On a finite interval with suitable boundary conditions, the wave equation has an associated conserved energy, involving both $u_t^2$ and $u_x^2$, which remains constant in time rather than decaying. This conservation of energy reflects the oscillatory, propagation-dominated nature of hyperbolic equations, in sharp contrast with the dissipative, smoothing behavior of the parabolic heat equation.

\medskip

In summary, the heat equation $u_t = k u_{xx}$, when written in standard second-order form in $(x,t)$, has discriminant $D = 0$ and a principal coefficient matrix with one positive and one zero eigenvalue. This places it in the parabolic class and aligns with its physical and analytical behavior: solutions tend to smooth out and lose energy over time, akin to an infinite-dimensional gradient flow. This example illustrates how the abstract classification of linear second-order PDEs by their principal part encodes essential qualitative features of their solutions.
\end{solution}

% ===== Example 4: The Wave Equation and Hyperbolic Type (inquiry-based) =====
\begin{problem}[The Wave Equation and Hyperbolic Type]
The one-dimensional wave equation
\[
u_{tt} = c^{2} u_{xx}, \qquad c>0,
\]
models, for instance, small vertical vibrations of a taut string and sound waves in a thin tube. In contrast to the heat equation, which smooths out disturbances, wave propagation transports oscillations along straight rays with a finite speed $c$. In this problem we use the general classification scheme for linear second-order partial differential equations to understand what it means, mathematically, for the wave equation to be of \emph{hyperbolic} type, and how this is reflected in its characteristic curves.

Recall that a linear second-order PDE in two variables $x$ and $y$ is often written in the form
\[
A(x,y)\,u_{xx} + 2 B(x,y)\,u_{xy} + C(x,y)\,u_{yy} + \text{(lower order terms)} = 0,
\]
and is called \emph{hyperbolic} at a point if $B^2 - AC > 0$ there.

\smallskip

(a) Rewrite the wave equation $u_{tt} = c^2 u_{xx}$ in the standard form
\[
A(x,t)\,u_{xx} + 2 B(x,t)\,u_{xt} + C(x,t)\,u_{tt} + \text{(lower order terms)} = 0,
\]
using $(x,t)$ as the independent variables. Identify explicitly the coefficients $A(x,t)$, $B(x,t)$, and $C(x,t)$.

\smallskip

(b) Compute the discriminant $B^2 - AC$ for the wave equation, and use it to classify the equation as elliptic, parabolic, or hyperbolic. State clearly which sign of the discriminant corresponds to each type.

\smallskip

(c) For a general equation
\[
A u_{xx} + 2 B u_{xt} + C u_{tt} + \cdots = 0,
\]
the characteristic curves in the $(x,t)$-plane are defined (locally) by solving the ordinary differential equation
\[
A \,(dt)^2 - 2B \,dx\,dt + C \,(dx)^2 = 0,
\]
or, equivalently,
\[
A \left(\frac{dt}{dx}\right)^2 - 2B \left(\frac{dt}{dx}\right) + C = 0.
\]
Write down this quadratic equation for $\dfrac{dt}{dx}$ specifically for the wave equation, and solve for $\dfrac{dt}{dx}$. What are the two families of characteristic curves in the $(x,t)$-plane?

\emph{Hint:} In part (c), remember that for the wave equation you found $A$, $B$, and $C$ in part (a). You should obtain two constant slopes, corresponding to straight lines.

\smallskip

(d) A standard way to simplify a hyperbolic equation is to introduce new variables $\xi$ and $\eta$ that are constant along the characteristic curves. For the wave equation, consider the change of variables
\[
\xi = x - c t, \qquad \eta = x + c t.
\]
(i) Verify that $\xi$ and $\eta$ are indeed constant along the characteristic lines you found in part (c). That is, check that along one family of characteristic curves $\xi$ is constant and along the other family $\eta$ is constant.

(ii) Using the chain rule, express $u_{x}$ and $u_{t}$ in terms of $u_{\xi}$ and $u_{\eta}$, and then compute $u_{xx}$ and $u_{tt}$ in terms of $u_{\xi\xi}$, $u_{\xi\eta}$, and $u_{\eta\eta}$. Substitute these into the wave equation to show that, in the $(\xi,\eta)$ variables, the equation takes the simpler \emph{canonical form}
\[
u_{\xi\eta} = 0.
\]

\emph{Hint:} First write $u(x,t) = U(\xi,\eta)$, where $U$ is $u$ expressed in the new variables. Then use
\[
u_x = U_{\xi}\,\xi_x + U_{\eta}\,\eta_x, \qquad
u_t = U_{\xi}\,\xi_t + U_{\eta}\,\eta_t,
\]
and differentiate once more to find $u_{xx}$ and $u_{tt}$. Be systematic and keep track of coefficients of $U_{\xi\xi}$, $U_{\xi\eta}$, and $U_{\eta\eta}$.

\smallskip

(e) \textbf{What if / extensions.}
\begin{enumerate}
\item[(i)] Consider instead the diffusion equation $u_t = k u_{xx}$, with $k>0$. How could you rewrite this in the form
\[
A u_{xx} + 2 B u_{xt} + C u_{tt} + \cdots = 0
\]
by moving all terms to one side? What are $A$, $B$, and $C$ in this case, and what is the discriminant $B^2 - AC$? How is this type (elliptic, parabolic, or hyperbolic) different from the wave equation?

\item[(ii)] Suppose we change the sign in the wave equation and consider
\[
u_{tt} + c^2 u_{xx} = 0.
\]
Classify this equation by finding $A$, $B$, $C$, and $B^2 - AC$. Is it hyperbolic, elliptic, or parabolic? Briefly discuss how you expect the qualitative behavior of solutions to differ from the original wave equation.
\end{enumerate}

\end{problem}

% ===== Example 4: The Wave Equation and Hyperbolic Type (full solution) =====
\begin{problem}[The Wave Equation and Hyperbolic Type]
Consider the one-dimensional wave equation
\[
u_{tt} = c^{2} u_{xx}, \qquad c>0,
\]
with independent variables $(x,t)$.

\begin{enumerate}
\item[(a)] Rewrite this equation in the general second-order form
\[
A(x,t)\,u_{xx} + 2 B(x,t)\,u_{xt} + C(x,t)\,u_{tt} + \text{(lower order terms)} = 0,
\]
identify $A$, $B$, and $C$, and compute the discriminant $B^2 - AC$. Classify the equation as elliptic, parabolic, or hyperbolic.

\item[(b)] Using the general characteristic equation
\[
A \left(\frac{dt}{dx}\right)^2 - 2B \left(\frac{dt}{dx}\right) + C = 0,
\]
find the characteristic curves of the wave equation in the $(x,t)$-plane.

\item[(c)] Introduce the characteristic coordinates
\[
\xi = x - c t, \qquad \eta = x + c t,
\]
and use the chain rule to transform the wave equation into canonical form in the $(\xi,\eta)$ variables. Show that the equation becomes
\[
u_{\xi\eta} = 0.
\]
Briefly explain how this illustrates the hyperbolic nature of the wave equation.
\end{enumerate}
\end{problem}

\begin{solution}
We regard the wave equation as a second-order partial differential equation in the independent variables $x$ (space) and $t$ (time).

\medskip

\textbf{(a) Classification via the discriminant.}
We start from
\[
u_{tt} = c^{2} u_{xx}.
\]
To match the standard classification form
\[
A(x,t)\,u_{xx} + 2 B(x,t)\,u_{xt} + C(x,t)\,u_{tt} + \text{(lower order terms)} = 0,
\]
we move all terms to one side:
\[
u_{tt} - c^{2} u_{xx} = 0.
\]
Comparing with the standard form, we read off
\[
A(x,t) = -c^{2}, \qquad B(x,t) = 0, \qquad C(x,t) = 1.
\]
These coefficients are constant and independent of $(x,t)$.

The discriminant is
\[
B^{2} - A C = 0^{2} - (-c^{2})(1) = c^{2} > 0.
\]
By the usual convention,
\[
\begin{cases}
B^{2} - A C > 0 &\Rightarrow \text{hyperbolic},\\[2pt]
B^{2} - A C = 0 &\Rightarrow \text{parabolic},\\[2pt]
B^{2} - A C < 0 &\Rightarrow \text{elliptic}.
\end{cases}
\]
Since $c^{2} > 0$, the one-dimensional wave equation is of \emph{hyperbolic} type everywhere in the $(x,t)$-plane. This algebraic characterization is the starting point for understanding its propagation of waves along characteristic curves.

\medskip

\textbf{(b) Characteristic curves.}
For a second-order equation
\[
A u_{xx} + 2B u_{xt} + C u_{tt} + \cdots = 0,
\]
the characteristic curves in the $(x,t)$-plane are determined (locally) by the quadratic equation
\[
A \left(\frac{dt}{dx}\right)^2 - 2B \left(\frac{dt}{dx}\right) + C = 0.
\]
For the wave equation, we substitute $A = -c^{2}$, $B = 0$, and $C=1$ to obtain
\[
(-c^{2}) \left(\frac{dt}{dx}\right)^2 + 1 = 0,
\]
or, equivalently,
\[
\left(\frac{dt}{dx}\right)^2 = \frac{1}{c^{2}}.
\]
Taking square roots yields two constant slopes:
\[
\frac{dt}{dx} = \frac{1}{c} \quad\text{or}\quad \frac{dt}{dx} = -\frac{1}{c}.
\]
Integrating, we find the families of characteristic curves:
\[
t = \frac{1}{c} x + \text{constant}
\quad\Longleftrightarrow\quad
x - c t = \text{constant},
\]
and
\[
t = -\frac{1}{c} x + \text{constant}
\quad\Longleftrightarrow\quad
x + c t = \text{constant}.
\]
Thus, there are two distinct families of straight lines in the $(x,t)$-plane along which information propagates. These lines have slopes $\pm 1/c$ and represent signals moving to the right and to the left with speed $c$. The fact that the characteristic equation has two distinct real roots (and hence two distinct real characteristic directions) is another hallmark of hyperbolic type.

\medskip

\textbf{(c) Transformation to canonical form.}
A central idea in the classification of second-order PDEs is that elliptic, parabolic, and hyperbolic equations can often be simplified, via an appropriate change of variables, to a canonical form that reveals their essential behavior. For hyperbolic equations in two variables, the canonical second-order term is typically a mixed derivative of the form $u_{\xi\eta}$.

We introduce new independent variables $\xi$ and $\eta$ by
\[
\xi = x - c t, \qquad \eta = x + c t.
\]
From part (b), we recognize that $\xi$ is constant along the characteristic lines $x - ct = \text{constant}$ (right-moving waves), and $\eta$ is constant along the lines $x + ct = \text{constant}$ (left-moving waves). Thus these are \emph{characteristic coordinates}.

Let us write $u(x,t)$ as $U(\xi,\eta)$, where
\[
U(\xi,\eta) = u\bigl(x(\xi,\eta), t(\xi,\eta)\bigr).
\]
We compute the necessary derivatives using the chain rule. First, we compute the partial derivatives of $\xi$ and $\eta$:
\[
\xi_x = 1, \quad \xi_t = -c, \qquad
\eta_x = 1, \quad \eta_t = c.
\]
Then
\[
u_x = U_{\xi}\,\xi_x + U_{\eta}\,\eta_x
= U_{\xi} + U_{\eta},
\]
and
\[
u_t = U_{\xi}\,\xi_t + U_{\eta}\,\eta_t
= -c U_{\xi} + c U_{\eta}
= c(-U_{\xi} + U_{\eta}).
\]

We now differentiate once more with respect to $x$ and $t$. For $u_{xx}$ we have
\[
u_{xx} = \frac{\partial}{\partial x} (u_x)
= \frac{\partial}{\partial x}(U_{\xi} + U_{\eta}).
\]
Using the chain rule again,
\[
\frac{\partial}{\partial x}
= \xi_x \frac{\partial}{\partial \xi} + \eta_x \frac{\partial}{\partial \eta}
= \frac{\partial}{\partial \xi} + \frac{\partial}{\partial \eta},
\]
so
\[
u_{xx}
= (U_{\xi\xi} + U_{\xi\eta}) + (U_{\eta\xi} + U_{\eta\eta})
= U_{\xi\xi} + 2 U_{\xi\eta} + U_{\eta\eta},
\]
since $U_{\xi\eta} = U_{\eta\xi}$ by equality of mixed partial derivatives.

Next we find $u_{tt}$:
\[
u_{tt} = \frac{\partial}{\partial t} (u_t)
= \frac{\partial}{\partial t}\bigl(c(-U_{\xi} + U_{\eta})\bigr)
= c\bigl(-U_{\xi t} + U_{\eta t}\bigr).
\]
The $t$-derivative operator is
\[
\frac{\partial}{\partial t}
= \xi_t \frac{\partial}{\partial \xi} + \eta_t \frac{\partial}{\partial \eta}
= -c \frac{\partial}{\partial \xi} + c \frac{\partial}{\partial \eta}.
\]
Therefore,
\[
U_{\xi t}
= \left(-c \frac{\partial}{\partial \xi} + c \frac{\partial}{\partial \eta}\right)U_{\xi}
= -c U_{\xi\xi} + c U_{\xi\eta},
\]
and
\[
U_{\eta t}
= \left(-c \frac{\partial}{\partial \xi} + c \frac{\partial}{\partial \eta}\right)U_{\eta}
= -c U_{\eta\xi} + c U_{\eta\eta}
= -c U_{\xi\eta} + c U_{\eta\eta}.
\]
Thus
\[
u_{tt}
= c\bigl(-U_{\xi t} + U_{\eta t}\bigr)
= c\Bigl(-(-c U_{\xi\xi} + c U_{\xi\eta}) + (-c U_{\xi\eta} + c U_{\eta\eta})\Bigr).
\]
Simplifying inside the parentheses,
\[
-u_{\xi t} + u_{\eta t}
= c U_{\xi\xi} - c U_{\xi\eta} - c U_{\xi\eta} + c U_{\eta\eta}
= c U_{\xi\xi} - 2c U_{\xi\eta} + c U_{\eta\eta}.
\]
Thus
\[
u_{tt}
= c\bigl(c U_{\xi\xi} - 2c U_{\xi\eta} + c U_{\eta\eta}\bigr)
= c^{2}\bigl(U_{\xi\xi} - 2 U_{\xi\eta} + U_{\eta\eta}\bigr).
\]

We now substitute $u_{xx}$ and $u_{tt}$ into the wave equation $u_{tt} = c^{2} u_{xx}$:
\[
c^{2}\bigl(U_{\xi\xi} - 2 U_{\xi\eta} + U_{\eta\eta}\bigr)
= c^{2}\bigl(U_{\xi\xi} + 2 U_{\xi\eta} + U_{\eta\eta}\bigr).
\]
We may divide both sides by $c^{2} > 0$ and subtract the left-hand side from the right-hand side:
\[
0
= \bigl(U_{\xi\xi} + 2 U_{\xi\eta} + U_{\eta\eta}\bigr)
- \bigl(U_{\xi\xi} - 2 U_{\xi\eta} + U_{\eta\eta}\bigr)
= 4 U_{\xi\eta}.
\]
Therefore $U_{\xi\eta} = 0$, which we can write, reverting to the notation $u$ for the dependent variable in $(\xi,\eta)$ coordinates, as
\[
u_{\xi\eta} = 0.
\]
This is precisely the canonical form for a hyperbolic equation in two variables: the second-order part consists of a single mixed derivative.

From this canonical form, we can immediately integrate:
\[
u_{\xi\eta} = 0
\quad\Longrightarrow\quad
u(\xi,\eta) = F(\xi) + G(\eta),
\]
for arbitrary functions $F$ and $G$. Translating back to the original variables,
\[
u(x,t) = F(x - c t) + G(x + c t),
\]
which is the well-known d'Alembert solution. This exhibits the two families of traveling waves moving along the characteristic lines $x - c t = \text{constant}$ and $x + c t = \text{constant}$.

\medskip

\textbf{Connection with the classification theory.}
This example illustrates the main ideas of the classification of linear second-order PDEs in two variables:

\begin{itemize}
\item The sign of the discriminant $B^{2} - A C$ distinguishes elliptic, parabolic, and hyperbolic equations. For the wave equation, $B^{2} - A C > 0$, so it is hyperbolic.

\item For hyperbolic equations, the characteristic equation has two distinct real roots, leading to two families of real characteristic curves. Solutions typically propagate along these curves with finite speed.

\item A suitable change of variables, chosen so that the new coordinates are constant along characteristics, transforms the equation into a canonical form involving a mixed derivative $u_{\xi\eta}$. For the wave equation, this leads directly to the decomposition into right-moving and left-moving waves.
\end{itemize}

Thus, both the algebraic discriminant and the geometric picture of characteristics consistently reveal the hyperbolic nature of the one-dimensional wave equation and its role in modeling wave propagation.
\end{solution}

% ===== Example 5: Change of Variables and Canonical Forms (inquiry-based) =====
\begin{problem}[Change of Variables and Canonical Forms]
In this problem we explore how a linear change of variables can simplify a second-order partial differential equation with a mixed derivative term. The guiding idea is that the second-order (or \emph{principal}) part of such a PDE behaves like a quadratic form, just as in the study of conic sections. By choosing coordinates aligned with the eigenvectors of this quadratic form, we can eliminate the mixed derivative and obtain a simpler \emph{canonical form}. This makes it much easier to recognize whether the equation is elliptic, hyperbolic, or parabolic, and to compare it with standard model equations.

Consider the linear second-order PDE
\[
u_{xx} + 4u_{xy} + u_{yy} \;=\; 0,
\]
where $u = u(x,y)$ is an unknown function of two variables.

\smallskip

(a) Recall that the general second-order part of a PDE in two variables can be written as
\[
A u_{xx} + 2B u_{xy} + C u_{yy}.
\]
For the given PDE, identify the coefficients $A$, $B$, and $C$. Compute the discriminant
\[
D = B^2 - AC
\]
and use it to classify the PDE as elliptic, hyperbolic, or parabolic. Explain your reasoning in a sentence or two.

\medskip

(b) Associate to the second-order part of the PDE the symmetric matrix
\[
Q = \begin{pmatrix} A & B \\[4pt] B & C \end{pmatrix}.
\]
For our equation, write down this matrix explicitly. Then recall (or verify) that the quadratic form
\[
A x^2 + 2B x y + C y^2
\]
can be written compactly as $\begin{pmatrix} x & y \end{pmatrix} Q \begin{pmatrix} x \\ y \end{pmatrix}$. 

How is the classification of the PDE related to the signs of the eigenvalues of $Q$? How is this analogous to the classification of conic sections given by a quadratic form in $x$ and $y$?

\medskip

(c) Compute the eigenvalues and eigenvectors of the matrix
\[
Q = \begin{pmatrix} 1 & 2 \\[4pt] 2 & 1 \end{pmatrix}.
\]
Normalize your eigenvectors to obtain an orthonormal basis of $\mathbb{R}^2$.

Hint: First find the characteristic polynomial and its roots. Notice that the vectors $(1,1)$ and $(1,-1)$ are natural candidates to test as eigenvectors.

\medskip

(d) Let $(\xi,\eta)$ be the new coordinates aligned with the orthonormal eigenbasis you found in part (c). Concretely, define
\[
\begin{pmatrix} \xi \\[4pt] \eta \end{pmatrix}
= P^{T} \begin{pmatrix} x \\[4pt] y \end{pmatrix},
\]
where $P$ is the $2\times 2$ orthogonal matrix whose columns are the normalized eigenvectors of $Q$. 

(i) Write down the change of variables $(x,y) \mapsto (\xi,\eta)$ explicitly.

(ii) Using the viewpoint that the principal part of the PDE transforms like the quadratic form $Q$, argue that, in the new variables $(\xi,\eta)$, the PDE has no mixed derivative term. In other words, its principal part is of the form
\[
\lambda_1\, u_{\xi\xi} + \lambda_2\, u_{\eta\eta},
\]
where $\lambda_1$ and $\lambda_2$ are the eigenvalues of $Q$ that you computed in part (c).

(iii) Conclude that, in $(\xi,\eta)$–coordinates, the PDE takes the canonical form
\[
3\,u_{\xi\xi} - u_{\eta\eta} = 0.
\]
Explain briefly why this canonical form makes it obvious that the equation is hyperbolic.

Hint: You do not need to compute $u_{xx}$, $u_{xy}$, and $u_{yy}$ explicitly in terms of $u_{\xi\xi}$, $u_{\xi\eta}$, and $u_{\eta\eta}$. Instead, use the fact that $P$ diagonalizes $Q$ by $P^{T} Q P = \operatorname{diag}(\lambda_1,\lambda_2)$.

\medskip

(e) Extensions and “what if” questions.

(i) Consider the PDE
\[
u_{xx} + 2u_{xy} + u_{yy} = 0.
\]
Repeat the classification step (part (a)) for this equation. What is the discriminant $D$ and what is the type of the PDE? Without full details, what canonical form do you expect after a suitable linear change of variables?

(ii) Suppose now that the matrix $Q$ associated to the principal part of a PDE has one positive and one zero eigenvalue. What type of PDE would you expect (elliptic, hyperbolic, or parabolic)? What sort of canonical form do you anticipate in suitable coordinates?

Hint: Connect your answers to the sign patterns of eigenvalues for quadratic forms and to the model equations
\[
u_{xx} + u_{yy} = 0 \quad (\text{elliptic}), \qquad
u_{xx} - u_{yy} = 0 \quad (\text{hyperbolic}), \qquad
u_{xx} = 0 \quad (\text{parabolic}).
\]
\end{problem}

% ===== Example 5: Change of Variables and Canonical Forms (full solution) =====
\begin{problem}[Change of Variables and Canonical Forms]
Consider the linear second-order PDE
\[
u_{xx} + 4u_{xy} + u_{yy} = 0,
\]
where $u = u(x,y)$.

\begin{enumerate}
\item Write the principal part in the standard form $A u_{xx} + 2B u_{xy} + C u_{yy}$, identify $A,B,C$, and classify the PDE using the discriminant $D = B^2 - AC$.
\item Form the symmetric matrix
\[
Q = \begin{pmatrix} A & B \\ B & C \end{pmatrix},
\]
compute its eigenvalues and an orthonormal eigenbasis.
\item Using a linear change of variables $(x,y) \mapsto (\xi,\eta)$ corresponding to this orthonormal eigenbasis, transform the PDE into a canonical form without mixed derivatives. Write the resulting PDE explicitly in the new variables and state its type.
\end{enumerate}
\end{problem}

\begin{solution}
We are asked to classify a second-order PDE with a mixed derivative term and then to find a linear change of variables that removes this mixed term. The key idea is to interpret the second-order part of the PDE as a quadratic form determined by a symmetric matrix. Diagonalizing this matrix by an orthogonal change of variables leads to a canonical form.

\medskip

\noindent\textbf{(1) Coefficients and classification.}
The principal (second-order) part of the given PDE is
\[
u_{xx} + 4u_{xy} + u_{yy}.
\]
We match this with the general expression
\[
A u_{xx} + 2B u_{xy} + C u_{yy}.
\]
Comparing coefficients, we obtain
\[
A = 1, \qquad 2B = 4 \;\Rightarrow\; B = 2, \qquad C = 1.
\]
The discriminant is
\[
D = B^2 - AC = 2^2 - (1)(1) = 4 - 1 = 3 > 0.
\]
Since $D>0$, the PDE is \emph{hyperbolic} at every point (for this constant-coefficient equation, the classification is global). This is completely analogous to the classification of conic sections: $D>0$ corresponds to a hyperbola in the quadratic-form setting.

\medskip

\noindent\textbf{(2) Matrix form and eigenvalues.}
We now form the symmetric matrix $Q$ associated with the principal part:
\[
Q = \begin{pmatrix} A & B \\[4pt] B & C \end{pmatrix}
= \begin{pmatrix} 1 & 2 \\[4pt] 2 & 1 \end{pmatrix}.
\]
This matrix plays the same role as the matrix of a quadratic form
\[
A x^2 + 2B x y + C y^2
= \begin{pmatrix} x & y \end{pmatrix}
\begin{pmatrix} 1 & 2 \\[2pt] 2 & 1 \end{pmatrix}
\begin{pmatrix} x \\[2pt] y \end{pmatrix}.
\]
The classification in terms of $D$ can also be seen from the eigenvalues of $Q$: a hyperbolic equation corresponds to one positive and one negative eigenvalue; an elliptic equation corresponds to two eigenvalues of the same sign; and a parabolic equation corresponds to one nonzero eigenvalue and one zero eigenvalue.

We now compute the eigenvalues of $Q$. The characteristic polynomial is
\[
\det(Q - \lambda I)
= \det\begin{pmatrix} 1 - \lambda & 2 \\ 2 & 1 - \lambda \end{pmatrix}
= (1 - \lambda)^2 - 4.
\]
Thus
\[
(1 - \lambda)^2 - 4 = 0
\quad\Longrightarrow\quad
(1 - \lambda) = \pm 2,
\]
so the eigenvalues are
\[
\lambda_1 = 1 + 2 = 3, \qquad
\lambda_2 = 1 - 2 = -1.
\]
As expected for a hyperbolic equation, one eigenvalue is positive and the other is negative.

Next, we find corresponding eigenvectors.

For $\lambda_1 = 3$, we solve $(Q - 3I)v = 0$:
\[
\begin{pmatrix} 1 - 3 & 2 \\ 2 & 1 - 3 \end{pmatrix}
= \begin{pmatrix} -2 & 2 \\ 2 & -2 \end{pmatrix}.
\]
The equation $-2 v_1 + 2 v_2 = 0$ implies $v_1 = v_2$. A convenient eigenvector is
\[
v^{(1)} = \begin{pmatrix} 1 \\ 1 \end{pmatrix}.
\]
For $\lambda_2 = -1$, we solve $(Q + I)v = 0$:
\[
\begin{pmatrix} 1 + 1 & 2 \\ 2 & 1 + 1 \end{pmatrix}
= \begin{pmatrix} 2 & 2 \\ 2 & 2 \end{pmatrix}.
\]
The equation $2 v_1 + 2 v_2 = 0$ implies $v_1 = -v_2$. A convenient eigenvector is
\[
v^{(2)} = \begin{pmatrix} 1 \\ -1 \end{pmatrix}.
\]

We now normalize these vectors to obtain an orthonormal basis of $\mathbb{R}^2$. Each of $v^{(1)}$ and $v^{(2)}$ has length $\sqrt{1^2+1^2} = \sqrt{2}$, so we take
\[
e_1 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ 1 \end{pmatrix},
\qquad
e_2 = \frac{1}{\sqrt{2}}\begin{pmatrix} 1 \\ -1 \end{pmatrix}.
\]
The vectors $e_1$ and $e_2$ form an orthonormal eigenbasis, with
\[
Q e_1 = 3 e_1, \qquad Q e_2 = -1 \cdot e_2.
\]

Let $P$ be the orthogonal matrix whose columns are $e_1$ and $e_2$:
\[
P = \begin{pmatrix} \dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} \\[6pt]
                    \dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}} \end{pmatrix}.
\]
Then
\[
P^T Q P = 
\begin{pmatrix} 3 & 0 \\ 0 & -1 \end{pmatrix}.
\]

\medskip

\noindent\textbf{(3) Linear change of variables and canonical form.}
The crucial idea is that the principal part of the PDE transforms under a linear change of variables in exactly the same way as the associated quadratic form. If we introduce new variables $(\xi,\eta)$ that are aligned with the orthonormal eigenbasis, then the mixed term disappears and the matrix $Q$ becomes diagonal.

Define new coordinates $(\xi,\eta)$ by
\[
\begin{pmatrix} \xi \\[4pt] \eta \end{pmatrix}
= P^{T} \begin{pmatrix} x \\[4pt] y \end{pmatrix}
= \begin{pmatrix}
\dfrac{1}{\sqrt{2}} & \dfrac{1}{\sqrt{2}} \\[6pt]
\dfrac{1}{\sqrt{2}} & -\dfrac{1}{\sqrt{2}}
\end{pmatrix}
\begin{pmatrix} x \\ y \end{pmatrix}.
\]
Thus the change of variables is
\[
\xi = \frac{x + y}{\sqrt{2}},
\qquad
\eta = \frac{x - y}{\sqrt{2}}.
\]
Geometrically, this is a rotation of the coordinate axes by $45^\circ$, followed by a uniform scaling that preserves orthonormality.

Conversely, we can solve for $x$ and $y$ in terms of $\xi$ and $\eta$:
\[
x = \frac{\xi + \eta}{\sqrt{2}},
\qquad
y = \frac{\xi - \eta}{\sqrt{2}}.
\]

Under this linear change of variables, the matrix $Q$ transforms to $P^{T} Q P = \operatorname{diag}(3,-1)$. On the level of the PDE, this means that the principal part
\[
u_{xx} + 4u_{xy} + u_{yy}
\]
in $(x,y)$–coordinates becomes
\[
3\,u_{\xi\xi} - u_{\eta\eta}
\]
in $(\xi,\eta)$–coordinates, with no mixed derivative term $u_{\xi\eta}$.

Therefore, in the new variables $(\xi,\eta)$, the PDE takes the canonical form
\[
3\,u_{\xi\xi} - u_{\eta\eta} = 0.
\]
We could, if desired, divide by $3$ to get
\[
u_{\xi\xi} - \frac{1}{3} u_{\eta\eta} = 0,
\]
but the important feature is that there are no mixed derivatives and the coefficients of $u_{\xi\xi}$ and $u_{\eta\eta}$ have opposite signs. This immediately reveals that the equation is hyperbolic, since the principal part is of the form
\[
(\text{positive}) \cdot u_{\xi\xi} + (\text{negative}) \cdot u_{\eta\eta}.
\]

\medskip

\noindent\textbf{Conceptual summary.}
This example illustrates a central theme in the classification of linear second-order PDEs: the type of the equation (elliptic, hyperbolic, parabolic) is governed by the quadratic form associated with its principal part. The symmetric coefficient matrix $Q$ encodes this quadratic form. Its discriminant and eigenvalues determine the type, just as in the classification of conic sections. By diagonalizing $Q$ via an orthogonal change of variables, we eliminate mixed derivatives and obtain a canonical form (here, $3 u_{\xi\xi} - u_{\eta\eta} = 0$). In these canonical coordinates, the structure and behavior of the PDE become much more transparent.
\end{solution}

% ===== Example 6: A Mixed-Type Equation: Tricomi’s Equation (inquiry-based) =====
\begin{problem}[A Mixed-Type Equation: Tricomi’s Equation]
In models of transonic fluid flow, the same flow may be subsonic in some regions and supersonic in others. Mathematically, this leads to partial differential equations that behave like elliptic equations in one part of the domain and like hyperbolic equations in another part. A model example is \emph{Tricomi’s equation}
\[
y\,u_{xx} + u_{yy} = 0,
\]
whose coefficient \(y\) changes sign across the line \(y=0\). In this problem you will see how the usual classification of second-order linear PDEs applies pointwise, and how a single equation can be elliptic, hyperbolic, and parabolic in different regions.

We recall that a general second-order linear PDE in two variables can be written in the form
\[
A(x,y)\,u_{xx} + 2 B(x,y)\,u_{xy} + C(x,y)\,u_{yy} + \text{(lower-order terms)} = 0.
\]

(a) For Tricomi’s equation
\[
y\,u_{xx} + u_{yy} = 0,
\]
rewrite it in the standard form above, and identify the coefficient functions \(A(x,y)\), \(B(x,y)\), and \(C(x,y)\). Then compute the discriminant
\[
D(x,y) = B^2 - A C.
\]
Based on the sign of \(D\), classify the equation as elliptic, parabolic, or hyperbolic at points with \(y>0\), with \(y<0\), and on the line \(y=0\). Sketch the regions of different type in the \((x,y)\)-plane.

\textit{Hint:} Only the second-order part matters for the classification, and here there is no mixed derivative term \(u_{xy}\).

(b) In the hyperbolic region, we can look for characteristic curves. The characteristic curves for a second-order PDE
\[
A u_{xx} + 2 B u_{xy} + C u_{yy} + \dots = 0
\]
are defined (away from points where all \(A,B,C\) vanish) by the ordinary differential equation
\[
A(x,y)\,(dy)^2 - 2 B(x,y)\,dx\,dy + C(x,y)\,(dx)^2 = 0.
\]
Restrict attention to the hyperbolic region of Tricomi’s equation. Write down the corresponding characteristic equation in differential form, and then express it as an equation for the slope \(\dfrac{dy}{dx}\).

\textit{Hint:} For Tricomi’s equation, \(B=0\), so the characteristic equation simplifies considerably.

(c) Still in the hyperbolic region, solve the ordinary differential equation you found in part (b) to obtain an explicit family of characteristic curves. Try to write your answer in the form
\[
x \pm \frac{2}{3}(-y)^{3/2} = \text{constant},
\]
or some equivalent implicit description. Sketch a few of these curves in the half-plane \(y<0\), and indicate how they meet the line \(y=0\).

\textit{Hint:} Once you have an equation for \(\dfrac{dy}{dx}\), it may be simpler to invert it and solve for \(\dfrac{dx}{dy}\). You will encounter an integral of a power of \(-y\).

(d) Now step back and interpret your findings.

\quad (i) Use your computation of the discriminant \(D\) to explain why there are two distinct real characteristic directions when \(y<0\), but no real characteristics when \(y>0\).

\quad (ii) Imagine a bounded domain whose lower boundary lies along two characteristic curves in the hyperbolic region \(y<0\) and whose upper boundary lies in the elliptic region \(y>0\), intersecting the line \(y=0\). Based on the usual theory for elliptic and hyperbolic equations, what kind of data (boundary values vs.\ initial values along curves) would you expect to prescribe on each part of the boundary in order to have a well-posed problem?

\textit{Hint:} Compare with the Laplace equation for elliptic behavior, and with the wave equation for hyperbolic behavior.

(e) Explore some variations.

\quad (i) Consider the modified equation
\[
y\,u_{xx} + u_{yy} + u = 0.
\]
Does the additional zeroth-order term \(u\) change the classification into elliptic, hyperbolic, or parabolic regions? Explain your reasoning carefully.

\quad (ii) Consider the more general family of equations
\[
a(y)\,u_{xx} + u_{yy} = 0,
\]
where \(a(y)\) is a continuous function of \(y\) only. In terms of the sign of \(a(y)\), describe for which values of \(y\) the equation is elliptic, for which it is hyperbolic, and where it is parabolic. Under what condition on \(a(y)\) (as a function of \(y\)) does this family give another example of a mixed-type equation?

\textit{Hint:} Go back to the discriminant \(D=B^2-AC\) and think about what happens when \(a(y)\) changes sign. 
\end{problem}

% ===== Example 6: A Mixed-Type Equation: Tricomi’s Equation (full solution) =====
\begin{problem}[A Mixed-Type Equation: Tricomi’s Equation]
Consider the second-order partial differential equation
\[
y\,u_{xx} + u_{yy} = 0, \qquad (x,y)\in\mathbb{R}^2.
\]
(a) Write this equation in the standard form
\[
A(x,y)\,u_{xx} + 2 B(x,y)\,u_{xy} + C(x,y)\,u_{yy} + \dots = 0,
\]
identify \(A,B,C\), and compute the discriminant \(D=B^2-AC\). Classify the equation as elliptic, hyperbolic, or parabolic in the regions \(y>0\), \(y<0\), and on the line \(y=0\).

(b) In the hyperbolic region, find the real characteristic curves by solving the characteristic equation
\[
A(dy)^2 - 2 B\,dx\,dy + C(dx)^2 = 0.
\]
Give an implicit formula for the characteristics and sketch their qualitative shape in the half-plane \(y<0\).

(c) Using the same characteristic equation, explain why there are no real characteristic curves in the region \(y>0\).

(d) Finally, consider the more general family
\[
a(y)\,u_{xx} + u_{yy} = 0,
\]
with continuous \(a(y)\). Express the discriminant in terms of \(a(y)\), describe the type (elliptic, hyperbolic, parabolic) as a function of \(y\), and state a condition on \(a\) under which this PDE is of mixed type.
\end{problem}

\begin{solution}
We first recall the general framework for classifying second-order linear PDEs in two variables. A PDE of the form
\[
A(x,y)\,u_{xx} + 2 B(x,y)\,u_{xy} + C(x,y)\,u_{yy} + \text{(lower-order terms)} = 0
\]
is called elliptic, parabolic, or hyperbolic at a point \((x_0,y_0)\) according to the sign of the discriminant
\[
D(x_0,y_0) = B(x_0,y_0)^2 - A(x_0,y_0)\,C(x_0,y_0).
\]
Specifically, the equation is elliptic if \(D<0\), hyperbolic if \(D>0\), and parabolic (degenerate) if \(D=0\). This criterion is directly analogous to the classification of quadratic forms and conic sections.

\medskip

\textbf{(a) Coefficients and classification.}

For Tricomi’s equation,
\[
y\,u_{xx} + u_{yy} = 0,
\]
we match this with the standard form
\[
A\,u_{xx} + 2 B\,u_{xy} + C\,u_{yy} = 0.
\]
There is no mixed derivative term \(u_{xy}\), so
\[
A(x,y) = y,\qquad B(x,y) = 0,\qquad C(x,y) = 1.
\]
The discriminant is therefore
\[
D(x,y) = B^2 - A C = 0^2 - y\cdot 1 = -y.
\]

We now classify by the sign of \(D\):

- If \(y>0\), then \(D=-y<0\), so the equation is \emph{elliptic} at every point with \(y>0\).
- If \(y<0\), then \(D=-y>0\), so the equation is \emph{hyperbolic} at every point with \(y<0\).
- If \(y=0\), then \(D=0\), so along the entire line \(y=0\) the equation is \emph{parabolic} (degenerate).

Thus the same equation is elliptic in the upper half-plane, hyperbolic in the lower half-plane, and parabolic on the horizontal line that separates them. This is a prototypical example of a \emph{mixed-type} equation.

\medskip

\textbf{(b) Characteristic curves in the hyperbolic region \(y<0\).}

In the region where the equation is hyperbolic, it admits real characteristic curves. For a second-order PDE
\[
A u_{xx} + 2B u_{xy} + C u_{yy} + \dots = 0,
\]
the characteristic curves in the \((x,y)\)-plane satisfy the quadratic differential equation
\[
A(x,y)\,(dy)^2 - 2 B(x,y)\,dx\,dy + C(x,y)\,(dx)^2 = 0.
\]

In our case, \(A = y\), \(B = 0\), \(C = 1\), so the characteristic equation becomes
\[
y\,(dy)^2 + (dx)^2 = 0.
\]
We would like to rewrite this as an ordinary differential equation for \(y\) as a function of \(x\). Dividing by \((dx)^2\) (on nonvertical portions of the curves) gives
\[
y\left(\frac{dy}{dx}\right)^2 + 1 = 0.
\]
Equivalently,
\[
\left(\frac{dy}{dx}\right)^2 = -\frac{1}{y}.
\]

This equation has real solutions only when \(-1/y \ge 0\), that is, only when \(y<0\). This agrees with the classification: real characteristics exist only in the hyperbolic region.

In the region \(y<0\), we can therefore write
\[
\frac{dy}{dx} = \pm \frac{1}{\sqrt{-y}}.
\]
It is slightly more convenient to invert this relation and solve for \(\dfrac{dx}{dy}\):
\[
\frac{dx}{dy} = \pm \sqrt{-y}.
\]

We now integrate with respect to \(y\). Define \(s = -y\), so that \(s>0\) when \(y<0\) and \(ds = -dy\). Then
\[
\frac{dx}{dy} = \pm \sqrt{-y}
\quad\Longrightarrow\quad
\frac{dx}{dy} = \pm \sqrt{s},\quad dy = -ds.
\]
Thus
\[
dx = \pm \sqrt{s}\,dy = \pm \sqrt{s}\,(-ds)
= \mp \sqrt{s}\,ds.
\]
Integrating,
\[
x = \mp \int \sqrt{s}\,ds + \text{constant}
= \mp \frac{2}{3} s^{3/2} + \text{constant}.
\]
Recalling that \(s=-y\), we obtain
\[
x = \mp \frac{2}{3}(-y)^{3/2} + C,
\]
or, equivalently,
\[
x \pm \frac{2}{3}(-y)^{3/2} = C.
\]

Thus there are two families of characteristic curves in the hyperbolic region \(y<0\), given implicitly by
\[
x + \frac{2}{3}(-y)^{3/2} = \text{constant}
\quad\text{and}\quad
x - \frac{2}{3}(-y)^{3/2} = \text{constant}.
\]

For a qualitative sketch in the half-plane \(y<0\), note that for each fixed constant \(C\),
\[
x = C \mp \frac{2}{3}(-y)^{3/2}.
\]
As \(y \to 0^-\), we have \((-y)^{3/2} \to 0\), so the curves approach the line \(y=0\) with horizontal tangent. As \(y\to -\infty\), the term \(( -y)^{3/2}\) grows, so the curves move off to the left or right with increasing steepness. The two families intersect transversely, as is typical for a hyperbolic equation with two distinct real characteristic directions.

\medskip

\textbf{(c) Absence of real characteristics in the elliptic region \(y>0\).}

We now use the same characteristic equation to explain why there are no real characteristics for \(y>0\). Recall that the characteristic equation reduced to
\[
\left(\frac{dy}{dx}\right)^2 = -\frac{1}{y}.
\]
If \(y>0\), then the right-hand side \(-1/y\) is negative, so the equation demands that \(\left(\dfrac{dy}{dx}\right)^2\) be negative, which is impossible for real-valued functions. Thus, for \(y>0\), there are no real solutions for \(\dfrac{dy}{dx}\), and therefore no real characteristic curves in the plane.

This is exactly what one expects for an elliptic equation: elliptic equations, such as the Laplace equation, do not possess real characteristic curves, and information propagates in a more global, “smeared” way rather than along distinguished curves.

On the line \(y=0\) itself, the coefficient \(A=y\) vanishes and the characteristic equation degenerates (it becomes \((dx)^2=0\)), reflecting the parabolic nature of the equation there.

\medskip

\textbf{(d) The general family \(a(y)\,u_{xx} + u_{yy} = 0\).}

Finally, consider the more general equation
\[
a(y)\,u_{xx} + u_{yy} = 0,
\]
with \(a(y)\) a continuous function of \(y\) only. Again we write it in the standard form:
\[
A(x,y) = a(y),\qquad B(x,y) = 0,\qquad C(x,y) = 1.
\]
The discriminant is then
\[
D(x,y) = B^2 - A C = -a(y).
\]

The type of the equation at a point \((x,y)\) depends only on the sign of \(a(y)\):

- If \(a(y) > 0\), then \(D = -a(y) < 0\): the equation is elliptic at all points with that value of \(y\).
- If \(a(y) < 0\), then \(D = -a(y) > 0\): the equation is hyperbolic at all points with that value of \(y\).
- If \(a(y) = 0\), then \(D = 0\): the equation is parabolic (degenerate) along the horizontal line corresponding to that \(y\).

Thus the equation is of \emph{mixed type} precisely when the coefficient \(a(y)\) changes sign as \(y\) varies, that is, when there exist values \(y_1\) and \(y_2\) with \(a(y_1) > 0\) and \(a(y_2) < 0\). By continuity, there must then be some \(y_0\) with \(a(y_0) = 0\); the horizontal line \(y=y_0\) is the type-change curve separating an elliptic region from a hyperbolic region.

\medskip

\textbf{Conceptual remarks.}

This example illustrates several central ideas of the classification of linear second-order PDEs:

1. \emph{Pointwise classification.} The type of a PDE is determined \emph{pointwise} by the coefficients \(A,B,C\). It is possible, as in Tricomi’s equation, for a single PDE to change type across a curve in the domain.

2. \emph{Discriminant and characteristics.} The sign of the discriminant \(D = B^2 - AC\) governs both the classification and the nature of characteristic curves: when \(D>0\) there are two distinct real characteristic directions (hyperbolic case), when \(D<0\) there are no real characteristic directions (elliptic case), and when \(D=0\) there is a single repeated direction (parabolic case).

3. \emph{Mixed-type modeling.} In applications such as transonic flow, the change from subsonic (elliptic-like) to supersonic (hyperbolic-like) regimes is modeled by coefficients that change sign. Tricomi’s equation and its generalizations \(a(y)u_{xx} + u_{yy} = 0\) provide canonical mathematical models for this mixed behavior, and they highlight the subtlety of formulating well-posed problems when an equation changes type within the domain.
\end{solution}

\section{Elliptic PDEs: Method of Green Function}
% --- Narrative plan (auto-generated) ---
% This section develops the method of Green functions as a systematic way to solve boundary value problems for elliptic partial differential equations, with a particular focus on the Poisson and Laplace equations. A Green function encodes how the domain and its boundary respond to a point source, allowing us to build solutions for arbitrary forcing terms by superposition. In this way, what seems like a complicated differential equation turns into an integral representation that separates geometry, boundary conditions, and data.
%
% Green functions are central in applied mathematics because they appear whenever steady states or equilibrium configurations are governed by linear elliptic operators: electrostatic potentials in conductors, steady heat distributions, incompressible fluid flows, and deflection of elastic membranes all admit Green function formulations. The method connects naturally to several other topics: it is closely related to fundamental solutions and convolution from ODEs and PDEs, to eigenfunction expansions and Fourier series on bounded domains, and to complex analysis through harmonic functions and the Poisson kernel. Understanding Green functions thus builds a bridge between geometric intuition about domains, analytic tools from functional analysis and spectral theory, and concrete computational techniques used throughout applied mathematics.

% ===== Example 1: Green Function for the One-Dimensional Poisson Equation on an Interval (inquiry-based) =====
\begin{problem}[Green Function for the One-Dimensional Poisson Equation on an Interval]
Consider a thin, homogeneous rod of length $L$, occupying the interval $(0,L)$ on the $x$-axis. Let $u(x)$ denote the steady-state temperature along the rod. Suppose the ends of the rod are held at zero temperature, and there is a distributed heat source of intensity $f(x)$ along the rod. In steady state, $u$ satisfies a one-dimensional Poisson equation with homogeneous Dirichlet boundary conditions. Our goal in this problem is to construct the corresponding Green function and to use it to represent $u$ as an explicit integral involving $f$.

We study the boundary value problem
\[
- u''(x) \;=\; f(x), \qquad 0 < x < L, \qquad
u(0) = 0, \quad u(L) = 0.
\]

\smallskip

(a) First recall the associated homogeneous equation
\[
- u''(x) = 0, \qquad 0 < x < L.
\]
Solve this ordinary differential equation and write its general solution. Then, impose the boundary conditions $u(0)=u(L)=0$. What does this tell you about the homogeneous solution?

\smallskip

(b) To incorporate the forcing term $f(x)$, we introduce the Green function $G(x,\xi)$ for the differential operator $L = -\dfrac{d^{2}}{dx^{2}}$ on $(0,L)$ with homogeneous Dirichlet boundary conditions. For a fixed point $\xi \in (0,L)$, $G(\cdot,\xi)$ is defined as the solution of
\[
- \frac{d^{2}}{dx^{2}} G(x,\xi) = \delta(x-\xi), \qquad 0 < x < L,
\]
with
\[
G(0,\xi) = 0, \qquad G(L,\xi) = 0,
\]
where $\delta(x-\xi)$ is the Dirac delta concentrated at $\xi$.

\begin{enumerate}
\item[(i)] For $x \neq \xi$, what equation does $G(x,\xi)$ satisfy? Write down the general form of $G(x,\xi)$ separately on the intervals $(0,\xi)$ and $(\xi,L)$, introducing appropriate constants.
\item[(ii)] Use the boundary conditions at $x=0$ and $x=L$ to simplify these general forms as much as possible.
\end{enumerate}

% Hint: Away from $x = \xi$, the right-hand side is zero, so you are solving the homogeneous equation again. Because the operator is second order, $G(\cdot,\xi)$ will be linear in $x$ on each side of $\xi$.

\smallskip

(c) Next, we must determine how $G(x,\xi)$ behaves at the point $x = \xi$.

\begin{enumerate}
\item[(i)] Argue that $G(x,\xi)$ should be continuous at $x = \xi$, that is,
\[
\lim_{x \to \xi^-} G(x,\xi) = \lim_{x \to \xi^+} G(x,\xi).
\]
What condition does this give on the two linear pieces you found in part (b)?
\item[(ii)] To determine the jump in the derivative of $G(x,\xi)$ at $x=\xi$, integrate the defining equation
\[
- G''(x,\xi) = \delta(x-\xi)
\]
from $x = \xi - \varepsilon$ to $x = \xi + \varepsilon$, for a small $\varepsilon > 0$, and then let $\varepsilon \to 0$. Show that
\[
- G_x(\xi^+,\xi) + G_x(\xi^-,\xi) = 1.
\]
Explain briefly how the property of the Dirac delta function is used here.
\end{enumerate}

% Hint: Use the Fundamental Theorem of Calculus on the left-hand side, and the defining property $\int_{\xi - \varepsilon}^{\xi + \varepsilon} \delta(x-\xi)\,dx = 1$ on the right-hand side.

\smallskip

(d) Use the conditions from parts (b) and (c) to solve for all constants and obtain an explicit formula for the Green function $G(x,\xi)$, valid for $0 < x, \xi < L$. Write your answer in a piecewise form depending on whether $x \le \xi$ or $x \ge \xi$.

Then, using this Green function, show that the solution $u$ of the boundary value problem
\[
- u''(x) = f(x), \quad 0 < x < L, \qquad u(0) = u(L) = 0,
\]
can be written as
\[
u(x) = \int_0^L G(x,\xi)\, f(\xi)\, d\xi.
\]

% Hint: Differentiate under the integral sign (formally) to apply the operator $- \frac{d^{2}}{dx^{2}}$ to $u(x)$, and use the defining equation for $G(x,\xi)$.

\smallskip

(e) Finally, explore some variations and extensions.

\begin{enumerate}
\item[(i)] Check whether your Green function is symmetric: does $G(x,\xi) = G(\xi,x)$ hold for all $x,\xi \in (0,L)$? If so, verify this from your explicit formula. Why might such a symmetry be expected from the operator $L = -\dfrac{d^2}{dx^2}$ with homogeneous Dirichlet boundary conditions?
\item[(ii)] Suppose instead that the rod occupies the interval $(a,b)$, with $u(a)=u(b)=0$. Without redoing all of the calculations in detail, sketch how the formulas for the Green function would have to be modified. What are the analogues of the factors $x$, $L-x$, $\xi$, and $L-\xi$ in this more general case?
\end{enumerate}

% Hint: You can think of shifting and rescaling the interval $(a,b)$ to $(0,L)$, or you can directly repeat the piecewise linear construction with $0$ replaced by $a$ and $L$ replaced by $b$.
\end{problem}

% ===== Example 1: Green Function for the One-Dimensional Poisson Equation on an Interval (full solution) =====
\begin{problem}[Green Function for the One-Dimensional Poisson Equation on an Interval]
Consider the boundary value problem
\[
- u''(x) = f(x), \qquad 0 < x < L, \qquad u(0) = 0, \quad u(L) = 0.
\]
\begin{enumerate}
\item[(a)] For the operator $L u = -u''$ with these boundary conditions, construct the Green function $G(x,\xi)$ defined by
\[
- \frac{d^{2}}{dx^{2}} G(x,\xi) = \delta(x-\xi), \qquad 0 < x < L, \qquad G(0,\xi)=G(L,\xi)=0,
\]
for each fixed $\xi \in (0,L)$.
\item[(b)] Derive an explicit formula for $G(x,\xi)$, written piecewise depending on whether $x \le \xi$ or $x \ge \xi$, and show that $G(x,\xi) = G(\xi,x)$.
\item[(c)] Show that the unique solution of the boundary value problem can be written in the Green function form
\[
u(x) = \int_0^L G(x,\xi)\, f(\xi)\, d\xi.
\]
\end{enumerate}
\end{problem}

\begin{solution}
We study the one-dimensional Poisson equation with homogeneous Dirichlet boundary conditions on the finite interval $(0,L)$. The central idea of the Green function method is to construct the response of the system to a point source and then build the response to a general forcing term by superposition.

\medskip

\textbf{Step 1: Homogeneous problem and motivation.}
The associated homogeneous equation is
\[
- u''(x) = 0, \qquad 0 < x < L.
\]
Integrating twice, we obtain the general solution
\[
u(x) = Ax + B,
\]
for constants $A$ and $B$. The boundary conditions $u(0) = 0$ and $u(L) = 0$ give
\[
u(0) = B = 0, \qquad u(L) = AL + B = AL = 0,
\]
so $A = 0$. Thus $u \equiv 0$ is the unique solution of the homogeneous problem with these boundary conditions; there is no nontrivial solution that satisfies $u(0) = u(L) = 0$. This tells us that if we can find any particular solution for a given $f$, then it is automatically the unique solution.

The Green function will give us such particular solutions for arbitrary right-hand sides $f$.

\medskip

\textbf{Step 2: Definition and piecewise structure of the Green function.}
We now define the Green function $G(x,\xi)$ for the operator $L = -\dfrac{d^2}{dx^2}$ with homogeneous Dirichlet boundary conditions. For each fixed $\xi \in (0,L)$, the function $x \mapsto G(x,\xi)$ is defined by
\begin{equation}\label{eq:Green-def}
- G_{xx}(x,\xi) = \delta(x-\xi), \qquad 0 < x < L,
\end{equation}
with boundary conditions
\begin{equation}\label{eq:Green-BC}
G(0,\xi) = 0, \qquad G(L,\xi) = 0.
\end{equation}
Here $\delta(x-\xi)$ is the Dirac delta, representing a unit point source at $x=\xi$.

For $x \neq \xi$ the right-hand side of \eqref{eq:Green-def} vanishes, so $G$ satisfies the homogeneous equation
\[
- G_{xx}(x,\xi) = 0 \quad \Longleftrightarrow \quad G_{xx}(x,\xi) = 0,
\]
on each of the subintervals $(0,\xi)$ and $(\xi,L)$. Therefore $G(\cdot,\xi)$ is linear in $x$ on each side of $\xi$. We write
\begin{align*}
G(x,\xi) &= a_1 x + b_1, \qquad 0 < x < \xi, \\
G(x,\xi) &= a_2 x + b_2, \qquad \xi < x < L,
\end{align*}
for some constants $a_1, b_1, a_2, b_2$ that may depend on $\xi$.

The boundary conditions \eqref{eq:Green-BC} give two relations. At $x=0$,
\[
G(0,\xi) = a_1 \cdot 0 + b_1 = b_1 = 0,
\]
so $b_1 = 0$. At $x=L$,
\[
G(L,\xi) = a_2 L + b_2 = 0,
\]
so $b_2 = -a_2 L$. Thus we can rewrite
\[
G(x,\xi) =
\begin{cases}
a_1 x, & 0 < x < \xi,\\[4pt]
a_2 x - a_2 L, & \xi < x < L.
\end{cases}
\]

\medskip

\textbf{Step 3: Conditions at the point $x = \xi$.}
To determine $a_1$ and $a_2$, we use the behavior of $G$ at the source point $x = \xi$.

First, we require continuity of $G$ at $x = \xi$. Physically, this corresponds to the temperature (or potential) not having an infinite jump at a point source; the singularity appears in the second derivative instead. Mathematically, continuity is appropriate because the equation involves $G''$ but not lower derivatives with distributions other than the delta. Thus we impose
\[
\lim_{x \to \xi^-} G(x,\xi) = \lim_{x \to \xi^+} G(x,\xi).
\]
Using our expressions,
\[
G(\xi^-,\xi) = a_1 \xi, \qquad G(\xi^+,\xi) = a_2 \xi - a_2 L.
\]
Continuity gives
\begin{equation}\label{eq:continuity}
a_1 \xi = a_2 \xi - a_2 L = a_2(\xi - L).
\end{equation}

Second, we determine the jump in the derivative $G_x$ at $x = \xi$ by integrating the defining equation \eqref{eq:Green-def} across a small interval around $\xi$. Let $\varepsilon > 0$ be small, and integrate from $\xi - \varepsilon$ to $\xi + \varepsilon$:
\[
\int_{\xi - \varepsilon}^{\xi + \varepsilon} \bigl(-G_{xx}(x,\xi)\bigr)\, dx
=
\int_{\xi - \varepsilon}^{\xi + \varepsilon} \delta(x-\xi)\, dx.
\]
By the Fundamental Theorem of Calculus, the left-hand side is
\[
- G_x(\xi+\varepsilon,\xi) + G_x(\xi-\varepsilon,\xi).
\]
By the defining property of the Dirac delta, the right-hand side equals $1$ for every $\varepsilon > 0$ small enough that the interval contains $\xi$, so we obtain
\[
- G_x(\xi+\varepsilon,\xi) + G_x(\xi-\varepsilon,\xi) = 1.
\]
Letting $\varepsilon \to 0$, we conclude
\begin{equation}\label{eq:jump}
- G_x(\xi^+,\xi) + G_x(\xi^-,\xi) = 1.
\end{equation}

Now we compute $G_x$ from our piecewise linear forms. On $(0,\xi)$,
\[
G_x(x,\xi) = a_1,
\]
so $G_x(\xi^-,\xi) = a_1$. On $(\xi,L)$,
\[
G_x(x,\xi) = a_2,
\]
so $G_x(\xi^+,\xi) = a_2$. Substituting into \eqref{eq:jump} gives
\begin{equation}\label{eq:jump-a}
- a_2 + a_1 = 1, \quad \text{that is,} \quad a_1 - a_2 = 1.
\end{equation}

\medskip

\textbf{Step 4: Solving for the constants and explicit Green function.}
We now solve the system of two equations \eqref{eq:continuity} and \eqref{eq:jump-a} for $a_1$ and $a_2$.

From \eqref{eq:jump-a}, we have
\[
a_1 = 1 + a_2.
\]
Substituting this into \eqref{eq:continuity} yields
\[
(1 + a_2)\,\xi = a_2 (\xi - L).
\]
Expanding and rearranging,
\[
\xi + a_2 \xi = a_2 \xi - a_2 L.
\]
Cancel $a_2 \xi$ from both sides to obtain
\[
\xi = - a_2 L,
\]
so
\[
a_2 = - \frac{\xi}{L}.
\]
Then
\[
a_1 = 1 + a_2 = 1 - \frac{\xi}{L} = \frac{L - \xi}{L}.
\]

Substituting back, we obtain the explicit Green function
\[
G(x,\xi) =
\begin{cases}
\dfrac{L - \xi}{L}\, x, & 0 \le x \le \xi,\\[6pt]
-\dfrac{\xi}{L}\, x + \xi, & \xi \le x \le L.
\end{cases}
\]
It is common to rewrite the second branch in a more symmetric form. Noting that
\[
-\dfrac{\xi}{L}\, x + \xi = \xi\!\left(1 - \dfrac{x}{L}\right) = \dfrac{\xi(L-x)}{L},
\]
we can present the Green function as
\begin{equation}\label{eq:Green-final}
G(x,\xi) =
\begin{cases}
\dfrac{(L - \xi)\, x}{L}, & 0 \le x \le \xi,\\[6pt]
\dfrac{\xi\, (L - x)}{L}, & \xi \le x \le L.
\end{cases}
\end{equation}

This function is continuous on $[0,L]$, vanishes at $x = 0$ and $x = L$, is piecewise linear in $x$ with a corner at $x = \xi$, and its second derivative in the distributional sense is $- \delta(x-\xi)$, as required.

\medskip

\textbf{Step 5: Symmetry of the Green function.}
We next check that $G(x,\xi) = G(\xi,x)$ for all $x,\xi \in (0,L)$. From the explicit expression \eqref{eq:Green-final}, observe that
\[
G(x,\xi) =
\begin{cases}
\dfrac{(L - \xi)\, x}{L}, & x \le \xi,\\[6pt]
\dfrac{\xi\, (L - x)}{L}, & x \ge \xi.
\end{cases}
\]
Now interchange the roles of $x$ and $\xi$ to write $G(\xi,x)$:
\[
G(\xi,x) =
\begin{cases}
\dfrac{(L - x)\, \xi}{L}, & \xi \le x,\\[6pt]
\dfrac{x\, (L - \xi)}{L}, & \xi \ge x.
\end{cases}
\]
Consider the two cases.

\emph{Case 1: $x \le \xi$.} Then, by the definition of $G(x,\xi)$,
\[
G(x,\xi) = \frac{(L - \xi)\, x}{L}.
\]
On the other hand, since $\xi \ge x$, the second branch in the expression for $G(\xi,x)$ applies:
\[
G(\xi,x) = \frac{x\, (L - \xi)}{L}.
\]
These expressions coincide, so $G(x,\xi) = G(\xi,x)$ when $x \le \xi$.

\emph{Case 2: $x \ge \xi$.} Then
\[
G(x,\xi) = \frac{\xi\, (L - x)}{L},
\]
while now $\xi \le x$, so the first branch in $G(\xi,x)$ applies:
\[
G(\xi,x) = \frac{(L - x)\, \xi}{L}.
\]
Again these are identical, so $G(x,\xi) = G(\xi,x)$ for $x \ge \xi$ as well. Thus the Green function is symmetric:
\[
G(x,\xi) = G(\xi,x).
\]
This symmetry is typical for self-adjoint operators like $L = -\dfrac{d^{2}}{dx^{2}}$ with homogeneous boundary conditions.

\medskip

\textbf{Step 6: Representation formula for the solution.}
We now prove that for any given forcing term $f$ (say, sufficiently nice, for instance continuous), the unique solution of
\[
- u''(x) = f(x), \qquad 0 < x < L, \qquad u(0) = u(L) = 0,
\]
can be written as
\begin{equation}\label{eq:representation}
u(x) = \int_0^L G(x,\xi)\, f(\xi)\, d\xi.
\end{equation}

First, we check that $u$ given by \eqref{eq:representation} satisfies the differential equation. Formally differentiating under the integral sign twice with respect to $x$, we have
\[
u''(x) = \int_0^L \frac{\partial^2}{\partial x^2} G(x,\xi)\, f(\xi)\, d\xi.
\]
Multiplying by $-1$ gives
\[
- u''(x) = \int_0^L \Bigl(-\frac{\partial^2}{\partial x^2} G(x,\xi)\Bigr)\, f(\xi)\, d\xi.
\]
By the defining property of $G$, we have
\[
- \frac{\partial^2}{\partial x^2} G(x,\xi) = \delta(x-\xi),
\]
in the sense of distributions, so
\[
- u''(x) = \int_0^L \delta(x-\xi)\, f(\xi)\, d\xi.
\]
Using the sifting property of the Dirac delta,
\[
\int_0^L \delta(x-\xi)\, f(\xi)\, d\xi = f(x),
\]
for $x \in (0,L)$. Thus $u$ defined by \eqref{eq:representation} satisfies
\[
- u''(x) = f(x), \qquad 0 < x < L.
\]

Next, we verify the boundary conditions. At $x=0$,
\[
u(0) = \int_0^L G(0,\xi)\, f(\xi)\, d\xi = \int_0^L 0 \cdot f(\xi)\, d\xi = 0,
\]
because $G(0,\xi) = 0$ for all $\xi$. Similarly, at $x = L$,
\[
u(L) = \int_0^L G(L,\xi)\, f(\xi)\, d\xi = 0,
\]
since $G(L,\xi) = 0$ for all $\xi$. Therefore $u$ satisfies the boundary conditions $u(0)=u(L)=0$.

Finally, we argue uniqueness. Suppose $\tilde{u}$ is any solution of the boundary value problem. Then $w = u - \tilde{u}$ satisfies
\[
- w''(x) = 0, \qquad w(0) = 0, \quad w(L) = 0.
\]
As we saw in Step 1, the only such solution is $w \equiv 0$. Hence $u = \tilde{u}$, so the solution \eqref{eq:representation} is unique and therefore gives the unique solution of the original boundary value problem.

\medskip

\textbf{Conclusion and relation to the Green function method.}
In this example we have illustrated the key ideas of the Green function method for elliptic problems in a particularly simple setting:

\begin{itemize}
\item We identified the linear differential operator $L = -\dfrac{d^{2}}{dx^{2}}$ and its homogeneous boundary conditions.
\item We constructed a Green function $G(x,\xi)$ as the response to a point source, satisfying $L G(\cdot,\xi) = \delta(\cdot-\xi)$ and the same boundary conditions as $u$.
\item Exploiting linearity and the defining property of the Dirac delta, we expressed the solution to the inhomogeneous problem as a superposition (integral) of point-source responses:
\[
u(x) = \int_0^L G(x,\xi)\, f(\xi)\, d\xi.
\]
\end{itemize}

This one-dimensional case already shows the general pattern for elliptic partial differential equations: once an appropriate Green function is found, solutions with arbitrary source terms can be written explicitly as integral operators acting on the data. In higher dimensions, the construction of $G$ is more involved, but the underlying principles are the same.
\end{solution}

% ===== Example 2: Electrostatic Potential in a Rectangular Box via Eigenfunction Expansion (inquiry-based) =====
\begin{problem}[Electrostatic Potential in a Rectangular Box via Eigenfunction Expansion]
We consider a long rectangular box whose cross section is the rectangle
\[
\Omega = \{(x,y) : 0 < x < a,\; 0 < y < b\}.
\]
Assume the walls of the box are perfectly conducting and are held at zero potential. Inside the box there may be a static charge density $\rho(x,y)$, and we seek the resulting electrostatic potential $\Phi(x,y)$ in the cross section. Instead of trying to guess the Green function directly, we will build it systematically from eigenfunctions of the Laplacian that satisfy the same boundary conditions.

We work with Poisson's equation in two dimensions,
\[
- \Delta \Phi(x,y) = \frac{\rho(x,y)}{\varepsilon_0} \quad \text{in } \Omega, 
\qquad 
\Phi = 0 \quad \text{on } \partial\Omega,
\]
where $\varepsilon_0$ is the permittivity of free space.

\smallskip

(a) (Modeling and Green function definition.)  
Explain why the above Poisson problem models the steady electrostatic potential in the box with grounded walls. Then define precisely the \emph{Dirichlet Green function} $G(x,y;\xi,\eta)$ for the Laplacian on $\Omega$ with zero boundary conditions.  
In particular, write down the boundary value problem that $G$ must satisfy (be explicit about which variables the Laplacian acts on, and where the Dirac delta appears).

\medskip

(b) (Eigenfunctions of the Laplacian with Dirichlet boundary conditions.)  
To construct $G$, we look for eigenfunctions of the Laplacian that satisfy homogeneous Dirichlet boundary conditions. Consider the eigenvalue problem
\[
- \Delta \varphi(x,y) = \lambda \, \varphi(x,y) 
\quad \text{for } (x,y)\in\Omega, 
\qquad 
\varphi = 0 \quad \text{on } \partial\Omega.
\]
\begin{enumerate}
\item[(i)] Use separation of variables $\varphi(x,y) = X(x)Y(y)$ to derive ordinary differential equations for $X$ and $Y$, together with their boundary conditions.
\item[(ii)] Solve these ODEs and show that, up to normalization, the eigenfunctions are
\[
\varphi_{mn}(x,y) = \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right), 
\quad m,n = 1,2,\dots,
\]
with corresponding eigenvalues
\[
\lambda_{mn} = \left(\frac{m\pi}{a}\right)^2 + \left(\frac{n\pi}{b}\right)^2.
\]
\end{enumerate}
Hint: Recall the standard Sturm–Liouville problem
\[
X''(x) + k^2 X(x) = 0,\quad X(0)=X(a)=0
\]
and its sine solutions.

\medskip

(c) (Orthogonality and normalization.)  
The collection $\{\varphi_{mn}\}_{m,n\ge 1}$ forms an orthogonal basis in $L^2(\Omega)$ with respect to the inner product
\[
\langle u,v\rangle = \int_0^a\int_0^b u(x,y)\,v(x,y)\,dy\,dx.
\]
\begin{enumerate}
\item[(i)] Compute the integral
\[
\int_0^a \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{m'\pi x}{a}\right)\,dx
\]
and show that it vanishes when $m\ne m'$, and find its value when $m=m'$. Do the analogous computation in $y$.
\item[(ii)] Use (i) to show that
\[
\int_0^a\!\!\int_0^b \varphi_{mn}(x,y)\,\varphi_{m'n'}(x,y)\,dy\,dx
= C\,\delta_{mm'}\delta_{nn'}
\]
for some constant $C$ independent of $m,n$. Determine $C$, and then define normalized eigenfunctions $\phi_{mn}(x,y)$ so that
\[
\int_0^a\!\!\int_0^b \phi_{mn}(x,y)\,\phi_{m'n'}(x,y)\,dy\,dx
= \delta_{mm'}\delta_{nn'}.
\]
\end{enumerate}
Hint: Use the one-dimensional orthogonality relations to factor the two-dimensional integral.

\medskip

(d) (Eigenfunction expansion of the Green function.)  
We now expand the Green function in the orthonormal basis $\{\phi_{mn}\}$:
\[
G(x,y;\xi,\eta) = \sum_{m=1}^\infty \sum_{n=1}^\infty A_{mn}(\xi,\eta)\,\phi_{mn}(x,y).
\]
\begin{enumerate}
\item[(i)] Use the defining equation for $G$ from part (a) and the eigenvalue equation for $\phi_{mn}$ to show that the coefficients must have the form
\[
A_{mn}(\xi,\eta) = \frac{\phi_{mn}(\xi,\eta)}{\lambda_{mn}}.
\]
(Hint: Apply $- \Delta_{x,y}$ to the series for $G$, and use orthonormality to match the Dirac delta as an $L^2$-limit of eigenfunction expansions.)
\item[(ii)] Substitute your explicit formulas for $\phi_{mn}$ and $\lambda_{mn}$ to obtain a concrete double series expression
\[
G(x,y;\xi,\eta) = \sum_{m=1}^\infty \sum_{n=1}^\infty 
\frac{\text{(explicit product of sines)}}{\left(\frac{m\pi}{a}\right)^2 + \left(\frac{n\pi}{b}\right)^2}.
\]
Write out this series with the correct normalization constant.
\item[(iii)] Show formally that if $- \Delta \Phi = \rho/\varepsilon_0$ in $\Omega$ with $\Phi=0$ on $\partial\Omega$, then
\[
\Phi(x,y) = \frac{1}{\varepsilon_0}\int_0^a\!\!\int_0^b
G(x,y;\xi,\eta)\,\rho(\xi,\eta)\,d\eta\,d\xi
\]
solves the boundary value problem.
\end{enumerate}

\medskip

(e) (Extensions and “what if” questions.)
\begin{enumerate}
\item[(i)] Suppose now that there is \emph{no} charge in the box ($\rho\equiv 0$), but the top side at $y=b$ is held at a prescribed potential $f(x)$, while the other three sides remain grounded. How could you adapt the eigenfunction expansion method you used above to represent the solution $\Phi(x,y)$? (You do not need to carry out the full computation; outline the main steps.)
\item[(ii)] How would the eigenfunctions and eigenvalues change if, instead of Dirichlet boundary conditions $\Phi=0$, you imposed homogeneous Neumann boundary conditions $\partial\Phi/\partial n =0$ on all four sides of the rectangle? Describe the new separated solutions $X(x)$ and $Y(y)$ and the corresponding eigenvalues.
\end{enumerate}

\end{problem}

% ===== Example 2: Electrostatic Potential in a Rectangular Box via Eigenfunction Expansion (full solution) =====
\begin{problem}[Electrostatic Potential in a Rectangular Box via Eigenfunction Expansion]
Let $\Omega = (0,a)\times(0,b)$ be a rectangular domain. Consider the Dirichlet Poisson problem
\[
- \Delta \Phi(x,y) = f(x,y) \quad \text{in } \Omega,
\qquad
\Phi = 0 \quad \text{on } \partial\Omega.
\]
\begin{enumerate}
\item[(i)] Solve the eigenvalue problem
\[
- \Delta \varphi = \lambda \varphi \quad \text{in } \Omega,
\qquad
\varphi = 0 \quad \text{on } \partial\Omega,
\]
and obtain an orthonormal set of eigenfunctions $\{\phi_{mn}\}_{m,n\ge1}$ and corresponding eigenvalues $\lambda_{mn}$.
\item[(ii)] Using these eigenfunctions, construct the Dirichlet Green function $G(x,y;\xi,\eta)$ for $-\Delta$ on $\Omega$ and show that it can be written as the double series
\[
G(x,y;\xi,\eta)
= \sum_{m=1}^\infty \sum_{n=1}^\infty
\frac{\phi_{mn}(x,y)\,\phi_{mn}(\xi,\eta)}{\lambda_{mn}}.
\]
Compute this expression explicitly as a double Fourier sine series.
\item[(iii)] Show that the solution of the Poisson problem is given by
\[
\Phi(x,y) = \int_0^a\!\!\int_0^b G(x,y;\xi,\eta)\,f(\xi,\eta)\,d\eta\,d\xi.
\]
\end{enumerate}
Explain briefly how this example illustrates the method of Green functions for elliptic boundary value problems.
\end{problem}

\begin{solution}
We solve the problem in three steps: first we find the eigenfunctions of the Laplacian with homogeneous Dirichlet boundary conditions; then we use them to construct the Green function as a spectral expansion; finally we express the solution as a Green function integral.

\medskip

\noindent\textbf{(i) Eigenfunctions and eigenvalues.}
We consider
\[
- \Delta \varphi = \lambda \varphi \quad \text{in } \Omega = (0,a)\times(0,b),
\qquad
\varphi = 0 \quad \text{on } \partial\Omega.
\]
We use separation of variables, seeking solutions of the form $\varphi(x,y)=X(x)Y(y)$. Then
\[
- \Delta \varphi = -\bigl(X''(x)Y(y) + X(x)Y''(y)\bigr)
= \lambda X(x)Y(y).
\]
Dividing by $X(x)Y(y)$ (assuming nontrivial solutions), we get
\[
- \frac{X''(x)}{X(x)} - \frac{Y''(y)}{Y(y)} = \lambda.
\]
The left-hand side is a sum of a function of $x$ and a function of $y$, so each must be constant. We write
\[
- \frac{X''(x)}{X(x)} = \mu, 
\qquad
- \frac{Y''(y)}{Y(y)} = \nu,
\]
with $\mu,\nu$ constants and $\mu+\nu=\lambda$.

Thus $X$ and $Y$ satisfy the one-dimensional eigenvalue problems
\[
\begin{cases}
- X''(x) = \mu X(x), & 0 < x < a, \\
X(0)=X(a)=0,
\end{cases}
\qquad
\begin{cases}
- Y''(y) = \nu Y(y), & 0 < y < b, \\
Y(0)=Y(b)=0.
\end{cases}
\]

From the standard Sturm–Liouville theory on $(0,a)$ with Dirichlet boundary conditions, we know that nontrivial solutions occur only when
\[
\mu_m = \left(\frac{m\pi}{a}\right)^2,\quad 
X_m(x) = \sin\!\left(\frac{m\pi x}{a}\right),\quad m=1,2,\dots,
\]
and similarly on $(0,b)$,
\[
\nu_n = \left(\frac{n\pi}{b}\right)^2,\quad 
Y_n(y) = \sin\!\left(\frac{n\pi y}{b}\right),\quad n=1,2,\dots.
\]
Therefore the separated solutions are
\[
\varphi_{mn}(x,y) = X_m(x)Y_n(y)
= \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right),
\]
with eigenvalues
\[
\lambda_{mn} = \mu_m + \nu_n
= \left(\frac{m\pi}{a}\right)^2 + \left(\frac{n\pi}{b}\right)^2,
\qquad m,n=1,2,\dots.
\]
These functions satisfy $\varphi_{mn}=0$ on $x=0,a$ and $y=0,b$, as required.

We now normalize these eigenfunctions to form an orthonormal set in $L^2(\Omega)$ with respect to the inner product
\[
\langle u,v\rangle = \int_0^a\!\int_0^b u(x,y)\,v(x,y)\,dy\,dx.
\]
First, recall the one-dimensional orthogonality:
\[
\int_0^a \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{m'\pi x}{a}\right)\,dx
=
\begin{cases}
0, & m\neq m',\\[3pt]
\dfrac{a}{2}, & m = m'.
\end{cases}
\]
An analogous formula holds on $(0,b)$:
\[
\int_0^b \sin\!\left(\frac{n\pi y}{b}\right)\sin\!\left(\frac{n'\pi y}{b}\right)\,dy
=
\begin{cases}
0, & n\neq n',\\[3pt]
\dfrac{b}{2}, & n = n'.
\end{cases}
\]
Therefore
\[
\int_0^a\!\int_0^b \varphi_{mn}(x,y)\,\varphi_{m'n'}(x,y)\,dy\,dx
= \left(\frac{a}{2}\delta_{mm'}\right)\left(\frac{b}{2}\delta_{nn'}\right)
= \frac{ab}{4}\,\delta_{mm'}\delta_{nn'}.
\]
To obtain orthonormal eigenfunctions, we define
\[
\phi_{mn}(x,y)
:= \frac{2}{\sqrt{ab}}\,\sin\!\left(\frac{m\pi x}{a}\right)
\sin\!\left(\frac{n\pi y}{b}\right).
\]
Then
\[
\int_0^a\!\int_0^b \phi_{mn}(x,y)\,\phi_{m'n'}(x,y)\,dy\,dx
= \delta_{mm'}\delta_{nn'}.
\]
Thus $\{\phi_{mn}\}_{m,n\ge1}$ is an orthonormal set of eigenfunctions of $-\Delta$ with eigenvalues $\lambda_{mn}$ as above. On a rectangle, these eigenfunctions are known to be complete in $L^2(\Omega)$, so any square-integrable function can be expanded in this basis.

\medskip

\noindent\textbf{(ii) Construction of the Green function.}
We now construct the Green function $G(x,y;\xi,\eta)$ for $-\Delta$ with Dirichlet boundary conditions. By definition, $G$ satisfies
\[
\begin{cases}
- \Delta_{x,y} G(x,y;\xi,\eta) 
= \delta(x-\xi)\,\delta(y-\eta), & (x,y)\in\Omega,\\[3pt]
G(x,y;\xi,\eta) = 0, & (x,y)\in\partial\Omega,
\end{cases}
\]
where the Laplacian acts on the $(x,y)$ variables and $(\xi,\eta)$ play the role of parameters indicating the location of the point source.

The operator $-\Delta$ with Dirichlet boundary conditions is a positive, self-adjoint operator on $L^2(\Omega)$ with the orthonormal eigenfunctions $\phi_{mn}$ and eigenvalues $\lambda_{mn}>0$. The general spectral theory for such operators tells us that the inverse operator $(-\Delta)^{-1}$, when it exists, acts on each eigenfunction by division by the eigenvalue:
\[
(-\Delta)^{-1}\phi_{mn} = \frac{1}{\lambda_{mn}}\phi_{mn}.
\]
In terms of Green functions, this means that $G(x,y;\xi,\eta)$, viewed as the integral kernel of $(-\Delta)^{-1}$, admits the eigenfunction expansion
\[
G(x,y;\xi,\eta) = \sum_{m=1}^\infty\sum_{n=1}^\infty
\frac{\phi_{mn}(x,y)\,\phi_{mn}(\xi,\eta)}{\lambda_{mn}}.
\]

We can justify this formula more concretely. Fix $(\xi,\eta)\in\Omega$ and expand $G(\cdot,\cdot;\xi,\eta)$ in the orthonormal basis:
\[
G(x,y;\xi,\eta)
= \sum_{m=1}^\infty\sum_{n=1}^\infty A_{mn}(\xi,\eta)\,\phi_{mn}(x,y),
\]
with
\[
A_{mn}(\xi,\eta)
= \int_0^a\!\int_0^b G(x,y;\xi,\eta)\,\phi_{mn}(x,y)\,dy\,dx.
\]
Apply $- \Delta_{x,y}$ to both sides. Using $- \Delta \phi_{mn} = \lambda_{mn}\phi_{mn}$, we obtain (in the sense of distributions)
\[
- \Delta_{x,y} G(x,y;\xi,\eta)
= \sum_{m,n} A_{mn}(\xi,\eta)\,\lambda_{mn}\,\phi_{mn}(x,y).
\]
On the other hand, by definition of $G$,
\[
- \Delta_{x,y} G(x,y;\xi,\eta)
= \delta(x-\xi)\,\delta(y-\eta).
\]
The right-hand side can itself be expanded in the orthonormal basis $\{\phi_{mn}\}$:
\[
\delta(x-\xi)\,\delta(y-\eta)
= \sum_{m,n} \phi_{mn}(\xi,\eta)\,\phi_{mn}(x,y),
\]
since for any $v\in L^2(\Omega)$,
\[
\int_0^a\!\int_0^b \delta(x-\xi)\,\delta(y-\eta)\,v(x,y)\,dy\,dx
= v(\xi,\eta)
= \sum_{m,n} \big\langle v,\phi_{mn}\big\rangle \phi_{mn}(\xi,\eta)
\]
and the expansion coefficients match term by term.

Comparing the two series expansions for $- \Delta_{x,y} G$, we must have
\[
A_{mn}(\xi,\eta)\,\lambda_{mn} = \phi_{mn}(\xi,\eta),
\quad\text{so}\quad
A_{mn}(\xi,\eta) = \frac{\phi_{mn}(\xi,\eta)}{\lambda_{mn}}.
\]
Therefore
\[
G(x,y;\xi,\eta)
= \sum_{m=1}^\infty\sum_{n=1}^\infty
\frac{\phi_{mn}(x,y)\,\phi_{mn}(\xi,\eta)}{\lambda_{mn}},
\]
as claimed.

Substituting the explicit formulas for $\phi_{mn}$ and $\lambda_{mn}$, we obtain
\[
\phi_{mn}(x,y)\,\phi_{mn}(\xi,\eta)
= \left(\frac{2}{\sqrt{ab}}\right)^2
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{m\pi \xi}{a}\right)
\sin\!\left(\frac{n\pi y}{b}\right)\sin\!\left(\frac{n\pi \eta}{b}\right)
= \frac{4}{ab}\,\prod_{\zeta\in\{x,\xi\}}\sin\!\left(\frac{m\pi \zeta}{a}\right)
\prod_{\zeta\in\{y,\eta\}}\sin\!\left(\frac{n\pi \zeta}{b}\right),
\]
and
\[
\lambda_{mn} = \left(\frac{m\pi}{a}\right)^2 + \left(\frac{n\pi}{b}\right)^2.
\]
Thus the Green function is the double Fourier sine series
\[
G(x,y;\xi,\eta)
= \sum_{m=1}^\infty\sum_{n=1}^\infty
\frac{4}{ab}
\frac{\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{m\pi \xi}{a}\right)
\sin\!\left(\frac{n\pi y}{b}\right)\sin\!\left(\frac{n\pi \eta}{b}\right)}
{\left(\frac{m\pi}{a}\right)^2 + \left(\frac{n\pi}{b}\right)^2}.
\]
This is the desired explicit representation of the Dirichlet Green function for the rectangle.

\medskip

\noindent\textbf{(iii) Representation of the solution.}
We now show that the solution of
\[
- \Delta \Phi = f \quad \text{in }\Omega, \qquad \Phi = 0 \quad \text{on }\partial\Omega,
\]
is given by
\[
\Phi(x,y) = \int_0^a\!\!\int_0^b G(x,y;\xi,\eta)\,f(\xi,\eta)\,d\eta\,d\xi.
\]

First, note that by construction $G(\cdot,\cdot;\xi,\eta)$ vanishes on $\partial\Omega$ for each fixed $(\xi,\eta)$, so the integral expression for $\Phi$ will also satisfy $\Phi=0$ on $\partial\Omega$ (since the integral is a linear combination of functions each of which vanishes on the boundary).

Next, we apply $- \Delta_{x,y}$ to the integral. By linearity and suitable regularity assumptions (which allow us to interchange differentiation and integration), we have
\[
- \Delta_{x,y} \Phi(x,y)
= - \Delta_{x,y} \int_0^a\!\!\int_0^b G(x,y;\xi,\eta)\,f(\xi,\eta)\,d\eta\,d\xi
= \int_0^a\!\!\int_0^b \bigl(-\Delta_{x,y}G(x,y;\xi,\eta)\bigr)\,f(\xi,\eta)\,d\eta\,d\xi.
\]
By the defining property of $G$,
\[
- \Delta_{x,y}G(x,y;\xi,\eta)
= \delta(x-\xi)\,\delta(y-\eta),
\]
so
\[
- \Delta_{x,y} \Phi(x,y)
= \int_0^a\!\!\int_0^b \delta(x-\xi)\,\delta(y-\eta)\,f(\xi,\eta)\,d\eta\,d\xi
= f(x,y),
\]
since the double delta distribution evaluates the integrand at $(\xi,\eta)=(x,y)$.

Therefore the function
\[
\Phi(x,y) = \int_0^a\!\!\int_0^b G(x,y;\xi,\eta)\,f(\xi,\eta)\,d\eta\,d\xi
\]
satisfies both the differential equation $- \Delta\Phi=f$ in $\Omega$ and the homogeneous Dirichlet boundary condition $\Phi=0$ on $\partial\Omega$. Standard elliptic theory implies that this solution is unique, so the integral representation is indeed the solution of the Poisson problem.

\medskip

\noindent\textbf{Relation to the method of Green functions.}
This example illustrates the main ideas of the method of Green functions for elliptic partial differential equations:

\begin{itemize}
\item We first solve the \emph{homogeneous} eigenvalue problem for the operator (here $- \Delta$) with the given boundary conditions (here Dirichlet on a rectangle), obtaining an orthonormal basis of eigenfunctions and corresponding eigenvalues.

\item We then use the spectral representation of the inverse operator: the Green function is obtained by summing over the eigenfunctions, each weighted by the reciprocal of its eigenvalue. This provides an explicit kernel $G(x,y;\xi,\eta)$ such that applying the inverse operator becomes an integral against $G$.

\item Finally, the solution of the \emph{inhomogeneous} boundary value problem is expressed as a Green function integral, which in this case becomes a double Fourier sine series when $f$ is expanded in the same eigenbasis.
\end{itemize}

In summary, the method of Green functions converts the task of solving a boundary value problem into constructing an integral kernel, here done by eigenfunction expansion. This approach is particularly effective in simple geometries, such as rectangles, where the eigenfunctions are explicitly known and orthogonal.
\end{solution}

% ===== Example 3: Steady-State Heat in a Disk and the Poisson Kernel (inquiry-based) =====
\begin{problem}[Steady-State Heat in a Disk and the Poisson Kernel]
Consider a thin, homogeneous, circular metal plate occupying the unit disk
\[
D := \{ x \in \mathbb{R}^2 : |x| < 1\}.
\]
We assume the plate has reached a steady-state temperature distribution $u(x)$.
On the boundary circle $\partial D$, the temperature is held at a prescribed
profile $g(\theta)$ (where $\theta$ is the polar angle), and inside the plate
there may or may not be heat sources. Our goal is to construct the Green
function for the Laplacian in the unit disk and to see how, from this function,
the classical Poisson kernel and the Poisson integral formula naturally appear.

Throughout, it will be convenient to identify points $x = (x_1,x_2)\in\mathbb{R}^2$
with complex numbers $z = x_1 + i x_2 \in \mathbb{C}$, and similarly write
$\zeta = \xi_1 + i \xi_2$ for another point.

\medskip

(a) {\bf Fundamental solution in the plane.}
In two dimensions, the fundamental solution of the Laplacian is the function
$\Phi:\mathbb{R}^2 \setminus \{0\} \to \mathbb{R}$ satisfying
\[
-\Delta_x \Phi(x) = \delta_0(x),
\]
in the sense of distributions, and decaying suitably at infinity.

\quad(i) Recall or verify that a fundamental solution for the Laplacian in $\mathbb{R}^2$ is
\[
\Phi(x) \;=\; -\frac{1}{2\pi}\log|x|.
\]
Explain in words what it means that $-\Delta_x \Phi(x-\xi) = \delta_\xi(x)$ for a fixed point $\xi\in\mathbb{R}^2$.

\quad(ii) Let us denote
\[
\Phi(x,\xi) := -\frac{1}{2\pi}\log|x-\xi|.
\]
Why is $\Phi(\cdot,\xi)$ harmonic (that is, satisfies $\Delta_x \Phi(x,\xi)=0$) on $\mathbb{R}^2\setminus\{\xi\}$?

\medskip

(b) {\bf From fundamental solution to Green function in a domain.}
For the unit disk $D$, the Green function with zero Dirichlet boundary condition is a function
$G_D(x,\xi)$ such that
\[
\Delta_x G_D(x,\xi) = \delta_\xi(x) \quad\text{in } D,\qquad
G_D(x,\xi)=0 \quad\text{for } x\in \partial D,
\]
for each fixed $\xi\in D$.

\quad(i) Argue that for each fixed $\xi\in D$, any candidate $G_D(\cdot,\xi)$ must look like
\[
G_D(x,\xi) = \Phi(x,\xi) + H(x,\xi),
\]
where $H(\cdot,\xi)$ is harmonic in $D$. Why is it natural to try to construct $H(\cdot,\xi)$ so that $G_D(\cdot,\xi)$ satisfies the boundary condition $G_D(x,\xi)=0$ on $\partial D$?

\quad(ii) Explain why the function $H(\cdot,\xi)$ must be smooth in $\overline{D}$ and cannot introduce any new singularities in $D$.

\medskip

(c) {\bf Guessing and verifying the Green function in the unit disk.}
We now restrict attention to the unit disk $D=\{z\in\mathbb{C}:|z|<1\}$ and write
$z$ for $x$ and $\zeta$ for $\xi$.

\quad(i) Consider the complex-valued function
\[
f(z) = \log(1 - \overline{\zeta}\,z),
\]
for fixed $\zeta\in D$ and variable $z\in D$. Show that $f$ is analytic in $z$ on $D$. Conclude that
\[
h(z,\zeta) := \Re f(z) = \log|1-\overline{\zeta} z|
\]
is harmonic in $z$ on $D$.  (Recall that the real part of a holomorphic function is harmonic.)

\quad(ii) Motivated by part (b), consider the candidate Green function
\[
G_D(z,\zeta) := -\frac{1}{2\pi}\log|z-\zeta| + \frac{1}{2\pi}\log|1-\overline{\zeta} z|.
\]
Using part (a) and the fact that $h(\cdot,\zeta)$ is harmonic in $D$, show that for fixed $\zeta\in D$,
\[
\Delta_z G_D(z,\zeta) = \delta_\zeta(z) \quad \text{in } D.
\]

\quad(iii) Verify the boundary condition.
Let $|z|=1$ (so $z$ is on the unit circle). Show that
\[
|z-\zeta| = |1 - \overline{\zeta}z|
\]
for all $|z|=1$, and deduce that $G_D(z,\zeta)=0$ for all $z\in\partial D$.

\emph{Hint:} Use $|z|=1$ to rewrite $z^{-1} = \overline{z}$ and compare 
$z-\zeta$ with $1-\overline{\zeta}z$ by factoring out $z$.

\medskip

(d) {\bf From the Green function to the Poisson kernel and Poisson integral.}
The Green representation formula for a $C^2$ function $u$ on $\overline{D}$ says that, for each $x\in D$,
\[
u(x) = \int_{\partial D} \Bigl(u(y)\,\frac{\partial G_D}{\partial n_y}(x,y)
      - G_D(x,y)\,\frac{\partial u}{\partial n_y}(y)\Bigr)\,ds_y
      + \int_{D} G_D(x,y)\,(-\Delta u(y))\,dy.
\]
Here $\partial/\partial n_y$ denotes the outward normal derivative at $y\in\partial D$.

\quad(i) Assume $u$ is harmonic in $D$ (so $\Delta u = 0$) and $u=g$ on $\partial D$. Explain why in this case the formula simplifies to
\[
u(x) = \int_{\partial D} g(y)\,\frac{\partial G_D}{\partial n_y}(x,y)\,ds_y.
\]

\quad(ii) Parametrize the boundary $\partial D$ by $y=e^{i\varphi}$, with arc length element $ds_y = d\varphi$, and write $x$ in polar form $x=re^{i\theta}$. Using the explicit formula for $G_D$, compute the outward normal derivative
\[
-\frac{\partial G_D}{\partial n_y}(x,y)\Big|_{|y|=1}
\]
and show that it equals
\[
P(r,\theta-\varphi) := \frac{1-r^2}{1-2r\cos(\theta-\varphi)+r^2},
\]
which is called the \emph{Poisson kernel} of the unit disk.

\emph{Hint:} First write $G_D(re^{i\theta},r'e^{i\varphi})$ with $0<r'<1$ and compute $\partial/\partial r'$ at $r'=1$. You may find it easier to differentiate $\log|re^{i\theta}-r'e^{i\varphi}|^2$ and express the result in terms of $r$, $r'$, and $\theta-\varphi$.

\quad(iii) Deduce the \emph{Poisson integral formula}: for every harmonic function $u$ in $D$ with boundary data $u(e^{i\varphi}) = g(\varphi)$,
\[
u(re^{i\theta})
= \frac{1}{2\pi}\int_0^{2\pi} P(r,\theta-\varphi)\,g(\varphi)\,d\varphi.
\]

\medskip

(e) {\bf Extensions and variations.}

\quad(i) Suppose now that $u$ solves the Poisson equation
\[
-\Delta u = f \quad\text{in } D,\qquad u=g\quad\text{on }\partial D,
\]
with a given source term $f$. Using the Green representation formula and your explicit $G_D$, write down an integral formula for $u$ in terms of $f$, $g$, and $G_D$.

\quad(ii) How do you expect the Green function and Poisson kernel to change if the disk has radius $R>0$ instead of $1$? Describe (without full derivation) how you would modify the formulas for $G_D$ and $P$ when $D=\{x:|x|<R\}$.
\end{problem}

% ===== Example 3: Steady-State Heat in a Disk and the Poisson Kernel (full solution) =====
%%% INQUIRY START %%%
\begin{problem}[Steady-State Heat in a Disk and the Poisson Kernel]
Consider a thin, homogeneous, circular metal plate occupying the unit disk
\[
D := \{ x \in \mathbb{R}^2 : |x| < 1\}.
\]
We assume the plate has reached a steady-state temperature distribution $u(x)$.
On the boundary circle $\partial D$, the temperature is held at a prescribed
profile $g(\theta)$ (where $\theta$ is the polar angle), and inside the plate
there may or may not be heat sources. Our goal is to construct the Green
function for the Laplacian in the unit disk and to see how, from this function,
the classical Poisson kernel and the Poisson integral formula naturally appear.

Throughout, it will be convenient to identify points $x = (x_1,x_2)\in\mathbb{R}^2$
with complex numbers $z = x_1 + i x_2 \in \mathbb{C}$, and similarly write
$\zeta = \xi_1 + i \xi_2$ for another point.

\medskip

(a) {\bf Fundamental solution in the plane.}
In two dimensions, the fundamental solution of the Laplacian is the function
$\Phi:\mathbb{R}^2 \setminus \{0\} \to \mathbb{R}$ satisfying
\[
-\Delta_x \Phi(x) = \delta_0(x),
\]
in the sense of distributions, and decaying suitably at infinity.

\quad(i) Recall or verify that a fundamental solution for the Laplacian in $\mathbb{R}^2$ is
\[
\Phi(x) \;=\; -\frac{1}{2\pi}\log|x|.
\]
Explain in words what it means that $-\Delta_x \Phi(x-\xi) = \delta_\xi(x)$ for a fixed point $\xi\in\mathbb{R}^2$.

\quad(ii) Let us denote
\[
\Phi(x,\xi) := -\frac{1}{2\pi}\log|x-\xi|.
\]
Why is $\Phi(\cdot,\xi)$ harmonic (that is, satisfies $\Delta_x \Phi(x,\xi)=0$) on $\mathbb{R}^2\setminus\{\xi\}$?

\medskip

(b) {\bf From fundamental solution to Green function in a domain.}
For the unit disk $D$, the Green function with zero Dirichlet boundary condition is a function
$G_D(x,\xi)$ such that
\[
\Delta_x G_D(x,\xi) = \delta_\xi(x) \quad\text{in } D,\qquad
G_D(x,\xi)=0 \quad\text{for } x\in \partial D,
\]
for each fixed $\xi\in D$.

\quad(i) Argue that for each fixed $\xi\in D$, any candidate $G_D(\cdot,\xi)$ must look like
\[
G_D(x,\xi) = \Phi(x,\xi) + H(x,\xi),
\]
where $H(\cdot,\xi)$ is harmonic in $D$. Why is it natural to try to construct $H(\cdot,\xi)$ so that $G_D(\cdot,\xi)$ satisfies the boundary condition $G_D(x,\xi)=0$ on $\partial D$?

\quad(ii) Explain why the function $H(\cdot,\xi)$ must be smooth in $\overline{D}$ and cannot introduce any new singularities in $D$.

\medskip

(c) {\bf Guessing and verifying the Green function in the unit disk.}
We now restrict attention to the unit disk $D=\{z\in\mathbb{C}:|z|<1\}$ and write
$z$ for $x$ and $\zeta$ for $\xi$.

\quad(i) Consider the complex-valued function
\[
f(z) = \log(1 - \overline{\zeta}\,z),
\]
for fixed $\zeta\in D$ and variable $z\in D$. Show that $f$ is analytic in $z$ on $D$. Conclude that
\[
h(z,\zeta) := \Re f(z) = \log|1-\overline{\zeta} z|
\]
is harmonic in $z$ on $D$.  (Recall that the real part of a holomorphic function is harmonic.)

\quad(ii) Motivated by part (b), consider the candidate Green function
\[
G_D(z,\zeta) := -\frac{1}{2\pi}\log|z-\zeta| + \frac{1}{2\pi}\log|1-\overline{\zeta} z|.
\]
Using part (a) and the fact that $h(\cdot,\zeta)$ is harmonic in $D$, show that for fixed $\zeta\in D$,
\[
\Delta_z G_D(z,\zeta) = \delta_\zeta(z) \quad \text{in } D.
\]

\quad(iii) Verify the boundary condition.
Let $|z|=1$ (so $z$ is on the unit circle). Show that
\[
|z-\zeta| = |1 - \overline{\zeta}z|
\]
for all $|z|=1$, and deduce that $G_D(z,\zeta)=0$ for all $z\in\partial D$.

\emph{Hint:} Use $|z|=1$ to rewrite $z^{-1} = \overline{z}$ and compare 
$z-\zeta$ with $1-\overline{\zeta}z$ by factoring out $z$.

\medskip

(d) {\bf From the Green function to the Poisson kernel and Poisson integral.}
The Green representation formula for a $C^2$ function $u$ on $\overline{D}$ says that, for each $x\in D$,
\[
u(x) = \int_{\partial D} \Bigl(u(y)\,\frac{\partial G_D}{\partial n_y}(x,y)
      - G_D(x,y)\,\frac{\partial u}{\partial n_y}(y)\Bigr)\,ds_y
      + \int_{D} G_D(x,y)\,(-\Delta u(y))\,dy.
\]
Here $\partial/\partial n_y$ denotes the outward normal derivative at $y\in\partial D$.

\quad(i) Assume $u$ is harmonic in $D$ (so $\Delta u = 0$) and $u=g$ on $\partial D$. Explain why in this case the formula simplifies to
\[
u(x) = \int_{\partial D} g(y)\,\frac{\partial G_D}{\partial n_y}(x,y)\,ds_y.
\]

\quad(ii) Parametrize the boundary $\partial D$ by $y=e^{i\varphi}$, with arc length element $ds_y = d\varphi$, and write $x$ in polar form $x=re^{i\theta}$. Using the explicit formula for $G_D$, compute the outward normal derivative
\[
-\frac{\partial G_D}{\partial n_y}(x,y)\Big|_{|y|=1}
\]
and show that it equals
\[
P(r,\theta-\varphi) := \frac{1-r^2}{1-2r\cos(\theta-\varphi)+r^2},
\]
which is called the \emph{Poisson kernel} of the unit disk.

\emph{Hint:} First write $G_D(re^{i\theta},r'e^{i\varphi})$ with $0<r'<1$ and compute $\partial/\partial r'$ at $r'=1$. You may find it easier to differentiate $\log|re^{i\theta}-r'e^{i\varphi}|^2$ and express the result in terms of $r$, $r'$, and $\theta-\varphi$.

\quad(iii) Deduce the \emph{Poisson integral formula}: for every harmonic function $u$ in $D$ with boundary data $u(e^{i\varphi}) = g(\varphi)$,
\[
u(re^{i\theta})
= \frac{1}{2\pi}\int_0^{2\pi} P(r,\theta-\varphi)\,g(\varphi)\,d\varphi.
\]

\medskip

(e) {\bf Extensions and variations.}

\quad(i) Suppose now that $u$ solves the Poisson equation
\[
-\Delta u = f \quad\text{in } D,\qquad u=g\quad\text{on }\partial D,
\]
with a given source term $f$. Using the Green representation formula and your explicit $G_D$, write down an integral formula for $u$ in terms of $f$, $g$, and $G_D$.

\quad(ii) How do you expect the Green function and Poisson kernel to change if the disk has radius $R>0$ instead of $1$? Describe (without full derivation) how you would modify the formulas for $G_D$ and $P$ when $D=\{x:|x|<R\}$.
\end{problem}
%%% INQUIRY END %%%

%%% SOLUTION START %%%
\begin{problem}[Steady-State Heat in a Disk and the Poisson Kernel]
Let $D = \{x\in\mathbb{R}^2 : |x|<1\}$ be the unit disk. 

(a) Construct the Dirichlet Green function $G_D(x,\xi)$ for the Laplacian in $D$, that is, a function such that for each fixed $\xi\in D$,
\[
\Delta_x G_D(x,\xi) = \delta_\xi(x)\quad\text{in }D,\qquad
G_D(x,\xi) = 0\quad\text{for }x\in\partial D.
\]
Show that, in complex notation $z=x_1+ix_2$, $\zeta=\xi_1+i\xi_2$,
\[
G_D(z,\zeta)
= -\frac{1}{2\pi}\log|z-\zeta| + \frac{1}{2\pi}\log|1-\overline{\zeta} z|.
\]

(b) Using the Green representation formula and part (a), compute the outward normal derivative of $G_D$ on the boundary and deduce that the Poisson kernel of the unit disk is
\[
P(r,\theta-\varphi)
= \frac{1-r^2}{1-2r\cos(\theta-\varphi)+r^2},
\]
for $0\le r<1$ and angles $\theta,\varphi\in\mathbb{R}$. Show that any harmonic function $u$ in $D$ with boundary values $u(e^{i\varphi}) = g(\varphi)$ satisfies the Poisson integral formula
\[
u(re^{i\theta})
= \frac{1}{2\pi}\int_0^{2\pi} P(r,\theta-\varphi)\,g(\varphi)\,d\varphi.
\]

(c) State the corresponding Green representation formula for the Poisson equation
\[
-\Delta u = f \quad\text{in } D,\qquad u=g\quad\text{on }\partial D,
\]
in terms of $G_D$, $f$, and $g$.

Explain briefly how this example illustrates the method of Green functions for elliptic partial differential equations.
\end{problem}

\begin{solution}
We begin by recalling the role of a Green function in solving boundary value problems for elliptic operators. For the Laplacian on a domain $D$, the Dirichlet Green function encodes both the singular behavior (through the fundamental solution) and the boundary condition. Once this function is known, it provides an integral representation of solutions to both homogeneous (Laplace) and inhomogeneous (Poisson) equations.

\medskip

\noindent\textbf{(a) Green function in the unit disk.}
In two dimensions, a fundamental solution of the Laplacian is
\[
\Phi(x) = -\frac{1}{2\pi}\log|x|, \qquad x\in\mathbb{R}^2\setminus\{0\},
\]
in the sense that $-\Delta \Phi = \delta_0$ in the distributional sense. For a fixed point $\xi\in\mathbb{R}^2$, the translated function
\[
\Phi(x,\xi) := -\frac{1}{2\pi}\log|x-\xi|
\]
satisfies $-\Delta_x\Phi(x,\xi) = \delta_\xi(x)$ and is harmonic in $x$ on $\mathbb{R}^2\setminus\{\xi\}$.

Let $D$ be the unit disk and fix $\xi\in D$. A Dirichlet Green function $G_D(\cdot,\xi)$ should satisfy
\[
\Delta_x G_D(x,\xi) = \delta_\xi(x)\quad\text{in }D,\qquad
G_D(x,\xi)=0\quad\text{for }x\in\partial D.
\]
Since $\Phi(\cdot,\xi)$ already has the correct singularity at $x=\xi$, any Green function for $D$ must differ from $\Phi(\cdot,\xi)$ by a function harmonic in $D$. Thus we seek
\[
G_D(x,\xi) = \Phi(x,\xi) + H(x,\xi)
= -\frac{1}{2\pi}\log|x-\xi| + H(x,\xi),
\]
where, for each fixed $\xi\in D$, the function $H(\cdot,\xi)$ is harmonic in $D$ and chosen so that $G_D(\cdot,\xi)$ vanishes on $\partial D$. Since we require $G_D$ to be bounded on $\partial D$ and smooth away from $x=\xi$, we also require $H(\cdot,\xi)$ to be smooth on $\overline{D}$ (it cannot introduce any new singularities inside $D$).

To exploit the geometry of the disk, we pass to complex notation. We identify $x=(x_1,x_2)$ with $z=x_1+ix_2\in\mathbb{C}$ and $\xi=(\xi_1,\xi_2)$ with $\zeta=\xi_1+i\xi_2\in\mathbb{C}$. The Euclidean distance is then $|x-\xi|=|z-\zeta|$. We look for a harmonic correction of the form
\[
H(z,\zeta) = \frac{1}{2\pi}\log|1-\overline{\zeta} z|.
\]

To justify this choice, note first that for fixed $\zeta\in D$, the map
\[
f(z) := \log(1-\overline{\zeta}z)
\]
is holomorphic as a function of $z$ on $D$, since $|\overline{\zeta}z|<1$ for all $z\in D$ and one can take the principal branch of the logarithm near $1$. Therefore the real part
\[
h(z,\zeta) := \Re f(z) = \log|1-\overline{\zeta} z|
\]
is harmonic in $z$ on $D$. Consequently $H(\cdot,\zeta)$ is harmonic on $D$, as required.

We then define
\[
G_D(z,\zeta)
:= -\frac{1}{2\pi}\log|z-\zeta| + \frac{1}{2\pi}\log|1-\overline{\zeta} z|.
\]
By construction, the singular part agrees with the fundamental solution $\Phi(z,\zeta)$ and the correction term is harmonic in $z$. It follows that, for fixed $\zeta\in D$,
\[
\Delta_z G_D(z,\zeta) = \Delta_z\Phi(z,\zeta) + \Delta_z H(z,\zeta)
= \delta_\zeta(z) + 0 = \delta_\zeta(z) \quad\text{in } D.
\]

It remains to check the boundary condition $G_D(z,\zeta)=0$ for $|z|=1$. Let $|z|=1$. We claim that
\[
|z-\zeta| = |1-\overline{\zeta}z|.
\]
Indeed,
\[
1-\overline{\zeta}z = \overline{z}\,(z-\zeta),
\]
because $|z|=1$ implies $\overline{z} = 1/z$ and thus
\[
\overline{z}\,(z-\zeta) = \overline{z}z - \overline{z}\,\zeta
= 1 - \overline{\zeta} z.
\]
Taking moduli and using $|\overline{z}|=1$ gives
\[
|1-\overline{\zeta}z| = |\overline{z}|\,|z-\zeta| = |z-\zeta|.
\]
Therefore on $|z|=1$ we have
\[
G_D(z,\zeta)
= -\frac{1}{2\pi}\log|z-\zeta| + \frac{1}{2\pi}\log|1-\overline{\zeta} z|
= -\frac{1}{2\pi}\log|z-\zeta| + \frac{1}{2\pi}\log|z-\zeta| = 0.
\]
This verifies that $G_D$ satisfies the Dirichlet boundary condition. Thus
\[
G_D(z,\zeta)
= -\frac{1}{2\pi}\log|z-\zeta| + \frac{1}{2\pi}\log|1-\overline{\zeta} z|
\]
is the Dirichlet Green function of the unit disk.

\medskip

\noindent\textbf{(b) Poisson kernel and Poisson integral formula.}
The Green representation formula for the Laplacian on a domain $D$ reads as follows. If $u\in C^2(D)\cap C^1(\overline{D})$, then for each $x\in D$,
\begin{equation}\label{GRF}
u(x)
= \int_{\partial D} \Bigl(u(y)\,\frac{\partial G_D}{\partial n_y}(x,y)
      - G_D(x,y)\,\frac{\partial u}{\partial n_y}(y)\Bigr)\,ds_y
      + \int_{D} G_D(x,y)\,(-\Delta u(y))\,dy,
\end{equation}
where $\partial/\partial n_y$ denotes differentiation in the outward normal direction at $y\in\partial D$ and $ds_y$ is the boundary arc length element.

Suppose now that $u$ is harmonic in $D$, that is, $\Delta u = 0$, and that $u$ has prescribed boundary values $u(y)=g(y)$ on $\partial D$. Then the volume integral in \eqref{GRF} vanishes, and we obtain
\[
u(x)
= \int_{\partial D} \left(g(y)\,\frac{\partial G_D}{\partial n_y}(x,y)
  - G_D(x,y)\,\frac{\partial u}{\partial n_y}(y)\right)\,ds_y.
\]
Since $G_D(x,y) = 0$ for $y\in\partial D$, the term involving $G_D(x,y)\,\partial u/\partial n_y(y)$ also vanishes. Therefore
\[
u(x) = \int_{\partial D} g(y)\,\frac{\partial G_D}{\partial n_y}(x,y)\,ds_y.
\]

Next we parametrize the boundary of the unit disk by
\[
y = e^{i\varphi},\qquad \varphi\in[0,2\pi),
\]
and write $x$ in polar coordinates $x = re^{i\theta}$, with $0\le r<1$. The outward unit normal at $y$ is just $n_y = y$, so the outward normal derivative at $y$ is
\[
\frac{\partial}{\partial n_y} = \nabla_y\cdot n_y = \frac{\partial}{\partial r'}
\quad\text{at } y = r'e^{i\varphi},\; r'=1.
\]
Moreover, the arc length element on the unit circle is $ds_y = d\varphi$.

We now compute the normal derivative of $G_D$ with respect to the boundary variable $y$. To emphasize which variable we differentiate in, we write
\[
G_D\bigl(re^{i\theta}, r'e^{i\varphi}\bigr)
= -\frac{1}{2\pi}\log\bigl|re^{i\theta}-r'e^{i\varphi}\bigr|
  + \frac{1}{2\pi}\log\bigl|1-\overline{r'e^{i\varphi}}\,re^{i\theta}\bigr|.
\]
We have $\overline{r'e^{i\varphi}} = r'e^{-i\varphi}$, so
\[
G_D\bigl(re^{i\theta}, r'e^{i\varphi}\bigr)
= -\frac{1}{4\pi}\log\Bigl(|re^{i\theta}-r'e^{i\varphi}|^2\Bigr)
  + \frac{1}{4\pi}\log\Bigl(|1-r're^{i(\theta-\varphi)}|^2\Bigr).
\]
Set $\alpha := \theta-\varphi$. A direct computation gives
\[
|re^{i\theta}-r'e^{i\varphi}|^2
= r^2 + r'^2 - 2rr'\cos(\theta-\varphi)
= r^2 + r'^2 - 2rr'\cos\alpha,
\]
and
\[
|

% ===== Example 4: Mixed and Robin Boundary Conditions for One-Dimensional Problems (inquiry-based) =====
\begin{problem}[Mixed and Robin Boundary Conditions for One-Dimensional Problems]
Consider a thin, insulated rod occupying the interval $0 < x < L$. The temperature $u(x)$ in steady state satisfies a one-dimensional Poisson equation. Suppose the left end $x=0$ is held at a fixed temperature, while the right end $x=L$ is exposed to a surrounding medium so that heat is lost according to Newton's law of cooling (a Robin boundary condition). In this example, you will construct the Green function that incorporates this mixed system of boundary conditions and then use it to represent the solution.

We study the boundary value problem
\[
- u''(x) = f(x), \qquad 0 < x < L,
\]
with boundary conditions
\[
u(0) = 0, \qquad u'(L) + h\,u(L) = 0,
\]
where $h \ge 0$ is a given constant (the heat transfer coefficient).

\medskip

(a) \textbf{Warm-up: homogeneous solutions and boundary conditions.}  
Consider the associated homogeneous equation
\[
- u''(x) = 0, \qquad 0 < x < L.
\]
\begin{itemize}
  \item[(i)] Find the general solution of $-u''(x) = 0$.
  \item[(ii)] Construct a special solution $\phi(x)$ that satisfies the \emph{left} boundary condition and a convenient normalization:
  \[
  \phi(0) = 0, \qquad \phi'(0) = 1.
  \]
  \item[(iii)] Construct a second special solution $\psi(x)$ that satisfies the \emph{right} boundary condition
  \[
  \psi'(L) + h\,\psi(L) = 0,
  \]
  together with the normalization
  \[
  \psi(L) = 1.
  \]
\end{itemize}
Hint: Use your general solution from part (i) and impose the conditions one by one.

\medskip

(b) \textbf{Setting up the Green function piecewise.}  
For a fixed source point $\xi \in (0,L)$, the Green function $G(x,\xi)$ should satisfy
\[
- \frac{\partial^2}{\partial x^2} G(x,\xi) = \delta(x - \xi),
\]
with the same boundary conditions in the $x$-variable:
\[
G(0,\xi) = 0, \qquad G_x(L,\xi) + h\,G(L,\xi) = 0.
\]
Away from $x = \xi$, the function $G(\cdot,\xi)$ solves the homogeneous equation.
\begin{itemize}
  \item[(i)] Argue that for $x \neq \xi$ the function $G(x,\xi)$ must be a solution of $-G''(x,\xi) = 0$, and therefore is linear in $x$ on each of the intervals $(0,\xi)$ and $(\xi,L)$.
  \item[(ii)] Explain why we may write
  \[
  G(x,\xi) =
  \begin{cases}
    A(\xi)\,x + B(\xi), & 0 \le x < \xi,\\[0.5ex]
    C(\xi)\,x + D(\xi), & \xi < x \le L,
  \end{cases}
  \]
  for some coefficients $A(\xi)$, $B(\xi)$, $C(\xi)$, and $D(\xi)$ depending on $\xi$.
\end{itemize}
Hint: The dependence on $\xi$ is parametric; treat $\xi$ as temporarily fixed.

\medskip

(c) \textbf{Matching conditions at the source point.}  
The Green function is continuous, but its derivative has a jump determined by the delta function.
\begin{itemize}
  \item[(i)] Impose continuity of $G$ at $x = \xi$:
  \[
  G(\xi^-,\xi) = G(\xi^+,\xi).
  \]
  Translate this into an equation relating $A(\xi)$, $B(\xi)$, $C(\xi)$, and $D(\xi)$.
  \item[(ii)] Derive the jump condition for the derivative $G_x$ at $x = \xi$ by integrating the differential equation
  \[
  - G''(x,\xi) = \delta(x - \xi)
  \]
  across a small interval $(\xi-\varepsilon,\xi+\varepsilon)$ and letting $\varepsilon \to 0^+$. Show that
  \[
  G_x(\xi^+,\xi) - G_x(\xi^-,\xi) = -1.
  \]
  Express this as an equation relating $A(\xi)$ and $C(\xi)$.
\end{itemize}
Hint: Remember that $\displaystyle \int_{\xi-\varepsilon}^{\xi+\varepsilon} \delta(x-\xi)\,dx = 1$.

\medskip

(d) \textbf{Using boundary conditions and solving for the coefficients.}  
Now use the boundary conditions in $x$ together with the matching conditions to determine the unknown coefficients.
\begin{itemize}
  \item[(i)] Apply the boundary condition at $x=0$ to the left piece of $G(x,\xi)$ and deduce a relation between $A(\xi)$ and $B(\xi)$.
  \item[(ii)] Apply the Robin boundary condition at $x=L$ to the right piece of $G(x,\xi)$ and obtain an equation relating $C(\xi)$ and $D(\xi)$.
  \item[(iii)] Combine \emph{all four} equations (from continuity, jump condition, and the two boundary conditions) to solve for $A(\xi)$, $B(\xi)$, $C(\xi)$, and $D(\xi)$ explicitly.
\end{itemize}
Hint: It may be helpful to eliminate $B(\xi)$ and $D(\xi)$ first, leaving a $2\times 2$ system for $A(\xi)$ and $C(\xi)$.

Once you have found $G(x,\xi)$ in explicit form, write the Green representation formula for the solution:
\[
u(x) = \int_0^L G(x,\xi)\,f(\xi)\,d\xi.
\]

\medskip

(e) \textbf{Extensions and limiting cases.}
\begin{itemize}
  \item[(i)] What happens to your Green function when $h = 0$? Interpret the resulting boundary value problem and verify that the formula simplifies to the corresponding mixed (Dirichlet--Neumann) case.
  \item[(ii)] Consider the limit $h \to \infty$ (very strong Newton cooling at $x=L$). Show formally that the Green function tends to the Dirichlet--Dirichlet Green function on $(0,L)$, and explain the physical meaning of this limit.
\end{itemize}
Hint: For $h \to \infty$, factor out $h$ from numerator and denominator of your expression for $G(x,\xi)$ before taking the limit.
\end{problem}

% ===== Example 4: Mixed and Robin Boundary Conditions for One-Dimensional Problems (full solution) =====
\begin{problem}[Mixed and Robin Boundary Conditions for One-Dimensional Problems]
Consider the boundary value problem
\[
- u''(x) = f(x), \qquad 0 < x < L,
\]
with mixed boundary conditions
\[
u(0) = 0, \qquad u'(L) + h\,u(L) = 0,
\]
where $h \ge 0$ is a constant. 

(a) Construct the Green function $G(x,\xi)$ satisfying
\[
- \frac{\partial^2}{\partial x^2} G(x,\xi) = \delta(x - \xi), \quad 0 < x,\xi < L,
\]
together with
\[
G(0,\xi) = 0, \qquad G_x(L,\xi) + h\,G(L,\xi) = 0.
\]

(b) Use $G$ to derive an integral representation
\[
u(x) = \int_0^L G(x,\xi)\,f(\xi)\,d\xi
\]
for the solution $u$.

(c) Show that when $h=0$ your Green function reduces to the Green function for the Dirichlet--Neumann problem $u(0)=0$, $u'(L)=0$, and that in the limit $h \to \infty$ it converges to the Dirichlet--Dirichlet Green function on $(0,L)$.
\end{problem}

\begin{solution}
We are asked to construct the Green function for the one-dimensional operator
\[
L u := -u''(x)
\]
on the interval $(0,L)$, with a Dirichlet condition at the left endpoint and a Robin (Newton cooling) condition at the right endpoint. This is a simple but instructive example of how Green functions adjust to mixed boundary conditions and illustrates the flexibility of the Green function method for elliptic-type problems.

\medskip

\textbf{1. Homogeneous solutions and general structure of the Green function.}

We first analyze the associated homogeneous equation
\[
- u''(x) = 0 \quad \Longleftrightarrow \quad u''(x) = 0.
\]
Integrating twice shows that every solution has the form
\[
u(x) = a x + b,
\]
where $a$ and $b$ are constants.

For a fixed source point $\xi \in (0,L)$, the Green function $G(\cdot,\xi)$ satisfies
\[
- G''(x,\xi) = \delta(x - \xi), \qquad 0 < x < L,
\]
with boundary conditions
\[
G(0,\xi) = 0, \qquad G_x(L,\xi) + h\,G(L,\xi) = 0.
\]
Thus, for $x \neq \xi$, the right-hand side vanishes and $G(\cdot,\xi)$ solves the homogeneous equation, hence is linear on each side of $x = \xi$.

Accordingly, for each fixed $\xi$ we may write
\[
G(x,\xi) =
\begin{cases}
A(\xi)\,x + B(\xi), & 0 \le x < \xi,\\[0.5ex]
C(\xi)\,x + D(\xi), & \xi < x \le L,
\end{cases}
\]
for some coefficient functions $A(\xi)$, $B(\xi)$, $C(\xi)$, and $D(\xi)$ yet to be determined.

\medskip

\textbf{2. Boundary conditions in the $x$-variable.}

We next impose the boundary conditions.

\emph{At $x=0$}, we use $G(0,\xi) = 0$. Evaluating the left-hand piece at $x=0$ gives
\[
G(0,\xi) = A(\xi)\cdot 0 + B(\xi) = B(\xi),
\]
hence
\[
B(\xi) = 0.
\]
Therefore, on $(0,\xi)$ we have the simpler expression
\[
G(x,\xi) = A(\xi)\,x, \quad 0 \le x < \xi.
\]

\emph{At $x=L$}, we use the Robin boundary condition in the $x$-variable:
\[
G_x(L,\xi) + h\,G(L,\xi) = 0.
\]
On the right-hand interval $(\xi,L)$, we have
\[
G(x,\xi) = C(\xi)\,x + D(\xi), \qquad
G_x(x,\xi) = C(\xi).
\]
Evaluating at $x=L$ yields
\[
C(\xi) + h\big( C(\xi)\,L + D(\xi) \big) = 0,
\]
or equivalently,
\[
(1 + hL)\,C(\xi) + h\,D(\xi) = 0.
\]
This gives one linear relation between $C(\xi)$ and $D(\xi)$.

\medskip

\textbf{3. Matching conditions at the source point $x=\xi$.}

The defining equation $-G'' = \delta$ imposes two additional conditions at $x = \xi$.

\emph{Continuity of $G$.}  
The Green function is continuous at $x=\xi$:
\[
G(\xi^-,\xi) = G(\xi^+,\xi).
\]
Using the piecewise form,
\[
A(\xi)\,\xi = C(\xi)\,\xi + D(\xi),
\]
so
\[
D(\xi) = A(\xi)\,\xi - C(\xi)\,\xi = (A(\xi) - C(\xi))\,\xi.
\]

\emph{Jump in the derivative.}  
To obtain the jump in $G_x$, integrate the differential equation across a small interval about $\xi$:
\[
\int_{\xi-\varepsilon}^{\xi+\varepsilon} \big(- G''(x,\xi)\big)\,dx
= \int_{\xi-\varepsilon}^{\xi+\varepsilon} \delta(x-\xi)\,dx = 1.
\]
The left-hand side integrates to
\[
- \big[ G_x(x,\xi) \big]_{x=\xi-\varepsilon}^{x=\xi+\varepsilon}
= - \big( G_x(\xi^+,\xi) - G_x(\xi^-,\xi) \big).
\]
Taking the limit $\varepsilon \to 0^+$, we obtain
\[
- \big( G_x(\xi^+,\xi) - G_x(\xi^-,\xi) \big) = 1
\quad \Longrightarrow \quad
G_x(\xi^+,\xi) - G_x(\xi^-,\xi) = -1.
\]
In terms of the coefficients, this becomes
\[
C(\xi) - A(\xi) = -1.
\]

\medskip

\textbf{4. Solving for the coefficients.}

We now have four equations for the four unknowns:
\begin{align*}
&\text{(i)} & B(\xi) &= 0,\\
&\text{(ii)} & D(\xi) &= (A(\xi) - C(\xi))\,\xi,\\
&\text{(iii)} & C(\xi) - A(\xi) &= -1,\\
&\text{(iv)} & (1 + hL)\,C(\xi) + h\,D(\xi) &= 0.
\end{align*}
Using (iii), we can write $C(\xi) = A(\xi) - 1$. Substituting this into (ii) gives
\[
D(\xi) = \big(A(\xi) - (A(\xi)-1)\big)\,\xi = \xi.
\]
Thus $D(\xi)$ is simply $\xi$, independent of $A(\xi)$.

Substituting $C(\xi) = A(\xi) - 1$ and $D(\xi) = \xi$ into (iv) yields
\[
(1 + hL)\big(A(\xi) - 1\big) + h\,\xi = 0.
\]
Solving for $A(\xi)$,
\[
(1 + hL)\,A(\xi) - (1 + hL) + h\,\xi = 0,
\]
so
\[
A(\xi) = \frac{1 + hL - h\xi}{1 + hL}.
\]
Then
\[
C(\xi) = A(\xi) - 1
= \frac{1 + hL - h\xi}{1 + hL} - 1
= -\,\frac{h\xi}{1 + hL},
\]
and we already have $B(\xi) = 0$ and $D(\xi) = \xi$.

Therefore the Green function is
\[
G(x,\xi) =
\begin{cases}
A(\xi)\,x, & 0 \le x \le \xi,\\[0.5ex]
C(\xi)\,x + D(\xi), & \xi \le x \le L.
\end{cases}
\]
Substituting the expressions for $A(\xi)$, $C(\xi)$, and $D(\xi)$ gives the explicit form
\[
G(x,\xi) =
\begin{cases}
\dfrac{1 + hL - h\xi}{1 + hL}\,x, & 0 \le x \le \xi \le L,\\[1.2ex]
-\,\dfrac{h\xi}{1 + hL}\,x + \xi, & 0 \le \xi \le x \le L.
\end{cases}
\]

It is often convenient to write this in a more symmetric form. For $x \le \xi$,
\[
G(x,\xi) = x\,\frac{1 + hL - h\xi}{1 + hL}
= x\,\frac{1 + h(L - \xi)}{1 + hL},
\]
and for $x \ge \xi$,
\[
G(x,\xi) = \xi - \frac{h\xi}{1 + hL}\,x
= \xi\,\frac{1 + hL - h x}{1 + hL}
= \xi\,\frac{1 + h(L - x)}{1 + hL}.
\]
Thus a compact symmetric expression is
\[
G(x,\xi) =
\begin{cases}
\dfrac{x\big(1 + hL - h\xi\big)}
\[
G(x,\xi) =
\begin{cases}
\dfrac{x\big(1 + hL - h\xi\big)}{1 + hL}, & 0 \le x \le \xi \le L,\\[1.2ex]
\dfrac{\xi\big(1 + hL - h x\big)}{1 + hL}, & 0 \le \xi \le x \le L.
\end{cases}
\]
Equivalently,
\[
G(x,\xi) =
\begin{cases}
\dfrac{x\big[1 + h(L-\xi)\big]}{1 + hL}, & 0 \le x \le \xi \le L,\\[1.2ex]
\dfrac{\xi\big[1 + h(L-x)\big]}{1 + hL}, & 0 \le \xi \le x \le L,
\end{cases}
\]
and one easily checks that $G(x,\xi) = G(\xi,x)$ (symmetry of the Green function for this self-adjoint problem).

\medskip

\textbf{5. Integral representation for the solution.}

Given $f$, define
\[
u(x) := \int_0^L G(x,\xi)\,f(\xi)\,d\xi.
\]
Because $-G_{xx}(x,\xi) = \delta(x-\xi)$ in the sense of distributions, we obtain
\[
- u''(x)
= - \int_0^L G_{xx}(x,\xi)\,f(\xi)\,d\xi
= \int_0^L \delta(x-\xi)\,f(\xi)\,d\xi
= f(x),
\]
so $u$ solves $-u''=f$.

At the boundaries, the conditions hold because they are imposed pointwise on $G$:

- At $x=0$, $G(0,\xi)=0$ for all $\xi$, hence
  \[
  u(0) = \int_0^L G(0,\xi)\,f(\xi)\,d\xi = 0.
  \]
- At $x=L$, we have $G_x(L,\xi) + h\,G(L,\xi)=0$ for all $\xi$, so
  \[
  u'(L) + h\,u(L)
  = \int_0^L \big(G_x(L,\xi) + h\,G(L,\xi)\big)\,f(\xi)\,d\xi = 0.
  \]

Thus
\[
u(x) = \int_0^L G(x,\xi)\,f(\xi)\,d\xi
\]
is the desired Green representation of the solution.

\medskip

\textbf{6. Limiting cases in the Robin parameter $h$.}

\emph{(i) Case $h=0$ (Dirichlet--Neumann).}

Setting $h=0$ in the above formula,
\[
G(x,\xi) =
\begin{cases}
\dfrac{x(1 + 0)}{1 + 0} = x, & 0 \le x \le \xi \le L,\\[0.8ex]
\dfrac{\xi(1 + 0)}{1 + 0} = \xi, & 0 \le \xi \le x \le L.
\end{cases}
\]
So
\[
G(x,\xi) =
\begin{cases}
x, & x \le \xi,\\
\xi, & \xi \le x,
\end{cases}
\]
which is exactly the Green function for
\[
- u'' = f, \qquad u(0)=0,\quad u'(L)=0
\]
(the Dirichlet--Neumann problem).

\medskip

\emph{(ii) Limit $h \to \infty$ (Dirichlet--Dirichlet).}

For $h>0$, write
\[
G(x,\xi) =
\begin{cases}
\dfrac{x\big(1 + hL - h\xi\big)}{1 + hL}, & x \le \xi,\\[1.2ex]
\dfrac{\xi\big(1 + hL - h x\big)}{1 + hL}, & x \ge \xi.
\end{cases}
\]
Factor out $h$ from numerator and denominator. For $x \le \xi$,
\[
G(x,\xi)
= x\,\frac{1 + h(L-\xi)}{1 + hL}
= x\,\frac{h(L-\xi)\big(1 + \tfrac{1}{h(L-\xi)}\big)}{hL\big(1 + \tfrac{1}{hL}\big)}
\;\xrightarrow[h\to\infty]{}\; x\,\frac{L-\xi}{L}.
\]
Similarly, for $x \ge \xi$,
\[
G(x,\xi)
= \xi\,\frac{1 + h(L-x)}{1 + hL}
\xrightarrow[h\to\infty]{}\; \xi\,\frac{L-x}{L}.
\]
Therefore,
\[
\lim_{h\to\infty} G(x,\xi)
=
\begin{cases}
\dfrac{x(L-\xi)}{L}, & x \le \xi,\\[0.8ex]
\dfrac{\xi(L-x)}{L}, & x \ge \xi,
\end{cases}
\]
which is precisely the Green function for the Dirichlet--Dirichlet problem
\[
- u'' = f, \qquad u(0) = 0,\quad u(L) = 0.
\]

Physically, increasing $h$ strengthens the heat loss at $x=L$ (Newton cooling). In the limit $h\to\infty$, the endpoint is forced to remain at the ambient temperature (here taken as $0$), so the Robin boundary condition behaves like a Dirichlet condition $u(L)=0$, as reflected by the limiting Green function.

\end{solution}

% ===== Example 5: Green Functions, Resolvents, and Eigenfunction Expansions (inquiry-based) =====
\begin{problem}[Green Functions, Resolvents, and Eigenfunction Expansions]
In this problem we connect three viewpoints on solving elliptic boundary value problems: eigenfunction expansions, resolvents of self-adjoint operators, and Green functions. We work in a concrete one-dimensional setting, but the ideas extend to general self-adjoint elliptic operators on bounded domains. The main goal is to see that the Green function is nothing but the integral kernel of the inverse operator, written in an eigenfunction basis.

Consider the Dirichlet boundary value problem on the interval $(0,\pi)$:
\[
\begin{cases}
- u''(x) = f(x), & 0 < x < \pi,\\[4pt]
u(0) = 0,\quad u(\pi) = 0,
\end{cases}
\]
where $f \in L^2(0,\pi)$ is given. Let $L$ denote the differential operator
\[
L u := -u'', \qquad \mathcal{D}(L) := H^2(0,\pi) \cap H_0^1(0,\pi),
\]
viewed as an unbounded operator on the Hilbert space $L^2(0,\pi)$.

\medskip

(a) First, recall the spectral data of $L$.

\quad(i) Solve the eigenvalue problem
\[
- \phi''(x) = \lambda \,\phi(x), \qquad \phi(0) = \phi(\pi) = 0,
\]
and find all eigenvalues $\lambda_n$ and corresponding eigenfunctions $\phi_n$.

\quad(ii) Normalize your eigenfunctions to obtain an orthonormal family $\{\phi_n\}_{n\ge 1}$ in $L^2(0,\pi)$. State the orthonormality relation explicitly as an integral.

\quad(iii) Without giving a full proof, recall (or briefly justify) why the family $\{\phi_n\}_{n\ge 1}$ is complete in $L^2(0,\pi)$; that is, every $f \in L^2(0,\pi)$ admits a sine series expansion in this basis.

\medskip

(b) Use the eigenfunctions to solve the boundary value problem.

\quad(i) Let $f \in L^2(0,\pi)$. Using part (a), write $f$ as a Fourier sine series
\[
f(x) = \sum_{n=1}^{\infty} f_n \,\phi_n(x),
\]
and express the coefficients $f_n$ in terms of $f$ and $\phi_n$.

\quad(ii) Look for a solution of $L u = f$ in the form
\[
u(x) = \sum_{n=1}^{\infty} u_n \,\phi_n(x).
\]
Insert this series into the equation $-u'' = f$ and use the fact that $L\phi_n = \lambda_n \phi_n$ to derive an explicit formula for $u_n$ in terms of $f_n$ and $\lambda_n$.

\quad(iii) Conclude that the (formal) solution can be written as
\[
u(x) = \sum_{n=1}^{\infty} \frac{f_n}{\lambda_n}\,\phi_n(x)
      = \sum_{n=1}^{\infty} \frac{\langle f,\phi_n\rangle}{\lambda_n}\,\phi_n(x),
\]
where $\langle \cdot,\cdot\rangle$ denotes the $L^2(0,\pi)$ inner product. How does this formula show that the operator $L$ is invertible and that its inverse $L^{-1}$ is diagonal in the eigenbasis?

\medskip

(c) Now we introduce the Green function as the kernel of the inverse operator.

Suppose there exists a function $G : (0,\pi)\times(0,\pi) \to \mathbb{R}$ (the Green function) with the property that, for each $f \in L^2(0,\pi)$, the function
\[
u(x) := \int_{0}^{\pi} G(x,y)\,f(y)\,dy
\]
solves $L u = f$ with $u(0)=u(\pi)=0$.

\quad(i) Motivated by the eigenfunction expansion in part (b), define
\[
G(x,y) := \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(y)}{\lambda_n}.
\]
Assuming that this series converges in a suitable sense, compute
\[
\int_{0}^{\pi} G(x,y)\,f(y)\,dy
\]
by interchanging sum and integral, and using orthonormality of $\{\phi_n\}$. Show that you recover exactly the series for $u(x)$ obtained in part (b).

\emph{Hint:} Write $f(y)$ as $\sum f_n \phi_n(y)$, and use that
\[
\int_{0}^{\pi} \phi_n(y)\,\phi_m(y)\,dy = \delta_{nm}.
\]

\quad(ii) Explain why this shows that $G$ is the \emph{integral kernel} of the inverse operator
\[
L^{-1} : L^2(0,\pi) \to \mathcal{D}(L) \subset L^2(0,\pi).
\]
In other words, express $L^{-1}f$ both as a series in the eigenbasis and as an integral operator with kernel $G$, and compare.

\medskip

(d) Interpreting the Green function as a solution with a point source.

Fix a point $x \in (0,\pi)$, and consider the function of $y$
\[
v_x(y) := G(x,y).
\]

\quad(i) Using the eigenfunction expansion of $G(x,y)$, compute $L_y v_x(y)$, where $L_y$ means that $L$ acts on the $y$-variable. Show that, for any eigenfunction $\phi_n$,
\[
\int_{0}^{\pi} (-v_x''(y))\,\phi_n(y)\,dy = \phi_n(x).
\]

\emph{Hint:} Use that $- \phi_n''(y) = \lambda_n \phi_n(y)$ and integrate term-by-term.

\quad(ii) Argue that the identity in part (d)(i) implies
\[
- \frac{d^2}{dy^2} G(x,y) = \delta_x(y)
\]
in the sense of distributions, where $\delta_x$ is the Dirac mass at $y=x$, and that $G(x,0)=G(x,\pi)=0$. Explain how this connects with the usual definition of a Green function as a solution of $L_y G(x,y) = \delta_x(y)$ with homogeneous boundary conditions.

\medskip

(e) Extensions and variations.

\quad(i) Suppose we replace $L$ by a more general self-adjoint Sturm--Liouville operator
\[
A u := -u''(x) + q(x) u(x),
\]
with $u(0)=u(\pi)=0$ and a smooth potential $q(x)$. Assume $A$ has a discrete set of eigenvalues $\mu_1 \le \mu_2 \le \cdots$ with corresponding orthonormal eigenfunctions $\psi_1,\psi_2,\dots$ forming a basis of $L^2(0,\pi)$. Based on what you have done above, what would be the natural candidate for the Green function $G_A(x,y)$ of $A$? Write down a formal series for $G_A$ in terms of the eigenpairs $(\mu_n,\psi_n)$.

\quad(ii) In our example, all eigenvalues $\lambda_n$ of $L$ are strictly positive. What do you expect to change if the operator has a zero eigenvalue? As a concrete case, consider the Neumann problem
\[
- u''(x) = f(x), \qquad u'(0)=u'(\pi)=0.
\]
What is the eigenfunction associated with the eigenvalue $0$, and what compatibility condition on $f$ is needed to invert the operator? How would the presence of $\lambda=0$ affect the Green function expansion?

\end{problem}

% ===== Example 5: Green Functions, Resolvents, and Eigenfunction Expansions (full solution) =====
\begin{problem}[Green Functions, Resolvents, and Eigenfunction Expansions]
Consider the Dirichlet problem on $(0,\pi)$
\[
\begin{cases}
- u''(x) = f(x), & 0 < x < \pi,\\[4pt]
u(0) = u(\pi) = 0,
\end{cases}
\]
with $f \in L^2(0,\pi)$. Let $L u := -u''$ with domain $\mathcal{D}(L)=H^2(0,\pi)\cap H_0^1(0,\pi)$, viewed as a self-adjoint operator on $L^2(0,\pi)$.

(a) Find the eigenvalues $\lambda_n$ and an orthonormal basis of eigenfunctions $\{\phi_n\}_{n\ge 1}$ of $L$ in $L^2(0,\pi)$.

(b) Show that for each $f \in L^2(0,\pi)$, the unique solution $u$ of $Lu=f$ with $u(0)=u(\pi)=0$ is given by the convergent series
\[
u(x) = \sum_{n=1}^{\infty} \frac{\langle f,\phi_n\rangle_{L^2}}{\lambda_n}\,\phi_n(x).
\]

(c) Define the function
\[
G(x,y) := \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(y)}{\lambda_n}, \qquad 0<x,y<\pi.
\]
Show that, for every $f \in L^2(0,\pi)$,
\[
u(x) := \int_{0}^{\pi} G(x,y)\,f(y)\,dy
\]
equals the series in part (b), and hence $G$ is the integral kernel of $L^{-1}$.

(d) For fixed $x$, regard $G(x,\cdot)$ as a function of $y$. Show that
\[
- \frac{d^2}{dy^2} G(x,y) = \delta_x(y)
\]
in the sense of distributions, with $G(x,0)=G(x,\pi)=0$. Thus $G$ is the Green function of $L$ with Dirichlet boundary conditions.

Briefly comment on how this eigenfunction expansion of the Green function illustrates the general relationship between Green functions, resolvents, and eigenfunction expansions for self-adjoint elliptic operators on bounded domains.
\end{problem}

\begin{solution}
We solve the boundary value problem by exploiting the spectral properties of the self-adjoint operator $L$ and then interpret the Green function as the kernel of $L^{-1}$ written in an eigenfunction basis. This connects the integral representation of solutions with the spectral decomposition of $L$.

\medskip

\textbf{(a) Eigenvalues and eigenfunctions of $L$.}

We solve the eigenvalue problem
\[
- \phi''(x) = \lambda\,\phi(x), \qquad \phi(0)=\phi(\pi)=0.
\]
There are three classical cases.

\emph{Case 1: $\lambda = 0$.} Then $\phi''(x)=0$, so $\phi(x)=ax+b$. The boundary conditions $\phi(0)=0$ and $\phi(\pi)=0$ give $b=0$ and $a\pi=0$, so $a=0$. Hence $\phi\equiv0$ is the only solution, which we discard. Thus $\lambda=0$ is not an eigenvalue for Dirichlet boundary conditions.

\emph{Case 2: $\lambda < 0$.} Write $\lambda = -\mu^2$ with $\mu>0$. The equation becomes $\phi''=\mu^2\phi$, whose general solution is $\phi(x)=A e^{\mu x}+B e^{-\mu x}$. The boundary conditions give
\[
\phi(0)=A+B=0,\qquad
\phi(\pi) = A e^{\mu\pi}+B e^{-\mu\pi}=0.
\]
From $A=-B$, the second condition becomes $A(e^{\mu\pi}-e^{-\mu\pi})=0$, so $A=0$ and again $\phi\equiv0$. Thus no negative eigenvalues exist.

\emph{Case 3: $\lambda > 0$.} Write $\lambda = \alpha^2$ with $\alpha>0$. The equation becomes $\phi''=-\alpha^2\phi$, with general solution $\phi(x)=A\cos(\alpha x)+B\sin(\alpha x)$. The boundary conditions yield
\[
\phi(0)=A=0,\qquad
\phi(\pi) = B \sin(\alpha\pi) = 0.
\]
For a nontrivial solution we require $B\neq 0$ and hence $\sin(\alpha\pi)=0$, which implies $\alpha\pi = n\pi$ for some integer $n\ge 1$. Thus $\alpha=n$ and
\[
\lambda_n = n^2,\qquad
\phi_n(x) = B \sin(nx), \quad n=1,2,\dots.
\]
We normalize these eigenfunctions in $L^2(0,\pi)$. The $L^2$ norm is
\[
\int_{0}^{\pi} \sin^2(nx)\,dx = \frac{\pi}{2},
\]
independent of $n$. Choosing $B = \sqrt{2/\pi}$, we obtain an orthonormal family
\[
\phi_n(x) = \sqrt{\frac{2}{\pi}}\sin(nx), \qquad n=1,2,\dots,
\]
satisfying
\[
\int_{0}^{\pi} \phi_n(x)\,\phi_m(x)\,dx = \delta_{nm}.
\]

It is a standard result of Fourier analysis that the set $\{\phi_n\}_{n\ge1}$ is complete in $L^2(0,\pi)$: every $f\in L^2(0,\pi)$ admits a Fourier sine series expansion in this orthonormal basis. Equivalently, the closure of the span of $\{\phi_n\}$ is all of $L^2(0,\pi)$.

\medskip

\textbf{(b) Solving $Lu=f$ by eigenfunction expansion.}

Let $f\in L^2(0,\pi)$. Since $\{\phi_n\}$ is an orthonormal basis, $f$ has the expansion
\[
f(x) = \sum_{n=1}^{\infty} f_n\,\phi_n(x),
\quad\text{where}\quad
f_n = \langle f,\phi_n\rangle_{L^2} = \int_{0}^{\pi} f(y)\,\phi_n(y)\,dy.
\]

We look for a solution $u$ of
\[
- u''(x) = f(x),\qquad u(0)=u(\pi)=0,
\]
in the form
\[
u(x) = \sum_{n=1}^{\infty} u_n\,\phi_n(x)
\]
for some coefficients $\{u_n\}$ to be determined.

Because $L\phi_n = -\phi_n'' = \lambda_n \phi_n$ with $\lambda_n=n^2$, linearity gives
\[
L u = -u'' = \sum_{n=1}^{\infty} u_n\,L\phi_n
           = \sum_{n=1}^{\infty} u_n\,\lambda_n\,\phi_n.
\]
We want $Lu=f = \sum f_n\phi_n$. Equality of two $L^2$-convergent expansions in an orthonormal basis implies equality of coefficients:
\[
u_n\,\lambda_n = f_n \quad\text{for all }n.
\]
Since $\lambda_n>0$, we obtain
\[
u_n = \frac{f_n}{\lambda_n} = \frac{\langle f,\phi_n\rangle_{L^2}}{\lambda_n}.
\]
Thus the solution is
\[
u(x) = \sum_{n=1}^{\infty} \frac{f_n}{\lambda_n}\,\phi_n(x)
     = \sum_{n=1}^{\infty} \frac{\langle f,\phi_n\rangle_{L^2}}{\lambda_n}\,\phi_n(x).
\]

This series converges in the appropriate Sobolev space (indeed in $H_0^1(0,\pi)\cap H^2(0,\pi)$), and $u$ is the unique weak (and in fact classical) solution of the boundary value problem. From the operator viewpoint, this shows that $L$ is invertible from $\mathcal{D}(L)$ onto $L^2(0,\pi)$, and that its inverse $L^{-1}$ acts diagonally on the eigenbasis:
\[
L^{-1}\phi_n = \frac{1}{\lambda_n}\,\phi_n.
\]
Hence $L^{-1}$ is precisely the operator that multiplies each Fourier sine coefficient by $1/\lambda_n$.

\medskip

\textbf{(c) The Green function as the kernel of $L^{-1}$.}

We now define
\[
G(x,y) := \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(y)}{\lambda_n},
\qquad 0<x,y<\pi.
\]
The series converges in $L^2((0,\pi)\times(0,\pi))$, and in fact defines a continuous function away from the diagonal $x=y$, but for our purposes the $L^2$ convergence is sufficient.

Let $f\in L^2(0,\pi)$, and define
\[
u(x) := \int_{0}^{\pi} G(x,y)\,f(y)\,dy.
\]
We claim that this integral expression exactly reproduces the series from part (b). Formally,
\[
\begin{aligned}
u(x)
&= \int_{0}^{\pi} \left( \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(y)}{\lambda_n} \right) f(y)\,dy \\
&= \sum_{n=1}^{\infty} \frac{\phi_n(x)}{\lambda_n} \int_{0}^{\pi} \phi_n(y)\,f(y)\,dy
   \quad\text{(interchanging sum and integral)}\\
&= \sum_{n=1}^{\infty} \frac{\langle f,\phi_n\rangle_{L^2}}{\lambda_n}\,\phi_n(x).
\end{aligned}
\]
The interchange of sum and integral is justified by standard theorems (e.g.\ Fubini/Tonelli) using the square-summability of the coefficients and the boundedness of the eigenfunctions.

The final series is precisely the expression for $u$ found in part (b). Therefore, for every $f\in L^2(0,\pi)$,
\[
(L^{-1} f)(x)
= \sum_{n=1}^{\infty} \frac{\langle f,\phi_n\rangle}{\lambda_n}\,\phi_n(x)
= \int_{0}^{\pi} G(x,y)\,f(y)\,dy.
\]
In other words, $G$ is the \emph{integral kernel} of the inverse operator $L^{-1}$. This is exactly the resolvent operator at zero, since here we are inverting $L$ itself.

\medskip

\textbf{(d) $G$ solves $L_y G(x,y) = \delta_x(y)$.}

We now check that $G$ satisfies the defining equation of a Green function. Fix $x\in(0,\pi)$, and consider
\[
v_x(y) := G(x,y) = \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(y)}{\lambda_n},
\]
as a function of $y$.

First, note that for each $n$, $\phi_n(0)=\phi_n(\pi)=0$, so
\[
v_x(0) = \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(0)}{\lambda_n} = 0,
\qquad
v_x(\pi) = \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(\pi)}{\lambda_n} = 0.
\]
Thus $G(x,0)=G(x,\pi)=0$ for all $x$.

Next, we compute $L_y v_x(y)$ in the sense of distributions. Formally differentiating term-by-term with respect to $y$, we have
\[
- \frac{d^2}{dy^2} v_x(y)
= - \sum_{n=1}^{\infty} \frac{\phi_n(x)}{\lambda_n} \phi_n''(y)
= \sum_{n=1}^{\infty} \frac{\phi_n(x)}{\lambda_n} (\lambda_n \phi_n(y))
= \sum_{n=1}^{\infty} \phi_n(x)\,\phi_n(y).
\]
To interpret this properly, we test against an arbitrary function $\psi\in L^2(0,\pi)$. Expanding $\psi = \sum_{m=1}^{\infty} \psi_m \phi_m$ with $\psi_m = \langle \psi,\phi_m\rangle$, we compute
\[
\begin{aligned}
\int_{0}^{\pi} \left( - v_x''(y) \right) \psi(y)\,dy
&= \int_{0}^{\pi} \left( \sum_{n=1}^{\infty} \phi_n(x)\,\phi_n(y) \right) \psi(y)\,dy \\
&= \sum_{n=1}^{\infty} \phi_n(x) \int_{0}^{\pi} \phi_n(y)\,\psi(y)\,dy \\
&= \sum_{n=1}^{\infty} \phi_n(x)\,\psi_n.
\end{aligned}
\]
On the other hand, since $\psi = \sum \psi_n \phi_n$, we have
\[
\psi(x) = \sum_{n=1}^{\infty} \psi_n\,\phi_n(x).
\]
Therefore,
\[
\int_{0}^{\pi} \left(- v_x''(y)\right)\psi(y)\,dy = \psi(x)
\]
for all $\psi \in L^2(0,\pi)$.

But $\psi \mapsto \psi(x)$ is exactly the action of the Dirac distribution $\delta_x$ at $y=x$. Thus, in distributional notation,
\[
- \frac{d^2}{dy^2} G(x,y) = \delta_x(y)
\]
with homogeneous Dirichlet boundary conditions in $y$. This is precisely the usual defining property of the Green function associated with the operator $L$:
\[
L_y G(x,y) = \delta_x(y),\qquad G(x,0)=G(x,\pi)=0.
\]

Combining this with part (c), we see both characterizations of the Green function:
\begin{itemize}
  \item As the solution (in the $y$-variable) of $L_y G(x,y) = \delta_x(y)$ with homogeneous boundary conditions, and
  \item As the integral kernel of the inverse operator $L^{-1}$, given spectrally by
  \[
  G(x,y) = \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(y)}{\lambda_n}.
  \]
\end{itemize}

\medskip

\textbf{Connection to the general theory.}

This example illustrates the main ideas of the method of Green functions for elliptic PDEs on bounded domains:

\begin{enumerate}
  \item For a self-adjoint, positive, elliptic operator $L$ on $L^2(\Omega)$ with suitable boundary conditions, the spectrum consists of a discrete sequence of eigenvalues $0<\lambda_1\le\lambda_2\le\cdots\to\infty$, with corresponding orthonormal eigenfunctions $\{\phi_n\}$ forming a basis of $L^2(\Omega)$.
  \item The inverse operator $L^{-1}$ (the resolvent at $0$) is well-defined and bounded on $L^2(\Omega)$, and acts diagonally on the eigenbasis by $L^{-1}\phi_n = \lambda_n^{-1}\phi_n$.
  \item The Green function is the integral kernel of $L^{-1}$ and admits the spectral expansion
  \[
  G(x,y) = \sum_{n=1}^{\infty} \frac{\phi_n(x)\,\phi_n(y)}{\lambda_n}.
  \]
  4. Solutions of the elliptic equation $Lu=f$ with homogeneous boundary conditions can be represented equivalently as eigenfunction series
  \[
  u(x) = \sum_{n=1}^{\infty} \frac{\langle f,\phi_n\rangle}{\lambda_n}\,\phi_n(x)
  \]
  or as a Green function integral
  \[
  u(x) = \int_{\Omega} G(x,y)\,f(y)\,dy.
  \]
\end{enumerate}

Thus, the Green function representation and the eigenfunction expansion are two faces of the same underlying spectral decomposition of a self-adjoint elliptic operator. In higher dimensions and for more general elliptic operators, the same structure persists: the Green function is the kernel of the resolvent, and its expansion in eigenfunctions reveals explicitly how the geometry of the domain and the boundary conditions influence the solution.
\end{solution}

\section{Waves in a Homogeneous Medium: Hyperbolic PDE (*)}
% --- Narrative plan (auto-generated) ---
% In this section we study wave phenomena in media that are, at least to a first approximation, spatially uniform. The governing equations are hyperbolic partial differential equations, with the one-dimensional wave equation as the central model. We will see how such equations encode the finite speed of propagation of signals, the superposition of traveling waves, and the emergence of characteristic lines along which information is transported.
%
% This material matters across applied mathematics, from modeling vibrations of strings and membranes, to understanding sound waves, electromagnetic waves, and even certain aspects of fluid flow and elasticity. Hyperbolic equations provide a complementary viewpoint to parabolic diffusion and elliptic equilibrium problems, and learning to solve them builds fluency with separation of variables, Fourier series, and transform methods. Along the way, we will connect to ordinary differential equations through eigenvalue problems, to Fourier analysis through series and integrals representing wave fields, and to more advanced topics such as distributions and complex analysis when we interpret fundamental solutions and apply contour integration to evaluate certain integrals.
%
% Our approach emphasizes starting from simple, concrete boundary-value problems and gradually introducing more sophisticated tools. We will analyze waves on finite and infinite domains, explore how initial and boundary data shape the resulting motion, and use the method of characteristics to understand how disturbances propagate. The goal is not only to learn solution formulas, but also to build an intuitive picture of how hyperbolic PDEs behave and how they fit into the broader framework of dynamical systems and applied analysis.

% ===== Example 1: Transverse Vibrations of a Stretched String (inquiry-based) =====
\begin{problem}[Transverse Vibrations of a Stretched String]
A thin, flexible string of length $L$ is stretched tightly between two fixed supports at $x=0$ and $x=L$. The string is displaced from its equilibrium position into some initial shape $f(x)$ and then released from rest. Assuming small transverse vibrations and a homogeneous string with constant wave speed $c>0$, the vertical displacement $u(x,t)$ of the string satisfies the one-dimensional wave equation. In this problem we explore how to solve this model using separation of variables and Fourier sine series, and how the notion of normal modes and standing waves naturally appears.

We consider the initial--boundary value problem
\[
\begin{cases}
u_{tt}(x,t) = c^{2} u_{xx}(x,t), & 0<x<L,\ t>0,\\[0.5ex]
u(0,t) = 0,\quad u(L,t)=0, & t\ge 0,\\[0.5ex]
u(x,0) = f(x),\quad u_t(x,0)=0, & 0\le x\le L.
\end{cases}
\]

\begin{enumerate}[(a)]
\item (Setting up separation of variables.) Suppose that $u$ can be written as a product of a purely spatial factor and a purely temporal factor,
\[
u(x,t) = X(x)\,T(t),
\]
with $X$ not identically zero and $T$ not identically zero. Substitute this form into the wave equation and show that
\[
\frac{T''(t)}{c^{2} T(t)} = \frac{X''(x)}{X(x)} = -\lambda
\]
for some constant $\lambda$ (the separation constant).

\emph{Hint:} Rearrange your expression so that all terms depending on $x$ are on one side and all terms depending on $t$ are on the other, then argue that both sides must be equal to the same constant.

\item (The spatial eigenvalue problem.) Focus on the spatial part
\[
X''(x) + \lambda X(x) = 0,\qquad X(0)=0,\quad X(L)=0.
\]
Analyze the possible signs of $\lambda$:
\[
\lambda<0,\quad \lambda=0,\quad \lambda>0.
\]
For each case, solve the ordinary differential equation and check whether there are nontrivial solutions satisfying the boundary conditions.

What values of $\lambda$ admit nontrivial solutions, and what are the corresponding eigenfunctions $X_n(x)$?

\emph{Hint:} You should find a discrete set of eigenvalues $\lambda_n$ and corresponding eigenfunctions that look like sines. Be explicit about the allowed values of $n$.

\item (Time dependence and standing waves.) For each admissible separation constant $\lambda_n$ from part (b), the time factor $T_n(t)$ satisfies
\[
T_n''(t) + c^{2}\lambda_n T_n(t) = 0.
\]
Solve this equation and write down the corresponding separated solutions $u_n(x,t) = X_n(x) T_n(t)$.

Show that each $u_n$ can be written in the form
\[
u_n(x,t) = \left(A_n\cos(\omega_n t) + B_n\sin(\omega_n t)\right)\sin\!\left(\frac{n\pi x}{L}\right),
\]
for some constants $A_n$ and $B_n$, and determine the frequency $\omega_n$ in terms of $c$, $n$, and $L$.

\emph{Hint:} Relate $\omega_n$ to the square root of the separation constant $\lambda_n$.

\item (Superposition and the initial displacement.) We now use linearity of the wave equation to build more general solutions from the basic separated solutions $u_n$.

Assuming that the solution can be written as a superposition of such modes,
\[
u(x,t) = \sum_{n=1}^{\infty} \left(A_n\cos(\omega_n t) + B_n\sin(\omega_n t)\right)\sin\!\left(\frac{n\pi x}{L}\right),
\]
use the initial conditions $u(x,0)=f(x)$ and $u_t(x,0)=0$ to determine conditions on the coefficients $A_n$ and $B_n$.

\begin{enumerate}[(i)]
\item Show that the zero initial velocity forces $B_n=0$ for all $n$.
\item Show that the initial displacement $f(x)$ can be expanded in a \emph{Fourier sine series}
\[
f(x) = \sum_{n=1}^{\infty} A_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
and derive a formula for $A_n$ in terms of $f$.

\emph{Hint:} Use the orthogonality of the functions $\sin\!\left(\frac{n\pi x}{L}\right)$ on the interval $[0,L]$:
\[
\int_0^L \sin\!\left(\frac{m\pi x}{L}\right)\sin\!\left(\frac{n\pi x}{L}\right)\,dx =
\begin{cases}
0, & m\ne n,\\[0.5ex]
\displaystyle \frac{L}{2}, & m=n.
\end{cases}
\]
\end{enumerate}

Assemble your results into a final formula for $u(x,t)$ in terms of $f(x)$ and the wave speed $c$.

\item (Extensions and variations.)
\begin{enumerate}[(i)]
\item Suppose instead that the string is released with \emph{zero displacement} and a prescribed initial velocity $u_t(x,0)=g(x)$, while still having $u(0,t)=u(L,t)=0$. How would your general solution change, and what integrals would appear in the formulas for the coefficients?

\item Qualitatively, how do the frequencies $\omega_n$ depend on $n$? What does this tell you about which modes oscillate faster or slower? Explain how this leads to the interpretation of $\sin\!\left(\frac{n\pi x}{L}\right)$ as the $n$-th normal mode or standing wave of the string.

\emph{Hint:} You may wish to sketch the first few eigenfunctions and discuss the number of interior nodes (points where $u=0$) for each mode.
\end{enumerate}
\end{enumerate}
\end{problem}

% ===== Example 1: Transverse Vibrations of a Stretched String (full solution) =====
\begin{problem}[Transverse Vibrations of a Stretched String]
Consider a taut string of length $L$ with fixed endpoints at $x=0$ and $x=L$. Let $u(x,t)$ denote the transverse displacement of the string at position $x$ and time $t$. Assume that $u$ satisfies the one-dimensional wave equation with constant wave speed $c>0$,
\[
u_{tt} = c^{2} u_{xx},\quad 0<x<L,\ t>0,
\]
together with homogeneous Dirichlet boundary conditions
\[
u(0,t)=0,\quad u(L,t)=0,\quad t\ge 0,
\]
and initial conditions corresponding to an initial shape $f$ and zero initial velocity,
\[
u(x,0)=f(x),\quad u_t(x,0)=0,\quad 0\le x\le L.
\]
Solve this initial--boundary value problem by separation of variables, and show that the solution can be written as a Fourier sine series in the normal modes of the string. Give explicit formulas for the time-dependent solution $u(x,t)$ and for the Fourier coefficients in terms of $f$.
\end{problem}

\begin{solution}
We solve the wave equation with fixed-end boundary conditions using separation of variables and Fourier series. This example illustrates how a hyperbolic partial differential equation on a bounded interval leads to a discrete set of eigenfrequencies and standing wave modes.

\medskip

\noindent\textbf{1. Separation of variables.}
We look for nontrivial solutions of the form
\[
u(x,t) = X(x)\,T(t),
\]
where $X$ depends only on $x$ and $T$ depends only on $t$. Substituting this product into the wave equation $u_{tt}=c^{2}u_{xx}$ gives
\[
X(x)\,T''(t) = c^{2} X''(x)\,T(t).
\]
We assume $X$ and $T$ are not identically zero, so we may divide both sides by $c^{2}X(x)T(t)$ to obtain
\[
\frac{T''(t)}{c^{2}T(t)} = \frac{X''(x)}{X(x)}.
\]
The left-hand side depends only on $t$ and the right-hand side depends only on $x$. Therefore both sides must equal a constant, which we denote by $-\lambda$. This yields the separated ordinary differential equations
\[
X''(x) + \lambda X(x) = 0,\qquad T''(t) + c^{2}\lambda T(t) = 0.
\]
The boundary conditions $u(0,t)=u(L,t)=0$ translate into
\[
X(0)T(t)=0,\quad X(L)T(t)=0\quad\text{for all }t.
\]
For a nontrivial time factor $T(t)$, this implies
\[
X(0)=0,\quad X(L)=0.
\]
Thus we are led first to study the spatial eigenvalue problem
\[
X''(x) + \lambda X(x) = 0,\qquad X(0)=0,\quad X(L)=0.
\]

\medskip

\noindent\textbf{2. Spatial eigenvalues and eigenfunctions.}
We analyze the possible signs of $\lambda$.

\emph{Case 1: $\lambda<0$.} Write $\lambda = -\mu^{2}$ with $\mu>0$. Then the equation becomes
\[
X''(x) - \mu^{2}X(x) = 0,
\]
whose general solution is
\[
X(x) = C_{1}e^{\mu x} + C_{2}e^{-\mu x}.
\]
Imposing $X(0)=0$ gives $C_{1}+C_{2}=0$, so $C_{2}=-C_{1}$ and $X(x)=C_{1}(e^{\mu x}-e^{-\mu x})=2C_{1}\sinh(\mu x)$. Then $X(L)=0$ implies $\sinh(\mu L)=0$, which forces $\mu L=0$, hence $\mu=0$, contradicting $\mu>0$. Therefore there are no nontrivial solutions for $\lambda<0$.

\emph{Case 2: $\lambda=0$.} The equation is $X''(x)=0$, with general solution $X(x)=C_{1}+C_{2}x$. The boundary condition $X(0)=0$ gives $C_{1}=0$, so $X(x)=C_{2}x$. Then $X(L)=0$ implies $C_{2}L=0$, hence $C_{2}=0$. Thus only the trivial solution exists for $\lambda=0$.

\emph{Case 3: $\lambda>0$.} Write $\lambda=\mu^{2}$ with $\mu>0$. Then
\[
X''(x) + \mu^{2}X(x) = 0,
\]
whose general solution is
\[
X(x) = C_{1}\cos(\mu x) + C_{2}\sin(\mu x).
\]
The condition $X(0)=0$ implies $C_{1}=0$, so $X(x)=C_{2}\sin(\mu x)$. Imposing $X(L)=0$ gives $C_{2}\sin(\mu L)=0$. For a nontrivial solution, $C_{2}\ne 0$, so we require
\[
\sin(\mu L)=0.
\]
This holds if and only if
\[
\mu L = n\pi,\quad n=1,2,3,\dots.
\]
Thus
\[
\mu_n = \frac{n\pi}{L},\qquad \lambda_n = \mu_n^{2} = \left(\frac{n\pi}{L}\right)^{2}.
\]
For each positive integer $n$, we obtain an eigenfunction
\[
X_n(x) = \sin\!\left(\frac{n\pi x}{L}\right),
\]
which is nontrivial and satisfies $X_n(0)=X_n(L)=0$. There is no admissible eigenvalue for $n=0$ because $X_0(x)=\sin(0)=0$ is the trivial solution.

The collection $\{X_n\}_{n=1}^{\infty}$ forms a countable set of spatial eigenfunctions, each corresponding to an eigenvalue $\lambda_n$.

\medskip

\noindent\textbf{3. Time dependence and standing waves.}
For each eigenvalue $\lambda_n$, the corresponding time factor $T_n(t)$ must satisfy
\[
T_n''(t) + c^{2}\lambda_n T_n(t) = 0,
\]
that is,
\[
T_n''(t) + c^{2}\left(\frac{n\pi}{L}\right)^{2} T_n(t) = 0.
\]
This is the equation of a simple harmonic oscillator with angular frequency
\[
\omega_n = c\frac{n\pi}{L}.
\]
Its general solution is
\[
T_n(t) = A_n\cos(\omega_n t) + B_n\sin(\omega_n t),
\]
where $A_n$ and $B_n$ are constants.

The separated solution associated with the $n$-th eigenvalue is therefore
\[
u_n(x,t) = X_n(x)T_n(t)
= \left(A_n\cos(\omega_n t) + B_n\sin(\omega_n t)\right)\sin\!\left(\frac{n\pi x}{L}\right).
\]
Each such $u_n$ represents a \emph{standing wave}: the spatial profile $\sin\!\left(\frac{n\pi x}{L}\right)$ remains fixed in space, while its amplitude oscillates sinusoidally in time with frequency $\omega_n$. The integer $n$ counts the number of half-wavelengths that fit into the interval $[0,L]$.

\medskip

\noindent\textbf{4. Superposition and imposition of initial conditions.}
Because the wave equation is linear and homogeneous, any finite linear combination of the separated solutions $u_n$ is again a solution. Under appropriate regularity assumptions on $f$, we may consider an infinite superposition and seek a solution of the form
\[
u(x,t) = \sum_{n=1}^{\infty}
\left(A_n\cos(\omega_n t) + B_n\sin(\omega_n t)\right)\sin\!\left(\frac{n\pi x}{L}\right),
\]
where $\omega_n = c n\pi/L$. By construction, this satisfies the wave equation and the boundary conditions for all choices of coefficients $\{A_n,B_n\}$.

We now determine the coefficients using the initial conditions.

\emph{First initial condition: zero initial velocity.}
Differentiating $u$ with respect to $t$ gives
\[
u_t(x,t) = \sum_{n=1}^{\infty}
\left(-A_n\omega_n\sin(\omega_n t) + B_n\omega_n\cos(\omega_n t)\right)\sin\!\left(\frac{n\pi x}{L}\right).
\]
At $t=0$, this reduces to
\[
u_t(x,0) = \sum_{n=1}^{\infty} B_n\omega_n \sin\!\left(\frac{n\pi x}{L}\right).
\]
The initial condition $u_t(x,0)=0$ for $0\le x\le L$ therefore implies
\[
\sum_{n=1}^{\infty} B_n\omega_n \sin\!\left(\frac{n\pi x}{L}\right) = 0\quad\text{for all }x\in[0,L].
\]
Since the sine functions $\{\sin(n\pi x/L)\}_{n=1}^{\infty}$ are orthogonal and form a complete system in a suitable function space on $[0,L]$, the only way this sum can vanish identically is if
\[
B_n = 0\quad\text{for all }n\ge 1.
\]
Thus the solution simplifies to
\[
u(x,t) = \sum_{n=1}^{\infty} A_n\cos(\omega_n t)\,\sin\!\left(\frac{n\pi x}{L}\right).
\]

\emph{Second initial condition: prescribed initial displacement.}
At time $t=0$ we have
\[
u(x,0) = \sum_{n=1}^{\infty} A_n\cos(0)\,\sin\!\left(\frac{n\pi x}{L}\right)
= \sum_{n=1}^{\infty} A_n\sin\!\left(\frac{n\pi x}{L}\right).
\]
The initial displacement condition $u(x,0)=f(x)$ therefore requires that $f$ admit a Fourier sine series expansion
\[
f(x) = \sum_{n=1}^{\infty} A_n\sin\!\left(\frac{n\pi x}{L}\right),\quad 0\le x\le L.
\]
The coefficients $A_n$ are determined by the standard sine series formula. The orthogonality relation
\[
\int_0^L \sin\!\left(\frac{m\pi x}{L}\right)\sin\!\left(\frac{n\pi x}{L}\right)\,dx
=
\begin{cases}
0, & m\neq n,\\[0.5ex]
\displaystyle \frac{L}{2}, & m=n,
\end{cases}
\]
allows us to isolate $A_n$. Multiply both sides of the expansion for $f(x)$ by $\sin\!\left(\frac{n\pi x}{L}\right)$ and integrate over $[0,L]$:
\[
\int_0^L f(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx
= \sum_{k=1}^{\infty} A_k
\int_0^L \sin\!\left(\frac{k\pi x}{L}\right)\sin\!\left(\frac{n\pi x}{L}\right)\,dx.
\]
By orthogonality, all terms with $k\ne n$ vanish, and we obtain
\[
\int_0^L f(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx
= A_n \int_0^L \sin^{2}\!\left(\frac{n\pi x}{L}\right)\,dx
= A_n \cdot \frac{L}{2}.
\]
Therefore
\[
A_n = \frac{2}{L}\int_0^L f(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx,\quad n=1,2,3,\dots.
\]

\medskip

\noindent\textbf{5. Final formula and interpretation.}
Combining these results, the unique solution of the initial--boundary value problem is
\[
u(x,t) = \sum_{n=1}^{\infty}
\left[\frac{2}{L}\int_0^L f(\xi)\sin\!\left(\frac{n\pi \xi}{L}\right)\,d\xi\right]
\cos\!\left(\frac{n\pi c t}{L}\right)
\sin\!\left(\frac{n\pi x}{L}\right),
\]
valid (under standard regularity assumptions on $f$) for $0<x<L$ and $t\ge 0$.

Each term in this series represents a \emph{normal mode} or \emph{standing wave} of the string, with spatial profile $\sin\!\left(\frac{n\pi x}{L}\right)$ and natural frequency
\[
\omega_n = c\frac{n\pi}{L}.
\]
The lowest mode, with $n=1$, has the longest wavelength and oscillates most slowly; higher modes have more interior nodes and oscillate faster, with frequencies proportional to $n$. The initial displacement $f(x)$ is decomposed into these modes by its Fourier sine series, and the time evolution is a superposition of independent harmonic oscillations.

This example illustrates the key ideas of the chapter section on waves in a homogeneous medium and hyperbolic partial differential equations: the wave equation on a bounded domain leads to an eigenvalue problem in space, a discrete spectrum of eigenvalues, orthogonal eigenfunctions forming a basis, and a representation of the solution as a Fourier series in normal modes, each evolving in time according to a simple harmonic oscillator.
\end{solution}

% ===== Example 2: Semi-Infinite String and the d’Alembert Formula (inquiry-based) =====
\begin{problem}[Semi-Infinite String and the d’Alembert Formula]
A taut, homogeneous string of linear density $\rho$ and tension $T$ is stretched along a line. Its small vertical displacement $u(x,t)$ satisfies the one-dimensional wave equation
\[
u_{tt} = c^{2} u_{xx}, \qquad c = \sqrt{T/\rho},
\]
where $x$ denotes position along the string and $t$ is time. In this problem you will first derive the d’Alembert representation formula for the wave equation on the entire real line, and then adapt it to a semi-infinite string fixed at one end. Along the way, you will see explicitly how initial disturbances decompose into left- and right-traveling waves and how a fixed end can be understood as a ``mirror'' that reflects waves with a sign change.

Consider first the Cauchy problem on the whole line:
\[
\begin{cases}
u_{tt}(x,t) = c^{2} u_{xx}(x,t), & x \in \mathbb{R},\ t>0,\\[0.3em]
u(x,0) = f(x), & x \in \mathbb{R},\\[0.3em]
u_t(x,0) = g(x), & x \in \mathbb{R},
\end{cases}
\]
where $f$ and $g$ are given, sufficiently smooth and decaying functions.

\smallskip

(a) (\textbf{Traveling wave building blocks.}) Suppose $w(x,t)$ is of the form $w(x,t) = F(x-ct)$ for some twice differentiable function $F$. 

\quad(i) Compute $w_t$ and $w_{tt}$ in terms of derivatives of $F$ and $x-ct$. Similarly, compute $w_x$ and $w_{xx}$.

\quad(ii) Show that any such $w$ satisfies the wave equation $w_{tt} = c^{2} w_{xx}$. Do the same for functions of the form $w(x,t) = G(x+ct)$.

\quad(iii) Conclude that any function of the form
\[
u(x,t) = F(x-ct) + G(x+ct)
\]
solves the wave equation. Why is it natural to interpret the first term as a right-moving wave and the second as a left-moving wave?

\emph{Hint:} Look at the graphs of $x \mapsto F(x)$ and $x \mapsto F(x-ct)$ for fixed $t$ and see how they move as $t$ increases.

\smallskip

(b) (\textbf{Using the initial displacement.}) Suppose that $u(x,t) = F(x-ct) + G(x+ct)$ solves the Cauchy problem. 

\quad(i) Use the initial condition $u(x,0) = f(x)$ to obtain a relation between $F$ and $G$ evaluated at $x$.

\quad(ii) Differentiate the representation for $u$ with respect to $t$, evaluate at $t=0$, and use $u_t(x,0)=g(x)$ to obtain a relation between $F'$ and $G'$ evaluated at $x$.

\quad(iii) Treat $F(x)$ and $G(x)$ as unknown functions and write your two relations as a system of equations for $F'(x)$ and $G'(x)$ in terms of $f'(x)$ and $g(x)$.

\emph{Hint:} First write
\[
F(x)+G(x)=f(x),
\qquad
-cF'(x)+cG'(x)=g(x),
\]
then add and subtract suitable multiples of these equations.

\smallskip

(c) (\textbf{Solving for $F$ and $G$ and obtaining d’Alembert’s formula.}) 

\quad(i) Solve the system from part (b) for $F'(x)$ and $G'(x)$, and then integrate with respect to $x$ to obtain formulas for $F(x)$ and $G(x)$ in terms of $f$ and $g$. Be careful about constants of integration.

\quad(ii) Show that, after combining the expressions and choosing the constants appropriately, you can write $u(x,t)$ in the form
\[
u(x,t) 
= \frac{1}{2} \bigl(f(x-ct) + f(x+ct)\bigr)
+ \frac{1}{2c} \int_{x-ct}^{x+ct} g(s)\,ds.
\]

\quad(iii) Explain how this formula expresses $u(x,t)$ entirely in terms of the initial displacement $f$ and the initial velocity $g$ over the interval $[x-ct,\,x+ct]$.

\emph{Hint:} For the constants of integration, imagine that $f$ and $g$ are compactly supported and that $u$ should vanish for large $|x|$ and small $t>0$.

\smallskip

Now consider a semi-infinite string occupying $x>0$ with its left end fixed at $x=0$. The displacement still satisfies
\[
u_{tt} = c^{2} u_{xx}, \qquad x>0,\ t>0,
\]
but now with a boundary condition at $x=0$,
\[
u(0,t) = 0, \qquad t \ge 0,
\]
and initial data given only for $x>0$:
\[
u(x,0) = f(x), \qquad u_t(x,0) = g(x), \qquad x>0.
\]
Assume $f$ and $g$ are sufficiently smooth on $[0,\infty)$ and satisfy $f(0)=0$ and $g(0)=0$.

\smallskip

(d) (\textbf{Odd reflection and the semi-infinite string.})

\quad(i) Define the odd extensions $\tilde f$ and $\tilde g$ of $f$ and $g$ to the whole line by
\[
\tilde f(x) =
\begin{cases}
f(x), & x \ge 0,\\
-f(-x), & x < 0,
\end{cases}
\qquad
\tilde g(x) =
\begin{cases}
g(x), & x \ge 0,\\
-g(-x), & x < 0.
\end{cases}
\]
Explain why $\tilde f$ and $\tilde g$ are continuous (and sufficiently smooth) at $x=0$ under the assumptions on $f$ and $g$.

\quad(ii) Let $\tilde u(x,t)$ be the solution of the Cauchy problem on the whole line with initial data $\tilde f$ and $\tilde g$:
\[
\tilde u_{tt} = c^{2} \tilde u_{xx}, \quad x \in \mathbb{R},\ t>0, 
\qquad
\tilde u(x,0)=\tilde f(x),\quad \tilde u_t(x,0)=\tilde g(x).
\]
Show that $\tilde u$ is an odd function of $x$ for each fixed $t$ (that is, $\tilde u(-x,t) = -\tilde u(x,t)$).

\emph{Hint:} Apply uniqueness for the Cauchy problem: if $v(x,t) = -\tilde u(-x,t)$, check that $v$ satisfies the same PDE and the same initial data as $\tilde u$.

\quad(iii) Show that, for $x>0$, the restriction $u(x,t) := \tilde u(x,t)$ satisfies the original semi-infinite problem, including the boundary condition at $x=0$.

\emph{Hint:} Use the oddness of $\tilde u$ to evaluate $u(0,t)$.

\smallskip

(e) (\textbf{Extensions and interpretations.})

\quad(i) Write an explicit d’Alembert-type formula for $u(x,t)$ on $x>0$ by substituting $\tilde f$ and $\tilde g$ into the formula from part (c) and then restricting to $x>0$. Try to simplify your answer by splitting the integral $\displaystyle \int_{x-ct}^{x+ct}$ into parts where the integration variable is positive or negative.

\emph{Hint:} When $x-ct<0<x+ct$, you can write
\[
\int_{x-ct}^{x+ct} \tilde g(s)\,ds
= \int_{x-ct}^{0} \tilde g(s)\,ds + \int_{0}^{x+ct} \tilde g(s)\,ds
\]
and use the oddness of $\tilde g$.

\quad(ii) Suppose instead that the boundary condition at $x=0$ is $u_x(0,t)=0$ (a free or Neumann end). What kind of extension of $f$ and $g$ to the whole line (odd or even?) would you try, and why? Formulate but do not fully carry out the analogous construction.

\quad(iii) Interpret physically how a pulse traveling toward the fixed end at $x=0$ behaves upon reaching the boundary, in terms of the odd extension and the reflected wave.

\end{problem}

% ===== Example 2: Semi-Infinite String and the d’Alembert Formula (full solution) =====
\begin{problem}[Semi-Infinite String and the d’Alembert Formula]
Consider the one-dimensional wave equation
\[
u_{tt} = c^{2} u_{xx}, \qquad c>0,
\]
with initial conditions $u(x,0) = f(x)$ and $u_t(x,0) = g(x)$.

(a) On the whole line $x\in\mathbb{R}$, derive the d’Alembert formula
\[
u(x,t) 
= \frac{1}{2}\bigl(f(x-ct)+f(x+ct)\bigr)
+ \frac{1}{2c}\int_{x-ct}^{x+ct} g(s)\,ds
\]
for sufficiently smooth and decaying $f$ and $g$.

(b) Now consider a semi-infinite string $x>0$ with fixed end at $x=0$:
\[
u_{tt} = c^{2} u_{xx}, \quad x>0,\ t>0; 
\quad u(0,t)=0,\ t\ge 0;
\quad u(x,0)=f(x),\ u_t(x,0)=g(x),\ x>0,
\]
where $f$ and $g$ are smooth on $[0,\infty)$, satisfy $f(0)=g(0)=0$, and are suitably decaying.

Using an odd reflection of the initial data across $x=0$, express the solution $u(x,t)$ for $x>0$ in terms of the d’Alembert formula applied on the whole line. Clearly indicate how the boundary condition is enforced and give an explicit representation of $u(x,t)$ in terms of $f$ and $g$.
\end{problem}

\begin{solution}
We begin with the wave equation on the whole real line and derive the d’Alembert representation formula. Then we adapt it to the semi-infinite domain by a reflection argument (method of images). This illustrates the basic structure of solutions to hyperbolic equations: propagation along characteristic lines and the role of boundary conditions as ``mirrors'' for waves.

\medskip

\textbf{(a) d’Alembert formula on the whole line.}

We consider the Cauchy problem
\[
u_{tt} = c^{2} u_{xx}, \qquad x\in\mathbb{R},\ t>0,
\]
with
\[
u(x,0)=f(x), \qquad u_t(x,0)=g(x).
\]

\emph{Step 1: General form of solutions via traveling waves.}

We first look for solutions of the special form $u(x,t) = F(x-ct)$, where $F$ is twice differentiable. Let $\xi = x-ct$. Using the chain rule we compute
\[
u_t(x,t) = -c F'(\xi), \qquad
u_{tt}(x,t) = c^{2} F''(\xi),
\]
and
\[
u_x(x,t) = F'(\xi), \qquad
u_{xx}(x,t) = F''(\xi).
\]
Substituting into the wave equation gives
\[
u_{tt} - c^{2} u_{xx} = c^{2} F''(\xi) - c^{2} F''(\xi) = 0.
\]
Hence any traveling profile of the form $F(x-ct)$ solves the wave equation. The same computation with $u(x,t) = G(x+ct)$ yields another family of solutions, now traveling to the left.

Because the wave equation is linear, any linear combination of such solutions is again a solution. Thus every function of the form
\[
u(x,t) = F(x-ct) + G(x+ct)
\]
satisfies $u_{tt}=c^{2}u_{xx}$. One can show (for sufficiently smooth solutions) that this is in fact the general solution of the one-dimensional wave equation; this can be made precise by a change of variables to characteristic coordinates $\xi = x-ct$ and $\eta = x+ct$, in which the equation reduces to $u_{\xi\eta}=0$ and hence $u(\xi,\eta)=A(\xi)+B(\eta)$.

Moreover, the function $x\mapsto F(x-ct)$ at a fixed time $t$ is just the graph of $F$ shifted to the right by $ct$, so $F(x-ct)$ is a right-traveling wave with speed $c$. Similarly, $G(x+ct)$ is a left-traveling wave.

\emph{Step 2: Use initial data to determine $F$ and $G$.}

We now impose the initial conditions. At $t=0$ we have
\[
u(x,0) = F(x) + G(x) = f(x).
\]
This is our first relation between $F$ and $G$.

Next, we differentiate $u$ with respect to $t$:
\[
u_t(x,t) = -c F'(x-ct) + c G'(x+ct).
\]
Evaluating at $t=0$ gives
\[
u_t(x,0) = -c F'(x) + c G'(x) = g(x),
\]
or equivalently
\[
-cF'(x)+cG'(x)=g(x).
\]

Thus $F$ and $G$ must satisfy the system
\[
F(x) + G(x) = f(x), \qquad -cF'(x)+cG'(x)=g(x).
\]

Differentiating the first relation with respect to $x$ yields
\[
F'(x) + G'(x) = f'(x).
\]
Together with $-cF'(x)+cG'(x)=g(x)$, this is now a pair of linear equations for the unknowns $F'(x)$ and $G'(x)$ at each point $x$:
\[
\begin{cases}
F'(x) + G'(x) = f'(x),\\[0.2em]
-cF'(x) + cG'(x) = g(x).
\end{cases}
\]

We can solve this system by standard algebra. Adding the two equations after multiplying the first by $c$ gives
\[
cF'(x)+cG'(x) = c f'(x),
\]
and then adding this to $-cF'(x)+cG'(x)=g(x)$ yields
\[
2cG'(x) = c f'(x) + g(x).
\]
Thus
\[
G'(x) = \frac{1}{2} f'(x) + \frac{1}{2c} g(x).
\]
Similarly, subtracting the second equation from $c$ times the first gives
\[
2cF'(x) = c f'(x) - g(x),
\]
and hence
\[
F'(x) = \frac{1}{2} f'(x) - \frac{1}{2c} g(x).
\]

\emph{Step 3: Integrate to find $F$ and $G$; derive d’Alembert’s formula.}

We integrate these identities with respect to $x$:
\[
F(x) = \frac{1}{2} f(x) - \frac{1}{2c} \int^{x} g(s)\,ds + C_1,
\]
\[
G(x) = \frac{1}{2} f(x) + \frac{1}{2c} \int^{x} g(s)\,ds + C_2,
\]
where $C_1$ and $C_2$ are constants of integration. Substituting into $u(x,t)=F(x-ct)+G(x+ct)$ yields
\[
\begin{aligned}
u(x,t)
&= \frac{1}{2} f(x-ct) - \frac{1}{2c} \int^{x-ct} g(s)\,ds + C_1 \\
&\quad + \frac{1}{2} f(x+ct) + \frac{1}{2c} \int^{x+ct} g(s)\,ds + C_2.
\end{aligned}
\]
We can combine the constants to a single constant $C_1+C_2$, and we can also combine the integrals:
\[
\int^{x+ct} g(s)\,ds - \int^{x-ct} g(s)\,ds = \int_{x-ct}^{x+ct} g(s)\,ds.
\]
Thus
\[
u(x,t)
= \frac{1}{2} \bigl(f(x-ct) + f(x+ct)\bigr)
+ \frac{1}{2c}\int_{x-ct}^{x+ct} g(s)\,ds
+ (C_1+C_2).
\]

To determine $C_1+C_2$, we may impose a mild growth or decay condition at infinity. If $f$ and $g$ have compact support or decay as $|x|\to\infty$, then it is natural to require that $u(x,t)\to 0$ as $|x|\to\infty$ for fixed $t>0$. This forces the constant to vanish, $C_1+C_2=0$. Therefore we obtain the classical d’Alembert formula
\[
u(x,t)
= \frac{1}{2} \bigl(f(x-ct) + f(x+ct)\bigr)
+ \frac{1}{2c}\int_{x-ct}^{x+ct} g(s)\,ds.
\]

This expression shows that $u(x,t)$ depends only on the values of the initial data $f$ and $g$ on the interval $[x-ct,x+ct]$, which is the intersection of the initial line $t=0$ with the characteristic cone emanating from the point $(x,t)$. This is a hallmark of hyperbolic equations: finite propagation speed and a clear domain of dependence.

\medskip

\textbf{(b) Semi-infinite string with a fixed end: odd reflection.}

We now consider the problem on the half-line $x>0$:
\[
u_{tt} = c^{2} u_{xx}, \quad x>0,\ t>0,
\]
with boundary condition
\[
u(0,t)=0,\qquad t\ge 0,
\]
and initial conditions
\[
u(x,0) = f(x),\qquad u_t(x,0)=g(x),\qquad x>0,
\]
where $f$ and $g$ are smooth on $[0,\infty)$, satisfy $f(0)=g(0)=0$, and decay sufficiently fast as $x\to\infty$.

The central idea is to extend the problem from the half-line to the whole line in such a way that the boundary condition at $x=0$ is automatically enforced by symmetry. For a fixed end with $u(0,t)=0$, we want an odd symmetry in $x$.

\emph{Step 1: Odd extensions of the initial data.}

We define the odd extensions $\tilde f$ and $\tilde g$ of $f$ and $g$ to the whole real line by
\[
\tilde f(x) =
\begin{cases}
f(x), & x \ge 0,\\
-f(-x), & x < 0,
\end{cases}
\qquad
\tilde g(x) =
\begin{cases}
g(x), & x \ge 0,\\
-g(-x), & x < 0.
\end{cases}
\]
Since $f(0)=0$ and $g(0)=0$, the defining formulas from the right and from the left coincide at $x=0$. Moreover, if $f$ and $g$ are, say, $C^{1}$ on $[0,\infty)$, then the left and right derivatives at $0$ also match, so $\tilde f$ and $\tilde g$ are at least $C^{1}$ across $x=0$. With more regularity of $f$ and $g$, the extensions inherit the same level of smoothness. Thus we have constructed globally defined, smooth, odd functions $\tilde f$ and $\tilde g$.

\emph{Step 2: Solve the Cauchy problem on $\mathbb{R}$ with odd data.}

We now consider the Cauchy problem on the entire line:
\[
\tilde u_{tt} = c^{2} \tilde u_{xx}, \quad x\in\mathbb{R},\ t>0,
\]
with
\[
\tilde u(x,0) = \tilde f(x), \qquad \tilde u_t(x,0)=\tilde g(x).
\]
By part (a), the unique (sufficiently regular) solution is given by d’Alembert’s formula:
\[
\tilde u(x,t)
= \frac{1}{2}\bigl(\tilde f(x-ct) + \tilde f(x+ct)\bigr)
+ \frac{1}{2c}\int_{x-ct}^{x+ct} \tilde g(s)\,ds.
\]

We now show that $\tilde u$ is an odd function of $x$ for each fixed $t$. Define
\[
v(x,t) := -\tilde u(-x,t).
\]
Because the change $x\mapsto -x$ simply reflects space, we have
\[
v_{tt}(x,t) = -\tilde u_{tt}(-x,t), \qquad
v_{xx}(x,t) = -\tilde u_{xx}(-x,t),
\]
so
\[
v_{tt}(x,t) - c^{2} v_{xx}(x,t)
= -\tilde u_{tt}(-x,t) + c^{2}\tilde u_{xx}(-x,t)
= -\bigl(\tilde u_{tt} - c^{2}\tilde u_{xx}\bigr)(-x,t)=0.
\]
Thus $v$ satisfies the same wave equation as $\tilde u$.

At $t=0$ we have
\[
v(x,0) = -\tilde u(-x,0) = -\tilde f(-x) = \tilde f(x),
\]
since $\tilde f$ is odd. Similarly,
\[
v_t(x,0) = -\tilde u_t(-x,0) = -\tilde g(-x) = \tilde g(x),
\]
since $\tilde g$ is odd. Therefore $v$ and $\tilde u$ satisfy the same PDE and the same initial data. By uniqueness of solutions to the Cauchy problem on $\mathbb{R}$, we must have $v(x,t) = \tilde u(x,t)$, that is,
\[
-\tilde u(-x,t) = \tilde u(x,t),
\]
or equivalently $\tilde u(-x,t) = -\tilde u(x,t)$. Hence $\tilde u$ is odd in $x$ for each $t$.

\emph{Step 3: Restrict to the half-line and verify the boundary condition.}

We now define
\[
u(x,t) := \tilde u(x,t), \qquad x>0,\ t\ge 0.
\]
On the domain $x>0$, $t>0$, the function $u$ satisfies the wave equation and inherits the initial conditions:
\[
u(x,0) = \tilde u(x,0) = \tilde f(x) = f(x), \quad x>0,
\]
\[
u_t(x,0) = \tilde u_t(x,0) = \tilde g(x) = g(x), \quad x>0.
\]
At the boundary $x=0$, using oddness of $\tilde u$ we obtain
\[
u(0,t) = \tilde u(0,t) = -\tilde u(0,t),
\]
so $2\tilde u(0,t)=0$, hence $\tilde u(0,t)=0$. Therefore
\[
u(0,t)=0, \qquad t\ge 0,
\]
and the boundary condition at the fixed end is satisfied.

Thus, the semi-infinite problem is solved by taking the odd extension of the initial data to $\mathbb{R}$, solving the Cauchy problem there, and then restricting the result back to $x>0$.

\emph{Step 4: Explicit representation formula.}

Substituting the specific form of $\tilde f$ and $\tilde g$ into the d’Alembert formula yields, for any $x>0$ and $t\ge 0$,
\[
u(x,t)
= \frac{1}{2}\bigl(\tilde f(x-ct) + \tilde f(x+ct)\bigr)
+ \frac{1}{2c}\int_{x-ct}^{x+ct} \tilde g(s)\,ds.
\]
We can make this more explicit in terms of $f$ and $g$ by considering the sign of $x\pm ct$.

For concreteness, suppose $t>0$ is such that $x-ct<0<x+ct$; this is the interesting regime in which the characteristic through $(x,t)$ intersects both positive and negative parts of the $x$-axis. Then
\[
\tilde f(x-ct) = -f(ct-x),\qquad \tilde f(x+ct)=f(x+ct),
\]
and
\[
\int_{x-ct}^{x+ct} \tilde g(s)\,ds
= \int_{x-ct}^{0} \tilde g(s)\,ds + \int_{0}^{x+ct} \tilde g(s)\,ds.
\]
Using the oddness of $\tilde g$ and the substitution $s=-r$ on $[x-ct,0]$, we obtain
\[
\int_{x-ct}^{0} \tilde g(s)\,ds
= \int_{ct-x}^{0} g(r)\,dr = -\int_{0}^{ct-x} g(r)\,dr.
\]
Hence
\[
\int_{x-ct}^{x+ct} \tilde g(s)\,ds
= -\int_{0}^{ct-x} g(r)\,dr + \int_{0}^{x+ct} g(r)\,dr
= \int_{ct-x}^{x+ct} g(r)\,dr.
\]

Combining these expressions gives, for $x>0$ and $t>0$,
\[
u(x,t)
= \frac{1}{2}\bigl(f(x+ct) - f(ct-x)\bigr)
+ \frac{1}{2c}\int_{ct-x}^{x+ct} g(s)\,ds
\]
when $x-ct<0<x+ct$. For times $t$ such that both $x-ct>0$ and $x+ct>0$, the interval $[x-ct,x+ct]$ stays in the positive half-line, and the odd extension simply coincides with $f$ and $g$ there, so the formula reduces to the standard d’Alembert representation with $f$ and $g$ on $(0,\infty)$:
\[
u(x,t)
= \frac{1}{2}\bigl(f(x-ct)+f(x+ct)\bigr)
+ \frac{1}{2c}\int_{x-ct}^{x+ct} g(s)\,ds
\quad \text{if } x-ct>0.
\]
For $x+ct<0$ (which cannot occur when $x>0$ and $t\ge 0$), a symmetric formula in terms of $f(-\cdot)$ and $g(-\cdot)$ would apply.

From the physical viewpoint, this construction shows that a pulse traveling toward $x=0$ reflects as if it continued into $x<0$ but with a sign change. The odd extension encodes that the reflected wave has opposite sign, consistent with a fixed end that inverts the displacement at the boundary.

\medskip

\textbf{Connection to the chapter theme.} 

This example illustrates several central ideas about waves in a homogeneous medium and hyperbolic partial differential equations:

\begin{itemize}
  \item The reduction of the PDE to characteristic variables $x\pm ct$ reveals that solutions are built from traveling waves, and the d’Alembert formula is an explicit representation expressing this structure.
  \item The domain of dependence is finite: $u(x,t)$ depends only on the initial data in the interval $[x-ct,x+ct]$, reflecting finite propagation speed.
  \item For the semi-infinite domain, the method of images (via odd extension) shows how boundary conditions correspond to symmetry conditions on the solution, and how boundaries reflect waves. Dirichlet and Neumann boundary conditions correspond naturally to odd and even reflections, respectively.
\end{itemize}

These features are typical of hyperbolic PDEs and will reappear in more complex settings, such as higher-dimensional wave equations, variable-coefficient media, and boundary-value problems on bounded domains.
\end{solution}

% ===== Example 3: Method of Characteristics for First-Order Wave Transport (inquiry-based) =====
\begin{problem}[Method of Characteristics for First-Order Wave Transport]
A narrow, straight river carries along a dissolved pollutant with constant downstream velocity $c>0$. Let $u(x,t)$ denote the pollutant concentration at position $x\in\mathbb{R}$ along the river and at time $t\ge 0$. If the river flow simply transports the pollutant without creating, destroying, or diffusing it, one arrives at the linear \emph{transport equation}
\[
u_t + c\,u_x = 0.
\]
In this problem you will discover how to solve this equation by following the motion of individual fluid parcels, that is, by tracking ``characteristic curves'' in the $(x,t)$–plane.

We will study the initial value problem
\[
u_t + c\,u_x = 0, \qquad x\in\mathbb{R},\ t>0, \qquad u(x,0) = f(x),
\]
where $f$ is a given initial concentration profile.

\medskip

(a) Imagine a marked parcel of water that is at position $x_0$ at time $t=0$. Assume the water flows at the constant speed $c>0$ in the positive $x$–direction.

\quad(i) Write down an ordinary differential equation (an ODE) for the position $X(t)$ of this parcel as a function of time, together with the appropriate initial condition.

\quad(ii) Solve this ODE explicitly and sketch several such trajectories $t\mapsto X(t)$ in the $(x,t)$–plane. These curves are called the \emph{characteristics} of the flow.

\medskip

(b) Now think about how the pollutant concentration behaves along such a characteristic curve. If a water parcel simply carries its concentration with it, without changing in time, what does this say about $u(X(t),t)$ along the trajectory you found in part (a)?

\quad(i) Express the rate of change of the concentration experienced by this parcel as a total derivative:
\[
\frac{d}{dt}u(X(t),t) = \; ? 
\]
Use the chain rule to write this derivative in terms of $u_t$ and $u_x$.

\quad(ii) Use the partial differential equation $u_t + c\,u_x = 0$ to simplify your expression for $\dfrac{d}{dt}u(X(t),t)$. What ordinary differential equation does $u(X(t),t)$ satisfy along a characteristic?

\emph{Hint:} You should find a very simple ODE for $u(X(t),t)$, reflecting that the concentration carried by each parcel does not change in time.

\medskip

(c) Combine your answers from parts (a) and (b).

\quad(i) Solve the characteristic equation for $X(t)$ obtained in part (a), and write it in the form
\[
X(t) = \xi + c t,
\]
where $\xi$ is a parameter labeling different characteristics. Rewrite this relation as an equation in $x$ and $t$:
\[
\text{(characteristic curves)}\quad x - c t = \xi.
\]

\quad(ii) Solve the ODE for $u(X(t),t)$ obtained in part (b) along these curves. Use the initial condition $u(x,0) = f(x)$ to determine the constant of integration in terms of $f$ and the parameter $\xi$.

\emph{Hint:} The idea is that along the characteristic passing through $(\xi,0)$ you must have $u(\xi,0) = f(\xi)$. How does this relate to $u(x,t)$ when $(x,t)$ lies on the same characteristic?

\medskip

(d) Use part (c) to derive an explicit formula for the solution $u(x,t)$ at an arbitrary point $(x,t)$ in terms of the initial profile $f$.

\quad(i) Show that for each $(x,t)$ there is a unique $\xi$ such that $(x,t)$ lies on the characteristic through $(\xi,0)$, and express $\xi$ in terms of $x$ and $t$.

\quad(ii) Conclude that the solution of the initial value problem satisfies
\[
u(x,t) = \, ? \quad\text{(fill in the expression in terms of $f$, $x$, and $t$)}.
\]

\quad(iii) Interpret your formula geometrically: how does the graph of $x\mapsto u(x,t)$ at time $t>0$ compare to the initial graph $x\mapsto f(x)$ at time $t=0$? In which direction does the profile move, and with what speed?

\emph{Hint:} Think in terms of translating the initial graph horizontally in the $x$–direction.

\medskip

(e) (Extensions and ``what if'' questions.)

\quad(i) Suppose instead that the transport equation were
\[
u_t + c\,u_x = 0 \quad\text{with } c<0.
\]
Repeat (at least informally) the reasoning above. In which direction does the initial profile now move? What characteristic curves do you obtain?

\quad(ii) Consider the \emph{inhomogeneous} transport equation
\[
u_t + c\,u_x = g(x,t),
\]
where $g$ represents a source term (for example, additional pollutant being added per unit time). How would you modify the characteristic method to handle this equation conceptually? What ODE would $u(X(t),t)$ satisfy along each characteristic?

\emph{Hint:} The total derivative $\dfrac{d}{dt}u(X(t),t)$ will no longer be zero; it will be related to $g$ along the trajectory.
\end{problem}

% ===== Example 3: Method of Characteristics for First-Order Wave Transport (full solution) =====
\begin{problem}[Method of Characteristics for First-Order Wave Transport]
Let $c\in\mathbb{R}$ be a nonzero constant. Consider the linear transport equation
\[
u_t + c\,u_x = 0, \qquad x\in\mathbb{R},\ t>0,
\]
with initial condition
\[
u(x,0) = f(x), \qquad x\in\mathbb{R},
\]
where $f$ is a given function.

(a) Solve this initial value problem using the method of characteristics and express $u(x,t)$ explicitly in terms of $f$, $x$, and $t$.

(b) For $c>0$ and $c<0$, describe in words how the initial profile $f$ propagates in time. In particular, specify the direction and speed of propagation.

\end{problem}

\begin{solution}
We solve the transport equation
\[
u_t + c\,u_x = 0,\qquad c\neq 0,
\]
with initial data $u(x,0)=f(x)$ by the method of characteristics. The central idea is that this first-order hyperbolic equation describes the advection of $u$ along straight lines in the $(x,t)$–plane, and that along those curves, called characteristics, the solution satisfies an ordinary differential equation.

\medskip

\textbf{Step 1: Characteristic curves.}
We seek curves in the $(x,t)$–plane along which $u$ behaves in a simple way. Let such a curve be parametrized as
\[
t \mapsto (X(t),t),
\]
that is, at time $t$ the point on the curve has spatial coordinate $X(t)$. Along this curve, we consider the function
\[
U(t) := u\bigl(X(t),t\bigr),
\]
which is the value of the solution as seen by a ``moving observer'' traveling along the curve $x=X(t)$.

By the chain rule, the total derivative of $U(t)$ is
\[
\frac{dU}{dt} = \frac{d}{dt}u(X(t),t)
= u_t(X(t),t) + X'(t)\,u_x(X(t),t).
\]
The partial differential equation $u_t + c\,u_x = 0$ implies
\[
u_t = -c\,u_x.
\]
Substituting this into the expression for $dU/dt$ gives
\[
\frac{dU}{dt}
= -c\,u_x(X(t),t) + X'(t)\,u_x(X(t),t)
= \bigl(X'(t)-c\bigr)\,u_x(X(t),t).
\]

We want the value of $u$ to satisfy an ordinary differential equation that does not involve $u_x$, and the simplest possibility is to arrange for $dU/dt$ to vanish. This happens if we choose $X(t)$ to satisfy
\[
X'(t) = c.
\]
Thus the characteristic curves for this equation are precisely those along which the spatial coordinate moves with constant speed $c$.

Solving the ordinary differential equation $X'(t)=c$ with initial condition $X(0)=\xi$ gives
\[
X(t) = \xi + c t,
\]
where the parameter $\xi\in\mathbb{R}$ labels different curves. Eliminating $t$ and $\xi$, we see that the characteristic curves in the $(x,t)$–plane are the straight lines
\[
x - c t = \xi = \text{constant.}
\]

\medskip

\textbf{Step 2: Evolution of $u$ along characteristics.}
Along a characteristic $x=X(t)$ with $X'(t)=c$, we just computed
\[
\frac{d}{dt}u(X(t),t)
= \bigl(X'(t)-c\bigr)\,u_x(X(t),t) = 0.
\]
Thus $U(t) = u(X(t),t)$ is constant along each characteristic. In other words, if $(x,t)$ and $(\xi,0)$ lie on the same characteristic line $x-ct=\xi$, then
\[
u(x,t) = u(\xi,0).
\]

We now use the initial condition to identify this constant. At time $t=0$, the solution satisfies
\[
u(\xi,0) = f(\xi).
\]
Therefore, along the characteristic line $x-ct=\xi$, the solution is given by
\[
u(x,t) = f(\xi)
\quad\text{whenever}\quad x-ct = \xi.
\]

\medskip

\textbf{Step 3: Expressing the solution in terms of $(x,t)$.}
For a fixed point $(x,t)$ with $t>0$, there is a unique characteristic passing through it. This characteristic can be written as
\[
x-ct = \xi,
\]
so the parameter $\xi$ is determined by $(x,t)$ via
\[
\xi = x - c t.
\]
Substituting this into the expression for $u(x,t)$ along characteristics, we obtain
\[
u(x,t) = f(\xi) = f(x-ct).
\]
This formula holds for all $x\in\mathbb{R}$ and all $t\ge 0$. It is straightforward to verify directly that $u(x,t)=f(x-ct)$ satisfies both the transport equation and the initial condition:
\begin{align*}
u_t(x,t) &= f'(x-ct)\cdot(-c),\\
u_x(x,t) &= f'(x-ct),
\end{align*}
so
\[
u_t + c\,u_x = -c f'(x-ct) + c f'(x-ct) = 0,
\]
and at $t=0$ we have $u(x,0)=f(x)$ as required.

This completes part (a): the solution to the initial value problem is
\[
\boxed{\,u(x,t) = f(x-ct)\,}.
\]

\medskip

\textbf{Step 4: Propagation of the initial profile.}
The explicit solution $u(x,t)=f(x-ct)$ has a simple geometric interpretation. For each fixed time $t$, the function $x\mapsto u(x,t)$ is obtained by evaluating the initial profile at the shifted argument $x-ct$. This means that the graph of $u(\cdot,t)$ is the graph of $f$ translated rigidly in the $x$–direction.

To see the direction and speed of propagation, consider the effect of time increasing from $0$ to some $t>0$:

\begin{itemize}
  \item If $c>0$, then for each fixed $x$ we evaluate the initial data at the point $x-ct$, which lies to the \emph{left} of $x$ by a distance $ct$. Equivalently, the shape $f$ moves to the \emph{right} with speed $c$. In other words, the initial profile is convected in the positive $x$–direction without distortion.

  \item If $c<0$, we may write $c=-|c|$ and the solution becomes
  \[
  u(x,t) = f(x - (-|c|)t) = f(x + |c|t).
  \]
  In this case, the initial profile is translated to the \emph{left} with speed $|c|$, that is, in the negative $x$–direction.
\end{itemize}

Thus the answer to part (b) is: for $c>0$ the wave profile $f$ propagates rigidly to the right with speed $|c|=c$, and for $c<0$ it propagates rigidly to the left with speed $|c|=-c$.

\medskip

\textbf{Connection to hyperbolic PDE and wave propagation.}
This example illustrates several central features of hyperbolic partial differential equations in a homogeneous medium. The equation $u_t + c\,u_x = 0$ is a prototypical first-order hyperbolic equation. Its characteristics are straight lines in the $(x,t)$–plane, along which information (here, the values of $u$) is transported. The solution shows \emph{finite-speed propagation}: data prescribed at a point $(x_0,0)$ influences the solution at exactly those points $(x,t)$ that lie on the characteristic through $(x_0,0)$, that is, on the line $x=x_0+ct$. There is no spreading or smoothing; the initial shape is simply carried along by the flow. These ideas—characteristics, propagation along curves, and finite signal speed—reappear in more complex form for the second-order wave equations studied later in this chapter.
\end{solution}

% ===== Example 4: Vibrating String with Damping and Forcing (inquiry-based) =====
\begin{problem}[Vibrating String with Damping and Forcing]
Consider a taut string of length $L$ fixed at both ends, vibrating in a vertical plane. In the idealized undamped, unforced case, its transverse displacement $u(x,t)$ satisfies the one-dimensional wave equation, and its motion can be described in terms of normal modes. Now suppose the string experiences viscous damping, proportional to its velocity, and is driven by an external periodic forcing that acts along the entire length of the string with a prescribed spatial profile. Physically, one expects that the damping removes energy while the forcing injects it, and that for large times a steady oscillatory regime may emerge in which input and dissipation balance.

We will model this situation, reduce the partial differential equation to a family of ordinary differential equations for the modes, and analyze the long-time behavior.

\smallskip

Let $u(x,t)$ denote the transverse displacement of the string at position $x\in(0,L)$ and time $t>0$. Suppose that $u$ satisfies
\[
u_{tt}(x,t) + 2\beta\,u_t(x,t) \;=\; c^2\,u_{xx}(x,t) \;+\; f(x)\cos(\omega t),
\qquad 0<x<L,\ t>0,
\]
where $c>0$ is the wave speed, $\beta>0$ is a damping coefficient, and $f(x)$ is a given smooth function describing how the external forcing is distributed along the string. Assume fixed ends,
\[
u(0,t)=u(L,t)=0,\qquad t>0,
\]
and zero initial displacement and velocity,
\[
u(x,0)=0,\quad u_t(x,0)=0,\qquad 0<x<L.
\]

\begin{enumerate}[(a)]
  \item \textbf{Modeling and qualitative behavior.}
  \begin{enumerate}[(i)]
    \item Explain in words what each term in the equation
    \[
    u_{tt} + 2\beta u_t = c^2 u_{xx} + f(x)\cos(\omega t)
    \]
    represents physically. In particular, which term corresponds to inertia, which to restoring forces, which to damping, and which to the external driving? Why is it reasonable to take the damping term proportional to $u_t$?
    \item If $\beta>0$ is fixed and $f\equiv 0$, what do you expect to happen to any initial vibration of the string as $t\to\infty$? How does your answer change when $f\not\equiv 0$ but is time-periodic as above?
  \end{enumerate}
  % Hint: Think about energy: which terms can store energy, which can remove it, and which can add it to the system?

  \item \textbf{Spatial modes and eigenfunction expansion.}
  \begin{enumerate}[(i)]
    \item Recall that in the undamped, unforced case,
    \[
    u_{tt} = c^2 u_{xx},\quad u(0,t)=u(L,t)=0,
    \]
    one seeks separated solutions $u(x,t)=X(x)T(t)$ and arrives at an eigenvalue problem for $X$. Write down this eigenvalue problem for $X$ and determine all eigenfunctions and corresponding eigenvalues.
    % Hint: You should obtain a Sturm--Liouville problem for $-X''=\lambda X$ with homogeneous Dirichlet boundary conditions.

    \item Show that the eigenfunctions you found in part (i) form an orthogonal basis of $L^2(0,L)$ (you may quote this as a standard theorem once you have identified the eigenfunctions). Write expansions of the form
    \[
    u(x,t) = \sum_{n=1}^\infty y_n(t)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr),\qquad
    f(x) = \sum_{n=1}^\infty f_n\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr),
    \]
    where you should give a formula for the coefficients $f_n$ in terms of $f$.
    % Hint: Use orthogonality of the sine functions on $(0,L)$.
  \end{enumerate}

  \item \textbf{Reduction to a family of forced, damped oscillators.}
  Substitute the series expansion for $u(x,t)$ into the damped, forced wave equation and use the orthogonality of the eigenfunctions to derive an ordinary differential equation for each modal coefficient $y_n(t)$.
  \begin{enumerate}[(i)]
    \item Show that each $y_n(t)$ satisfies an equation of the form
    \[
    y_n''(t) + 2\beta\,y_n'(t) + \Omega_n^2\,y_n(t) \;=\; f_n\cos(\omega t),
    \]
    where you should identify $\Omega_n$ explicitly in terms of $c$, $L$, and $n$.
    % Hint: Carefully compute $u_{tt}$ and $u_{xx}$ termwise, then compare coefficients of $\sin(n\pi x/L)$.

    \item As a warm-up, ignore the index $n$ and consider the scalar equation
    \[
    y''(t) + 2\beta\,y'(t) + \Omega^2\,y(t) = F\cos(\omega t),\qquad y(0)=0,\ y'(0)=0,
    \]
    where $\beta>0$, $\Omega>0$, and $F$ are constants. Solve this ODE and identify the part of the solution that persists as $t\to\infty$.
    % Hint: You may use the method of undetermined coefficients with an ansatz $y_p(t)=A\cos(\omega t)+B\sin(\omega t)$ for a particular solution, and then discuss the homogeneous solution separately.
  \end{enumerate}

  \item \textbf{Assembling the steady-state solution and resonance.}
  \begin{enumerate}[(i)]
    \item Returning to the full string problem, use your result from part (c) to write down the \emph{steady-state} (long-time periodic) solution in the form
    \[
    u_{\text{ss}}(x,t) = \sum_{n=1}^\infty \Bigl( A_n\cos(\omega t) + B_n\sin(\omega t) \Bigr)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr),
    \]
    and express $A_n$ and $B_n$ in terms of $f_n$, $\beta$, $c$, $L$, $n$, and $\omega$.

    \item Show that for each fixed $n$, the oscillation in the $n$th mode has amplitude
    \[
    R_n(\omega) = \frac{|f_n|}{\sqrt{\bigl(c^2(n\pi/L)^2 - \omega^2\bigr)^2 + 4\beta^2\omega^2}}.
    \]
    For a given mode $n$, for which driving frequencies $\omega$ is this amplitude largest? Describe qualitatively how increasing the damping coefficient $\beta$ affects the maximum amplitude and how sharply it peaks near the ``natural frequency'' $\omega\approx c\,n\pi/L$.
    % Hint: View $R_n(\omega)$ as a function of $\omega$ and relate its graph to the familiar resonance curve of a forced, damped harmonic oscillator.
  \end{enumerate}

  \item \textbf{Extensions and variations.}
  \begin{enumerate}[(i)]
    \item What changes in your analysis if the damping is turned off, that is, if $\beta=0$? In particular, what happens to the steady-state amplitude $R_n(\omega)$ near the natural frequencies? Relate your answer to the physical phenomenon of resonance and to the role of damping in realistic systems.

    \item Suppose instead that the forcing acts only in a single mode, say
    \[
    f(x) = F_1\sin\!\Bigl(\frac{\pi x}{L}\Bigr),
    \]
    so that $f_1=F_1$ and $f_n=0$ for $n\ge2$. How does your series simplify, and what can you say in this case about the motion of the string and its energy distribution among modes?
    % Hint: Think about whether higher modes can be excited if they are not directly forced and the equation is linear with constant coefficients.
  \end{enumerate}
\end{enumerate}
\end{problem}

% ===== Example 4: Vibrating String with Damping and Forcing (full solution) =====
\begin{problem}[Vibrating String with Damping and Forcing]
Let $u(x,t)$ denote the transverse displacement of a taut string of length $L$, fixed at $x=0$ and $x=L$. The string is subject to viscous damping with coefficient $\beta>0$ and an external periodic forcing of the form $f(x)\cos(\omega t)$, where $f$ is a given smooth function on $(0,L)$. The motion is modeled by
\[
u_{tt}(x,t) + 2\beta\,u_t(x,t) = c^2\,u_{xx}(x,t) + f(x)\cos(\omega t),\qquad 0<x<L,\ t>0,
\]
with boundary conditions
\[
u(0,t)=u(L,t)=0,\qquad t>0,
\]
and initial conditions
\[
u(x,0)=0,\qquad u_t(x,0)=0,\qquad 0<x<L.
\]

\begin{enumerate}[(a)]
  \item Using the eigenfunctions of the one-dimensional Dirichlet Laplacian on $(0,L)$, expand $u$ and $f$ in a sine series and reduce the partial differential equation to an infinite family of ordinary differential equations for the modal coefficients $y_n(t)$ of $u$.

  \item Solve each of these ordinary differential equations and determine the steady-state (long-time periodic) solution $u_{\mathrm{ss}}(x,t)$. Express $u_{\mathrm{ss}}$ as a Fourier sine series
  \[
  u_{\mathrm{ss}}(x,t) = \sum_{n=1}^\infty \bigl(A_n\cos(\omega t)+B_n\sin(\omega t)\bigr)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr),
  \]
  with explicit formulae for $A_n$ and $B_n$ in terms of $f$, $\beta$, $c$, $L$, $n$, and $\omega$.

  \item For each fixed mode $n$, show that the amplitude of oscillation in that mode is
  \[
  R_n(\omega)=\frac{|f_n|}{\sqrt{\bigl(c^2(n\pi/L)^2 - \omega^2\bigr)^2 + 4\beta^2\omega^2}},
  \]
  where $f_n$ are the sine coefficients of $f$. Discuss how $R_n(\omega)$ depends on the driving frequency $\omega$ and the damping coefficient $\beta$, and explain how this illustrates resonance and its damping in the context of the damped, forced wave equation.
\end{enumerate}
\end{problem}

\begin{solution}
We analyze the problem by expanding the solution in the spatial eigenfunctions of the Dirichlet Laplacian. This reduces the partial differential equation to a countable family of ordinary differential equations, one for each mode, which are precisely the equations of forced, damped harmonic oscillators. The long-time behavior is then read off from the known solutions of those ordinary differential equations.

\medskip

\noindent\textbf{(a) Eigenfunction expansion and reduction to ODEs.}
We begin with the standard eigenvalue problem associated with the one-dimensional Laplacian with homogeneous Dirichlet boundary conditions:
\[
-X''(x) = \lambda X(x),\qquad 0<x<L,\qquad X(0)=X(L)=0.
\]
Nontrivial solutions exist exactly when
\[
X_n(x) = \sin\!\Bigl(\frac{n\pi x}{L}\Bigr),\qquad \lambda_n = \biggl(\frac{n\pi}{L}\biggr)^2,\qquad n=1,2,3,\dots.
\]
The family $\{X_n\}_{n=1}^\infty$ is an orthogonal basis of $L^2(0,L)$, and we normalize it in the usual way if desired. In particular, any function in $L^2(0,L)$ can be expanded in this basis, with convergence in $L^2$ and, for smooth functions, pointwise.

We therefore expand the forcing and the solution as
\[
f(x) = \sum_{n=1}^\infty f_n\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr),\qquad
u(x,t) = \sum_{n=1}^\infty y_n(t)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr),
\]
where the coefficients of $f$ are given by the usual Fourier sine series formula,
\[
f_n = \frac{2}{L}\int_0^L f(x)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr)\,dx.
\]

We now substitute the series for $u$ into the damped, forced wave equation
\[
u_{tt} + 2\beta u_t = c^2 u_{xx} + f(x)\cos(\omega t).
\]
Termwise differentiation (justified by standard convergence results for Fourier series of smooth functions) gives
\[
u_t(x,t) = \sum_{n=1}^\infty y_n'(t)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr),\qquad
u_{tt}(x,t) = \sum_{n=1}^\infty y_n''(t)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr),
\]
and, using $X_n''(x) = -\lambda_n X_n(x)$ with $\lambda_n=(n\pi/L)^2$,
\[
u_{xx}(x,t) = \sum_{n=1}^\infty y_n(t)\,X_n''(x)
             = -\sum_{n=1}^\infty \lambda_n\,y_n(t)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr).
\]

Substituting these into the PDE, we obtain
\[
\sum_{n=1}^\infty y_n''(t)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr)
+ 2\beta \sum_{n=1}^\infty y_n'(t)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr)
= -c^2 \sum_{n=1}^\infty \lambda_n y_n(t)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr)
  + \cos(\omega t)\sum_{n=1}^\infty f_n\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr).
\]

Rearranging terms and comparing coefficients of the orthogonal family $\{\sin(n\pi x/L)\}$, we see that for each $n\ge1$,
\[
y_n''(t) + 2\beta\,y_n'(t) + c^2\lambda_n y_n(t) = f_n\cos(\omega t),
\]
where
\[
\lambda_n = \biggl(\frac{n\pi}{L}\biggr)^2.
\]
Thus each modal coefficient $y_n$ satisfies a forced, damped harmonic oscillator equation
\[
y_n'' + 2\beta y_n' + \Omega_n^2\,y_n = f_n\cos(\omega t),
\]
with ``natural frequency'' $\Omega_n = c\sqrt{\lambda_n} = c n\pi/L$. The initial conditions $u(x,0)=0$ and $u_t(x,0)=0$ imply
\[
y_n(0) = 0,\qquad y_n'(0)=0,\qquad n=1,2,\dots,
\]
since the sine basis is complete and orthogonal.

This completes the reduction to an infinite family of ordinary differential equations.

\medskip

\noindent\textbf{(b) Solution of the modal ODEs and the steady state.}
We now solve, for fixed $n$,
\[
y_n''(t) + 2\beta\,y_n'(t) + \Omega_n^2\,y_n(t) = f_n\cos(\omega t),
\qquad y_n(0)=0,\ y_n'(0)=0,
\]
where $\Omega_n = c n\pi/L$ and $\beta>0$. Since the structure is identical for each $n$, we first treat the general scalar equation
\[
y''(t) + 2\beta\,y'(t) + \Omega^2 y(t) = F\cos(\omega t),
\]
and then specialize.

The corresponding homogeneous equation
\[
y'' + 2\beta y' + \Omega^2 y = 0
\]
has characteristic polynomial $r^2+2\beta r+\Omega^2=0$ with roots
\[
r = -\beta \pm \sqrt{\beta^2-\Omega^2}.
\]
In the underdamped case $\beta<\Omega$, which is the typical physical situation, these roots are complex conjugates
\[
r = -\beta \pm i\,\sqrt{\Omega^2-\beta^2},
\]
and the homogeneous solution can be written as
\[
y_{\mathrm{hom}}(t) = e^{-\beta t}\Bigl(C_1\cos(\omega_d t)+C_2\sin(\omega_d t)\Bigr),
\quad \omega_d := \sqrt{\Omega^2-\beta^2}.
\]
In any case, $y_{\mathrm{hom}}(t)$ decays exponentially to zero as $t\to\infty$ because $\beta>0$.

To find a particular solution of the inhomogeneous equation, we use the method of undetermined coefficients. We seek $y_p(t)$ of the form
\[
y_p(t) = A\cos(\omega t) + B\sin(\omega t),
\]
where $A$ and $B$ are constants to be determined. Differentiating,
\[
y_p'(t) = -A\omega\sin(\omega t) + B\omega\cos(\omega t),
\]
\[
y_p''(t) = -A\omega^2\cos(\omega t) - B\omega^2\sin(\omega t).
\]
Substituting into the differential equation yields
\[
\begin{aligned}
&\bigl(-A\omega^2\cos\omega t - B\omega^2\sin\omega t\bigr)
+ 2\beta\bigl(-A\omega\sin\omega t + B\omega\cos\omega t\bigr)\\
&\qquad + \Omega^2\bigl(A\cos\omega t + B\sin\omega t\bigr)
= F\cos(\omega t).
\end{aligned}
\]
Grouping terms with $\cos(\omega t)$ and $\sin(\omega t)$ separately, we have
\[
\Bigl[(-\omega^2+\Omega^2)A + 2\beta\omega B\Bigr]\cos(\omega t)
+ \Bigl[(-\omega^2+\Omega^2)B - 2\beta\omega A\Bigr]\sin(\omega t)
= F\cos(\omega t).
\]
For this identity to hold for all $t$, the sine and cosine coefficients must match:
\[
(-\omega^2+\Omega^2)A + 2\beta\omega B = F,\qquad
(-\omega^2+\Omega^2)B - 2\beta\omega A = 0.
\]
Solving this $2\times2$ linear system for $A$ and $B$ gives
\[
A = \frac{F(\Omega^2-\omega^2)}{(\Omega^2-\omega^2)^2 + 4\beta^2\omega^2},\qquad
B = \frac{2\beta\omega F}{(\Omega^2-\omega^2)^2 + 4\beta^2\omega^2}.
\]
Thus a particular solution is
\[
y_p(t) = A\cos(\omega t) + B\sin(\omega t)
\]
with $A$ and $B$ as above. The general solution of the inhomogeneous equation is
\[
y(t) = y_{\mathrm{hom}}(t) + y_p(t).
\]

Returning now to the modal equation for $y_n$, we set $F=f_n$ and $\Omega=\Omega_n=c n\pi/L$. With zero initial data $y_n(0)=0$, $y_n'(0)=0$, the homogeneous constants $C_1$ and $C_2$ are chosen so that the total solution satisfies these initial conditions. However, regardless of their precise values, the homogeneous contribution $y_{\mathrm{hom},n}(t)$ decays exponentially because $\beta>0$. Therefore, the \emph{steady-state} or long-time periodic solution is given simply by the particular solution
\[
y_{n,\mathrm{ss}}(t) = A_n\cos(\omega t) + B_n\sin(\omega t),
\]
where
\[
A_n = \frac{f_n(\Omega_n^2-\omega^2)}{(\Omega_n^2-\omega^2)^2 + 4\beta^2\omega^2},
\qquad
B_n = \frac{2\beta\omega f_n}{(\Omega_n^2-\omega^2)^2 + 4\beta^2\omega^2},
\]
and
\[
\Omega_n^2 = c^2\lambda_n = c^2\biggl(\frac{n\pi}{L}\biggr)^2.
\]

Substituting back into the series for $u$, the steady-state solution of the original partial differential equation is
\[
u_{\mathrm{ss}}(x,t)
= \sum_{n=1}^\infty \Bigl(A_n\cos(\omega t) + B_n\sin(\omega t)\Bigr)\,\sin\!\Bigl(\frac{n\pi x}{L}\Bigr).
\]
This is a time-periodic solution with the same driving frequency $\omega$ as the forcing, and with spatial profile given by a Fourier sine series whose coefficients depend on $\omega$, $\beta$, and the forcing profile $f$ through the numbers $f_n$.

\medskip

\noindent\textbf{(c) Amplitudes, frequency dependence, and resonance.}
For each fixed mode $n$, the steady-state motion is sinusoidal in time with frequency $\omega$ and some phase shift relative to the forcing. It is convenient to compute the amplitude of oscillation in the $n$th mode.

The modal contribution is
\[
y_{n,\mathrm{ss}}(t) = A_n\cos(\omega t) + B_n\sin(\omega t).
\]
This can be rewritten in the single-sine form
\[
y_{n,\mathrm{ss}}(t) = R_n(\omega)\,\cos\bigl(\omega t - \phi_n\bigr),
\]
where
\[
R_n(\omega) = \sqrt{A_n^2 + B_n^2}
\]
is the amplitude and $\phi_n$ is a phase shift determined by $\tan\phi_n = B_n/A_n$ (with appropriate quadrant conventions).

Using the previous expressions for $A_n$ and $B_n$, we obtain
\[
\begin{aligned}
A_n^2 + B_n^2
&= \frac{f_n^2(\Omega_n^2-\omega^2)^2}{\bigl[(\Omega_n^2-\omega^2)^2 + 4\beta^2\omega^2\bigr]^2}
 + \frac{4\beta^2\omega^2 f_n^2}{\bigl[(\Omega_n^2-\omega^2)^2 + 4\beta^2\omega^2\bigr]^2}\\
&= \frac{f_n^2\bigl[(\Omega_n^2-\omega^2)^2 + 4\beta^2\omega^2\bigr]}
         {\bigl[(\Omega_n^2-\omega^2)^2 + 4\beta^2\omega^2\bigr]^2}\\
&= \frac{f_n^2}{(\Omega_n^2-\omega^2)^2 + 4\beta^2\omega^2}.
\end{aligned}
\]
Therefore the amplitude is
\[
R_n(\omega)
= \frac{|f_n|}{\sqrt{(\Omega_n^2-\omega^2)^2 + 4\beta^2\omega^2}}
= \frac{|f_n|}
       {\sqrt{\bigl(c^2(n\pi/L)^2-\omega^2\bigr)^2 + 4\beta^2\omega^2}}.
\]
This is precisely the formula requested in the problem statement.

Now we discuss the dependence on $\omega$ and $\beta$. As a function of $\omega$, for fixed $n$ and fixed parameters $c$, $L$, $\beta$, and $f_n$, the amplitude $R_n(\omega)$ behaves like the familiar resonance curve of a forced, damped harmonic oscillator:

\begin{itemize}
  \item The quantity $\Omega_n = c n\pi/L$ is the natural angular frequency of the $n$th mode of the undamped, unforced string. The denominator of $R_n(\omega)$ is smallest, and hence $R_n(\omega)$ is largest, for driving frequencies $\omega$ near $\Omega_n$, up to a small shift due to damping. This is the resonance phenomenon: the system responds most strongly when driven near its natural frequency.

  \item The presence of $\beta>0$ prevents the denominator from vanishing. In the undamped case $\beta=0$, the denominator would be $|\Omega_n^2-\omega^2|$, which becomes arbitrarily small as $\omega\to\Omega_n$, leading to unbounded growth of the amplitude in time (true resonance). With damping, however, the term $4\beta^2\omega^2$ ensures that the denominator remains positive, and the amplitude remains finite for all $\omega$.

  \item Increasing $\beta$ broadens and lowers the resonance peak: the maximum of $R_n(\omega)$ decreases, and the frequency interval over which the amplitude is relatively large widens. Physically, stronger damping dissipates energy more quickly, so even when the system is driven near resonance it cannot build up very large oscillations.

  \item For driving frequencies far from resonance ($|\omega|\gg\Omega_n$ or $|\omega|\ll\Omega_n$), the denominator is large and $R_n(\omega)$ is small, meaning that the forcing is inefficient at exciting that mode.
\end{itemize}

Putting all the modes together, the steady-state displacement is a superposition of sinusoidal oscillations at the driving frequency $\omega$, each weighted by the amplitude $R_n(\omega)$ and by the spatial mode shape $\sin(n\pi x/L)$. The forcing profile $f(x)$ enters only through its Fourier coefficients $
$f_n$; modes for which $|f_n|$ is small are weakly excited even if the driving frequency is close to their natural frequency, whereas modes with large $|f_n|$ dominate the steady-state response.

\end{solution}

% ===== Example 5: Two-Dimensional Membrane Vibrations (inquiry-based) =====
\begin{problem}[Two-Dimensional Membrane Vibrations]
A thin, elastic membrane is stretched over a rectangular frame, like a drumhead, and clamped along the entire boundary so that its edge cannot move. Let $u(x,y,t)$ denote the vertical displacement of the membrane above the $(x,y)$-plane at time $t$. Under standard small-amplitude and homogeneous-tension assumptions, $u$ satisfies the two-dimensional wave equation on the interior of the rectangle. In this problem you will rediscover how to separate variables in two spatial dimensions, identify the normal modes of vibration, and express the general motion as a superposition of these modes. Along the way, notice how the two one-dimensional problems in $x$ and $y$ combine to give a richer structure of eigenvalues and mode shapes.

Consider a rectangular membrane occupying the region
\[
0 < x < a,\qquad 0 < y < b,
\]
with wave speed $c>0$, whose boundary is fixed (clamped) along the entire rectangle. The vertical displacement $u(x,y,t)$ satisfies the two-dimensional wave equation
\[
u_{tt} = c^2 \bigl(u_{xx}+u_{yy}\bigr)
\]
for $0<x<a$, $0<y<b$, $t>0$.

\medskip

(a) Write down the boundary conditions that correspond to the membrane being fixed along all four sides of the rectangle. Then state a general form of initial conditions describing an initial displacement and initial velocity of the membrane. That is, give $u(x,y,0)$ and $u_t(x,y,0)$ in terms of arbitrary functions $f(x,y)$ and $g(x,y)$.

\medskip

(b) We aim to solve the initial–boundary value problem using separation of variables in the form
\[
u(x,y,t) = X(x)\,Y(y)\,T(t).
\]
Substitute this ansatz into the wave equation and separate the variables.

\begin{itemize}
\item[(i)] After substituting $u = X Y T$, divide the resulting equation so that each term depends on only one variable (or combination of variables). Show that you can rewrite the PDE in the form
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)}.
\]
Why must both sides of this equation be equal to a \emph{constant}? Introduce a separation constant and explain your choice of its sign.

\item[(ii)] Perform a second separation to split the $x$- and $y$-dependent parts. Introduce appropriate separation constants so that you arrive at three ordinary differential equations: one for $T(t)$, one for $X(x)$, and one for $Y(y)$.
\end{itemize}

Hint: Follow the pattern from the one-dimensional wave equation: try to arrange things so that the time equation is of the form $T'' + \omega^2 T = 0$ with $\omega^2>0$, giving oscillatory behavior in time.

\medskip

(c) The boundary conditions at $x=0$ and $x=a$ imply homogeneous boundary conditions for $X(x)$, and similarly the conditions at $y=0$ and $y=b$ imply conditions for $Y(y)$.

\begin{itemize}
\item[(i)] Using your separated ODEs, write the boundary value problems satisfied by $X(x)$ and $Y(y)$. Show that you obtain two Sturm–Liouville eigenvalue problems of the form
\[
X''(x) + \lambda_x X(x)=0,\quad X(0)=X(a)=0,
\]
\[
Y''(y) + \lambda_y Y(y)=0,\quad Y(0)=Y(b)=0,
\]
for suitable eigenvalues $\lambda_x$ and $\lambda_y$.

\item[(ii)] Solve each of these one-dimensional eigenvalue problems. Determine the eigenvalues and corresponding eigenfunctions explicitly. You should find discrete families
\[
\lambda_x = \left(\frac{m\pi}{a}\right)^2,\qquad X_m(x) = \sin\!\left(\frac{m\pi x}{a}\right),\quad m=1,2,3,\dots,
\]
and an analogous description for $Y_n(y)$.

\item[(iii)] Using your separation constants, express the corresponding time equation for the $(m,n)$-mode as an ODE for $T_{m,n}(t)$ and solve it.
\end{itemize}

Hint: For part (ii), recall the one-dimensional string fixed at both ends, and reuse that analysis carefully.

\medskip

(d) Combine your results to construct the normal modes of vibration and the general solution.

\begin{itemize}
\item[(i)] Show that for each pair of positive integers $(m,n)$ you obtain a separated solution of the form
\[
u_{m,n}(x,y,t) = \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\Bigl(A_{m,n}\cos(\omega_{m,n} t) + B_{m,n}\sin(\omega_{m,n} t)\Bigr),
\]
and identify the angular frequency $\omega_{m,n}$ in terms of $c$, $a$, $b$, $m$, and $n$.

\item[(ii)] Argue (using the superposition principle) that the general solution satisfying the homogeneous boundary conditions can be written as a double series
\[
u(x,y,t) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\Bigl(A_{m,n}\cos(\omega_{m,n} t) + B_{m,n}\sin(\omega_{m,n} t)\Bigr).
\]

\item[(iii)] Use the initial conditions to derive formulas for $A_{m,n}$ and $B_{m,n}$ as double Fourier sine coefficients of $f(x,y)$ and $g(x,y)$. Write these formulas explicitly as integrals over the rectangle $(0,a)\times(0,b)$.

\end{itemize}

Hint: Use the orthogonality of the sine functions in $x$ and in $y$ separately, and then combine them. For example,
\[
\int_0^a \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{m'\pi x}{a}\right)\,dx
= \begin{cases}
0, & m\neq m',\\[4pt]
\frac{a}{2}, & m=m'.
\end{cases}
\]

\medskip

(e) Extensions and variations.

\begin{itemize}
\item[(i)] \emph{Neumann boundary conditions:} Suppose the membrane is attached to a frictionless frame so that the slope normal to the boundary is zero, but the edge is free to move up and down. This is modeled by homogeneous Neumann boundary conditions $u_x(0,y,t)=u_x(a,y,t)=0$ and $u_y(x,0,t)=u_y(x,b,t)=0$. How does the spatial eigenvalue problem change? What do the $X_m(x)$ and $Y_n(y)$ eigenfunctions look like in this case?

\item[(ii)] \emph{Circular membrane:} Suppose instead that the membrane is a circular drum of radius $R$ and you use polar coordinates $(r,\theta)$. Without carrying out all computations, outline how the separation-of-variables procedure would proceed starting from the wave equation
\[
u_{tt} = c^2\left(u_{rr} + \frac{1}{r}u_r + \frac{1}{r^2}u_{\theta\theta}\right),
\]
with $u(R,\theta,t)=0$. Which new special functions arise in the radial eigenvalue problem, and why is this problem more intricate than the rectangular case?
\end{itemize}

\end{problem}

% ===== Example 5: Two-Dimensional Membrane Vibrations (full solution) =====
\begin{problem}[Two-Dimensional Membrane Vibrations]
Consider a rectangular membrane occupying $0<x<a$, $0<y<b$, with constant wave speed $c>0$ and clamped edges. The vertical displacement $u(x,y,t)$ satisfies
\[
u_{tt} = c^2(u_{xx}+u_{yy}),\qquad 0<x<a,\ 0<y<b,\ t>0,
\]
with boundary conditions
\[
u(0,y,t)=u(a,y,t)=u(x,0,t)=u(x,b,t)=0,
\]
and initial data
\[
u(x,y,0) = f(x,y),\qquad u_t(x,y,0) = g(x,y).
\]
Using separation of variables, find the normal modes of vibration and their frequencies, and show that the solution can be written as
\[
u(x,y,t) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\Bigl(A_{m,n}\cos(\omega_{m,n} t) + B_{m,n}\sin(\omega_{m,n} t)\Bigr),
\]
for suitable $\omega_{m,n}$, $A_{m,n}$, and $B_{m,n}$. Express $A_{m,n}$ and $B_{m,n}$ explicitly as double Fourier sine coefficients of $f$ and $g$.
\end{problem}

\begin{solution}
We solve the two-dimensional wave equation on a rectangle with homogeneous Dirichlet boundary conditions by separation of variables. This example illustrates the main ideas for hyperbolic partial differential equations in a homogeneous medium: separation into simpler eigenvalue problems, identification of normal modes, and expansion of general solutions as orthogonal series.

\medskip

\noindent\textbf{1. Separation of variables.}
We seek separated solutions of the form
\[
u(x,y,t) = X(x)\,Y(y)\,T(t).
\]
Substituting into the PDE $u_{tt}=c^2(u_{xx}+u_{yy})$ gives
\[
X(x)Y(y)T''(t) = c^2\bigl(X''(x)Y(y)T(t) + X(x)Y''(y)T(t)\bigr).
\]
Assuming $X$, $Y$, and $T$ are nonzero, we divide both sides by $c^2 X(x)Y(y)T(t)$ to obtain
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)}.
\]
The left-hand side depends on $t$ only, while the right-hand side depends on $x$ and $y$. Therefore both sides must be equal to a constant; denote this constant by $-\lambda$:
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)} = -\lambda.
\]
We will see that $\lambda>0$, leading to oscillatory time dependence.

Rewriting,
\[
\frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)} = -\lambda.
\]
Rearrange to group $x$- and $y$-dependence separately:
\[
\frac{X''(x)}{X(x)} + \lambda = -\,\frac{Y''(y)}{Y(y)}.
\]
The left side depends on $x$ only, the right side on $y$ only, so both must equal another constant, say $\mu$:
\[
\frac{X''(x)}{X(x)} + \lambda = \mu, \qquad -\frac{Y''(y)}{Y(y)} = \mu.
\]
Thus we obtain
\[
X''(x) = (\mu - \lambda) X(x), \qquad Y''(y) = -\mu\,Y(y),
\]
and the time equation
\[
T''(t) + c^2\lambda\,T(t) = 0.
\]

It is more convenient to parameterize by nonnegative constants that will become spatial eigenvalues. Write
\[
X''(x) + \lambda_x X(x) = 0,\qquad Y''(y) + \lambda_y Y(y) = 0,
\]
with
\[
\lambda_x>0,\quad \lambda_y>0,\quad \lambda_x + \lambda_y = \lambda,
\]
so that the time equation becomes
\[
T''(t) + c^2(\lambda_x+\lambda_y) T(t) = 0.
\]

\medskip

\noindent\textbf{2. Boundary conditions and spatial eigenvalue problems.}
The boundary conditions on $u$ induce boundary conditions on $X$ and $Y$.

From $u(0,y,t)=0$ for all $y$ and $t$, and $u(x,y,t)=X(x)Y(y)T(t)$, we have
\[
X(0)Y(y)T(t) = 0 \quad \text{for all } y,t.
\]
Nontrivial solutions require $Y$ and $T$ not identically zero, so we must have
\[
X(0) = 0.
\]
Similarly, from $u(a,y,t)=0$ for all $y,t$ we get $X(a)=0$. The conditions $u(x,0,t)=0$ and $u(x,b,t)=0$ give
\[
Y(0) = 0,\qquad Y(b)=0.
\]
Hence $X$ and $Y$ satisfy the Sturm–Liouville problems
\[
X''(x) + \lambda_x X(x)=0,\quad X(0)=X(a)=0,
\]
\[
Y''(y) + \lambda_y Y(y)=0,\quad Y(0)=Y(b)=0.
\]

These are the same eigenvalue problems that arise for a one-dimensional string fixed at both ends.

\medskip

\noindent\textbf{3. Solving the one-dimensional eigenvalue problems.}
Consider first $X'' + \lambda_x X = 0$ with $X(0)=X(a)=0$. As in the one-dimensional case, nontrivial solutions occur only for
\[
\lambda_x = \left(\frac{m\pi}{a}\right)^2,\quad m=1,2,3,\dots,
\]
with eigenfunctions
\[
X_m(x) = \sin\!\left(\frac{m\pi x}{a}\right).
\]
Similarly, for $Y$ we find
\[
\lambda_y = \left(\frac{n\pi}{b}\right)^2,\quad n=1,2,3,\dots,
\]
with eigenfunctions
\[
Y_n(y) = \sin\!\left(\frac{n\pi y}{b}\right).
\]
Each pair of positive integers $(m,n)$ yields a spatial eigenfunction
\[
\Phi_{m,n}(x,y) = X_m(x)Y_n(y)
= \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right).
\]

For this $(m,n)$-mode, the total spatial eigenvalue in the time equation is
\[
\lambda = \lambda_x + \lambda_y
= \left(\frac{m\pi}{a}\right)^2 + \left(\frac{n\pi}{b}\right)^2.
\]
Thus the time equation becomes
\[
T_{m,n}''(t) + c^2\biggl[\left(\frac{m\pi}{a}\right)^2 + \left(\frac{n\pi}{b}\right)^2\biggr]\,T_{m,n}(t) = 0.
\]
Set
\[
\omega_{m,n} = c\,\pi \sqrt{\left(\frac{m}{a}\right)^2 + \left(\frac{n}{b}\right)^2},
\]
so that the solution of the time ODE is
\[
T_{m,n}(t) = A_{m,n}\cos(\omega_{m,n} t) + B_{m,n}\sin(\omega_{m,n} t),
\]
for arbitrary constants $A_{m,n}$ and $B_{m,n}$.

\medskip

\noindent\textbf{4. Normal modes and general solution.}
Combining the spatial and temporal factors, the separated solution corresponding to the pair $(m,n)$ is
\[
u_{m,n}(x,y,t) =
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\Bigl(A_{m,n}\cos(\omega_{m,n} t) + B_{m,n}\sin(\omega_{m,n} t)\Bigr).
\]
Each $u_{m,n}$ is a \emph{normal mode} of vibration, with fixed spatial pattern
\[
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\]
and time-oscillation frequency $\omega_{m,n}$. 

The wave equation is linear with homogeneous boundary conditions, so we may superpose all such modes. The most general solution satisfying the boundary conditions can therefore be expressed as the double series
\[
u(x,y,t) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\Bigl(A_{m,n}\cos(\omega_{m,n} t) + B_{m,n}\sin(\omega_{m,n} t)\Bigr).
\]

\medskip

\noindent\textbf{5. Imposing the initial conditions.}
We now determine $A_{m,n}$ and $B_{m,n}$ from the given initial data
\[
u(x,y,0) = f(x,y),\qquad u_t(x,y,0) = g(x,y).
\]

At $t=0$ we have
\[
u(x,y,0) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\Bigl(A_{m,n}\cos 0 + B_{m,n}\sin 0\Bigr)
= \sum_{m,n} A_{m,n}
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right).
\]
Thus $f$ has a double sine series expansion
\[
f(x,y) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
A_{m,n}\,\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right).
\]

Similarly, differentiate $u$ with respect to $t$:
\[
u_t(x,y,t) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\Bigl(-A_{m,n}\omega_{m,n}\sin(\omega_{m,n} t) + B_{m,n}\omega_{m,n}\cos(\omega_{m,n} t)\Bigr).
\]
At $t=0$,
\[
u_t(x,y,0) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\bigl(B_{m,n}\omega_{m,n}\bigr).
\]
Thus $g$ has the double sine series expansion
\[
g(x,y) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
B_{m,n}\omega_{m,n}\,\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right).
\]

We use orthogonality of sines on $(0,a)$ and $(0,b)$ to compute the coefficients. Recall
\[
\int_0^a \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{m'\pi x}{a}\right)\,dx
= \begin{cases}
0, & m\neq m',\\[4pt]
\frac{a}{2}, & m=m',
\end{cases}
\]
and analogously
\[
\int_0^b \sin\!\left(\frac{n\pi y}{b}\right)\sin\!\left(\frac{n'\pi y}{b}\right)\,dy
= \begin{cases}
0, & n\neq n',\\[4pt]
\frac{b}{2}, & n=n'.
\end{cases}
\]

To obtain $A_{m,n}$, multiply the expansion for $f(x,y)$ by
\[
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\]
and integrate over the rectangle:
\[
\int_0^a\int_0^b f(x,y)
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)\,dy\,dx
= \sum_{m',n'} A_{m',n'}
\int_0^a \sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{m'\pi x}{a}\right)dx
\int_0^b \sin\!\left(\frac{n\pi y}{b}\right)\sin\!\left(\frac{n'\pi y}{b}\right)dy.
\]
By orthogonality, all terms vanish except when $m'=m$ and $n'=n$. Hence
\[
\int_0^a\int_0^b f(x,y)
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)\,dy\,dx
= A_{m,n}\,\frac{a}{2}\,\frac{b}{2} = A_{m,n}\,\frac{ab}{4}.
\]
Therefore
\[
A_{m,n} = \frac{4}{ab}\int_0^a\int_0^b f(x,y)
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)\,dy\,dx.
\]

The same procedure applied to $g(x,y)$ yields
\[
\int_0^a\int_0^b g(x,y)
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)\,dy\,dx
= B_{m,n}\omega_{m,n}\,\frac{ab}{4},
\]
so
\[
B_{m,n} = \frac{4}{ab\,\omega_{m,n}}
\int_0^a\int_0^b g(x,y)
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)\,dy\,dx.
\]

\medskip

\noindent\textbf{6. Final form and interpretation.}
We have shown that the displacement of the rectangular membrane can be written as
\[
u(x,y,t) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)
\Bigl(A_{m,n}\cos(\omega_{m,n} t) + B_{m,n}\sin(\omega_{m,n} t)\Bigr),
\]
with
\[
\omega_{m,n} = c\,\pi \sqrt{\left(\frac{m}{a}\right)^2 + \left(\frac{n}{b}\right)^2},
\]
and
\[
A_{m,n} = \frac{4}{ab}\int_0^a\int_0^b f(x,y)
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)\,dy\,dx,
\]
\[
B_{m,n} = \frac{4}{ab\,\omega_{m,n}}
\int_0^a\int_0^b g(x,y)
\sin\!\left(\frac{m\pi x}{a}\right)\sin\!\left(\frac{n\pi y}{b}\right)\,dy\,dx.
\]

Each normal mode labeled by $(m,n)$ is an eigenfunction of the Laplacian $-\Delta$ with Dirichlet boundary conditions, and its oscillation frequency is determined by the corresponding eigenvalue. The solution is a superposition of these modes with coefficients chosen to match the initial displacement and velocity. This is the two-dimensional analogue of the vibrating string, and it encapsulates the central ideas of waves in a homogeneous medium and hyperbolic partial differential equations: decomposition into eigenmodes, orthogonality, and the role of the spectrum of the spatial operator in determining the temporal behavior.
\end{solution}

\section{Diffusion Equation}
% --- Narrative plan (auto-generated) ---
% This section introduces the diffusion, or heat, equation as a central model for how quantities such as temperature, chemicals, or probability densities spread out in space over time. We begin with concrete physical and probabilistic pictures, such as heat flowing along a metal rod or dye dispersing in water, and progressively uncover the mathematical structure behind these phenomena. Along the way, we learn how local balancing laws, like conservation of energy or mass, naturally lead to partial differential equations that govern the evolution of entire systems.
%
% The diffusion equation sits at an important crossroads in applied mathematics. It is simple enough to be solved explicitly in many settings, yet rich enough to exhibit phenomena such as smoothing, long-time equilibration, and sensitivity to geometry and boundary conditions. Its solutions can often be expressed in terms of Fourier series or Fourier transforms, so studying diffusion gives us an opportunity to practice separation of variables and spectral methods, and to see how tools from ordinary differential equations reappear in the analysis of partial differential equations.
%
% Conceptually, this section connects to several major themes. The Gaussian function arises naturally as the fundamental solution of the diffusion equation, linking our study to probability theory and to complex analysis, where the same Gaussian integrals appear in contour methods. The long-time behavior of solutions can be understood using eigenvalues and eigenfunctions of associated spatial operators, tying diffusion to dynamical systems and linear algebra. By the end of the section, the reader will see how diffusion provides a model laboratory where techniques from Fourier analysis, linear ODE theory, and PDE theory all come together.

% ===== Example 1: Heat Flow in a Finite Rod with Fixed End Temperatures (inquiry-based) =====
\begin{problem}[Heat Flow in a Finite Rod with Fixed End Temperatures]
Consider a thin, homogeneous metal rod of length $L$, lying along the $x$-axis from $x=0$ to $x=L$. The rod is perfectly insulated along its lateral surface, so heat can flow only along the $x$-direction. The left end is held at temperature $0$, and the right end is held at a fixed temperature $T_0>0$. At time $t=0$ the temperature distribution along the rod is given by a prescribed function $f(x)$. We want to derive and solve the mathematical model that describes the temperature $u(x,t)$ in the rod for $t>0$.

(a) Use conservation of energy and Fourier's law of heat conduction to derive the one-dimensional diffusion (heat) equation for the temperature $u(x,t)$ in the rod. Clearly state any physical assumptions you make, and indicate the role of the thermal diffusivity $\kappa>0$.
\medskip

(b) Translate the physical description into mathematical initial and boundary conditions for $u(x,t)$. Write down the initial value problem you obtain in the form
\[
\begin{cases}
u_t = \kappa u_{xx}, & 0<x<L,\ t>0, \\
\text{(boundary conditions)}, & t>0, \\
\text{(initial condition)}, & 0<x<L.
\end{cases}
\]
What is the type of boundary condition at each end (Dirichlet, Neumann, or Robin), and are they homogeneous or inhomogeneous?
\medskip

(c) A standard strategy for dealing with inhomogeneous Dirichlet boundary conditions is to subtract off a steady-state solution. 

\begin{itemize}
\item[(i)] Find a time-independent temperature profile $v(x)$ that solves the \emph{steady-state} boundary value problem
\[
\kappa v''(x) = 0,\quad 0<x<L,\qquad
v(0) = 0,\quad v(L) = T_0.
\]
\item[(ii)] Define a new unknown $w(x,t) = u(x,t) - v(x)$. Show that $w$ satisfies a diffusion equation with \emph{homogeneous} boundary conditions. Write down explicitly the PDE, the boundary conditions, and the initial condition for $w$.
\end{itemize}
Hint: Carefully differentiate $w$ with respect to $t$ and $x$, and use the equation for $u$ and the fact that $v$ is independent of $t$.
\medskip

(d) Now solve the initial-boundary value problem for $w$ by separation of variables and Fourier series.

\begin{itemize}
\item[(i)] Look for separated solutions of the form $w(x,t)=X(x)T(t)$ and derive the two ordinary differential equations for $X$ and $T$, together with the boundary conditions for $X$.
\item[(ii)] Solve the spatial eigenvalue problem for $X(x)$ with homogeneous Dirichlet boundary conditions at $x=0$ and $x=L$. Identify the eigenvalues and the corresponding eigenfunctions.
\item[(iii)] Use these eigenfunctions to write down the general solution for $w(x,t)$ as an infinite series. How are the time-dependent factors determined?
\item[(iv)] Impose the initial condition for $w$ to obtain formulas for the Fourier coefficients in the series. Express the final answer for $u(x,t)$ in terms of $f(x)$, $T_0$, and $\kappa$.
\end{itemize}
Hint: You should obtain a sine series expansion for the function $f(x)-v(x)$ on the interval $(0,L)$.
\medskip

(e) Explore one or two variations on this model.

\begin{itemize}
\item[(i)] Suppose instead that \emph{both} ends of the rod are held at zero temperature, that is, $u(0,t)=u(L,t)=0$, but the initial condition $f(x)$ is nonzero. How does the method of separation of variables simplify in this purely homogeneous Dirichlet case? Which parts of your work in parts (c) and (d) are no longer needed?
\item[(ii)] Suppose the right end of the rod is perfectly insulated rather than held at fixed temperature; that is, the heat flux at $x=L$ is zero. How would you express this as a boundary condition on $u(x,t)$, and how would that change the eigenvalue problem in part (d)? (You do not need to solve the new problem completely, but describe in words and formulas what changes.)
\end{itemize}
\end{problem}

% ===== Example 1: Heat Flow in a Finite Rod with Fixed End Temperatures (full solution) =====
\begin{problem}[Heat Flow in a Finite Rod with Fixed End Temperatures]
A homogeneous rod of length $L$ lies along $0<x<L$ and is insulated along its sides. Its left end is held at temperature $0$ and its right end at temperature $T_0>0$, for all $t>0$. Let $u(x,t)$ denote the temperature at position $x$ and time $t$, and suppose the initial temperature profile is $u(x,0)=f(x)$.

\begin{enumerate}
\item[(i)] Derive the one-dimensional diffusion equation for $u(x,t)$ using conservation of energy and Fourier's law of heat conduction.
\item[(ii)] Solve the resulting initial–boundary value problem
\[
\begin{cases}
u_t = \kappa u_{xx}, & 0<x<L,\ t>0,\\[4pt]
u(0,t)=0,\quad u(L,t)=T_0, & t>0,\\[4pt]
u(x,0)=f(x), & 0<x<L,
\end{cases}
\]
by reducing to a problem with homogeneous boundary conditions and using separation of variables.
\end{enumerate}
Express your final answer for $u(x,t)$ in terms of $f(x)$, $T_0$, and $\kappa$.
\end{problem}

\begin{solution}
We first derive the governing equation and then solve the initial–boundary value problem. This example illustrates the central ideas for the diffusion equation: modeling via conservation and Fourier's law, reduction to homogeneous boundary conditions, and solution by eigenfunction expansions.

\medskip\noindent
\textbf{(i) Derivation of the diffusion equation.}
Let $u(x,t)$ denote the temperature in the rod at position $x$ and time $t$. Consider a small segment of the rod between $x$ and $x+\Delta x$. The amount of heat energy in this piece at time $t$ is
\[
\text{(heat in segment)} = c \rho A\, u(x,t)\,\Delta x,
\]
where $c$ is the specific heat, $\rho$ the density, and $A$ the cross-sectional area. These are assumed constant along the rod because it is homogeneous.

The rate of change of heat in the segment equals the net heat flux into the segment. Denote by $q(x,t)$ the heat flux (heat per unit area per unit time) along the $x$-direction, taken positive in the positive $x$-direction. Fourier's law of heat conduction states that
\[
q(x,t) = -k\,u_x(x,t),
\]
where $k>0$ is the thermal conductivity. Thus heat flows from hotter regions to colder regions.

The total heat flowing into the segment through its left face at $x$ per unit time is $-A q(x,t)$ (because flux in the positive $x$-direction corresponds to heat leaving through the right face). Similarly, the total heat flowing out through the right face at $x+\Delta x$ per unit time is $A q(x+\Delta x,t)$. Therefore the net rate of heat flow \emph{into} the segment is
\[
-A q(x,t) - A q(x+\Delta x,t)
= -A q(x,t) + A\bigl(-q(x+\Delta x,t)\bigr)
= -A\bigl(q(x+\Delta x,t) - q(x,t)\bigr).
\]
By conservation of energy,
\[
\frac{\partial}{\partial t}\bigl(c\rho A\,u(x,t)\,\Delta x\bigr)
= -A\bigl(q(x+\Delta x,t) - q(x,t)\bigr).
\]
Divide by $A\,\Delta x$ and rewrite the right-hand side as a difference quotient:
\[
c\rho\,u_t(x,t)
= -\,\frac{q(x+\Delta x,t) - q(x,t)}{\Delta x}.
\]
Letting $\Delta x\to 0$ and using the definition of the spatial derivative, we obtain
\[
c\rho\,u_t(x,t) = -q_x(x,t).
\]
Substituting Fourier's law $q=-k u_x$ yields
\[
c\rho\,u_t(x,t) = -\bigl(-k u_x(x,t)\bigr)_x = k u_{xx}(x,t).
\]
We define the \emph{thermal diffusivity}
\[
\kappa = \frac{k}{c\rho} > 0,
\]
which has units of length$^2$ per unit time. Then the governing equation becomes
\[
u_t = \kappa\,u_{xx},\qquad 0<x<L,\ t>0,
\]
which is the one-dimensional diffusion (or heat) equation.

\medskip\noindent
\textbf{(ii) Initial and boundary conditions.}
The ends are held at fixed temperatures:
\[
u(0,t) = 0,\qquad u(L,t) = T_0,\qquad t>0.
\]
These are Dirichlet boundary conditions, and they are inhomogeneous because the prescribed values are not both zero. The initial condition is
\[
u(x,0) = f(x),\qquad 0<x<L.
\]
Thus the initial–boundary value problem is exactly as stated in the problem.

\medskip\noindent
\textbf{Reduction to homogeneous boundary conditions.}
The inhomogeneous Dirichlet boundary conditions are not directly compatible with the usual sine-series separation of variables. A standard remedy is to subtract a steady-state solution that matches the boundary data.

We look for a time-independent function $v(x)$ satisfying the steady-state version of the PDE and the boundary conditions:
\[
\kappa v''(x)=0,\quad 0<x<L;\qquad v(0)=0,\quad v(L)=T_0.
\]
Integrating $v''(x)=0$ twice, we find
\[
v(x) = a x + b,
\]
for constants $a$ and $b$. The boundary conditions give
\[
v(0)=b=0,\qquad v(L) = aL + b = aL = T_0,
\]
so $a = T_0/L$. Thus
\[
v(x) = \frac{T_0}{L}x.
\]
This profile is the unique steady-state linear temperature distribution connecting the two end temperatures.

Now define
\[
w(x,t) = u(x,t) - v(x).
\]
We compute its derivatives:
\[
w_t = u_t - v_t = u_t,
\]
since $v$ is independent of time, and
\[
w_{xx} = u_{xx} - v_{xx}.
\]
Because $v''(x)=0$, we have $v_{xx}=0$, and hence $w_{xx}=u_{xx}$. Since $u$ satisfies $u_t = \kappa u_{xx}$, it follows that
\[
w_t = u_t = \kappa u_{xx} = \kappa w_{xx}.
\]
Thus $w$ satisfies the same diffusion equation:
\[
w_t = \kappa w_{xx},\qquad 0<x<L,\ t>0.
\]

Next we check the boundary conditions. At $x=0$ we have
\[
w(0,t) = u(0,t) - v(0) = 0 - 0 = 0,
\]
and at $x=L$,
\[
w(L,t) = u(L,t) - v(L) = T_0 - T_0 = 0.
\]
Hence $w$ satisfies \emph{homogeneous} Dirichlet boundary conditions:
\[
w(0,t)=0,\qquad w(L,t)=0,\qquad t>0.
\]
Finally, the initial condition becomes
\[
w(x,0) = u(x,0) - v(x) = f(x) - \frac{T_0}{L}x,\qquad 0<x<L.
\]

We have reduced the original problem to the following one for $w$:
\[
\begin{cases}
w_t = \kappa w_{xx}, & 0<x<L,\ t>0,\\[4pt]
w(0,t)=0,\quad w(L,t)=0, & t>0,\\[4pt]
w(x,0)=f(x) - \dfrac{T_0}{L}x, & 0<x<L.
\end{cases}
\]

\medskip\noindent
\textbf{Solution by separation of variables and Fourier series.}
We now solve the homogeneous Dirichlet problem for $w$. We look for separated solutions of the form
\[
w(x,t) = X(x)T(t).
\]
Substituting into the PDE gives
\[
X(x)T'(t) = \kappa X''(x)T(t).
\]
Assuming $X$ and $T$ are not identically zero, we divide by $\kappa X(x)T(t)$ to obtain
\[
\frac{T'(t)}{\kappa T(t)} = \frac{X''(x)}{X(x)} = -\lambda,
\]
where $-\lambda$ is a separation constant, independent of $x$ and $t$. This leads to the pair of ordinary differential equations
\[
\begin{cases}
X''(x) + \lambda X(x) = 0,\\[4pt]
T'(t) + \kappa\lambda T(t) = 0.
\end{cases}
\]
The boundary conditions on $w$ give conditions on $X$:
\[
w(0,t)=0 \implies X(0)T(t)=0 \implies X(0)=0, \quad
w(L,t)=0 \implies X(L)=0.
\]
Thus $X$ must satisfy
\[
X''(x) + \lambda X(x) = 0,\quad 0<x<L;\qquad X(0)=0,\ X(L)=0.
\]

This is a classical Sturm–Liouville eigenvalue problem. Nontrivial solutions exist only for certain values of $\lambda$, the eigenvalues, with corresponding eigenfunctions $X(x)$.

To find them, we analyze cases for $\lambda$:

\emph{Case 1:} $\lambda=0$. Then $X''(x)=0$, so $X(x)=ax+b$. The boundary conditions yield $X(0)=b=0$, $X(L)=aL+b=aL=0$, so $a=0$ as well. Thus $X\equiv 0$, which is trivial and discarded.

\emph{Case 2:} $\lambda<0$. Write $\lambda=-\mu^2$ with $\mu>0$. Then
\[
X''(x) - \mu^2 X(x) = 0,
\]
with general solution $X(x)=A e^{\mu x}+B e^{-\mu x}$. The boundary condition $X(0)=0$ gives $A+B=0$, so $B=-A$ and $X(x)=A(e^{\mu x}-e^{-\mu x})=2A\sinh(\mu x)$. The second boundary condition $X(L)=0$ implies $\sinh(\mu L)=0$, which forces $\mu=0$, contradicting $\mu>0$. Hence there are no nontrivial solutions for $\lambda<0$.

\emph{Case 3:} $\lambda>0$. Write $\lambda=\mu^2$ with $\mu>0$. Then
\[
X''(x) + \mu^2 X(x) = 0,
\]
with general solution $X(x)=A\cos(\mu x)+B\sin(\mu x)$. The condition $X(0)=0$ gives $A=0$, so $X(x)=B\sin(\mu x)$. The condition $X(L)=0$ then requires
\[
B\sin(\mu L)=0.
\]
For a nontrivial solution we need $B\neq 0$, so $\sin(\mu L)=0$, which implies
\[
\mu L = n\pi,\qquad n=1,2,3,\dots.
\]
Thus $\mu_n = \dfrac{n\pi}{L}$, and the eigenvalues are
\[
\lambda_n = \mu_n^2 = \left(\frac{n\pi}{L}\right)^2,\qquad n=1,2,3,\dots,
\]
with corresponding eigenfunctions
\[
X_n(x) = \sin\left(\frac{n\pi x}{L}\right).
\]

For each $\lambda_n$, the temporal equation $T'(t)+\kappa\lambda_n T(t)=0$ has solution
\[
T_n(t) = C_n \exp\bigl(-\kappa\lambda_n t\bigr)
= C_n \exp\left(-\kappa \left(\frac{n\pi}{L}\right)^2 t\right),
\]
where $C_n$ is a constant.

Multiplying $X_n$ and $T_n$, we obtain separated solutions
\[
w_n(x,t)=\sin\left(\frac{n\pi x}{L}\right)\exp\left(-\kappa\left(\frac{n\pi}{L}\right)^2 t\right).
\]
By linearity of the PDE and boundary conditions, any linear combination of these is again a solution. Hence the general solution satisfying homogeneous Dirichlet boundary conditions is
\[
w(x,t) = \sum_{n=1}^{\infty} b_n \sin\left(\frac{n\pi x}{L}\right)\exp\left(-\kappa\left(\frac{n\pi}{L}\right)^2 t\right),
\]
where the coefficients $b_n$ are determined from the initial condition.

We impose $w(x,0)=f(x)-\dfrac{T_0}{L}x$. At $t=0$ the exponential factors equal $1$, so
\[
w(x,0) = \sum_{n=1}^{\infty} b_n \sin\left(\frac{n\pi x}{L}\right)
= f(x) - \frac{T_0}{L}x,\qquad 0<x<L.
\]
This expresses the function $f(x)-\dfrac{T_0}{L}x$ as a Fourier sine series on $(0,L)$. The standard formula for the sine coefficients gives
\[
b_n = \frac{2}{L}\int_0^L\left(f(\xi) - \frac{T_0}{L}\xi\right)\sin\left(\frac{n\pi \xi}{L}\right)\,d\xi,
\qquad n=1,2,3,\dots.
\]

\medskip\noindent
\textbf{Final expression for the temperature.}
Recall that $u(x,t) = v(x) + w(x,t)$, with $v(x) = \dfrac{T_0}{L}x$. Therefore
\[
u(x,t) = \frac{T_0}{L}x
+ \sum_{n=1}^{\infty} b_n \sin\left(\frac{n\pi x}{L}\right)
\exp\left(-\kappa\left(\frac{n\pi}{L}\right)^2 t\right),
\]
where
\[
b_n = \frac{2}{L}\int_0^L\left(f(\xi) - \frac{T_0}{L}\xi\right)\sin\left(\frac{n\pi \xi}{L}\right)\,d\xi.
\]

This solution consists of a steady-state linear temperature profile plus a transient part expressed as a sine series with exponentially decaying modes. Each mode $\sin\left(\dfrac{n\pi x}{L}\right)$ decays at a rate proportional to $\exp\left(-\kappa\left(\dfrac{n\pi}{L}\right)^2 t\right)$, so higher spatial frequencies (larger $n$) decay faster. This behavior—smoothing in space and decay of high-frequency components in time—is characteristic of solutions of the diffusion equation and is a central qualitative feature of parabolic partial differential equations.
\end{solution}

% ===== Example 2: Infinite Line and the Gaussian Fundamental Solution (inquiry-based) =====
\begin{problem}[Infinite Line and the Gaussian Fundamental Solution]
Consider heat diffusion along an infinitely long, homogeneous rod. We assume the material properties are constant, and there are no internal sources or sinks of heat. At time $t=0$ we place a very concentrated amount of heat near $x=0$, so that the initial temperature is sharply peaked there and essentially zero far away. Symmetry and translation invariance suggest that the resulting temperature profile should spread out in a way that depends only on the distance from the origin and on time, and that the ``shape'' of this profile might be self-similar as time evolves.

We model the temperature $u(x,t)$ for $x\in\mathbb{R}$ and $t>0$ by the \emph{diffusion equation}
\[
u_t = k\,u_{xx}, \qquad -\infty < x < \infty,\ t>0,
\]
where $k>0$ is the thermal diffusivity.

\smallskip

(a) Physically, for an infinite rod with no sources or sinks, the \emph{total heat} should be conserved in time. Argue that the total heat at time $t$ is given by
\[
H(t) = \int_{-\infty}^{\infty} u(x,t)\,dx,
\]
and use the diffusion equation to show that $H'(t)=0$ provided $u$ and its derivatives decay sufficiently fast as $|x|\to\infty$. In other words, show that $H(t)$ is constant in time.  
Hint: Differentiate $H(t)$ under the integral sign and integrate by parts.

\smallskip

(b) We idealize a ``point heat source'' at the origin by requiring that the initial condition has unit total heat concentrated at $x=0$, that is,
\[
H(0)=\int_{-\infty}^{\infty} u(x,0)\,dx = 1,
\]
and that $u(x,0)$ is extremely localized near $x=0$. On the infinite line, the diffusion equation has several symmetries: translations in $x$, reflections $x\mapsto -x$, and a scaling $(x,t)\mapsto(\lambda x,\lambda^2 t)$ for any $\lambda>0$.

Explain why it is reasonable, based on these symmetries and on conservation of total heat, to look for a solution of the form
\[
u(x,t) = t^{-\alpha}\,\phi\!\left(\eta\right), \qquad \eta = \frac{x}{\sqrt{t}},
\]
for some exponent $\alpha$ and some profile function $\phi\colon\mathbb{R}\to\mathbb{R}$.  
(i) Use conservation of total heat to determine $\alpha$.  
(ii) Briefly describe the physical meaning of the scaling variable $\eta=x/\sqrt{t}$.

\smallskip

(c) Let $\alpha$ be the value you found in part (b), and write
\[
u(x,t) = t^{-\alpha}\,\phi(\eta),\qquad \eta = \frac{x}{\sqrt{t}}.
\]
Compute $u_t$ and $u_{xx}$ in terms of $\phi$ and its derivatives, and substitute into the diffusion equation $u_t = k u_{xx}$ to obtain an ordinary differential equation (ODE) for $\phi$. Write this ODE explicitly.  
Hint: Use the chain rule carefully:
\[
\eta = x t^{-1/2},\quad \frac{\partial\eta}{\partial t} = -\frac{\eta}{2t},\quad
\frac{\partial\eta}{\partial x} = t^{-1/2}.
\]

\smallskip

(d) Solve the ODE you obtained in part (c). Show that, under natural conditions of symmetry (evenness in $x$) and boundedness as $|\eta|\to\infty$, the profile must have a Gaussian form
\[
\phi(\eta) = C\,e^{-a\eta^2}
\]
for some constants $C>0$ and $a>0$, and determine $a$ in terms of the diffusivity $k$. Then use the condition that the total heat is $1$ to determine $C$, and write the explicit formula
\[
u(x,t) = G(x,t) = \frac{1}{\sqrt{4\pi k t}}\,
\exp\!\left(-\frac{x^2}{4kt}\right).
\]
Check directly (by differentiating) that $G$ satisfies the diffusion equation, and that $\displaystyle\int_{-\infty}^{\infty} G(x,t)\,dx = 1$ for all $t>0$.  
Hint: To evaluate the integral, use the change of variables $y = x/\sqrt{4kt}$ and the standard Gaussian integral $\displaystyle\int_{-\infty}^{\infty} e^{-y^2}\,dy = \sqrt{\pi}$.

\smallskip

(e) (Extensions and ``what if'' questions.)

(i) The function $G(x,t)$ is called the \emph{fundamental solution} of the diffusion equation on the line. Suppose now that the initial temperature distribution is some integrable function $f(x)$, not just a point source. Based on the linearity and translation invariance of the diffusion equation, formulate a conjecture for $u(x,t)$ in terms of $G$ and $f$. (You do not need to prove it rigorously yet.)

(ii) How do you expect the fundamental solution to change if the diffusion coefficient is a different positive constant $\kappa\neq k$? What would change in the formula, and what would stay the same?

(iii) Finally, speculate about what the fundamental solution might look like in higher spatial dimensions (for instance, on $\mathbb{R}^2$ or $\mathbb{R}^3$). Which parts of your derivation above might carry over, and which parts might need to be modified?
\end{problem}

% ===== Example 2: Infinite Line and the Gaussian Fundamental Solution (full solution) =====
\begin{problem}[Infinite Line and the Gaussian Fundamental Solution]
Consider the diffusion equation on the whole real line
\[
u_t = k\,u_{xx}, \qquad -\infty < x < \infty,\ t>0,
\]
with a point source of unit total heat at the origin at time $t=0$, modeled by the initial condition $u(x,0)=\delta(x)$ in the sense of distributions.  

(a) Using conservation of total heat and the scaling invariance of the equation, seek a self-similar solution of the form
\[
u(x,t) = t^{-\alpha}\,\phi\!\left(\frac{x}{\sqrt{t}}\right),
\]
and determine the correct exponent $\alpha$.  

(b) Derive the corresponding ordinary differential equation for the profile $\phi$ and solve it under natural conditions to show that $\phi$ must be a Gaussian.  

(c) Determine the normalization constant using $\displaystyle\int_{-\infty}^{\infty} u(x,t)\,dx = 1$, and conclude that the fundamental solution is
\[
G(x,t) = \frac{1}{\sqrt{4\pi k t}}\,
\exp\!\left(-\frac{x^2}{4kt}\right).
\]

(d) Verify directly that $G$ satisfies $G_t = k G_{xx}$ for $t>0$, that $\displaystyle\int_{-\infty}^{\infty} G(x,t)\,dx = 1$ for all $t>0$, and that $G(\cdot,t)$ tends to $\delta$ as $t\to 0^+$ in the sense of distributions.
\end{problem}

\begin{solution}
We consider the diffusion equation
\[
u_t = k\,u_{xx}, \qquad x\in\mathbb{R},\ t>0,
\]
with a unit point source at the origin at time $t=0$. The constant $k>0$ is the thermal diffusivity.

\medskip

\noindent\textbf{1. Conservation of total heat and the similarity ansatz.}

The total heat at time $t$ is
\[
H(t) = \int_{-\infty}^{\infty} u(x,t)\,dx.
\]
Assuming $u$ and $u_x$ decay sufficiently rapidly as $|x|\to\infty$, we differentiate under the integral sign and use the PDE:
\[
\frac{dH}{dt}
= \int_{-\infty}^{\infty} u_t(x,t)\,dx
= \int_{-\infty}^{\infty} k\,u_{xx}(x,t)\,dx.
\]
Integrating by parts gives
\[
\int_{-\infty}^{\infty} u_{xx}(x,t)\,dx
= u_x(x,t)\big|_{x=-\infty}^{x=\infty} = 0,
\]
so $H'(t)=0$. Thus the total heat is conserved:
\[
H(t) \equiv H(0).
\]
For a point source of unit strength at $t=0$ we impose $H(0)=1$, so we expect
\[
\int_{-\infty}^{\infty} u(x,t)\,dx = 1 \quad\text{for all } t>0.
\]

On the whole line, the diffusion equation is invariant under space translations and under the scaling
\[
x\mapsto \lambda x,\qquad t\mapsto \lambda^2 t,
\]
for any $\lambda>0$. The initial condition is localized at $x=0$, and there is no distinguished length scale in the problem. This suggests that the profile of $u(x,t)$ at different times should be related by such a scaling, so we seek a self-similar solution of the form
\[
u(x,t) = t^{-\alpha}\,\phi(\eta),\qquad \eta = \frac{x}{\sqrt{t}},
\]
for some exponent $\alpha$ and some profile function $\phi$.

We determine $\alpha$ using conservation of total heat. Compute
\[
\int_{-\infty}^{\infty} u(x,t)\,dx
= \int_{-\infty}^{\infty} t^{-\alpha}\,\phi\!\left(\frac{x}{\sqrt{t}}\right)\,dx.
\]
Make the change of variables $\eta = x/\sqrt{t}$, so that $x = \eta\sqrt{t}$ and $dx = \sqrt{t}\,d\eta$. Then
\[
\int_{-\infty}^{\infty} u(x,t)\,dx
= t^{-\alpha} \int_{-\infty}^{\infty} \phi(\eta)\,\sqrt{t}\,d\eta
= t^{-\alpha + 1/2} \int_{-\infty}^{\infty} \phi(\eta)\,d\eta.
\]
For this to be independent of $t$, we must have $-\alpha + 1/2 = 0$, so
\[
\alpha = \frac{1}{2}.
\]
Hence we consider
\[
u(x,t) = t^{-1/2}\,\phi(\eta), \qquad \eta = \frac{x}{\sqrt{t}}.
\]
The variable $\eta$ is a dimensionless similarity variable: as time grows, the characteristic spatial scale of diffusion grows like $\sqrt{t}$, so the solution at different times can be compared by rescaling $x$ by $\sqrt{t}$.

\medskip

\noindent\textbf{2. Deriving the ODE for the similarity profile.}

With
\[
u(x,t) = t^{-1/2}\,\phi(\eta),\qquad \eta = x t^{-1/2},
\]
we compute $u_t$ and $u_{xx}$ via the chain rule. First,
\[
u_t = \frac{\partial}{\partial t}\Bigl(t^{-1/2}\Bigr)\,\phi(\eta)
      + t^{-1/2}\,\phi'(\eta)\,\eta_t,
\]
where the prime denotes differentiation with respect to $\eta$. We have
\[
\frac{\partial}{\partial t}\Bigl(t^{-1/2}\Bigr)
= -\frac{1}{2}t^{-3/2}
\]
and
\[
\eta_t = \frac{\partial}{\partial t}(x t^{-1/2})
       = x\left(-\frac{1}{2}t^{-3/2}\right)
       = -\frac{1}{2} t^{-1}\,\eta.
\]
Therefore
\[
u_t = -\frac{1}{2} t^{-3/2}\,\phi(\eta)
      + t^{-1/2}\,\phi'(\eta)\left(-\frac{1}{2}t^{-1}\eta\right)
    = t^{-3/2}\Bigl(-\tfrac{1}{2}\phi(\eta)
                     - \tfrac{1}{2}\eta\phi'(\eta)\Bigr).
\]

Next, we compute $u_x$ and $u_{xx}$. We have
\[
u_x = t^{-1/2}\,\phi'(\eta)\,\eta_x.
\]
Since $\eta = x t^{-1/2}$, we have $\eta_x = t^{-1/2}$, so
\[
u_x = t^{-1/2}\,\phi'(\eta)\,t^{-1/2}
    = t^{-1}\,\phi'(\eta).
\]
Differentiating again,
\[
u_{xx} = \frac{\partial}{\partial x}\Bigl(t^{-1}\,\phi'(\eta)\Bigr)
       = t^{-1}\,\phi''(\eta)\,\eta_x
       = t^{-1}\,\phi''(\eta)\,t^{-1/2}
       = t^{-3/2}\,\phi''(\eta).
\]

Substituting into the PDE $u_t = k u_{xx}$ gives
\[
t^{-3/2}\Bigl(-\tfrac{1}{2}\phi - \tfrac{1}{2}\eta\phi'\Bigr)
= k\,t^{-3/2}\,\phi''.
\]
Cancelling the factor $t^{-3/2}$, we obtain the ordinary differential equation
\[
-\frac{1}{2}\phi(\eta) - \frac{1}{2}\eta\phi'(\eta)
= k\,\phi''(\eta).
\]
Multiplying by $2$ and rearranging, this can be written as
\[
2k\,\phi''(\eta) + \eta\,\phi'(\eta) + \phi(\eta) = 0.
\]

\medskip

\noindent\textbf{3. Solving the ODE: emergence of the Gaussian.}

We now solve
\[
2k\,\phi''(\eta) + \eta\,\phi'(\eta) + \phi(\eta) = 0.
\]
We are looking for a profile that is even in $\eta$ (because the problem is symmetric under $x\mapsto -x$) and that decays at infinity. A natural ansatz is a Gaussian:
\[
\phi(\eta) = C\,e^{-a\eta^2},
\]
where $C>0$ and $a>0$ are constants to be determined. Differentiating,
\[
\phi'(\eta) = -2a\eta\,\phi(\eta),
\]
\[
\phi''(\eta) = (-2a + 4a^2\eta^2)\,\phi(\eta).
\]
Substitute these into the ODE:
\[
2k(-2a + 4a^2\eta^2)\,\phi(\eta)
+ \eta(-2a\eta\,\phi(\eta))
+ \phi(\eta) = 0.
\]
Factor out $\phi(\eta)$:
\[
\bigl[2k(-2a + 4a^2\eta^2) - 2a\eta^2 + 1\bigr]\phi(\eta) = 0.
\]
Since $\phi(\eta)\neq 0$ for $\eta$ near $0$, the bracket must vanish for all $\eta$:
\[
2k(-2a + 4a^2\eta^2) - 2a\eta^2 + 1 = 0.
\]
Separate constant and $\eta^2$ terms. The constant term is
\[
2k(-2a) + 1 = -4ka + 1,
\]
and the coefficient of $\eta^2$ is
\[
2k(4a^2) - 2a = 8ka^2 - 2a.
\]
Thus we require
\[
-4ka + 1 = 0,
\qquad
8ka^2 - 2a = 0.
\]
From the first equation we obtain $a = 1/(4k)$. Substituting into the second gives
\[
8k\left(\frac{1}{4k}\right)^2 - 2\left(\frac{1}{4k}\right)
= \frac{8k}{16k^2} - \frac{1}{2k}
= \frac{1}{2k} - \frac{1}{2k} = 0,
\]
so the two conditions are consistent. Therefore the Gaussian ansatz indeed solves the ODE, with
\[
\phi(\eta) = C\,\exp\!\left(-\frac{\eta^2}{4k}\right).
\]

Thus the similarity solution has the form
\[
u(x,t) = t^{-1/2}\,C\,\exp\!\left(-\frac{1}{4k}\,\frac{x^2}{t}\right)
       = C\,t^{-1/2}\,\exp\!\left(-\frac{x^2}{4kt}\right).
\]

\medskip

\noindent\textbf{4. Determining the normalization constant.}

We now choose $C$ so that the total heat is $1$. Compute
\[
\int_{-\infty}^{\infty} u(x,t)\,dx
= \int_{-\infty}^{\infty} C\,t^{-1/2}\,
   \exp\!\left(-\frac{x^2}{4kt}\right)\,dx.
\]
Let $y = x/\sqrt{4kt}$, so that $x = y\sqrt{4kt}$ and $dx = \sqrt{4kt}\,dy$. Then
\[
\int_{-\infty}^{\infty} u(x,t)\,dx
= C\,t^{-1/2} \int_{-\infty}^{\infty}
  \exp(-y^2)\,\sqrt{4kt}\,dy
= C\,t^{-1/2}\,\sqrt{4kt} \int_{-\infty}^{\infty} e^{-y^2}\,dy.
\]
We use the standard Gaussian integral
\[
\int_{-\infty}^{\infty} e^{-y^2}\,dy = \sqrt{\pi},
\]
to obtain
\[
\int_{-\infty}^{\infty} u(x,t)\,dx
= C\,t^{-1/2}\,\sqrt{4kt}\,\sqrt{\pi}
= C\,\sqrt{4k\pi}.
\]
This expression is independent of $t$, as expected, and must equal $1$. Therefore
\[
C\,\sqrt{4k\pi} = 1
\quad\Longrightarrow\quad
C = \frac{1}{\sqrt{4\pi k}}.
\]
Thus the self-similar solution is
\[
G(x,t) = \frac{1}{\sqrt{4\pi k t}}\,
\exp\!\left(-\frac{x^2}{4kt}\right).
\]
This is the fundamental solution of the diffusion equation on the real line.

\medskip

\noindent\textbf{5. Verification: PDE, mass conservation, and initial data.}

\emph{(i) The PDE $G_t = k G_{xx}$.}
We verify directly that $G$ satisfies the diffusion equation for $t>0$. Write
\[
G(x,t) = A(t)\,\exp\!\left(-\frac{x^2}{4kt}\right),
\qquad
A(t) = \frac{1}{\sqrt{4\pi k t}}.
\]
First compute $G_t$. We have
\[
A'(t) = -\frac{1}{2}\frac{1}{\sqrt{4\pi k}}\,t^{-3/2}
      = -\frac{1}{2t}\,A(t),
\]
and
\[
\frac{\partial}{\partial t}\left(-\frac{x^2}{4kt}\right)
= \frac{x^2}{4kt^2}.
\]
Hence
\[
G_t = A'(t)\,e^{-x^2/(4kt)}
     + A(t)\,e^{-x^2/(4kt)}\,\frac{x^2}{4kt^2}
   = G\left(-\frac{1}{2t} + \frac{x^2}{4kt^2}\right).
\]

Next compute $G_x$ and $G_{xx}$. Since $A(t)$ does not depend on $x$,
\[
G_x = A(t)\,e^{-x^2/(4kt)}\left(-\frac{2x}{4kt}\right)
    = -\frac{x}{2kt}\,G.
\]
Differentiating once more,
\[
G_{xx}
= -\frac{1}{2kt}\,G -\frac{x}{2kt}\,G_x
= -\frac{1}{2kt}\,G -\frac{x}{2kt}\left(-\frac{x}{2kt}\,G\right)
= -\frac{1}{2kt}\,G + \frac{x^2}{4k^2t^2}\,G.
\]
Therefore
\[
k G_{xx}
= G\left(-\frac{1}{2t} + \frac{x^2}{4kt^2}\right)
= G_t,
\]
as required.

\medskip

\emph{(ii) Conservation of total heat.}
We have already computed
\[
\int_{-\infty}^{\infty} G(x,t)\,dx = 1
\]
for all $t>0$ when choosing $C = 1/\sqrt{4\pi k}$. This is the mathematical expression of conservation of total heat for the fundamental solution.

\medskip

\emph{(iii) Initial data in the sense of distributions.}
Finally, we check that $G(\cdot,t)$ tends to $\delta$ as $t\to 0^+$ in the sense of distributions. Let $\varphi$ be a smooth test function with compact support. We need to show
\[
\lim_{t\to 0^+} \int_{-\infty}^{\infty} G(x,t)\,\varphi(x)\,dx
= \varphi(0).
\]
Perform the same change of variables as before: $y = x/\sqrt{4kt}$, so $x = y\sqrt{4kt}$ and $dx = \sqrt{4kt}\,dy$. Then
\[
\int_{-\infty}^{\infty} G(x,t)\,\varphi(x)\,dx
= \frac{1}{\sqrt{4\pi k t}}
  \int_{-\infty}^{\infty}
  \exp(-y^2)\,\varphi\!\bigl(y\sqrt{4kt}\bigr)\,\sqrt{4kt}\,dy
= \frac{1}{\sqrt{\pi}}
  \int_{-\infty}^{\infty}
  e^{-y^2}\,\varphi\!\bigl(y\sqrt{4kt}\bigr)\,dy.
\]
For each fixed $y$, we have $y\sqrt{4kt}\to 0$ as $t\to 0^+$, so $\varphi(y\sqrt{4kt})\to \varphi(0)$. Moreover, $|\varphi(y\sqrt{4kt})|\le \|\varphi\|_\infty$ and $e^{-y^2}$ is integrable on $\mathbb{R}$. By the dominated convergence theorem,
\[
\lim_{t\to 0^+} \int_{-\infty}^{\infty} G(x,t)\,\varphi(x)\,dx
= \frac{1}{\sqrt{\pi}}
  \int_{-\infty}^{\infty} e^{-y^2}\,\varphi(0)\,dy
= \varphi(0)
  \frac{1}{\sqrt{\pi}}\int_{-\infty}^{\infty} e^{-y^2}\,dy
= \varphi(0).
\]
Thus $G(\cdot,t)\to \delta$ as $t\to 0^+$ in the sense of distributions.

\medskip

\noindent\textbf{6. Interpretation and connection to diffusion.}

The function
\[
G(x,t) = \frac{1}{\sqrt{4\pi k t}}\,
\exp\!\left(-\frac{x^2}{4kt}\right)
\]
is the fundamental solution of the diffusion equation on $\mathbb{R}$. It is Gaussian in space, with variance proportional to $t$: as time increases, the Gaussian spreads out (its standard deviation grows like $\sqrt{t}$) and its peak height decreases like $t^{-1/2}$, while the total area under the curve remains equal to $1$. This behavior is characteristic of diffusion: localized disturbances spread and smooth out over time, with a characteristic length scale $\sqrt{kt}$.

From the viewpoint of the theory of partial differential equations, this example illustrates several central ideas for the diffusion equation: conservation laws (total heat), scaling and self-similarity (the $\sqrt{t}$ spatial scaling), the smoothing effect of parabolic equations, and the role of fundamental solutions. For more general initial data $f$, the solution can be represented as a convolution
\[
u(x,t) = (G(\cdot,t)*f)(x)
= \int_{-\infty}^{\infty} G(x-y,t)\,f(y)\,dy,
\]
showing that the diffusion equation generates a linear smoothing semigroup whose kernel is precisely this Gaussian fundamental solution.
\end{solution}

% ===== Example 3: Random Walks and the Diffusion Equation (inquiry-based) =====
\begin{problem}[Random Walks and the Diffusion Equation]
Many diffusion phenomena in nature, such as heat spreading in a metal rod or dye dispersing in water, can be modeled by the diffusion equation. Surprisingly, this smooth partial differential equation can emerge from a very simple random, discrete model: a particle that takes random steps on a lattice. In this problem you will build, step by step, a bridge from a one-dimensional random walk to the diffusion equation, and see how the continuum description arises as an approximation to the discrete model.

Consider a particle moving on the one-dimensional lattice $\Delta x \,\mathbb{Z} = \{\dotsc,-2\Delta x,-\Delta x,0,\Delta x,2\Delta x,\dotsc\}$. Time is discrete with step size $\Delta t>0$, so $t_n = n\Delta t$ for $n=0,1,2,\dotsc$. At each time step, the particle moves one step to the right or one step to the left, each with probability $\tfrac12$.

Let $P_n(k)$ denote the probability that the particle is at the lattice site $x_k = k\Delta x$ at time $t_n = n\Delta t$.

\smallskip

(a) \textbf{Discrete evolution law (``master equation'').}  
Explain why the probabilities $\{P_n(k)\}$ satisfy a recursive relation of the form
\[
P_{n+1}(k) = \frac12\,P_n(k-1) + \frac12\,P_n(k+1).
\]
Write this equation carefully in words, and then derive it from the definition of the random walk.

Hint: Think about how the particle can arrive at position $x_k$ at time $t_{n+1}$, given where it could have been at time $t_n$.

\smallskip

(b) \textbf{Introducing a continuum description.}  
We now seek a smooth function $u(x,t)$ that approximates the discrete probabilities in the limit of small lattice spacing $\Delta x$ and small time step $\Delta t$. A common correspondence is
\[
u(x_k,t_n) \approx \frac{1}{\Delta x}\,P_n(k),
\]
so that $u(x,t)$ is a probability \emph{density} with respect to $x$.

\begin{enumerate}
\item[(i)] Briefly justify why dividing by $\Delta x$ is natural if we want $u(x,t)$ to represent a probability density in the continuum limit.
\item[(ii)] Rewrite the discrete evolution law from part (a) in terms of $u(x_k,t_n)$, $\Delta x$, and $\Delta t$, so that $P_n(k)$ is eliminated in favor of $u$.
\end{enumerate}

Hint: Start from the relation $P_n(k) \approx u(x_k,t_n)\,\Delta x$ and substitute into the recursion for $P_{n+1}(k)$.

\smallskip

(c) \textbf{Connecting the difference equation to derivatives.}  
We next interpret the discrete evolution law as a finite-difference approximation to a partial differential equation. For this, we will use Taylor expansions of $u(x,t)$ around the point $(x_k,t_n)$.

\begin{enumerate}
\item[(i)] Write the second-order Taylor expansion of $u(x_k\pm\Delta x,t_n)$ in powers of $\Delta x$ around $(x_k,t_n)$, and of $u(x_k,t_n+\Delta t)$ in powers of $\Delta t$ around $(x_k,t_n)$. Keep terms up to order $(\Delta x)^2$ and $\Delta t$, respectively.
\item[(ii)] Use these expansions to express the right-hand side and left-hand side of your equation from part (b)(ii) in terms of $u$ and its derivatives evaluated at $(x_k,t_n)$, plus higher-order error terms.
\end{enumerate}

Hint: You should obtain an expression where $u_t$ (a time derivative) is approximately proportional to $u_{xx}$ (a second spatial derivative). Be explicit about which terms you discard and why it is reasonable to neglect them in the limit of small $\Delta t$ and $\Delta x$.

\smallskip

(d) \textbf{Taking the diffusion scaling limit.}  
The continuum limit requires letting $\Delta x\to 0$ and $\Delta t\to 0$ in a coordinated way. Suppose we impose the \emph{diffusive scaling}
\[
\frac{(\Delta x)^2}{2\Delta t} \to D \quad\text{as}\quad \Delta x,\Delta t\to 0,
\]
where $D>0$ is a fixed constant called the diffusion coefficient.

\begin{enumerate}
\item[(i)] Starting from your expression in part (c)(ii), divide both sides by $\Delta t$ and then pass formally to the limit $\Delta x,\Delta t \to 0$ under the diffusive scaling above. Derive the continuous equation satisfied by $u(x,t)$.
\item[(ii)] State clearly the resulting partial differential equation, and explain in a sentence or two why it is called the \emph{diffusion equation}.
\end{enumerate}

Hint: After dividing by $\Delta t$, the factor $(\Delta x)^2/(2\Delta t)$ should appear multiplying $u_{xx}$. Replace this factor by $D$ in the limit and discard terms that vanish as $\Delta t,\Delta x\to 0$.

\smallskip

(e) \textbf{Extensions and variations.}  
Now explore how changes in the random walk change the limiting equation.

\begin{enumerate}
\item[(i)] Suppose that at each step the particle moves right with probability $p$ and left with probability $q=1-p$, with $p\neq q$. Write the new discrete evolution equation for $P_{n+1}(k)$ in terms of $P_n(k-1)$ and $P_n(k+1)$, and repeat (very briefly) the Taylor expansion reasoning of parts (b)--(d) to guess the form of the continuum limit. What new term appears in the partial differential equation?
\item[(ii)] How would you expect the diffusion coefficient $D$ and any new parameters in the continuum equation to depend on the microscopic step size $\Delta x$, time step $\Delta t$, and probabilities $p$ and $q$? Provide heuristic formulas and a short explanation.
\end{enumerate}

Hint: In the biased case, you should discover a first derivative term $u_x$ in addition to the $u_{xx}$ term. This corresponds to a \emph{drift} or \emph{advection} term in the limiting equation.
\end{problem}

% ===== Example 3: Random Walks and the Diffusion Equation (full solution) =====
\begin{problem}[Random Walks and the Diffusion Equation]
Consider a particle on the one-dimensional lattice $\Delta x\,\mathbb{Z}$ that moves in discrete time steps of length $\Delta t>0$. At each time step it moves one site to the right or left with equal probability $1/2$. Let $P_n(k)$ be the probability that the particle is at $x_k = k\Delta x$ at time $t_n = n\Delta t$, and define a probability density approximation
\[
u(x_k,t_n) \approx \frac{1}{\Delta x}\,P_n(k).
\]
\begin{enumerate}
\item[(a)] Derive the discrete evolution equation (master equation)
\[
P_{n+1}(k) = \frac12\,P_n(k-1) + \frac12\,P_n(k+1),
\]
and rewrite it in terms of $u(x_k,t_n)$, $\Delta x$, and $\Delta t$.
\item[(b)] Use Taylor expansions of $u(x_k\pm\Delta x,t_n)$ and $u(x_k,t_n+\Delta t)$ about $(x_k,t_n)$, up to second order in $\Delta x$ and first order in $\Delta t$, to interpret this discrete equation as a finite-difference approximation to a partial differential equation.
\item[(c)] Under the diffusive scaling
\[
\frac{(\Delta x)^2}{2\Delta t} \to D>0 \quad\text{as}\quad \Delta x,\Delta t\to 0,
\]
pass formally to the limit and derive the diffusion equation satisfied by $u(x,t)$.
\item[(d)] Briefly indicate how the result changes if the walk is biased: at each step the particle moves right with probability $p$ and left with probability $q=1-p\neq\tfrac12$. Identify the additional term that appears in the limiting equation and express its coefficient in terms of $\Delta x$, $\Delta t$, $p$, and $q$.
\end{enumerate}
\end{problem}

\begin{solution}
We begin from the discrete random walk and systematically pass to a continuum description. This example illustrates how the diffusion equation can arise as the large-scale, long-time limit of a microscopic random motion.

\medskip

\emph{(a) Discrete evolution and rewriting in terms of $u$.}  
At time $t_n$, the particle is at some lattice site $x_j = j\Delta x$. In one time step, it moves either to $x_{j+1}$ or $x_{j-1}$, each with probability $1/2$, independently of the past. To be at $x_k$ at time $t_{n+1}$, the particle must have been at $x_{k-1}$ at time $t_n$ and then step right, or at $x_{k+1}$ at time $t_n$ and then step left. Therefore
\[
P_{n+1}(k)
= \frac12\,P_n(k-1) + \frac12\,P_n(k+1),
\]
which is the master equation.

We want to express this in terms of a continuous density $u(x,t)$. By definition
\[
u(x_k,t_n) \approx \frac{1}{\Delta x}\,P_n(k),
\]
or equivalently
\[
P_n(k) \approx u(x_k,t_n)\,\Delta x.
\]
Substituting into the master equation gives
\[
u(x_k,t_{n+1})\,\Delta x
\approx \frac12\,u(x_{k-1},t_n)\,\Delta x
      + \frac12\,u(x_{k+1},t_n)\,\Delta x.
\]
We may cancel the common factor $\Delta x$ to obtain
\[
u(x_k,t_{n+1})
\approx \frac12\,u(x_{k-1},t_n)
      + \frac12\,u(x_{k+1},t_n).
\]
It will be convenient to write this in a form reminiscent of a finite-difference scheme:
\begin{equation} \label{eq:discrete-u}
u(x_k,t_{n+1}) - u(x_k,t_n)
\approx \frac12\bigl(u(x_{k-1},t_n) - 2u(x_k,t_n) + u(x_{k+1},t_n)\bigr).
\end{equation}
We have simply subtracted $u(x_k,t_n)$ from both sides and regrouped terms.

\medskip

\emph{(b) Taylor expansions and interpretation as finite differences.}  
We now express the right- and left-hand sides of \eqref{eq:discrete-u} using Taylor expansions of $u$ around the point $(x_k,t_n)$.

First, expand in space at fixed time $t_n$:
\[
u(x_k \pm \Delta x, t_n)
= u(x_k,t_n)
  \pm \Delta x\,u_x(x_k,t_n)
  + \frac{(\Delta x)^2}{2}\,u_{xx}(x_k,t_n)
  + O\bigl((\Delta x)^3\bigr).
\]
Adding these two expansions, the first derivatives cancel and we obtain
\[
u(x_{k-1},t_n) + u(x_{k+1},t_n)
= 2u(x_k,t_n) + (\Delta x)^2 u_{xx}(x_k,t_n)
  + O\bigl((\Delta x)^4\bigr),
\]
since the odd powers of $\Delta x$ cancel and the first nonzero correction beyond $(\Delta x)^2$ is of order $(\Delta x)^4$.

Therefore
\[
u(x_{k-1},t_n) - 2u(x_k,t_n) + u(x_{k+1},t_n)
= (\Delta x)^2 u_{xx}(x_k,t_n) + O\bigl((\Delta x)^4\bigr).
\]
Substituting this into the right-hand side of \eqref{eq:discrete-u} yields
\[
\text{RHS of \eqref{eq:discrete-u}}
\approx \frac12 (\Delta x)^2 u_{xx}(x_k,t_n) + O\bigl((\Delta x)^4\bigr).
\]

Next, expand in time at fixed $x_k$:
\[
u(x_k,t_{n+1})
= u(x_k,t_n+\Delta t)
= u(x_k,t_n) + \Delta t\,u_t(x_k,t_n)
  + O\bigl((\Delta t)^2\bigr).
\]
Thus the left-hand side of \eqref{eq:discrete-u} becomes
\[
u(x_k,t_{n+1}) - u(x_k,t_n)
= \Delta t\,u_t(x_k,t_n) + O\bigl((\Delta t)^2\bigr).
\]

Substituting both approximations into \eqref{eq:discrete-u}, we obtain
\[
\Delta t\,u_t(x_k,t_n) + O\bigl((\Delta t)^2\bigr)
\approx \frac12 (\Delta x)^2 u_{xx}(x_k,t_n)
        + O\bigl((\Delta x)^4\bigr).
\]
Neglecting the higher-order terms $O((\Delta t)^2)$ and $O((\Delta x)^4)$ in a formal way, this relation suggests
\begin{equation} \label{eq:prelimit}
\Delta t\,u_t(x_k,t_n) \approx \frac{(\Delta x)^2}{2}\,u_{xx}(x_k,t_n).
\end{equation}
This is already in the form of a discrete approximation to a partial differential equation connecting $u_t$ and $u_{xx}$.

\medskip

\emph{(c) Diffusive scaling and continuum limit.}  
We now implement the diffusive scaling
\[
\frac{(\Delta x)^2}{2\Delta t} \to D > 0
\quad\text{as}\quad \Delta x,\Delta t\to 0.
\]
Divide both sides of \eqref{eq:prelimit} by $\Delta t$ to obtain
\[
u_t(x_k,t_n) \approx \frac{(\Delta x)^2}{2\Delta t}\,u_{xx}(x_k,t_n).
\]
Under the assumed scaling, the factor $(\Delta x)^2/(2\Delta t)$ tends to $D$ as $\Delta x,\Delta t\to 0$. If we now regard $x_k$ and $t_n$ as generic space and time points $(x,t)$ in the continuum, and let $\Delta x,\Delta t \to 0$, the above relation formally converges to
\[
u_t(x,t) = D\,u_{xx}(x,t).
\]
This is the one-dimensional diffusion equation (also called the heat equation) with diffusion coefficient $D$.

The name ``diffusion equation'' reflects the fact that this equation describes the temporal evolution of a quantity that spreads out over time, such as heat, mass, or probability density, in a manner consistent with Fick's law: flux is proportional to the gradient, and conservation of the quantity leads to a second derivative in space.

\medskip

\emph{(d) Biased random walk and advection--diffusion.}  
Now suppose that at each step the particle moves right with probability $p$ and left with probability $q = 1-p$, where $p\neq q$. The master equation becomes
\[
P_{n+1}(k) = p\,P_n(k-1) + q\,P_n(k+1).
\]
As before, substituting $P_n(k) \approx u(x_k,t_n)\,\Delta x$ and cancelling $\Delta x$ yields
\begin{equation} \label{eq:biased-discrete}
u(x_k,t_{n+1})
\approx p\,u(x_{k-1},t_n) + q\,u(x_{k+1},t_n).
\end{equation}
Subtract $u(x_k,t_n)$ from both sides:
\[
u(x_k,t_{n+1}) - u(x_k,t_n)
\approx p\bigl[u(x_{k-1},t_n) - u(x_k,t_n)\bigr]
      + q\bigl[u(x_{k+1},t_n) - u(x_k,t_n)\bigr].
\]

We again use Taylor expansions
\[
u(x_k\pm\Delta x,t_n)
= u(x_k,t_n) \pm \Delta x\,u_x(x_k,t_n)
  + \frac{(\Delta x)^2}{2}\,u_{xx}(x_k,t_n)
  + O\bigl((\Delta x)^3\bigr).
\]
Then
\[
u(x_{k-1},t_n) - u(x_k,t_n)
= -\Delta x\,u_x + \frac{(\Delta x)^2}{2}\,u_{xx} + O\bigl((\Delta x)^3\bigr),
\]
\[
u(x_{k+1},t_n) - u(x_k,t_n)
= \Delta x\,u_x + \frac{(\Delta x)^2}{2}\,u_{xx} + O\bigl((\Delta x)^3\bigr),
\]
where all derivatives are evaluated at $(x_k,t_n)$.

Substituting into the right-hand side gives
\begin{align*}
\text{RHS}
&\approx p\left(-\Delta x\,u_x + \frac{(\Delta x)^2}{2}\,u_{xx}\right)
      + q\left(\Delta x\,u_x + \frac{(\Delta x)^2}{2}\,u_{xx}\right)
      + O\bigl((\Delta x)^3\bigr) \\
&= (q-p)\,\Delta x\,u_x
   + \frac{p+q}{2}(\Delta x)^2 u_{xx}
   + O\bigl((\Delta x)^3\bigr).
\end{align*}
Since $p+q=1$, this simplifies to
\[
\text{RHS}
\approx (q-p)\,\Delta x\,u_x
   + \frac{(\Delta x)^2}{2} u_{xx}
   + O\bigl((\Delta x)^3\bigr).
\]

As before, the left-hand side satisfies
\[
u(x_k,t_{n+1}) - u(x_k,t_n)
= \Delta t\,u_t(x_k,t_n) + O\bigl((\Delta t)^2\bigr).
\]
Thus we obtain
\[
\Delta t\,u_t(x_k,t_n)
\approx (q-p)\,\Delta x\,u_x(x_k,t_n)
   + \frac{(\Delta x)^2}{2} u_{xx}(x_k,t_n)
   + \text{higher-order terms}.
\]
Dividing by $\Delta t$ leads to
\[
u_t(x_k,t_n)
\approx \frac{(q-p)\Delta x}{\Delta t}\,u_x(x_k,t_n)
   + \frac{(\Delta x)^2}{2\Delta t}\,u_{xx}(x_k,t_n).
\]

In the continuum limit, we again impose the diffusive scaling
\[
\frac{(\Delta x)^2}{2\Delta t} \to D>0.
\]
In addition, we assume that the ratio
\[
v := \frac{(p-q)\Delta x}{\Delta t}
\]
converges to a finite limit. Notice the sign: since $q-p = -(p-q)$, the coefficient of $u_x$ is $-(p-q)\Delta x/\Delta t$, so the limiting equation becomes
\[
u_t + v\,u_x = D\,u_{xx},
\quad\text{with}\quad v = \lim_{\Delta x,\Delta t\to 0} \frac{(p-q)\Delta x}{\Delta t}.
\]
This is the \emph{advection--diffusion equation}: the term $v u_x$ represents deterministic drift (advection) with velocity $v$ superimposed on diffusive spreading with coefficient $D$.

In summary, for the unbiased walk we obtain the diffusion equation
\[
u_t = D\,u_{xx},
\]
while for a biased walk the continuum limit is
\[
u_t + v\,u_x = D\,u_{xx}.
\]
These results show how a deterministic partial differential equation describing diffusion (and drift) can emerge from an underlying random walk model, which is precisely one of the central themes of the diffusion-equation section: macroscopic smoothing behavior encoded in $u_t = D u_{xx}$ arises from microscopically random motion with suitable scaling.
\end{solution}

% ===== Example 4: Steady States and the Connection to Laplace’s Equation (inquiry-based) =====
\begin{problem}[Steady States and the Connection to Laplace’s Equation]
In this problem, we explore what happens to a diffusing temperature distribution after a very long time, when the boundary temperatures are held fixed. Physically, one expects that eventually nothing changes in time: the system reaches a steady state. Mathematically, this steady state should solve a purely spatial equation. You will discover that this spatial equation is precisely Laplace's equation in one dimension, and see how this connects the time-dependent diffusion equation to time-independent boundary value problems.

Consider a thin, homogeneous rod of length $L$, lying along the $x$-axis from $x=0$ to $x=L$. Let $u(x,t)$ denote the temperature at position $x$ and time $t$. The temperature evolves according to the one-dimensional diffusion (heat) equation
\[
u_t = k\,u_{xx}, \qquad 0 < x < L,\ t>0,
\]
where $k>0$ is the thermal diffusivity. The ends of the rod are kept at fixed (time-independent) temperatures:
\[
u(0,t) = T_0, \qquad u(L,t) = T_L \quad \text{for all } t>0,
\]
where $T_0$ and $T_L$ are given constants. At time $t=0$ the initial temperature profile is some given function $f(x)$:
\[
u(x,0) = f(x), \qquad 0 \le x \le L.
\]

(a) A \emph{steady state} is a temperature distribution $v(x)$ that does not change in time, that is, $u(x,t) \equiv v(x)$ for all $t$. If $u(x,t) = v(x)$ is independent of $t$, what does the diffusion equation $u_t = k u_{xx}$ reduce to? Write down the resulting ordinary differential equation for $v(x)$ and the associated boundary conditions at $x=0$ and $x=L$.  
Hint: If $u(x,t) = v(x)$, then $u_t(x,t) = 0$ for all $x$ and $t$.

(b) Solve the boundary value problem you found in part (a). Show that there is a unique steady-state temperature profile $v(x)$, and express it explicitly in terms of $T_0$, $T_L$, and $L$.  
Hint: The equation $v''(x)=0$ has general solution $v(x) = ax + b$. Use the boundary conditions to determine $a$ and $b$.

(c) Now consider the \emph{difference} between the actual temperature and the steady state:
\[
w(x,t) := u(x,t) - v(x).
\]
Write down the partial differential equation and boundary conditions satisfied by $w(x,t)$. In particular, show that $w$ satisfies a diffusion equation with \emph{homogeneous} (zero) boundary conditions. What is the initial condition for $w(x,t)$ in terms of $f$ and $v$?  
Hint: Compute $w_t$ and $w_{xx}$ in terms of $u$ and $v$, and use the equations that $u$ and $v$ satisfy.

(d) Suppose (as in the usual separation-of-variables treatment of the heat equation) that $w(x,t)$ can be represented as a sine series
\[
w(x,t) = \sum_{n=1}^{\infty} b_n e^{-k\lambda_n t} \sin\!\left(\frac{n\pi x}{L}\right),
\]
for suitable constants $b_n$ and positive numbers $\lambda_n$.  

(i) Using the diffusion equation for $w$, determine the values of $\lambda_n$.  

(ii) Using this representation, explain why $\displaystyle \lim_{t\to\infty} w(x,t) = 0$ for each fixed $x \in (0,L)$.  

(iii) Conclude that
\[
\lim_{t\to\infty} u(x,t) = v(x),
\]
and interpret this statement physically.  
% Hint for (i): Plug the series into $w_t = k w_{xx}$ and match coefficients of $\sin\left(\frac{n\pi x}{L}\right)$.
% Hint for (ii): Recall that $e^{-k\lambda_n t} \to 0$ as $t\to\infty$ when $k\lambda_n>0$.

(e) Extensions and “what if” questions.

(i) In this problem we considered a one-dimensional rod, so the steady state $v$ satisfies $v''(x) = 0$, which is the one-dimensional form of Laplace’s equation. Suppose instead that we have a thin rectangular plate occupying a region $D \subset \mathbb{R}^2$, and the boundary of the plate is held at fixed temperatures that do not depend on time. Write down the diffusion equation for $u(x,y,t)$, and then write down the \emph{steady-state} equation that $v(x,y)$ should satisfy if $u(x,y,t) \equiv v(x,y)$ is independent of $t$.  

(ii) How do your answers to (a)–(d) suggest a general principle for diffusion problems with time-independent boundary conditions in higher dimensions? State this principle in words.  
Hint: Think about the role played by $v$ and $w = u - v$ in the one-dimensional case, and how the Laplacian $\Delta$ appears in higher dimensions.
\end{problem}

% ===== Example 4: Steady States and the Connection to Laplace’s Equation (full solution) =====
\begin{problem}[Steady States and the Connection to Laplace’s Equation]
Consider the one-dimensional diffusion (heat) equation
\[
u_t = k\,u_{xx}, \qquad 0 < x < L,\ t>0,
\]
with constant Dirichlet boundary conditions
\[
u(0,t) = T_0, \qquad u(L,t) = T_L \quad (t>0),
\]
and initial condition
\[
u(x,0) = f(x), \qquad 0 \le x \le L,
\]
where $k>0$ and $T_0, T_L$ are given constants.

(a) Find the steady-state temperature profile $v(x)$ that is independent of $t$, satisfies the same boundary conditions, and solves the appropriate ordinary differential equation.  

(b) Define $w(x,t) = u(x,t) - v(x)$. Derive the partial differential equation, boundary conditions, and initial condition satisfied by $w$.  

(c) By solving $w$ using separation of variables and a sine series expansion, show that $w(x,t) \to 0$ as $t\to\infty$ for each fixed $x\in(0,L)$. Conclude that
\[
\lim_{t\to\infty} u(x,t) = v(x),
\]
and explain how this illustrates the connection between the diffusion equation and Laplace’s equation.
\end{problem}

\begin{solution}
We are given a one-dimensional diffusion problem on a finite interval with constant boundary temperatures. The main goal is to identify the long-time limit of the temperature profile and to understand how this limit is governed by a time-independent equation, namely Laplace’s equation in one dimension.

\medskip

\noindent\textbf{(a) The steady state and Laplace’s equation.}

A steady-state solution is a temperature profile $v(x)$ that does not depend on time. If $u(x,t) = v(x)$ for all $t$, then $u_t(x,t) = 0$ for all $x$ and $t$. Substituting this into the diffusion equation
\[
u_t = k\,u_{xx}
\]
gives
\[
0 = k\,v''(x),
\]
so $v$ satisfies the ordinary differential equation
\[
v''(x) = 0, \qquad 0 < x < L.
\]
This is the one-dimensional form of Laplace’s equation. The steady state must also satisfy the same boundary conditions as $u$, namely
\[
v(0) = T_0, \qquad v(L) = T_L.
\]

The general solution of $v''(x)=0$ is
\[
v(x) = ax + b,
\]
for some constants $a$ and $b$. Imposing the boundary conditions gives
\[
v(0) = b = T_0,\qquad
v(L) = aL + b = T_L.
\]
From the second equation we obtain
\[
a = \frac{T_L - b}{L} = \frac{T_L - T_0}{L}.
\]
Therefore the unique steady-state solution is
\[
v(x) = T_0 + \frac{T_L - T_0}{L}\,x.
\]
This is a linear temperature profile interpolating between $T_0$ and $T_L$.

\medskip

\noindent\textbf{(b) The equation for the deviation $w = u - v$.}

Define the deviation from steady state by
\[
w(x,t) := u(x,t) - v(x).
\]
We now derive the equation and conditions satisfied by $w$.

First, compute the time and spatial derivatives:
\[
w_t = u_t - 0 = u_t,\qquad
w_{xx} = u_{xx} - v''.
\]
Using the original diffusion equation for $u$ and the steady-state equation for $v$, we have
\[
u_t = k\,u_{xx},\qquad v'' = 0.
\]
Therefore
\[
w_t = u_t = k\,u_{xx} = k\,(w_{xx} + v'') = k\,(w_{xx} + 0) = k\,w_{xx}.
\]
So $w$ satisfies the same diffusion equation:
\[
w_t = k\,w_{xx}, \qquad 0<x<L,\ t>0.
\]

Next, we determine the boundary conditions. At $x=0$,
\[
w(0,t) = u(0,t) - v(0) = T_0 - T_0 = 0.
\]
Similarly, at $x=L$,
\[
w(L,t) = u(L,t) - v(L) = T_L - T_L = 0.
\]
So $w$ satisfies homogeneous Dirichlet boundary conditions:
\[
w(0,t) = 0,\qquad w(L,t) = 0 \quad \text{for } t>0.
\]

Finally, the initial condition for $w$ is obtained from
\[
w(x,0) = u(x,0) - v(x) = f(x) - v(x).
\]
Putting this together, $w$ solves
\[
\begin{cases}
w_t = k\,w_{xx}, & 0 < x < L,\ t>0,\\[4pt]
w(0,t) = 0,\ w(L,t) = 0, & t>0,\\[4pt]
w(x,0) = f(x) - v(x), & 0 \le x \le L.
\end{cases}
\]

\medskip

\noindent\textbf{(c) Separation of variables, decay, and convergence to the steady state.}

The problem for $w$ is the standard heat equation on $(0,L)$ with homogeneous Dirichlet (zero) boundary conditions. The central idea for solving such problems is separation of variables together with expansion in eigenfunctions of the spatial operator. The relevant eigenfunctions here are sines, which are orthogonal on $(0,L)$ and satisfy the homogeneous Dirichlet boundary conditions.

We look for separated solutions of the form $X(x)T(t)$ for $w$, leading to
\[
X(x)T'(t) = k\,X''(x)T(t).
\]
Dividing by $k\,X(x)T(t)$ (assuming neither factor is identically zero), we obtain
\[
\frac{T'(t)}{k\,T(t)} = \frac{X''(x)}{X(x)} = -\lambda,
\]
where $-\lambda$ is a separation constant. This gives a spatial eigenvalue problem
\[
X''(x) + \lambda X(x) = 0,\qquad X(0) = 0,\ X(L)=0.
\]
The nontrivial solutions of this boundary value problem occur when $\lambda = \lambda_n := \left(\frac{n\pi}{L}\right)^2$ for integers $n\ge1$, with corresponding eigenfunctions
\[
X_n(x) = \sin\!\left(\frac{n\pi x}{L}\right).
\]
For each $n$, the temporal factor $T_n(t)$ satisfies
\[
T_n'(t) = -k\lambda_n T_n(t),
\]
so
\[
T_n(t) = e^{-k\lambda_n t} C_n = C_n e^{-k\left(\frac{n\pi}{L}\right)^2 t},
\]
for some constant $C_n$.

Superposing these separated solutions, we may represent $w$ as
\[
w(x,t) = \sum_{n=1}^{\infty} b_n e^{-k\left(\frac{n\pi}{L}\right)^2 t}\,\sin\!\left(\frac{n\pi x}{L}\right),
\]
where the coefficients $b_n$ are determined by the initial condition
\[
w(x,0) = f(x) - v(x).
\]
Specifically, we expand $f(x) - v(x)$ in a sine series:
\[
f(x) - v(x) = \sum_{n=1}^{\infty} b_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
with
\[
b_n = \frac{2}{L} \int_0^L \bigl(f(\xi) - v(\xi)\bigr) \sin\!\left(\frac{n\pi \xi}{L}\right)\,d\xi.
\]

Now we analyze the long-time behavior. For each fixed $n\ge1$ and fixed $x\in(0,L)$,
\[
e^{-k\left(\frac{n\pi}{L}\right)^2 t} \to 0 \quad \text{as } t\to\infty,
\]
because $k>0$ and $\left(\frac{n\pi}{L}\right)^2>0$. Therefore, term by term, we have
\[
b_n e^{-k\left(\frac{n\pi}{L}\right)^2 t}\,\sin\!\left(\frac{n\pi x}{L}\right) \to 0 \quad \text{as } t\to\infty.
\]
Under mild regularity hypotheses on $f$ (so that the sine series converges appropriately), we can pass the limit through the sum and conclude that, for each fixed $x\in(0,L)$,
\[
\lim_{t\to\infty} w(x,t) = 0.
\]

Since $u = v + w$, this implies
\[
\lim_{t\to\infty} u(x,t) = \lim_{t\to\infty} \bigl(v(x) + w(x,t)\bigr) = v(x) + 0 = v(x)
\]
for each fixed $x\in(0,L)$.

\medskip

\noindent\textbf{Interpretation and connection to Laplace’s equation.}

We have shown that the solution of the time-dependent diffusion problem converges, as $t\to\infty$, to the unique steady-state profile $v(x)$ that satisfies
\[
v''(x) = 0,\quad 0<x<L,\qquad v(0) = T_0,\ v(L)=T_L.
\]
This is exactly Laplace’s equation in one dimension with Dirichlet boundary conditions. Thus, for diffusion processes with time-independent Dirichlet boundary conditions, the long-time limit is governed by a \emph{static} boundary value problem for the Laplacian.

In higher dimensions, the same reasoning applies with the Laplacian $\Delta$ in place of the second derivative $d^2/dx^2$. If $u(\mathbf{x},t)$ satisfies the diffusion equation
\[
u_t = k\,\Delta u \quad \text{in a bounded region } D\subset\mathbb{R}^n,
\]
with time-independent boundary data, then any steady state $v(\mathbf{x})$ must satisfy
\[
\Delta v = 0 \quad \text{in } D
\]
with the same boundary values. This means the steady state is a harmonic function. Subtracting $v$ from $u$ again yields a function $w$ that satisfies the diffusion equation with homogeneous boundary conditions and decays to zero in time. 

This example illustrates a central idea in the study of diffusion equations: the time evolution smooths out initial data and drives the solution toward a harmonic equilibrium profile determined solely by the boundary conditions. The diffusion equation thus connects dynamical behavior (in time) to the static boundary value problems for Laplace’s equation.
\end{solution}

% ===== Example 5: Diffusion with Reaction or Decay (inquiry-based) =====
\begin{problem}[Diffusion with Reaction or Decay]
A chemical species diffuses along a thin one-dimensional rod of length $L$. In addition to diffusion, each particle of the species has a constant probability per unit time of disappearing, for example by radioactive decay or a first-order chemical reaction. We model the concentration $u(x,t)$ (mass per unit length) of the species at position $x\in(0,L)$ and time $t>0$. The rod ends are kept at zero concentration, representing perfect sinks. The governing equation is taken to be
\[
u_t = D\,u_{xx} - k\,u,\qquad 0<x<L,\ t>0,
\]
where $D>0$ is the diffusion coefficient and $k>0$ is the decay (reaction) rate, with boundary and initial conditions
\[
u(0,t)=u(L,t)=0,\qquad t>0,\qquad
u(x,0)=f(x),\qquad 0<x<L.
\]

\smallskip

(a) Explain in words why it is reasonable to add a term of the form $-k\,u$ to the diffusion equation to model decay or reaction. In particular:
\begin{itemize}
    \item Why should the loss rate be proportional to $u$?
    \item Why is the sign negative?
    \item What are the physical units of $k$?
\end{itemize}
How would you check, using the PDE, that the total mass $\displaystyle M(t)=\int_0^L u(x,t)\,dx$ is nonincreasing in time?
% Hint: Differentiate $M(t)$ with respect to $t$ and use the PDE and the boundary conditions.

\smallskip

(b) The reaction term $-k\,u$ makes the equation slightly more complicated than the pure diffusion equation. An important idea is to try a change of unknown that ``absorbs'' this term. 

Look for a new function $v(x,t)$ of the form
\[
v(x,t)=e^{\alpha t}\,u(x,t)
\]
for some constant $\alpha$ to be determined. Compute $v_t$ and $v_{xx}$ in terms of $u$, $u_t$, and $u_{xx}$, and use the PDE for $u$ to derive an equation for $v$. For which value of $\alpha$ does $v$ satisfy the \emph{pure} diffusion equation
\[
v_t = D\,v_{xx}\,?
\]
State also the boundary and initial conditions satisfied by $v$.
% Hint: First write $v_t = \alpha e^{\alpha t} u + e^{\alpha t} u_t$ and substitute $u_t$ from the original PDE.

\smallskip

(c) Now forget about $u$ for a moment and focus on the simpler problem you found for $v$ in part (b):
\[
v_t = D\,v_{xx},\quad 0<x<L,\ t>0,\qquad
v(0,t)=v(L,t)=0,\qquad
v(x,0)=f(x).
\]
Solve this initial–boundary value problem by separation of variables.

\begin{enumerate}
    \item[(i)] Assume $v(x,t)=X(x)T(t)$ and separate variables to obtain an eigenvalue problem for $X(x)$. What ordinary differential equation and boundary conditions does $X$ satisfy?
    \item[(ii)] Show that the nontrivial solutions occur for eigenvalues
    \[
    \lambda_n = \left(\frac{n\pi}{L}\right)^2,\qquad n=1,2,3,\dots,
    \]
    with corresponding eigenfunctions $X_n(x)=\sin\!\left(\frac{n\pi x}{L}\right)$.
    \item[(iii)] For each $n$, solve the corresponding time equation for $T_n(t)$.
    \item[(iv)] Write down the general solution for $v(x,t)$ as a series and express the coefficients in terms of the initial condition $f(x)$.
\end{enumerate}
Hint: You may recall from the heat equation that
\[
f(x) \sim \sum_{n=1}^\infty b_n \sin\!\left(\frac{n\pi x}{L}\right),\qquad
b_n = \frac{2}{L}\int_0^L f(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx.
\]

\smallskip

(d) Return now to the original concentration $u(x,t)$. Using your relation between $u$ and $v$ from part (b), express the solution $u(x,t)$ in terms of $f(x)$ and the parameters $D$, $k$, and $L$. 

Then analyze the long-time behavior:
\begin{itemize}
    \item What is $\displaystyle \lim_{t\to\infty} u(x,t)$?
    \item How does the presence of the reaction rate $k>0$ modify the rate at which the solution decays in time, compared with the pure diffusion case $k=0$?
\end{itemize}
% Hint: Compare the exponents in the time-dependent factors.

\smallskip

(e) ``What if'' questions and extensions.
\begin{enumerate}
    \item[(i)] Suppose instead that $k<0$, so that the reaction term is $-k\,u$ with $k<0$, corresponding to linear \emph{growth} rather than decay. How would your formula for $u(x,t)$ change, and what would you expect for the long-time behavior?
    \item[(ii)] Suppose the equation had an additional constant source term,
    \[
    u_t = D\,u_{xx} - k\,u + S,\qquad S>0\ \text{constant}.
    \]
    Outline (without full details) how you might adapt your method to handle this case. In particular, what kind of steady state solution would you look for, and how could you reduce the problem to one you already know how to solve?
    % Hint: Try to find a time-independent particular solution, then subtract it to obtain a homogeneous problem.
\end{enumerate}

\end{problem}

% ===== Example 5: Diffusion with Reaction or Decay (full solution) =====
\begin{problem}[Diffusion with Reaction or Decay]
Consider the one-dimensional reaction–diffusion equation on $0<x<L$,
\[
u_t = D\,u_{xx} - k\,u,\qquad D>0,\ k>0,
\]
with boundary conditions $u(0,t)=u(L,t)=0$ for $t>0$ and initial condition $u(x,0)=f(x)$ for $0<x<L$, where $f$ is sufficiently regular.

\begin{enumerate}
    \item[(i)] By introducing a transformed unknown $v(x,t)=e^{k t}u(x,t)$, show that $v$ satisfies the pure diffusion equation with homogeneous Dirichlet boundary conditions and initial data $v(x,0)=f(x)$.
    \item[(ii)] Solve the resulting initial–boundary value problem for $v$ by separation of variables, and hence obtain an explicit series representation for $u(x,t)$.
    \item[(iii)] Describe briefly how the reaction term $-k\,u$ affects the temporal decay rates and the long-time behavior, compared with the case $k=0$.
\end{enumerate}
\end{problem}

\begin{solution}
We are asked to solve a linear reaction–diffusion equation on a finite interval with homogeneous Dirichlet boundary conditions. The central ideas are to simplify the equation via an integrating-factor-type substitution, then to apply separation of variables and eigenfunction expansions, exactly as for the standard diffusion (heat) equation.

\medskip

\noindent\textbf{(i) Transforming to the pure diffusion equation.}
Define
\[
v(x,t)=e^{k t}u(x,t).
\]
We compute its derivatives. First,
\[
v_t(x,t) = k e^{kt} u(x,t) + e^{kt} u_t(x,t),
\]
and, since the exponential factor depends only on $t$,
\[
v_x(x,t) = e^{kt} u_x(x,t),\qquad v_{xx}(x,t) = e^{kt} u_{xx}(x,t).
\]
We substitute the given PDE for $u$,
\[
u_t = D u_{xx} - k u,
\]
into the expression for $v_t$:
\[
v_t = k e^{kt} u + e^{kt} (D u_{xx} - k u)
    = k e^{kt} u + D e^{kt} u_{xx} - k e^{kt} u
    = D e^{kt} u_{xx}.
\]
Using $v_{xx} = e^{kt} u_{xx}$, we obtain the simpler PDE
\[
v_t = D v_{xx},\qquad 0<x<L,\ t>0,
\]
which is the standard diffusion (heat) equation.

The boundary conditions for $v$ follow from those of $u$:
\[
v(0,t) = e^{kt} u(0,t) = 0,\qquad
v(L,t) = e^{kt} u(L,t) = 0,\qquad t>0.
\]
At $t=0$, we have
\[
v(x,0) = e^{k\cdot 0} u(x,0) = u(x,0) = f(x),\qquad 0<x<L.
\]
Thus $v$ solves the classical heat equation on $(0,L)$ with homogeneous Dirichlet boundary conditions and initial data $f$.

\medskip

\noindent\textbf{(ii) Separation of variables and series solution.}
We now solve
\[
\begin{cases}
v_t = D v_{xx}, & 0<x<L,\ t>0,\\[0.2em]
v(0,t)=v(L,t)=0, & t>0,\\[0.2em]
v(x,0)=f(x), & 0<x<L.
\end{cases}
\]
We use separation of variables. Seek solutions of the form $v(x,t) = X(x)T(t)$ with $X$ not identically zero and $T$ not identically zero. Substituting into the PDE gives
\[
X(x) T'(t) = D X''(x) T(t).
\]
For $X(x)T(t)\neq 0$, we divide both sides by $D X(x) T(t)$ to obtain
\[
\frac{T'(t)}{D T(t)} = \frac{X''(x)}{X(x)} = -\lambda
\]
for some separation constant $\lambda$ (independent of $x$ and $t$). We thus obtain the system
\[
\begin{cases}
X''(x) + \lambda X(x) = 0,\\
T'(t) + D \lambda T(t) = 0.
\end{cases}
\]

The boundary conditions for $v$ translate to
\[
X(0) T(t) = 0,\quad X(L) T(t) = 0\quad \text{for all } t>0.
\]
For nontrivial solutions with $T(t)\neq 0$, this requires
\[
X(0)=X(L)=0.
\]
Therefore $X$ satisfies the Sturm–Liouville problem
\[
X'' + \lambda X = 0,\quad X(0)=X(L)=0.
\]

It is standard (and easy to check) that nontrivial solutions arise precisely for
\[
\lambda_n = \left(\frac{n\pi}{L}\right)^2,\qquad n=1,2,3,\dots,
\]
with corresponding eigenfunctions
\[
X_n(x) = \sin\!\left(\frac{n\pi x}{L}\right).
\]
For each such $\lambda_n$, the time-dependent factor satisfies
\[
T_n'(t) + D \lambda_n T_n(t) = 0,
\]
whose solution is
\[
T_n(t) = A_n e^{-D \lambda_n t}
      = A_n \exp\!\left(-D\left(\frac{n\pi}{L}\right)^2 t\right),
\]
for some constant $A_n$.

By linearity and superposition, the general solution $v$ satisfying the boundary conditions may be expanded as
\[
v(x,t) = \sum_{n=1}^\infty b_n e^{-D (n\pi/L)^2 t}
          \sin\!\left(\frac{n\pi x}{L}\right),
\]
where the coefficients $\{b_n\}$ are determined by the initial condition $v(x,0)=f(x)$:
\[
f(x) = v(x,0) = \sum_{n=1}^\infty b_n \sin\!\left(\frac{n\pi x}{L}\right).
\]
This is precisely the sine series expansion of $f$ on $(0,L)$, so the coefficients are
\[
b_n = \frac{2}{L} \int_0^L f(x)\,\sin\!\left(\frac{n\pi x}{L}\right)\,dx,
\qquad n=1,2,3,\dots.
\]
Therefore
\[
v(x,t) = \sum_{n=1}^\infty 
\left[\frac{2}{L} \int_0^L f(\xi)\,\sin\!\left(\frac{n\pi \xi}{L}\right)\,d\xi\right]
\exp\!\left(-D\left(\frac{n\pi}{L}\right)^2 t\right)
\sin\!\left(\frac{n\pi x}{L}\right).
\]

Returning to $u$, we recall $u(x,t)=e^{-kt} v(x,t)$, so
\[
u(x,t) = e^{-kt} \sum_{n=1}^\infty b_n e^{-D (n\pi/L)^2 t}
          \sin\!\left(\frac{n\pi x}{L}\right)
       = \sum_{n=1}^\infty b_n
          \exp\!\left(-\bigl[D(n\pi/L)^2 + k\bigr] t\right)
          \sin\!\left(\frac{n\pi x}{L}\right),
\]
with
\[
b_n = \frac{2}{L} \int_0^L f(\xi)\,\sin\!\left(\frac{n\pi \xi}{L}\right)\,d\xi.
\]
This is the desired explicit series solution for $u$.

\medskip

\noindent\textbf{(iii) Effect of the reaction term on decay rates and long-time behavior.}
Each spatial mode $\sin\!\left(\frac{n\pi x}{L}\right)$ decays in time with a factor
\[
\exp\!\left(-\bigl[D(n\pi/L)^2 + k\bigr] t\right).
\]
For the pure diffusion equation ($k=0$), the decay rate of the $n$-th mode is $D (n\pi/L)^2$. The reaction term $-k u$ adds the constant $k$ to every decay rate, so each mode decays faster in time by a factor $\mathrm{e}^{-k t}$ relative to the $k=0$ case.

In particular, for any nontrivial initial data $f$ with at least one nonzero Fourier coefficient $b_n$, we have
\[
\lim_{t\to\infty} u(x,t) = 0
\]
for each fixed $x\in(0,L)$, since all the exponential factors tend to zero. The reaction term does not change the limiting state (which is already zero for the homogeneous Dirichlet heat equation), but it accelerates the approach to zero by imposing an additional uniform exponential decay.

\medskip

\noindent\textbf{Connection to the diffusion equation theme.}
This example illustrates two central ideas of the diffusion equation chapter:
\begin{itemize}
    \item A linear reaction term $-k u$ can often be removed by a simple time-dependent rescaling $v = e^{k t} u$, reducing the problem to the pure diffusion equation.
    \item Once reduced, the standard separation-of-variables method applies, leading to an eigenvalue problem for the spatial operator (here, the Laplacian with Dirichlet boundary conditions), and the full solution is expressed as a series of eigenfunctions with exponentially decaying time factors.
\end{itemize}
The reaction term therefore appears as a uniform shift in the eigenvalues governing temporal decay, leaving the spatial eigenfunctions unchanged.
\end{solution}

\section{Boundary Value Problems: Fourier Method}
% --- Narrative plan (auto-generated) ---
% In this section we study boundary value problems for partial differential equations using the Fourier method, which is built on separation of variables and Fourier series. The central idea is to convert a partial differential equation with spatial boundary conditions into a family of ordinary differential equations in time (or another variable) whose solutions can be written as infinite sums of spatial eigenfunctions. These eigenfunctions, typically sines and cosines, arise from solving a related Sturm–Liouville problem and form an orthogonal basis that allows us to expand the initial or boundary data.
%
% This approach is fundamental in applied mathematics because many physical models, such as heat flow, wave propagation, and diffusion in confined regions, naturally lead to boundary value problems. The Fourier method provides explicit formulas for solutions, reveals how different spatial modes evolve, and connects directly to notions of stability and long-time behavior in dynamical systems. Mathematically, it illustrates how tools from ordinary differential equations, linear algebra, and real and complex Fourier analysis come together to solve partial differential equations and prepares the ground for more advanced topics such as transform methods, spectral theory, and numerical approximation of PDEs.

% ===== Example 1: Heat Equation on a Finite Rod with Fixed End Temperatures (inquiry-based) =====
\begin{problem}[Heat Equation on a Finite Rod with Fixed End Temperatures]
Consider a thin, homogeneous rod of length $L$, lying along the $x$-axis from $x=0$ to $x=L$. The sides of the rod are perfectly insulated, so heat can flow only along the length of the rod. The two ends are kept in contact with ideal heat baths which maintain them at fixed temperatures (in this first model, at temperature zero). We are given an initial temperature distribution along the rod and we wish to determine the temperature at later times. In this problem you will discover how the Fourier method and separation of variables lead naturally to a solution in terms of a Fourier sine series.

(a) \textbf{Modeling the problem.}  
Assume that the temperature $u(x,t)$ in the rod satisfies the one-dimensional heat equation with constant thermal diffusivity $\kappa>0$. Write down the partial differential equation (PDE) for $u(x,t)$, the boundary conditions at $x=0$ and $x=L$ if both ends are held at temperature zero, and the initial condition if the initial temperature profile is a given function $f(x)$.

\medskip

(b) \textbf{Trying separation of variables.}  
We now look for solutions of the PDE that also satisfy the homogeneous boundary conditions, but not necessarily the initial condition yet. Suppose that
\[
u(x,t) = X(x)\,T(t)
\]
for some functions $X$ and $T$.

\begin{enumerate}
  \item[(i)] Substitute $u(x,t) = X(x)T(t)$ into the heat equation and simplify to obtain an equation where all dependence on $x$ and $t$ is separated.
  \item[(ii)] Explain why the separated equation must be equal to a constant, say $-\lambda$, and write down the resulting ordinary differential equations (ODEs) for $X$ and $T$.
  \item[(iii)] Translate the boundary conditions on $u$ into boundary conditions on $X$.
\end{enumerate}

Hint: After substitution and division by $\kappa X T$, you should obtain that a function of $x$ alone equals a function of $t$ alone. Conclude that both are constant.

\medskip

(c) \textbf{Solving the spatial eigenvalue problem.}  
You now have a boundary value problem for $X$ of the form
\[
X''(x) + \lambda X(x) = 0, \qquad X(0)=0, \quad X(L)=0,
\]
for some real parameter $\lambda$.

\begin{enumerate}
  \item[(i)] Consider separately the cases $\lambda<0$, $\lambda=0$, and $\lambda>0$. For each case, solve the ODE for $X$ and impose the boundary conditions to determine whether nontrivial solutions exist.
  \item[(ii)] For the case that admits nontrivial solutions, find the corresponding values of $\lambda$ and describe the corresponding eigenfunctions $X_n(x)$, $n=1,2,\dots$.
\end{enumerate}

Hint: You should find that $\lambda$ cannot be nonpositive if we want nontrivial solutions satisfying both boundary conditions. For $\lambda>0$, it is convenient to write $\lambda = \mu^2$ with $\mu>0$ and look for trigonometric solutions.

\medskip

(d) \textbf{Assembling the separated solutions and matching the initial condition.}  
For each admissible $\lambda_n$ you found, there is a corresponding time-dependent factor $T_n(t)$ solving the ODE from part (b). 

\begin{enumerate}
  \item[(i)] Solve the ODE for $T_n(t)$ and write down the product solution
  \[
  u_n(x,t) = X_n(x)\,T_n(t).
  \]
  \item[(ii)] Argue why a general solution $u(x,t)$ satisfying the PDE and homogeneous boundary conditions can be written as an (infinite) linear combination of these separated solutions:
  \[
  u(x,t) = \sum_{n=1}^\infty b_n\,X_n(x)\,T_n(t).
  \]
  \item[(iii)] Use the initial condition $u(x,0) = f(x)$ to derive a formula for the coefficients $b_n$ in terms of $f$. Express $b_n$ as an integral involving $f$ and $X_n$.
\end{enumerate}

Hint: At $t=0$ you obtain a Fourier series expansion of $f(x)$ in terms of the eigenfunctions $X_n(x)$. Use orthogonality of these eigenfunctions on the interval $[0,L]$.

\medskip

(e) \textbf{Extensions and variations.}  

\begin{enumerate}
  \item[(i)] Suppose now that the ends are held at the nonzero constant temperature $T_0$ instead of zero, that is, $u(0,t)=u(L,t)=T_0$. Outline how you would modify the above procedure to solve this problem. In particular, indicate how to reduce the boundary conditions to a homogeneous form.
  
  Hint: Look for a steady-state (time-independent) solution $v(x)$ that satisfies the boundary conditions, and then consider $w(x,t)=u(x,t)-v(x)$.
  
  \item[(ii)] In the original problem with zero end temperatures, what happens to $u(x,t)$ as $t\to\infty$? Give a qualitative explanation based on your series solution.
\end{enumerate}

\end{problem}

% ===== Example 1: Heat Equation on a Finite Rod with Fixed End Temperatures (full solution) =====
\begin{problem}[Heat Equation on a Finite Rod with Fixed End Temperatures]
Let $u(x,t)$ denote the temperature in a thin homogeneous rod of length $L$, for $0<x<L$ and $t>0$. The rod satisfies the one-dimensional heat equation
\[
u_t = \kappa u_{xx}, \qquad 0<x<L,\ t>0,
\]
with fixed end temperatures
\[
u(0,t)=0,\qquad u(L,t)=0,\qquad t>0,
\]
and initial temperature profile
\[
u(x,0)=f(x),\qquad 0<x<L,
\]
where $f$ is a given sufficiently nice function. Use the Fourier method (separation of variables and eigenfunction expansions) to find an explicit series representation of the solution $u(x,t)$, including formulas for the coefficients in terms of $f$. Describe also the qualitative behavior of $u(x,t)$ as $t\to\infty$.
\end{problem}

\begin{solution}
We seek to solve the initial–boundary value problem for the one-dimensional heat equation on the finite interval $(0,L)$ with homogeneous Dirichlet boundary conditions. This is a standard setting in which the Fourier method applies: we use separation of variables to obtain an eigenvalue problem in $x$, solve that boundary value problem, and then expand the initial data in the resulting eigenfunctions.

\medskip

\textbf{1. Separation of variables and the eigenvalue problem.}  
We first look for solutions of the PDE that satisfy the homogeneous boundary conditions, but not yet the initial condition, in the separated form
\[
u(x,t) = X(x)\,T(t).
\]
Substituting this into the PDE $u_t = \kappa u_{xx}$ gives
\[
X(x)\,T'(t) = \kappa X''(x)\,T(t).
\]
Assuming $X$ and $T$ are not identically zero, we divide by $\kappa X(x)T(t)$ to obtain
\[
\frac{T'(t)}{\kappa T(t)} = \frac{X''(x)}{X(x)}.
\]
The left-hand side depends only on $t$, while the right-hand side depends only on $x$. Therefore both sides must be equal to a constant, which we denote by $-\lambda$:
\[
\frac{T'(t)}{\kappa T(t)} = \frac{X''(x)}{X(x)} = -\lambda.
\]
This yields the system of ordinary differential equations
\[
\begin{cases}
X''(x) + \lambda X(x) = 0,\\[4pt]
T'(t) + \kappa \lambda T(t) = 0.
\end{cases}
\]
The boundary conditions on $u$ translate into conditions on $X$:
\[
u(0,t)=0 \ \Rightarrow\ X(0)T(t)=0 \ \Rightarrow\ X(0)=0,\qquad
u(L,t)=0 \ \Rightarrow\ X(L)T(t)=0 \ \Rightarrow\ X(L)=0,
\]
since we are not interested in the trivial solution $T\equiv 0$. Thus $X$ must solve the boundary value problem
\[
X''(x) + \lambda X(x) = 0,\qquad X(0)=0,\quad X(L)=0.
\]
This is a Sturm–Liouville eigenvalue problem, and its solutions will furnish the spatial modes for our Fourier expansion.

\medskip

\textbf{2. Solving the spatial boundary value problem.}  
We now determine the eigenvalues $\lambda$ for which there exist nontrivial solutions $X$ satisfying
\[
X'' + \lambda X = 0,\quad X(0)=0,\ X(L)=0.
\]
We consider three cases.

\medskip

\emph{Case 1: $\lambda=0$.} Then the ODE reduces to $X''(x)=0$, whose general solution is
\[
X(x) = A + Bx.
\]
The condition $X(0)=0$ implies $A=0$, so $X(x)=Bx$. The condition $X(L)=0$ then gives $B L = 0$, hence $B=0$. Thus $X\equiv 0$ is the only solution; there is no nontrivial eigenfunction when $\lambda=0$.

\medskip

\emph{Case 2: $\lambda<0$.} Write $\lambda=-\mu^2$ with $\mu>0$. The ODE becomes
\[
X''(x) - \mu^2 X(x)=0,
\]
whose general solution is
\[
X(x) = A e^{\mu x} + B e^{-\mu x}.
\]
Imposing $X(0)=0$ yields $A + B = 0$, so $B=-A$ and
\[
X(x) = A\left(e^{\mu x} - e^{-\mu x}\right) = 2A\sinh(\mu x).
\]
Then $X(L)=0$ implies $\sinh(\mu L)=0$, but $\sinh(y)=0$ only when $y=0$, so we must have $\mu L=0$, that is $\mu=0$, which contradicts $\mu>0$. Hence again only the trivial solution exists. There are no eigenvalues with $\lambda<0$.

\medskip

\emph{Case 3: $\lambda>0$.} Write $\lambda=\mu^2$ with $\mu>0$. The ODE becomes
\[
X''(x)+\mu^2 X(x)=0,
\]
whose general solution is
\[
X(x) = A\cos(\mu x) + B\sin(\mu x).
\]
The condition $X(0)=0$ gives $A\cos(0) + B\sin(0) = A=0$, so $X(x)=B\sin(\mu x)$. Then $X(L)=0$ implies $B\sin(\mu L)=0$. For a nontrivial solution we require $B\neq 0$, so we must have
\[
\sin(\mu L)=0 \quad\Longrightarrow\quad \mu L = n\pi,\quad n=1,2,3,\dots
\]
Thus
\[
\mu_n = \frac{n\pi}{L}, \qquad \lambda_n = \mu_n^2 = \left(\frac{n\pi}{L}\right)^2,
\]
and the corresponding eigenfunctions (up to scaling) are
\[
X_n(x) = \sin\left(\frac{n\pi x}{L}\right),\qquad n=1,2,3,\dots
\]
These eigenfunctions satisfy the boundary conditions $X_n(0)=X_n(L)=0$.

Therefore, the spatial part of the Fourier method provides an infinite sequence of eigenvalues $\lambda_n = (n\pi/L)^2$ and corresponding eigenfunctions $X_n(x)$ as above.

\medskip

\textbf{3. Time-dependent factors and separated solutions.}  
For each eigenvalue $\lambda_n$, the time-dependent function $T_n(t)$ solves
\[
T_n'(t) + \kappa \lambda_n T_n(t)=0.
\]
This is a first-order linear ODE whose general solution is
\[
T_n(t) = C_n e^{-\kappa \lambda_n t} = C_n \exp\!\left(-\kappa \left(\frac{n\pi}{L}\right)^2 t\right),
\]
where $C_n$ is an arbitrary constant. Absorbing $C_n$ into the overall multiplicative constant, one separated solution corresponding to mode $n$ is
\[
u_n(x,t) = \sin\!\left(\frac{n\pi x}{L}\right) e^{-\kappa \left(\frac{n\pi}{L}\right)^2 t}.
\]
Each $u_n$ satisfies the heat equation, the homogeneous boundary conditions, and is nontrivial.

The heat equation is linear and homogeneous, so any finite linear combination of the $u_n$ is also a solution. Under suitable assumptions on $f$ (for example, $f$ square-integrable or piecewise smooth), we may consider infinite series as well. Thus we look for the general solution in the form
\[
u(x,t) = \sum_{n=1}^\infty b_n \sin\!\left(\frac{n\pi x}{L}\right) e^{-\kappa \left(\frac{n\pi}{L}\right)^2 t},
\]
where the coefficients $b_n$ are to be determined from the initial condition.

\medskip

\textbf{4. Imposing the initial condition and Fourier sine coefficients.}  
At time $t=0$ the solution must satisfy
\[
u(x,0) = f(x).
\]
On the other hand, from the series representation we obtain
\[
u(x,0) = \sum_{n=1}^\infty b_n \sin\!\left(\frac{n\pi x}{L}\right) e^{0}
= \sum_{n=1}^\infty b_n \sin\!\left(\frac{n\pi x}{L}\right).
\]
Thus we require that $f$ admit an expansion in a Fourier sine series on $(0,L)$:
\[
f(x) \sim \sum_{n=1}^\infty b_n \sin\!\left(\frac{n\pi x}{L}\right).
\]
The sine functions $\{\sin(n\pi x/L)\}_{n=1}^\infty$ form an orthogonal set in $L^2(0,L)$ with respect to the standard inner product
\[
\langle g,h\rangle = \int_0^L g(x)h(x)\,dx.
\]
In fact, for $m,n\ge 1$,
\[
\int_0^L \sin\!\left(\frac{m\pi x}{L}\right)\sin\!\left(\frac{n\pi x}{L}\right)\,dx
= 
\begin{cases}
0, & m\neq n,\\[4pt]
\displaystyle \frac{L}{2}, & m=n.
\end{cases}
\]
To derive the coefficients $b_n$, multiply the series for $f$ by $\sin\left(\frac{m\pi x}{L}\right)$ and integrate from $0$ to $L$:
\[
\int_0^L f(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= \int_0^L \left(\sum_{n=1}^\infty b_n \sin\!\left(\frac{n\pi x}{L}\right)\right)\sin\!\left(\frac{m\pi x}{L}\right)\,dx.
\]
Under mild hypotheses on $f$ we may interchange sum and integral:
\[
\int_0^L f(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= \sum_{n=1}^\infty b_n \int_0^L \sin\!\left(\frac{n\pi x}{L}\right)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= b_m \frac{L}{2}.
\]
Therefore,
\[
b_m = \frac{2}{L}\int_0^L f(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx,\qquad m=1,2,3,\dots.
\]
Renaming the index, we have the explicit formula
\[
b_n = \frac{2}{L}\int_0^L f(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx,\qquad n\ge 1.
\]

\medskip

\textbf{5. Final series solution and long-time behavior.}  
Substituting these coefficients into the separated series, we arrive at the Fourier–series representation of the solution:
\[
u(x,t)
= \sum_{n=1}^\infty \left[ \frac{2}{L}\int_0^L f(\xi)\sin\!\left(\frac{n\pi \xi}{L}\right)\,d\xi \right]
\sin\!\left(\frac{n\pi x}{L}\right)
\exp\!\left(-\kappa \left(\frac{n\pi}{L}\right)^2 t\right).
\]
Here $\xi$ is a dummy integration variable. This function $u$ satisfies the heat equation, the homogeneous Dirichlet boundary conditions, and the initial condition in the sense of convergence of the Fourier series to $f$ (for example, pointwise at continuity points of $f$ and in $L^2$ if $f\in L^2(0,L)$).

To understand the behavior as $t\to\infty$, note that each term in the series has the factor
\[
\exp\!\left(-\kappa \left(\frac{n\pi}{L}\right)^2 t\right),
\]
which tends to zero exponentially fast as $t\to\infty$ for every $n\ge 1$. Therefore, $u(x,t)\to 0$ as $t\to\infty$, at least pointwise and in appropriate norms. Physically, this corresponds to the rod cooling down so that its temperature eventually becomes identically zero, consistent with the fact that the ends are held at zero temperature and the system has no internal heat sources.

\medskip

\textbf{6. Connection to the Fourier method for boundary value problems.}  
This example illustrates the main ideas of the section on boundary value problems and the Fourier method:

\begin{itemize}
  \item First, separation of variables reduces the PDE, together with homogeneous boundary conditions, to an eigenvalue problem for an ordinary differential operator in $x$.
  \item Second, the eigenfunctions of that boundary value problem (here, the sine functions) form an orthogonal basis appropriate to the geometry and boundary conditions of the problem.
  \item Third, the initial condition is enforced by expanding the initial data in this eigenfunction basis, using orthogonality to compute Fourier coefficients.
  \item Finally, the time evolution is encoded in simple exponential factors depending on the eigenvalues, leading to a series solution that makes qualitative features such as decay and smoothing of the temperature field transparent.
\end{itemize}

Thus the heat equation on a finite interval with fixed end temperatures provides a canonical and instructive model for the Fourier method applied to linear PDEs with boundary conditions.

\end{solution}

% ===== Example 2: Heat Equation with Insulated Ends (Neumann Boundary Conditions) (inquiry-based) =====
\begin{problem}[Heat Equation with Insulated Ends (Neumann Boundary Conditions)]
Consider a thin, homogeneous rod of length $L$ lying along the $x$-axis from $x=0$ to $x=L$. We again assume that heat flows only along the length of the rod and that the temperature is governed by the one-dimensional heat equation. This time, however, both ends of the rod are perfectly insulated, so no heat can flow in or out through $x=0$ or $x=L$. Physically, this means that whatever heat is initially in the rod will remain there forever; only its distribution can change. We will see that this physical picture is naturally reflected in the mathematics through Neumann boundary conditions and a special ``constant mode'' in the solution.

Throughout, let $u(x,t)$ denote the temperature at position $x$ and time $t$, and let $k>0$ denote the thermal diffusivity constant.

\medskip

(a) \textbf{Modeling the insulation.}  
Starting from the physical statement that “no heat flows through the ends of the rod,” explain why the appropriate boundary conditions at $x=0$ and $x=L$ are
\[
u_x(0,t) = 0, \qquad u_x(L,t) = 0, \qquad t>0.
\]
You may recall that the heat flux is proportional to $-u_x$. Why does the vanishing of $u_x$ correspond to perfect insulation?

\medskip

(b) \textbf{Setting up separation of variables.}  
Assume that $u$ satisfies the heat equation
\[
u_t = k\,u_{xx}, \qquad 0<x<L,\ t>0,
\]
with the Neumann boundary conditions from part (a) and an initial condition
\[
u(x,0) = f(x), \qquad 0\le x\le L,
\]
where $f$ is a given (reasonable) function.

Assume a separated solution of the form $u(x,t) = X(x)T(t)$ and substitute into the PDE and boundary conditions.

\begin{itemize}
  \item[(i)] Show that $X$ and $T$ must satisfy
  \[
  \frac{T'(t)}{k\,T(t)} = \frac{X''(x)}{X(x)} = -\lambda
  \]
  for some separation constant $\lambda \in \mathbb{R}$, and hence derive the ordinary differential equations
  \[
  X''(x) + \lambda X(x) = 0, \quad 0<x<L, \qquad T'(t) + k\lambda T(t) = 0.
  \]
  \item[(ii)] Translate the Neumann boundary conditions into conditions on $X$.
\end{itemize}

Hint: Focus on $X'(0)$ and $X'(L)$, using that $u_x(x,t) = X'(x)T(t)$.

\medskip

(c) \textbf{The spatial eigenvalue problem for insulated ends.}  
We now must solve the eigenvalue problem
\[
X''(x) + \lambda X(x) = 0,\quad 0<x<L,\qquad X'(0)=0,\quad X'(L)=0.
\]
Analyze this problem by considering three cases for $\lambda$:

\begin{itemize}
  \item[(i)] $\lambda < 0$. Show that any nontrivial solution violates at least one of the Neumann conditions, so there are no negative eigenvalues.
  \item[(ii)] $\lambda = 0$. Solve $X''(x) = 0$ and impose the Neumann conditions. What eigenfunctions arise in this case?
  \item[(iii)] $\lambda > 0$. Write $\lambda = \mu^2$ with $\mu>0$ and solve $X'' + \mu^2 X = 0$. Impose $X'(0)=0$ and $X'(L)=0$ to determine the allowable values of $\mu$ and the corresponding eigenfunctions.
\end{itemize}

Summarize your findings by listing the eigenvalues $\lambda_n$ and eigenfunctions $X_n(x)$, including the case $n=0$.

Hint: For $\lambda>0$, the general solution is $X(x)=A\cos(\mu x)+B\sin(\mu x)$.

\medskip

(d) \textbf{Assembling the general solution and matching the initial data.}  
Using the eigenfunctions from part (c), write the general separated solution as a linear combination
\[
u(x,t) = \sum_{n=0}^\infty c_n\,X_n(x)\,T_n(t),
\]
with each $T_n(t)$ solving the corresponding time ODE.

\begin{itemize}
  \item[(i)] Determine $T_n(t)$ for each eigenvalue $\lambda_n$. Pay special attention to the $n=0$ case.
  \item[(ii)] Using the orthogonality of the eigenfunctions $\{X_n\}$ on $[0,L]$, explain how to choose the constants $c_n$ so that $u(x,0)=f(x)$. Write down explicit formulas for the coefficients in terms of $f$ and integrals over $[0,L]$.
  \item[(iii)] What does the term corresponding to $n=0$ represent physically? Based on your formula for $c_0$, interpret this in terms of the \emph{average} temperature of the rod.
\end{itemize}

Hint: You should find that the spatial eigenfunctions are cosine functions, and that $c_0$ is proportional to the integral of $f$ over $[0,L]$.

\medskip

(e) \textbf{Extensions and qualitative behavior.}

\begin{itemize}
  \item[(i)] Show, directly from the PDE and Neumann boundary conditions (without using the explicit solution), that the total heat in the rod,
  \[
  H(t) = \int_0^L u(x,t)\,dx,
  \]
  is constant in time. How does this fact relate to your identification of the $n=0$ term in part (d)?
  
  Hint: Differentiate $H(t)$ with respect to $t$ and use the PDE and integration by parts.
  
  \item[(ii)] Suppose instead that the left end is held at zero temperature while the right end is insulated:
  \[
  u(0,t)=0,\quad u_x(L,t)=0.
  \]
  How would you expect the set of eigenfunctions to change compared to the purely insulated case? Would you still expect constant temperature modes?
  
  Hint: Think about how many derivative conditions versus value conditions you have, and what combinations of sines and cosines can satisfy them.
\end{itemize}

\end{problem}

% ===== Example 2: Heat Equation with Insulated Ends (Neumann Boundary Conditions) (full solution) =====
\begin{problem}[Heat Equation with Insulated Ends (Neumann Boundary Conditions)]
Consider the heat equation on a rod of length $L$ with insulated ends:
\[
u_t = k\,u_{xx}, \qquad 0<x<L,\ t>0,
\]
\[
u_x(0,t) = 0,\quad u_x(L,t) = 0,\qquad t>0,
\]
\[
u(x,0) = f(x), \qquad 0\le x\le L,
\]
where $k>0$ is constant and $f$ is a given function.

(a) Use separation of variables and the Fourier method to find the solution $u(x,t)$ in terms of a cosine series involving $f$.

(b) Identify the long-time limit $\displaystyle \lim_{t\to\infty} u(x,t)$ (if it exists), and express it in terms of $f$.

(c) Briefly explain how this example illustrates the role of Neumann boundary conditions and orthogonal eigenfunctions in the Fourier method for boundary value problems.
\end{problem}

\begin{solution}
We solve the initial–boundary value problem by the method of separation of variables and Fourier expansions adapted to Neumann boundary conditions.

\medskip

\emph{Step 1: Separation of variables and the eigenvalue problem.}  
We seek solutions of the form $u(x,t)=X(x)T(t)$. Substituting into the heat equation gives
\[
X(x)T'(t) = k X''(x)T(t).
\]
Assuming $X$ and $T$ are nonzero, we divide by $kX(x)T(t)$ to obtain
\[
\frac{T'(t)}{kT(t)} = \frac{X''(x)}{X(x)} = -\lambda,
\]
for some separation constant $\lambda\in\mathbb{R}$. Hence,
\[
X''(x)+\lambda X(x)=0,\qquad 0<x<L,
\]
\[
T'(t)+k\lambda T(t)=0,\qquad t>0.
\]
The boundary conditions become
\[
u_x(0,t)=X'(0)T(t)=0,\quad u_x(L,t)=X'(L)T(t)=0.
\]
Since we are interested in nontrivial solutions with $T(t)\neq 0$, these reduce to
\[
X'(0)=0,\qquad X'(L)=0.
\]
Thus, we must solve the eigenvalue problem
\[
X''(x)+\lambda X(x)=0,\quad 0<x<L,\qquad X'(0)=0,\ X'(L)=0.
\]

\medskip

\emph{Step 2: Solving the Neumann eigenvalue problem.}  
We analyze $X''+\lambda X=0$ with $X'(0)=X'(L)=0$.

\smallskip

\emph{Case 1: $\lambda<0$.}  
Let $\lambda=-\mu^2$ with $\mu>0$. Then
\[
X''(x)-\mu^2 X(x)=0,
\]
whose general solution is $X(x)=A e^{\mu x}+B e^{-\mu x}$. Differentiating gives
\[
X'(x)=A\mu e^{\mu x}-B\mu e^{-\mu x}.
\]
The condition $X'(0)=0$ implies $A\mu-B\mu=0$, so $A=B$. Then
\[
X'(L)=A\mu e^{\mu L}-A\mu e^{-\mu L}=A\mu\bigl(e^{\mu L}-e^{-\mu L}\bigr).
\]
For this to vanish, we must have either $A=0$ or $e^{\mu L}=e^{-\mu L}$, which is impossible for $\mu>0$. Hence $A=0$, and then $X\equiv 0$. Therefore, there is no nontrivial solution for $\lambda<0$.

\smallskip

\emph{Case 2: $\lambda=0$.}  
We solve $X''(x)=0$. The general solution is
\[
X(x)=A+Bx.
\]
Differentiating, $X'(x)=B$. The Neumann conditions give
\[
X'(0)=B=0,\quad X'(L)=B=0.
\]
Thus $B=0$ and $X(x)=A$ is constant. Any nonzero constant is an eigenfunction corresponding to $\lambda_0=0$. We usually take $X_0(x)\equiv 1$ as a normalized eigenfunction (up to a constant factor).

\smallskip

\emph{Case 3: $\lambda>0$.}  
Let $\lambda=\mu^2$ with $\mu>0$. Then
\[
X''(x)+\mu^2 X(x)=0,
\]
with general solution
\[
X(x)=A\cos(\mu x)+B\sin(\mu x).
\]
Differentiating,
\[
X'(x)=-A\mu\sin(\mu x)+B\mu\cos(\mu x).
\]
The boundary condition $X'(0)=0$ yields
\[
X'(0)=B\mu=0 \quad\Rightarrow\quad B=0.
\]
Therefore $X(x)=A\cos(\mu x)$ and $X'(x)=-A\mu\sin(\mu x)$. The second boundary condition $X'(L)=0$ gives
\[
-A\mu\sin(\mu L)=0.
\]
For a nontrivial eigenfunction, $A\neq 0$ and $\mu\neq 0$, so we require
\[
\sin(\mu L)=0 \quad\Rightarrow\quad \mu L=n\pi,\quad n=1,2,3,\dots.
\]
Thus,
\[
\mu_n = \frac{n\pi}{L},\qquad \lambda_n = \mu_n^2 = \left(\frac{n\pi}{L}\right)^2,\quad n=1,2,3,\dots,
\]
with corresponding eigenfunctions
\[
X_n(x)=\cos\left(\frac{n\pi x}{L}\right),\quad n\ge 1.
\]

\smallskip

Collecting all cases, the eigenvalues and eigenfunctions are
\[
\lambda_0=0,\quad X_0(x)\equiv 1;
\]
\[
\lambda_n=\left(\frac{n\pi}{L}\right)^2,\quad X_n(x)=\cos\left(\frac{n\pi x}{L}\right),\quad n=1,2,3,\dots.
\]
The family $\{X_n\}_{n=0}^\infty$ forms an orthogonal set in $L^2(0,L)$ with respect to the standard inner product.

\medskip

\emph{Step 3: Time dependence and general series solution.}  
For each eigenvalue $\lambda_n$, the time equation is
\[
T_n'(t)+k\lambda_n T_n(t)=0,
\]
whose solutions are exponentials:
\[
T_n(t)=C_n e^{-k\lambda_n t}.
\]

For $n\ge 1$, we have $\lambda_n=(n\pi/L)^2>0$, so
\[
T_n(t)=C_n\,e^{-k(n\pi/L)^2 t}.
\]
For $n=0$, we have $\lambda_0=0$, so the time equation is $T_0'(t)=0$, and therefore
\[
T_0(t)=C_0,
\]
a constant in time.

A general solution is an infinite linear combination of separated solutions,
\[
u(x,t) = A_0 X_0(x)T_0(t) + \sum_{n=1}^\infty A_n X_n(x)T_n(t).
\]
Absorbing constants into single coefficients, and using $X_0(x)\equiv 1$, $X_n(x)=\cos(n\pi x/L)$, we write
\[
u(x,t) = a_0 + \sum_{n=1}^\infty a_n e^{-k(n\pi/L)^2 t}\cos\left(\frac{n\pi x}{L}\right),
\]
for some constants $a_0,a_1,a_2,\dots$ to be determined from the initial condition.

\medskip

\emph{Step 4: Matching the initial condition and cosine coefficients.}  
At $t=0$, we must have
\[
u(x,0)=f(x)=a_0 + \sum_{n=1}^\infty a_n \cos\left(\frac{n\pi x}{L}\right).
\]
Thus $f$ is expanded as a cosine series on $[0,L]$. The orthogonality relations for the cosine functions are
\[
\int_0^L \cos\left(\frac{m\pi x}{L}\right)\cos\left(\frac{n\pi x}{L}\right)\,dx
=
\begin{cases}
L, & m=n=0,\\[4pt]
\frac{L}{2}, & m=n\ge 1,\\[4pt]
0, & m\neq n.
\end{cases}
\]

Multiplying the expansion of $f$ by $1$ and integrating from $0$ to $L$ gives the $n=0$ coefficient:
\[
\int_0^L f(x)\,dx
= \int_0^L a_0\,dx + \sum_{n=1}^\infty a_n \int_0^L \cos\left(\frac{n\pi x}{L}\right)\,dx
= a_0 L,
\]
since the integrals of the cosine terms vanish for $n\ge 1$. Thus
\[
a_0 = \frac{1}{L}\int_0^L f(x)\,dx.
\]

For $n\ge 1$, multiply the expansion by $\cos(n\pi x/L)$ and integrate:
\[
\int_0^L f(x)\cos\left(\frac{n\pi x}{L}\right)\,dx
=
a_0 \int_0^L \cos\left(\frac{n\pi x}{L}\right)\,dx
+ \sum_{m=1}^\infty a_m \int_0^L \cos\left(\frac{m\pi x}{L}\right)\cos\left(\frac{n\pi x}{L}\right)\,dx.
\]
The first integral on the right is zero, and the orthogonality relation yields
\[
\int_0^L f(x)\cos\left(\frac{n\pi x}{L}\right)\,dx
= a_n \cdot \frac{L}{2}.
\]
Hence
\[
a_n = \frac{2}{L}\int_0^L f(x)\cos\left(\frac{n\pi x}{L}\right)\,dx,\qquad n=1,2,3,\dots.
\]

Therefore the solution is
\[
u(x,t) = a_0 + \sum_{n=1}^\infty a_n e^{-k(n\pi/L)^2 t}\cos\left(\frac{n\pi x}{L}\right),
\]
where
\[
a_0 = \frac{1}{L}\int_0^L f(x)\,dx,\qquad
a_n = \frac{2}{L}\int_0^L f(x)\cos\left(\frac{n\pi x}{L}\right)\,dx,\quad n\ge 1.
\]

This answers part (a).

\medskip

\emph{Step 5: Long-time behavior and the steady state.}  
To analyze $\displaystyle \lim_{t\to\infty} u(x,t)$, we note that for each $n\ge 1$,
\[
e^{-k(n\pi/L)^2 t}\to 0\quad\text{as }t\to\infty.
\]
Hence all nonconstant modes decay to zero, and we are left with the constant mode:
\[
\lim_{t\to\infty} u(x,t) = a_0 = \frac{1}{L}\int_0^L f(x)\,dx.
\]
Thus the solution approaches a steady state that is spatially constant and equal to the initial average temperature. This answers part (b).

This behavior is consistent with the physical interpretation: the rod is thermally insulated at both ends, so no heat is gained or lost. The system redistributes the initial heat until it reaches a uniform temperature equal to the conserved average.

\medskip

\emph{Step 6: Conceptual remarks (role of Neumann conditions and Fourier method).}  
This example illustrates several key ideas of the Fourier method for boundary value problems:

\begin{itemize}
  \item The boundary conditions determine the appropriate family of spatial eigenfunctions. With Dirichlet (fixed temperature) conditions we obtain sine series, while with Neumann (insulated) conditions we obtain cosine series, including a constant mode.
  \item The spatial part of the PDE leads to a Sturm–Liouville eigenvalue problem. Here the problem $X''+\lambda X=0$ with $X'(0)=X'(L)=0$ yields a discrete set of eigenvalues and an orthogonal basis of eigenfunctions $\{1,\cos(n\pi x/L)\}$.
  \item The time dependence separates into exponentially decaying modes $e^{-k\lambda_n t}$, which encode diffusion. The zero eigenvalue $\lambda_0=0$ corresponds to a nondecaying steady-state mode, reflecting the conservation of total heat under Neumann boundary conditions.
  \item The initial condition is expanded in the eigenfunction basis, and the orthogonality of the eigenfunctions provides explicit formulas for the Fourier coefficients in terms of $f$.
\end{itemize}

Together, these features exemplify how the Fourier method turns a PDE with boundary conditions into an infinite system of decoupled ordinary differential equations, which can be solved explicitly and recombined into a series solution tailored to the geometry and boundary behavior of the problem.
\end{solution}

% ===== Example 3: Vibrating String with Fixed Ends (Wave Equation) (inquiry-based) =====
\begin{problem}[Vibrating String with Fixed Ends (Wave Equation)]
A thin, taut string of length $L$ is stretched between two rigid supports at $x=0$ and $x=L$. Let $u(x,t)$ denote the small transverse displacement of the string from its equilibrium position at point $x$ and time $t$. Under suitable physical assumptions (small vibrations, uniform density and tension), $u$ satisfies the one-dimensional wave equation. In this problem you will rediscover, step by step, how to solve the wave equation with fixed ends using separation of variables and Fourier series.

Assume that $u$ satisfies the wave equation
\[
u_{tt}(x,t) \;=\; c^2\,u_{xx}(x,t), \qquad 0<x<L,\; t>0,
\]
where $c>0$ is the wave speed, together with fixed-end boundary conditions
\[
u(0,t)=0,\qquad u(L,t)=0,\qquad t>0,
\]
and initial conditions
\[
u(x,0)=f(x),\qquad u_t(x,0)=g(x),\qquad 0<x<L,
\]
for some given functions $f$ and $g$.

\smallskip

(a) \textbf{Understanding the model.}  
Explain in words what each of the conditions above means physically:
\begin{itemize}
  \item the partial differential equation $u_{tt}=c^2 u_{xx}$,
  \item the boundary conditions $u(0,t)=u(L,t)=0$,
  \item the initial conditions $u(x,0)=f(x)$ and $u_t(x,0)=g(x)$.
\end{itemize}
What are some typical shapes for $f$ and $g$ that might arise from plucking or striking the string?

\medskip

(b) \textbf{Trying separation of variables.}  
We look for special solutions of the form
\[
u(x,t)=X(x)\,T(t),
\]
where $X$ depends only on $x$ and $T$ depends only on $t$.

\begin{enumerate}
  \item Substitute $u(x,t)=X(x)T(t)$ into the wave equation and show that
  \[
  \frac{T''(t)}{c^2\,T(t)} \;=\; \frac{X''(x)}{X(x)}.
  \]
  Explain why the left-hand side depends only on $t$ and the right-hand side only on $x$, and conclude that both sides must be equal to a constant, which we denote by $-\lambda$.
  
  \item Show that the boundary conditions imply
  \[
  X(0)=0,\qquad X(L)=0.
  \]
  (Why do the boundary conditions not involve $T$ at all?)
\end{enumerate}

What differential equations do $X$ and $T$ satisfy in terms of the separation constant $\lambda$?

\textit{Hint:} Carefully separate the variables and keep track of signs. Write $X''/X=-\lambda$ and $T''/(c^2 T)=-\lambda$.

\medskip

(c) \textbf{The spatial boundary value problem and eigenvalues.}  
The function $X$ must solve
\[
X''(x)+\lambda\,X(x)=0,\qquad X(0)=0,\quad X(L)=0.
\]
This is an eigenvalue problem. Its solutions depend qualitatively on the sign of $\lambda$.

\begin{enumerate}
  \item Analyze separately the three cases $\lambda<0$, $\lambda=0$, and $\lambda>0$. In each case, solve the ordinary differential equation for $X$ and impose the boundary conditions to determine when a nontrivial solution exists.
  
  \item Show that the only nontrivial solutions occur when
  \[
  \lambda_n = \left(\frac{n\pi}{L}\right)^2,\qquad n=1,2,3,\dots
  \]
  and that, up to a constant factor, the corresponding eigenfunctions are
  \[
  X_n(x) = \sin\!\left(\frac{n\pi x}{L}\right).
  \]
\end{enumerate}

\textit{Hint:} For $\lambda>0$, write $\lambda=\mu^2$ and solve $X''+\mu^2 X=0$. For $\lambda<0$, write $\lambda=-\mu^2$ and solve $X''-\mu^2 X=0$. Check which constants must vanish to satisfy $X(0)=0$ and $X(L)=0$.

\medskip

(d) \textbf{Putting space and time together; using Fourier series.}  

For each $n\ge 1$, you now have a spatial eigenfunction $X_n(x)$ and a temporal function $T_n(t)$ solving
\[
T_n''(t) + c^2 \lambda_n\, T_n(t) = 0, \qquad \lambda_n = \left(\frac{n\pi}{L}\right)^2.
\]

\begin{enumerate}
  \item Solve this ordinary differential equation for $T_n(t)$ and show that
  \[
  T_n(t) = A_n \cos\!\left(\frac{n\pi c t}{L}\right) + B_n \sin\!\left(\frac{n\pi c t}{L}\right),
  \]
  for some constants $A_n$ and $B_n$ depending on $n$.
  
  \item Argue that the general solution of the boundary value problem (with fixed ends) can be written as a superposition of these separated solutions:
  \[
  u(x,t) = \sum_{n=1}^{\infty} \left[ A_n \cos\!\left(\frac{n\pi c t}{L}\right) + B_n \sin\!\left(\frac{n\pi c t}{L}\right) \right]
  \sin\!\left(\frac{n\pi x}{L}\right).
  \]
  
  \item Use the initial displacement condition $u(x,0)=f(x)$ to show that
  \[
  f(x) = \sum_{n=1}^{\infty} A_n \sin\!\left(\frac{n\pi x}{L}\right),
  \]
  and use the initial velocity condition $u_t(x,0)=g(x)$ to show that
  \[
  g(x) = \sum_{n=1}^{\infty} \frac{n\pi c}{L} B_n \sin\!\left(\frac{n\pi x}{L}\right).
  \]
  Express $A_n$ and $B_n$ in terms of $f$ and $g$ using suitable integrals.
\end{enumerate}

\textit{Hint:} You will need the orthogonality of the sine functions
\[
\int_0^L \sin\!\left(\frac{m\pi x}{L}\right)\sin\!\left(\frac{n\pi x}{L}\right)\,dx
=
\begin{cases}
0, & m\ne n,\\[4pt]
\displaystyle \frac{L}{2}, & m=n.
\end{cases}
\]
To derive formulas for $A_n$ and $B_n$, multiply both sides of the series by $\sin\!\left(\frac{k\pi x}{L}\right)$ and integrate from $0$ to $L$.

\medskip

(e) \textbf{What if / extensions.}

\begin{enumerate}
  \item Suppose the initial velocity is zero, $g(x)\equiv 0$, and the initial displacement is $f(x)=h\,x(L-x)$ for some constant $h$. Write down the corresponding series for $u(x,t)$ as explicitly as you can. Where do the coefficients come from?
  
  \item How would the analysis change if the right end of the string were free instead of fixed, so that $u_x(L,t)=0$ instead of $u(L,t)=0$? What sort of spatial eigenfunctions would you expect in that case (sines, cosines, or something else), and why?
  
  \item (Conceptual) The functions $\sin(n\pi x/L)$ are called the normal modes or standing wave shapes of the string. How does the superposition of many such modes explain the way a plucked string vibrates and produces sound?
\end{enumerate}

\end{problem}

% ===== Example 3: Vibrating String with Fixed Ends (Wave Equation) (full solution) =====
\begin{problem}[Vibrating String with Fixed Ends (Wave Equation)]
Consider the initial–boundary value problem for the one-dimensional wave equation
\[
u_{tt}(x,t) = c^2 u_{xx}(x,t),\qquad 0<x<L,\ t>0,
\]
with fixed-end boundary conditions
\[
u(0,t)=0,\qquad u(L,t)=0,\qquad t>0,
\]
and initial conditions
\[
u(x,0)=f(x),\qquad u_t(x,0)=g(x),\qquad 0<x<L,
\]
where $c>0$ is constant and $f,g$ are given functions. Using separation of variables and Fourier series, find a series representation for the solution $u(x,t)$, and express its coefficients explicitly in terms of $f$ and $g$.
\end{problem}

\begin{solution}
We seek to solve the wave equation with homogeneous Dirichlet boundary conditions by the Fourier method. The central ideas are separation of variables, which leads to a spatial eigenvalue problem, and expansion of the initial data in terms of the corresponding eigenfunctions (a sine Fourier series).

\medskip

\textbf{1. Separation of variables and the eigenvalue problem.}

We look for special solutions of the form
\[
u(x,t) = X(x)\,T(t),
\]
where $X$ depends only on $x$ and $T$ on $t$. Substituting this product into the wave equation yields
\[
X(x) T''(t) = c^2 X''(x) T(t).
\]
Assuming $X$ and $T$ are nonzero, we divide both sides by $c^2 X(x) T(t)$ to obtain
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)}.
\]
The left-hand side depends only on $t$, and the right-hand side only on $x$, so both must be equal to a constant, which we denote by $-\lambda$:
\[
\frac{X''(x)}{X(x)} = \frac{T''(t)}{c^2 T(t)} = -\lambda.
\]

Thus we obtain two ordinary differential equations,
\begin{align*}
X''(x) + \lambda X(x) &= 0,\\
T''(t) + c^2 \lambda T(t) &= 0.
\end{align*}
The boundary conditions $u(0,t)=0$ and $u(L,t)=0$ impose conditions on $X$:
\[
u(0,t)=X(0)T(t)=0,\quad u(L,t)=X(L)T(t)=0\quad\text{for all }t.
\]
We are interested in nontrivial separated solutions for which $T$ is not identically zero, so we must have
\[
X(0)=0,\qquad X(L)=0.
\]
Hence $X$ satisfies the boundary value problem
\[
X''(x) + \lambda X(x) = 0,\qquad X(0)=0,\quad X(L)=0.
\]

This is a standard Sturm–Liouville problem. Its nontrivial solutions exist only for discrete values of $\lambda$, the eigenvalues, and the corresponding functions $X$ are eigenfunctions.

\medskip

\textbf{2. Solving the spatial problem; eigenvalues and eigenfunctions.}

We analyze the spatial problem
\[
X'' + \lambda X = 0, \quad X(0)=0,\ X(L)=0
\]
by considering the sign of $\lambda$.

\emph{Case 1: $\lambda = 0$.}  
The equation becomes $X''=0$, so $X(x)=ax+b$. The boundary condition $X(0)=0$ implies $b=0$, and $X(L)=0$ then implies $aL=0$, hence $a=0$. Thus $X\equiv 0$ is the only solution, and there is no nontrivial eigenfunction for $\lambda=0$.

\emph{Case 2: $\lambda < 0$.}  
Let $\lambda = -\mu^2$ with $\mu>0$. Then the equation is $X'' - \mu^2 X = 0$, whose general solution is
\[
X(x) = A e^{\mu x} + B e^{-\mu x}.
\]
The condition $X(0)=0$ gives $A+B=0$, so $B=-A$ and
\[
X(x) = A\bigl(e^{\mu x}-e^{-\mu x}\bigr) = 2A \sinh(\mu x).
\]
Then $X(L)=0$ gives $\sinh(\mu L)=0$. Since $\sinh(\mu L)\neq 0$ for $\mu>0$, we must have $A=0$, and again $X\equiv 0$ is the only solution. Thus there are no negative eigenvalues.

\emph{Case 3: $\lambda > 0$.}  
Let $\lambda = \mu^2$ with $\mu>0$. Then the equation is $X'' + \mu^2 X = 0$, whose general solution is
\[
X(x) = A\cos(\mu x) + B\sin(\mu x).
\]
The condition $X(0)=0$ implies $A=0$, so
\[
X(x) = B \sin(\mu x).
\]
The second boundary condition $X(L)=0$ demands $\sin(\mu L)=0$, so $\mu L = n\pi$ for some integer $n$. The case $n=0$ yields the trivial solution; for nontrivial solutions we require $n\neq 0$. Since replacing $n$ by $-n$ does not change $\sin(n\pi x/L)$ up to sign, we may restrict to positive integers $n=1,2,3,\dots$.

Thus
\[
\mu_n = \frac{n\pi}{L},\qquad \lambda_n = \mu_n^2 = \left(\frac{n\pi}{L}\right)^2,
\]
and, up to multiplicative constants, the eigenfunctions are
\[
X_n(x) = \sin\!\left(\frac{n\pi x}{L}\right),\qquad n=1,2,3,\dots.
\]

These eigenfunctions are orthogonal in $L^2(0,L)$:
\[
\int_0^L \sin\!\left(\frac{m\pi x}{L}\right)\sin\!\left(\frac{n\pi x}{L}\right)\,dx
=
\begin{cases}
0, & m\neq n,\\[4pt]
\displaystyle \frac{L}{2}, & m=n.
\end{cases}
\]

\medskip

\textbf{3. Temporal factors and separated solutions.}

For each eigenvalue $\lambda_n$, the temporal equation
\[
T_n''(t) + c^2 \lambda_n T_n(t) = 0
\]
becomes
\[
T_n''(t) + c^2 \left(\frac{n\pi}{L}\right)^2 T_n(t) = 0.
\]
This is a harmonic oscillator equation with frequency $\omega_n = \dfrac{n\pi c}{L}$. Its general solution is
\[
T_n(t) = A_n \cos\!\left(\frac{n\pi c t}{L}\right)
        + B_n \sin\!\left(\frac{n\pi c t}{L}\right),
\]
where $A_n$ and $B_n$ are constants to be determined from the initial data.

For each $n$, the product
\[
u_n(x,t) = T_n(t)\,X_n(x)
\]
yields a separated solution that satisfies both the wave equation and the boundary conditions.

\medskip

\textbf{4. Superposition and the general series solution.}

Because the wave equation is linear and homogeneous, any (possibly infinite) linear combination of solutions is again a solution. Therefore, we may form
\[
u(x,t) = \sum_{n=1}^\infty \left[
A_n \cos\!\left(\frac{n\pi c t}{L}\right)
+ B_n \sin\!\left(\frac{n\pi c t}{L}\right)
\right] \sin\!\left(\frac{n\pi x}{L}\right).
\]
Each term in the sum satisfies the boundary conditions, so the whole sum does as well (formally, and under suitable convergence assumptions).

The remaining task is to choose the coefficients $A_n$ and $B_n$ so that the initial conditions are satisfied.

\medskip

\textbf{5. Imposing the initial conditions via Fourier sine series.}

First, impose the initial displacement:
\[
u(x,0) = f(x).
\]
At $t=0$, the sine terms in time vanish and the cosine terms equal $1$, so
\[
u(x,0) = \sum_{n=1}^\infty A_n \sin\!\left(\frac{n\pi x}{L}\right) = f(x).
\]
Thus $f$ is represented by its sine Fourier series on $(0,L)$ with coefficients $A_n$.

Next, impose the initial velocity:
\[
u_t(x,0) = g(x).
\]
We differentiate the series term by term with respect to $t$:
\[
u_t(x,t) = \sum_{n=1}^\infty \left[
-A_n \frac{n\pi c}{L}\sin\!\left(\frac{n\pi c t}{L}\right)
+ B_n  \frac{n\pi c}{L}\cos\!\left(\frac{n\pi c t}{L}\right)
\right] \sin\!\left(\frac{n\pi x}{L}\right).
\]
Evaluating at $t=0$, the sine terms vanish and the cosines become $1$, giving
\[
u_t(x,0) = \sum_{n=1}^\infty \frac{n\pi c}{L} B_n \sin\!\left(\frac{n\pi x}{L}\right)
= g(x).
\]
Hence $g$ also has a sine series representation, with coefficients proportional to $B_n$.

To find $A_n$ and $B_n$ explicitly, we use the orthogonality of the sine functions. For a series
\[
f(x) \sim \sum_{n=1}^\infty a_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
the standard formula for the Fourier sine coefficients on $(0,L)$ is
\[
a_n = \frac{2}{L} \int_0^L f(x)\,\sin\!\left(\frac{n\pi x}{L}\right)\,dx.
\]
Comparing with
\[
f(x) = \sum_{n=1}^\infty A_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
we obtain
\[
A_n = \frac{2}{L} \int_0^L f(x)\,\sin\!\left(\frac{n\pi x}{L}\right)\,dx,\qquad n=1,2,3,\dots.
\]

Similarly, for
\[
g(x) = \sum_{n=1}^\infty \frac{n\pi c}{L} B_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
we may view $\dfrac{n\pi c}{L} B_n$ as the sine coefficient of $g$. Thus
\[
\frac{n\pi c}{L} B_n = \frac{2}{L} \int_0^L g(x)\,\sin\!\left(\frac{n\pi x}{L}\right)\,dx,
\]
which yields
\[
B_n = \frac{2}{n\pi c} \int_0^L g(x)\,\sin\!\left(\frac{n\pi x}{L}\right)\,dx,\qquad n=1,2,3,\dots.
\]

\medskip

\textbf{6. Final series solution and interpretation.}

Collecting all the pieces, the solution of the initial–boundary value problem is given formally by
\[
u(x,t) =
\sum_{n=1}^\infty
\left[
\left(\frac{2}{L} \int_0^L f(\xi)\,\sin\!\left(\frac{n\pi \xi}{L}\right)\,d\xi\right)
\cos\!\left(\frac{n\pi c t}{L}\right)
+
\left(\frac{2}{n\pi c} \int_0^L g(\xi)\,\sin\!\left(\frac{n\pi \xi}{L}\right)\,d\xi\right)
\sin\!\left(\frac{n\pi c t}{L}\right)
\right]
\sin\!\left(\frac{n\pi x}{L}\right).
\]

Each term
\[
\sin\!\left(\frac{n\pi x}{L}\right)\cos\!\left(\frac{n\pi c t}{L}\right)
\quad\text{and}\quad
\sin\!\left(\frac{n\pi x}{L}\right)\sin\!\left(\frac{n\pi c t}{L}\right)
\]
represents a standing wave mode of the string with spatial shape $\sin(n\pi x/L)$ and temporal oscillation at frequency $\omega_n = n\pi c / L$. The coefficients, determined by the Fourier sine series of the initial displacement $f$ and initial velocity $g$, specify how much of each normal mode is excited initially.

\medskip

\textbf{Relation to the Fourier method.}

This example illustrates the main ideas of the Fourier method for boundary value problems: separation of variables reduces the partial differential equation to a spatial eigenvalue problem and a temporal ODE; the boundary conditions lead to a discrete set of eigenfunctions (here, sine functions) that form an orthogonal basis; and the initial data are expanded in this basis using Fourier series. The resulting solution is a superposition of eigenmodes that evolves in time with frequencies determined by the eigenvalues.
\end{solution}

% ===== Example 4: Steady-State Temperature in a Rectangular Plate (Laplace’s Equation) (inquiry-based) =====
\begin{problem}[Steady-State Temperature in a Rectangular Plate (Laplace’s Equation)]
Consider a thin rectangular metal plate occupying the region
\[
0 < x < L, \qquad 0 < y < a,
\]
with negligible thickness in the $z$-direction. The plate is thermally insulated on its faces, so heat flows only in the $x$–$y$ plane, and we wait long enough for the temperature to reach a steady state. Along three edges of the plate we hold the temperature at zero, while along the top edge $y = a$ we prescribe a nonzero temperature profile. In this regime, the temperature $u(x,y)$ no longer depends on time and satisfies Laplace’s equation.

Suppose that
\[
u(x,0) = 0, \quad u(0,y) = 0, \quad u(L,y) = 0, \qquad 0< x < L,\; 0<y<a,
\]
while on the top edge we prescribe
\[
u(x,a) = f(x), \qquad 0<x<L,
\]
for some given function $f$.

\medskip

(a) Write down the partial differential equation and boundary conditions that describe the steady-state temperature $u(x,y)$ in the interior and on the boundary of the plate. Briefly explain (in words) why Laplace's equation is the correct steady-state model here.

\medskip

(b) A common way to approach such boundary value problems on rectangles is to use separation of variables. Assume a separated form
\[
u(x,y) = X(x)\,Y(y).
\]
Insert this ansatz into Laplace’s equation and derive the resulting pair of ordinary differential equations for $X$ and $Y$. Show that you obtain an equation of the form
\[
\frac{X''(x)}{X(x)} = -\frac{Y''(y)}{Y(y)} = -\lambda
\]
for some constant $\lambda$, called the separation constant.

\emph{Hint:} Carefully divide by $X(x)Y(y)$ at a point where both are nonzero, and argue why the resulting expression must be independent of both $x$ and $y$.

\medskip

(c) Now incorporate the boundary conditions at $x = 0$ and $x = L$ into the equation for $X(x)$.

\begin{itemize}
    \item[(i)] Use the fact that $u(0,y)=u(L,y)=0$ for all $0<y<a$ to deduce boundary conditions for $X(x)$. 
    \item[(ii)] Solve the eigenvalue problem
    \[
    X''(x) + \lambda X(x) = 0, \qquad X(0) = 0,\; X(L) = 0,
    \]
    and determine all admissible values of $\lambda$ and the corresponding eigenfunctions $X(x)$.
\end{itemize}

\emph{Hint:} Consider three cases: $\lambda = 0$, $\lambda < 0$, and $\lambda > 0$, and see which of them allow nontrivial (nonzero) solutions that satisfy both boundary conditions.

\medskip

(d) For each admissible $\lambda$, solve the corresponding ordinary differential equation for $Y(y)$:
\[
Y''(y) - \lambda Y(y) = 0.
\]
Use the boundary condition $u(x,0)=0$ to determine which constants in the general solution for $Y$ must vanish.

\begin{itemize}
    \item[(i)] Show that for the eigenvalues $\lambda_n$ you found in part (c), you obtain a family of separated solutions $u_n(x,y)$.
    \item[(ii)] Use the principle of superposition (linearity of Laplace's equation) to argue that a general solution satisfying the three homogeneous boundary conditions $u(0,y)=u(L,y)=u(x,0)=0$ can be written as a series
    \[
    u(x,y) = \sum_{n=1}^\infty A_n \,\phi_n(x)\,\psi_n(y),
    \]
    for suitable functions $\phi_n$ and $\psi_n$ that you should identify explicitly.
\end{itemize}

\emph{Hint:} Your $\phi_n$ should be the eigenfunctions in $x$, and your $\psi_n$ should be the corresponding solutions in $y$.

\medskip

(e) To incorporate the nonzero boundary condition $u(x,a)=f(x)$, evaluate your series from part (d) at $y=a$:
\[
u(x,a) = \sum_{n=1}^\infty A_n \,\phi_n(x)\,\psi_n(a).
\]
\begin{itemize}
    \item[(i)] Explain how this leads to representing $f(x)$ as a Fourier series in the eigenfunctions $\phi_n(x)$.
    \item[(ii)] Use orthogonality of the $\phi_n$ to derive a formula for the coefficients $A_n$ in terms of $f$.
    \item[(iii)] Write down the final series expression for $u(x,y)$ in terms of $f(x)$ and the eigenfunctions.
\end{itemize}

\emph{Hint:} You should end up with $f(x)$ expressed as a Fourier sine series on $(0,L)$, whose coefficients determine the $A_n$.

\medskip

(f) \textbf{Extensions and “what if” questions.}
\begin{itemize}
    \item[(i)] What changes in your solution method if the bottom edge has a nonzero temperature profile, say $u(x,0)=g(x)$ instead of zero? How might you reduce this to a problem with homogeneous (zero) boundary conditions?
    \item[(ii)] Suppose instead that the vertical edges at $x=0$ and $x=L$ are insulated, so that there is no heat flux through them, mathematically $u_x(0,y)=u_x(L,y)=0$. How would this modify the eigenvalue problem for $X(x)$, and what functions would you expect to appear instead of sines?
\end{itemize}

\end{problem}

% ===== Example 4: Steady-State Temperature in a Rectangular Plate (Laplace’s Equation) (full solution) =====
\begin{problem}[Steady-State Temperature in a Rectangular Plate (Laplace’s Equation)]
Let $u(x,y)$ denote the steady-state temperature in a thin rectangular plate
\[
0 < x < L,\qquad 0 < y < a.
\]
Assume $u$ satisfies Laplace’s equation
\[
u_{xx} + u_{yy} = 0
\]
in the interior of the rectangle and the boundary conditions
\[
u(x,0) = 0,\quad u(0,y) = 0,\quad u(L,y) = 0,\qquad 0 < x < L,\; 0 < y < a,
\]
and
\[
u(x,a) = f(x),\qquad 0 < x < L,
\]
for a given function $f$.

Use the Fourier method (separation of variables and Fourier series) to find a series expression for the steady-state solution $u(x,y)$ in terms of $f(x)$.
\end{problem}

\begin{solution}
In the steady state there is no time dependence, so conservation of heat combined with Fourier’s law yields Laplace’s equation
\[
u_{xx} + u_{yy} = 0
\]
for the temperature $u(x,y)$ inside the plate. The boundary conditions fix the temperature along all four sides, with three sides held at zero and one side prescribed by $f(x)$. This is a prototypical Dirichlet boundary value problem on a rectangle, well suited for the Fourier method.

\medskip

\textbf{1. Separation of variables and the eigenvalue problem.}

We seek separated solutions of the form
\[
u(x,y) = X(x)\,Y(y),
\]
where $X$ depends only on $x$ and $Y$ only on $y$. Substituting into Laplace’s equation gives
\[
X''(x)Y(y) + X(x)Y''(y) = 0.
\]
Assuming $X$ and $Y$ are not identically zero, we can divide by $X(x)Y(y)$ at points where both are nonzero:
\[
\frac{X''(x)}{X(x)} + \frac{Y''(y)}{Y(y)} = 0.
\]
The first term depends only on $x$, the second only on $y$, yet their sum is identically zero for all $(x,y)$ in the rectangle. Therefore, each term must be constant, say
\[
\frac{X''(x)}{X(x)} = -\lambda, \qquad \frac{Y''(y)}{Y(y)} = \lambda,
\]
for some constant $\lambda$ called the separation constant. Thus $X$ and $Y$ satisfy the ordinary differential equations
\[
X''(x) + \lambda X(x) = 0, \qquad Y''(y) - \lambda Y(y) = 0.
\]

We next impose the boundary conditions. From $u(0,y)=0$ and $u(L,y)=0$ for $0<y<a$, we obtain
\[
X(0)Y(y) = 0,\qquad X(L)Y(y) = 0 \quad \text{for all } 0<y<a.
\]
We are interested in nontrivial separated solutions, so $Y$ is not identically zero; therefore we must have
\[
X(0) = 0,\qquad X(L) = 0.
\]
The condition $u(x,0)=0$ yields
\[
X(x)Y(0) = 0 \quad \text{for } 0<x<L,
\]
and since $X$ is not identically zero, we deduce
\[
Y(0) = 0.
\]

Thus we must solve the eigenvalue problem
\[
X''(x) + \lambda X(x) = 0,\qquad X(0)=0,\; X(L)=0,
\]
and then, for each admissible $\lambda$, solve
\[
Y''(y) - \lambda Y(y) = 0,\qquad Y(0)=0.
\]

\medskip

\textbf{2. Solving the $X$-equation and determining eigenvalues.}

We consider the three standard cases for $\lambda$.

\emph{Case 1: $\lambda = 0$.} Then
\[
X''(x) = 0 \;\Rightarrow\; X(x) = A + Bx.
\]
Applying the boundary conditions $X(0)=0$ and $X(L)=0$ yields $A=0$ and $B L = 0$, so $B=0$ as well. Therefore $X$ is identically zero, and this case gives only the trivial solution, which we discard.

\emph{Case 2: $\lambda < 0$.} Write $\lambda = -\mu^2$ with $\mu>0$, then
\[
X''(x) - \mu^2 X(x) = 0,
\]
whose general solution is
\[
X(x) = A e^{\mu x} + B e^{-\mu x}.
\]
Imposing $X(0)=0$ gives $A + B = 0$, so $B = -A$ and
\[
X(x) = A(e^{\mu x} - e^{-\mu x}) = 2A \sinh(\mu x).
\]
Then $X(L)=0$ implies $2A \sinh(\mu L) = 0$. Since $\sinh(\mu L) \neq 0$ for $\mu>0$, we must have $A=0$. Thus again only the trivial solution arises, so $\lambda<0$ is not admissible.

\emph{Case 3: $\lambda > 0$.} Write $\lambda = \left(\frac{n\pi}{L}\right)^2$ for some $n>0$ (this form is anticipated; any positive $\lambda$ can be written as $\kappa^2$, and the boundary conditions will quantize $\kappa$). Then the equation
\[
X''(x) + \lambda X(x) = 0
\]
has general solution
\[
X(x) = A \cos\!\Bigl(\tfrac{n\pi}{L} x\Bigr) + B \sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr).
\]
Applying $X(0) = 0$ gives
\[
X(0) = A = 0,
\]
so
\[
X(x) = B \sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr).
\]
Then $X(L)=0$ becomes
\[
X(L) = B \sin(n\pi) = 0.
\]
This holds for any $B$ provided $\sin(n\pi) = 0$, that is, for every positive integer $n \in \mathbb{N}$. Hence the admissible eigenvalues and eigenfunctions are
\[
\lambda_n = \left(\frac{n\pi}{L}\right)^2, \qquad X_n(x) = \sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr), \quad n=1,2,3,\dots,
\]
where we absorb any multiplicative constant into later coefficients.

Thus the spatial dependence in the $x$-direction is a Fourier sine basis, reflecting the homogeneous Dirichlet conditions at $x=0$ and $x=L$.

\medskip

\textbf{3. Solving the $Y$-equation for each eigenvalue.}

For each $n$, we now solve
\[
Y_n''(y) - \lambda_n Y_n(y) = 0, \qquad Y_n(0) = 0,
\]
with $\lambda_n = (n\pi/L)^2$. The general solution of this linear ODE is
\[
Y_n(y) = C_n e^{\sqrt{\lambda_n}\,y} + D_n e^{-\sqrt{\lambda_n}\,y}
       = C_n e^{\frac{n\pi}{L} y} + D_n e^{-\frac{n\pi}{L} y}.
\]
The condition $Y_n(0) = 0$ gives
\[
Y_n(0) = C_n + D_n = 0 \;\Rightarrow\; D_n = -C_n.
\]
Hence
\[
Y_n(y) = C_n\left(e^{\frac{n\pi}{L} y} - e^{-\frac{n\pi}{L} y}\right)
       = 2C_n \sinh\!\Bigl(\tfrac{n\pi}{L} y\Bigr).
\]
We can incorporate the constant $2C_n$ into the overall coefficient later, so we may take
\[
Y_n(y) = \sinh\!\Bigl(\tfrac{n\pi}{L} y\Bigr).
\]

Thus a separated solution for each $n$ is
\[
u_n(x,y) = X_n(x)Y_n(y) = \sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr)\,\sinh\!\Bigl(\tfrac{n\pi}{L} y\Bigr).
\]

\medskip

\textbf{4. Superposition and satisfying the homogeneous boundary conditions.}

Laplace’s equation is linear and homogeneous, so any finite linear combination, and hence any (sufficiently convergent) infinite sum of solutions, is again a solution. Therefore, the general solution that satisfies the three homogeneous boundary conditions
\[
u(0,y) = u(L,y) = u(x,0) = 0
\]
is given by the series
\[
u(x,y) = \sum_{n=1}^{\infty} A_n\,\sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr)\,\sinh\!\Bigl(\tfrac{n\pi}{L} y\Bigr),
\]
for some coefficients $A_n$ yet to be determined.

Indeed, this form ensures:
\begin{itemize}
    \item $u(0,y)=0$ and $u(L,y)=0$ because $\sin(\tfrac{n\pi}{L} x)$ vanishes at $x=0$ and $x=L$;
    \item $u(x,0)=0$ because $\sinh(\tfrac{n\pi}{L} y)$ vanishes at $y=0$.
\end{itemize}

\medskip

\textbf{5. Imposing the nonhomogeneous boundary condition at $y=a$.}

We now use the remaining boundary condition on the top edge:
\[
u(x,a) = f(x),\qquad 0<x<L.
\]
Evaluating the series at $y=a$ gives
\[
u(x,a) = \sum_{n=1}^{\infty} A_n\,\sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr)\,\sinh\!\Bigl(\tfrac{n\pi}{L} a\Bigr).
\]
Thus
\[
f(x) = \sum_{n=1}^{\infty} \Bigl(A_n \sinh\!\Bigl(\tfrac{n\pi}{L} a\Bigr)\Bigr)\, \sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr).
\]
This expresses $f(x)$ as a Fourier sine series on the interval $(0,L)$, with sine basis functions
\[
\sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr),\quad n=1,2,\dots.
\]

The standard Fourier sine expansion of a function $f$ on $(0,L)$ (assuming $f$ is sufficiently regular) is
\[
f(x) = \sum_{n=1}^\infty b_n \sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr),
\]
where the coefficients are given by the orthogonality of sines:
\[
b_n = \frac{2}{L}\int_0^L f(x)\,\sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr)\,dx.
\]
Comparing with our expression, we identify
\[
b_n = A_n \sinh\!\Bigl(\tfrac{n\pi}{L} a\Bigr).
\]
Therefore
\[
A_n = \frac{b_n}{\sinh\!\bigl(\tfrac{n\pi}{L} a\bigr)}
    = \frac{2}{L\,\sinh\!\bigl(\tfrac{n\pi}{L} a\bigr)} \int_0^L f(x)\,\sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr)\,dx.
\]

\medskip

\textbf{6. Final solution formula.}

Substituting these coefficients back into our series for $u(x,y)$, we obtain the steady-state temperature distribution:
\[
u(x,y)
= \sum_{n=1}^{\infty} 
\left[
\frac{2}{L\,\sinh\!\bigl(\tfrac{n\pi}{L} a\bigr)} 
\int_0^L f(\xi)\,\sin\!\Bigl(\tfrac{n\pi}{L} \xi\Bigr)\,d\xi
\right]
\sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr)\,\sinh\!\Bigl(\tfrac{n\pi}{L} y\Bigr).
\]
It is often convenient to write this as
\[
u(x,y)
= \sum_{n=1}^{\infty} 
\left(\frac{2}{L} \int_0^L f(\xi)\,\sin\!\Bigl(\tfrac{n\pi}{L} \xi\Bigr)\,d\xi\right)
\frac{\sinh\!\bigl(\tfrac{n\pi}{L} y\bigr)}{\sinh\!\bigl(\tfrac{n\pi}{L} a\bigr)}
\,\sin\!\Bigl(\tfrac{n\pi}{L} x\Bigr).
\]

This series solves Laplace’s equation in the rectangle, satisfies $u(x,0)=u(0,y)=u(L,y)=0$, and matches the prescribed boundary data $u(x,a)=f(x)$.

\medskip

\textbf{7. Conceptual remarks and relation to the Fourier method.}

This example illustrates the key ideas of the Fourier method for boundary value problems:
\begin{itemize}
    \item \emph{Separation of variables} reduces a partial differential equation with two independent variables to a pair of ordinary differential equations, linked by a separation constant $\lambda$.
    \item The boundary conditions in one variable (here, in $x$) lead to an \emph{eigenvalue problem} for the corresponding ordinary differential equation. Only discrete values of $\lambda$ (the eigenvalues) admit nontrivial solutions satisfying the boundary conditions, and the corresponding eigenfunctions (here, sine functions) form an orthogonal basis.
    \item The solution in the other variable (here, $y$) is then determined for each eigenvalue, giving a family of separated solutions that can be superposed.
    \item The remaining nonhomogeneous boundary condition is enforced by expanding the prescribed boundary function (here, $f(x)$) into a \emph{Fourier series} in the eigenfunctions. Orthogonality of the eigenfunctions allows us to compute the coefficients explicitly.
\end{itemize}
In this way, the Fourier method turns the PDE boundary value problem into the problem of finding a Fourier series representation of the boundary data, yielding a solution as a convergent series of separated modes.

\end{solution}

% ===== Example 5: Heat Equation with Mixed (Robin) Boundary Conditions (inquiry-based) =====
\begin{problem}[Heat Equation with Mixed (Robin) Boundary Conditions]
A thin, homogeneous rod of length $L$ lies along the $x$-axis for $0<x<L$. Its lateral surface is perfectly insulated, but each end is exposed to air at a fixed ambient temperature $T_{\mathrm{env}}$. According to Newton’s law of cooling, the heat flux at each end is proportional to the temperature difference between the rod and the surrounding air. Let $u(x,t)$ denote the temperature in the rod at position $x$ and time $t$. We prescribe an initial temperature distribution $u(x,0)=f(x)$ and wish to understand how the temperature evolves.

The governing equation is the one-dimensional heat equation
\[
u_t = \kappa\, u_{xx},\qquad 0<x<L,\ t>0,
\]
where $\kappa>0$ is the thermal diffusivity. At the ends we impose Newton cooling:
\begin{align*}
- \kappa\, u_x(0,t) &= h\bigl(u(0,t) - T_{\mathrm{env}}\bigr),\\
\kappa\, u_x(L,t) &= h\bigl(u(L,t) - T_{\mathrm{env}}\bigr),
\end{align*}
where $h>0$ is the heat transfer coefficient. The initial condition is
\[
u(x,0) = f(x),\qquad 0<x<L.
\]

\medskip

(a) The boundary conditions are inhomogeneous because of the presence of $T_{\mathrm{env}}$. A standard trick is to measure temperature relative to the environment. Define
\[
v(x,t) = u(x,t) - T_{\mathrm{env}}.
\]
Write down the partial differential equation, boundary conditions, and initial condition satisfied by $v(x,t)$. Are the new boundary conditions homogeneous (that is, equal to zero)? Why is this useful for the method of separation of variables?

\medskip

(b) From now on, work with $v(x,t)$ and \emph{assume} that $v$ satisfies
\[
v_t = \kappa v_{xx},\qquad 0<x<L,\ t>0,
\]
with homogeneous Robin boundary conditions
\[
v_x(0,t) = -\alpha\, v(0,t),\qquad v_x(L,t) = \alpha\, v(L,t),
\]
where $\alpha = h/\kappa>0$, and initial condition $v(x,0)=g(x)$, where $g(x)=f(x)-T_{\mathrm{env}}$.

Use separation of variables $v(x,t)=X(x)T(t)$ to derive the ordinary differential equations for $X$ and $T$, together with the boundary conditions for $X$. Introduce a separation constant $-\lambda$ in the usual way. What eigenvalue problem does $X$ satisfy?

\emph{Hint:} After dividing by $\kappa X T$, you should find $T'/(\kappa T) = X''/X = -\lambda$, with an appropriate sign convention for $\lambda$ so that decay in time is possible.

\medskip

(c) In this part, analyze the spatial eigenvalue problem
\[
X''(x) + \lambda X(x) = 0,\qquad 0<x<L,
\]
with mixed (Robin) boundary conditions
\[
X'(0) = -\alpha X(0),\qquad X'(L) = \alpha X(L),
\]
where $\alpha>0$ is fixed.

\begin{enumerate}
\item[(i)] Show that $\lambda\le 0$ cannot give any nontrivial eigenfunctions. In particular, explain why $\lambda=0$ and $\lambda<0$ lead only to the trivial solution $X\equiv 0$.
\item[(ii)] Conclude that we must have $\lambda>0$, and write $\lambda=\mu^2$ with $\mu>0$. Show that any solution of $X''+\mu^2 X=0$ has the form
\[
X(x) = A\cos(\mu x) + B\sin(\mu x).
\]
\item[(iii)] Impose the two Robin boundary conditions to obtain a homogeneous linear system for $A$ and $B$. By eliminating $A$ and $B$, derive a transcendental equation for $\mu$ of the form
\[
F(\mu) = 0,
\]
where you should give $F(\mu)$ explicitly in terms of $\mu$, $\alpha$, and $L$.

\emph{Hint:} First use the condition at $x=0$ to express $B$ in terms of $A$; then substitute into the condition at $x=L$.
\item[(iv)] Argue (without fully solving $F(\mu)=0$) that there is an infinite sequence of positive roots
\[
0<\mu_1<\mu_2<\cdots<\mu_n<\cdots,\qquad \mu_n\to\infty,
\]
and that the corresponding eigenfunctions $X_n(x)$ form an orthogonal family in $L^2(0,L)$.
\end{enumerate}

\emph{Hint:} For orthogonality, recall the general Sturm–Liouville theory, or verify directly that eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to the usual inner product $\int_0^L X_m X_n\,dx$.

\medskip

(d) For each eigenvalue $\lambda_n=\mu_n^2$ from part (c), solve the time equation for $T_n(t)$ and write down the corresponding separated solution $v_n(x,t)$. Then form the general solution as a series
\[
v(x,t) = \sum_{n=1}^\infty c_n\, v_n(x,t).
\]

\begin{enumerate}
\item[(i)] What is the explicit formula for $T_n(t)$?
\item[(ii)] How do you determine the coefficients $c_n$ from the initial condition $v(x,0)=g(x)$?
\item[(iii)] Express $u(x,t)$ in terms of this series and the ambient temperature $T_{\mathrm{env}}$.
\end{enumerate}

\emph{Hint:} You should obtain a generalized Fourier series expansion of $g$ in terms of the eigenfunctions $\{X_n\}$.

\medskip

(e) Explorations and extensions.

\begin{enumerate}
\item[(i)] Consider the limiting cases $\alpha\to 0$ and $\alpha\to\infty$. What boundary conditions do you expect to recover at the ends of the rod? How might the eigenfunctions and eigenvalues simplify in these limits?
\item[(ii)] How would the analysis change if the ambient temperatures at the two ends were different, say $T_{\mathrm{env},0}$ at $x=0$ and $T_{\mathrm{env},L}$ at $x=L$? Sketch (in words or formulas) how you might first find a steady-state solution and then study the transient behavior.
\end{enumerate}

\end{problem}

% ===== Example 5: Heat Equation with Mixed (Robin) Boundary Conditions (full solution) =====
\begin{problem}[Heat Equation with Mixed (Robin) Boundary Conditions]
Let $u(x,t)$ denote the temperature in a homogeneous rod of length $L$ satisfying the heat equation
\[
u_t = \kappa u_{xx},\qquad 0<x<L,\ t>0,
\]
with Newton (Robin) boundary conditions at the ends:
\begin{align*}
- \kappa u_x(0,t) &= h\bigl(u(0,t)-T_{\mathrm{env}}\bigr),\\
\kappa u_x(L,t) &= h\bigl(u(L,t)-T_{\mathrm{env}}\bigr),
\end{align*}
where $\kappa>0$, $h>0$, and $T_{\mathrm{env}}$ are constants. The initial condition is $u(x,0)=f(x)$ for $0<x<L$.

Use the Fourier (separation of variables) method to obtain a series representation of $u(x,t)$ for $t>0$, expressing your answer in terms of the eigenvalues $\{\mu_n\}_{n=1}^{\infty}$ determined by an appropriate transcendental equation and the corresponding eigenfunctions. Clearly indicate the transformed problem, the eigenvalue problem, and how the coefficients of the series are determined from $f$.
\end{problem}

\begin{solution}
We begin by homogenizing the boundary conditions. The ambient temperature $T_{\mathrm{env}}$ is constant in space and time, so it is natural to measure temperature relative to this reference. Define
\[
v(x,t) = u(x,t) - T_{\mathrm{env}}.
\]
Since $T_{\mathrm{env}}$ is constant, we have $v_t = u_t$ and $v_{xx}=u_{xx}$, so $v$ satisfies the same heat equation:
\[
v_t = \kappa v_{xx},\qquad 0<x<L,\ t>0.
\]
The boundary conditions become
\[
- \kappa v_x(0,t) = h v(0,t),\qquad \kappa v_x(L,t) = h v(L,t).
\]
Dividing by $\kappa$ and defining $\alpha = h/\kappa>0$ gives the homogeneous Robin conditions
\[
v_x(0,t) = -\alpha\, v(0,t),\qquad v_x(L,t) = \alpha\, v(L,t).
\]
The initial condition becomes
\[
v(x,0) = g(x) := f(x) - T_{\mathrm{env}}.
\]
Thus our task is to solve
\begin{align*}
v_t &= \kappa v_{xx}, && 0<x<L,\ t>0,\\
v_x(0,t) &= -\alpha v(0,t), && t>0,\\
v_x(L,t) &= \alpha v(L,t), && t>0,\\
v(x,0) &= g(x), && 0<x<L.
\end{align*}
Once $v$ is known, we recover $u$ by $u(x,t)=v(x,t)+T_{\mathrm{env}}$.

\medskip

\textbf{Separation of variables and the eigenvalue problem.}
We apply the Fourier method by seeking separated solutions $v(x,t) = X(x)T(t)$ with $X\not\equiv 0$, $T\not\equiv 0$. Substituting into the PDE,
\[
X(x)T'(t) = \kappa X''(x)T(t).
\]
Assuming $X$ and $T$ are nonzero, we divide both sides by $\kappa X T$:
\[
\frac{T'(t)}{\kappa T(t)} = \frac{X''(x)}{X(x)} = -\lambda,
\]
where the common value must be a constant, which we denote by $-\lambda$ following the usual convention that leads to decaying-in-time solutions for $\lambda>0$.

This yields the system
\[
\frac{T'}{T} = -\kappa \lambda,\qquad X'' + \lambda X = 0.
\]
The boundary conditions on $v$ translate to $X$ (since $T(t)\neq 0$ for nontrivial separated solutions):
\[
X'(0) = -\alpha X(0),\qquad X'(L) = \alpha X(L).
\]
Hence $X$ must satisfy the eigenvalue problem
\[
X''(x) + \lambda X(x) = 0,\quad 0<x<L;\qquad
X'(0) = -\alpha X(0),\quad X'(L) = \alpha X(L).
\]

\medskip

\textbf{Analysis of the spatial eigenvalue problem.}
We must determine the admissible values of $\lambda$ (the eigenvalues) and the corresponding nontrivial functions $X$ (the eigenfunctions).

\emph{1. Excluding $\lambda\le 0$.} First consider $\lambda=0$. Then $X''=0$, so $X(x)=ax+b$. The boundary condition at $x=0$ gives
\[
X'(0) = a = -\alpha X(0) = -\alpha b.
\]
Thus $a=-\alpha b$. At $x=L$,
\[
X'(L) = a = \alpha X(L) = \alpha(aL + b).
\]
Using $a=-\alpha b$ in the second equation:
\[
-\alpha b = \alpha\bigl(-\alpha b\, L + b\bigr)
= \alpha b(1-\alpha L).
\]
If $b\neq 0$, dividing by $\alpha b$ yields $-1 = 1 - \alpha L$, or $\alpha L = 2$. But then $a=-\alpha b\neq 0$, so $X$ is nonconstant; however, substituting back shows this contradicts the previous boundary condition unless $\alpha L$ is tuned exactly, and even then one obtains at most a very special case. In the generic situation (and in Sturm–Liouville theory more generally), $\lambda=0$ does not produce a robust family of eigenfunctions. More systematically, one can check that for $\lambda=0$ the two boundary conditions force $a=b=0$, so $X\equiv 0$.

Now suppose $\lambda<0$. Write $\lambda=-\nu^2$ with $\nu>0$. Then $X''-\nu^2 X=0$, so
\[
X(x) = A e^{\nu x} + B e^{-\nu x}.
\]
Applying the boundary condition at $x=0$:
\[
X'(0) = \nu A - \nu B = -\alpha (A+B).
\]
At $x=L$:
\[
X'(L) = \nu A e^{\nu L} - \nu B e^{-\nu L}
= \alpha\bigl(A e^{\nu L} + B e^{-\nu L}\bigr).
\]
These two linear equations in $A$ and $B$ lead, after elimination, to a condition that cannot be satisfied for $\nu>0$ and $\alpha>0$ except trivially; equivalently, it is a standard fact that the Robin–Robin Sturm–Liouville problem with positive $\alpha$ has no negative eigenvalues. Thus $\lambda<0$ yields only the trivial solution $X\equiv 0$.

Therefore, the only relevant eigenvalues satisfy
\[
\lambda>0.
\]
We write $\lambda=\mu^2$ with $\mu>0$.

\emph{2. The case $\lambda=\mu^2>0$.} The equation
\[
X'' + \mu^2 X = 0
\]
has the general solution
\[
X(x) = A\cos(\mu x) + B\sin(\mu x).
\]
Its derivative is
\[
X'(x) = -A\mu\sin(\mu x) + B\mu\cos(\mu x).
\]

Applying $X'(0)=-\alpha X(0)$:
\[
X'(0) = B\mu = -\alpha X(0) = -\alpha A.
\]
Thus
\[
B = -\frac{\alpha}{\mu}A.
\]

Next apply $X'(L)=\alpha X(L)$:
\begin{align*}
X'(L) &= -A\mu\sin(\mu L) + B\mu\cos(\mu L)\\
&= -A\mu\sin(\mu L) - \alpha A\cos(\mu L),
\end{align*}
since $B\mu = -\alpha A$.
On the other hand,
\begin{align*}
X(L) &= A\cos(\mu L) + B\sin(\mu L)\\
&= A\cos(\mu L) - \frac{\alpha}{\mu}A\sin(\mu L).
\end{align*}
Thus the boundary condition $X'(L)=\alpha X(L)$ becomes
\[
- A\mu\sin(\mu L) - \alpha A\cos(\mu L)
= \alpha A\cos(\mu L) - \alpha^2\frac{A}{\mu}\sin(\mu L).
\]
Dividing by $A$ (which must be nonzero for a nontrivial eigenfunction) and multiplying by $\mu$ gives
\[
- \mu^2\sin(\mu L) - \alpha\mu\cos(\mu L)
= \alpha\mu\cos(\mu L) - \alpha^2\sin(\mu L).
\]
Bringing all terms to the left-hand side:
\[
(-\mu^2 + \alpha^2)\sin(\mu L) - 2\alpha\mu\cos(\mu L) = 0.
\]
Thus the eigenvalues $\lambda_n=\mu_n^2$ are determined by the transcendental equation
\[
F(\mu) := (\alpha^2-\mu^2)\sin(\mu L) - 2\alpha\mu\cos(\mu L) = 0.
\]
This equation has an infinite sequence of positive roots
\[
0<\mu_1<\mu_2<\cdots<\mu_n<\cdots,\qquad \mu_n\to\infty,
\]
as can be seen from general Sturm–Liouville theory or by inspecting $F(\mu)$ on successive intervals of length $\pi/L$.

For each root $\mu_n$, the corresponding eigenfunction can be taken as
\[
X_n(x) = \cos(\mu_n x) - \frac{\alpha}{\mu_n}\sin(\mu_n x),
\]
where we have chosen $A=1$ and $B=-\frac{\alpha}{\mu_n}$ for convenience. Each $X_n$ satisfies
\[
X_n'' + \mu_n^2 X_n = 0,\quad X_n'(0)=-\alpha X_n(0),\quad X_n'(L)=\alpha X_n(L).
\]
Standard Sturm–Liouville theory ensures that the family $\{X_n\}_{n=1}^\infty$ is orthogonal in $L^2(0,L)$ with respect to the usual inner product:
\[
\int_0^L X_m(x)X_n(x)\,dx = 0\quad\text{for }m\neq n.
\]
Moreover, under mild regularity assumptions on $g$, the eigenfunctions form a complete set, so $g$ can be expanded as a generalized Fourier series in $\{X_n\}$.

\medskip

\textbf{Time dependence and separated solutions.}
For each eigenvalue $\lambda_n=\mu_n^2$, the time equation
\[
T_n' = -\kappa\lambda_n T_n = -\kappa\mu_n^2 T_n
\]
has solution
\[
T_n(t) = e^{-\kappa\mu_n^2 t},
\]
up to a multiplicative constant that we absorb into the spatial coefficient.

Thus each separated solution has the form
\[
v_n(x,t) = X_n(x)\, e^{-\kappa\mu_n^2 t}.
\]
By linearity, the general solution that satisfies the Robin boundary conditions is a (possibly infinite) linear combination of these separated solutions:
\[
v(x,t) = \sum_{n=1}^\infty c_n X_n(x) e^{-\kappa\mu_n^2 t},
\]
where the coefficients $c_n$ are determined by the initial condition.

\medskip

\textbf{Determining the coefficients from the initial data.}
At $t=0$ we have
\[
v(x,0) = g(x) = f(x) - T_{\mathrm{env}} = \sum_{n=1}^\infty c_n X_n(x).
\]
This is a generalized Fourier expansion of $g$ in the orthogonal basis $\{X_n\}$.

Let us denote
\[
\|X_n\|^2 := \int_0^L X_n(x)^2\,dx.
\]
Orthogonality implies that
\[
c_n = \frac{\displaystyle \int_0^L g(x)\, X_n(x)\, dx}{\displaystyle \int_0^L X_n(x)^2\, dx}
= \frac{\displaystyle \int_0^L \bigl(f(x)-T_{\mathrm{env}}\bigr)\, X_n(x)\, dx}{\displaystyle \int_0^L X_n(x)^2\, dx}.
\]
In practice, the denominator can be computed explicitly (though the resulting expression is somewhat cumbersome), or left in integral form.

Therefore the solution $v$ is
\[
v(x,t) = \sum_{n=1}^\infty
\left(
\frac{\displaystyle \int_0^L \bigl(f(\xi)-T_{\mathrm{env}}\bigr)\, X_n(\xi)\, d\xi}{\displaystyle \int_0^L X_n(\xi)^2\, d\xi}
\right)
X_n(x)\, e^{-\kappa\mu_n^2 t},
\]
with
\[
X_n(x) = \cos(\mu_n x) - \frac{\alpha}{\mu_n}\sin(\mu_n x),
\]
and $\mu_n$ the positive roots of
\[
(\alpha^2-\mu^2)\sin(\mu L) - 2\alpha\mu\cos(\mu L) = 0.
\]

Finally, recalling that $u(x,t)=v(x,t)+T_{\mathrm{env}}$, we obtain
\[
u(x,t) = T_{\mathrm{env}}
+ \sum_{n=1}^\infty
\left(
\frac{\displaystyle \int_0^L \bigl(f(\xi)-T_{\mathrm{env}}\bigr)\, X_n(\xi)\, d\xi}{\displaystyle \int_0^L X_n(\xi)^2\, d\xi}
\right)
X_n(x)\, e^{-\kappa\mu_n^2 t}.
\]

\medskip

\textbf{Conceptual remarks.}
This example illustrates several central ideas of the Fourier method for boundary value problems:

\begin{itemize}
\item Homogenization of boundary conditions by subtracting an appropriate steady or reference state (here, the constant ambient temperature).
\item Reduction of the PDE with boundary conditions to a Sturm–Liouville eigenvalue problem for the spatial part, here with Robin (mixed) conditions involving both the function and its derivative.
\item The appearance of a transcendental eigenvalue equation, whose roots define the spectrum, and the use of the corresponding eigenfunctions as a generalized Fourier basis.
\item Expansion of the initial data in this basis and exponential decay of each mode according to its eigenvalue.
\end{itemize}

Even though the eigenfunctions are no longer the simple sine or cosine functions arising from Dirichlet or Neumann boundary conditions, the overall structure of the Fourier method is unchanged: one still builds the solution as a superposition of separated modes adapted to the given boundary conditions.
\end{solution}

% ===== Example 6: Nonhomogeneous Forcing in a Heat Equation Boundary Value Problem (inquiry-based) =====
\begin{problem}[Nonhomogeneous Forcing in a Heat Equation Boundary Value Problem]
Consider a thin, perfectly insulated rod of length $\pi$. Its ends at $x=0$ and $x=\pi$ are kept at zero temperature by contact with large ice baths. Inside the rod there is an internal heat source whose intensity varies in space and in time. We denote the temperature by $u(x,t)$ and suppose that the internal heat source is described by a function $S(x,t)$ added to the usual heat equation.

We take the simplest nontrivial case
\[
u_t = u_{xx} + S(x,t), \qquad 0<x<\pi,\ t>0,
\]
with Dirichlet boundary conditions
\[
u(0,t)=0,\qquad u(\pi,t)=0,\qquad t>0,
\]
and initial condition
\[
u(x,0)=0,\qquad 0<x<\pi.
\]
In this problem you will discover how to adapt the Fourier method to handle the nonhomogeneous forcing $S(x,t)$, and then work out a concrete example.

\medskip

(a) First recall the spatial eigenvalue problem that appears when using separation of variables for the \emph{homogeneous} heat equation (that is, with $S\equiv 0$). Consider
\[
X''(x) + \lambda X(x) = 0,\qquad X(0)=0,\quad X(\pi)=0.
\]
Find all eigenvalues $\lambda_n$ and corresponding eigenfunctions $X_n(x)$.

\emph{Hint:} You should recover a familiar sine series basis on the interval $(0,\pi)$.

\medskip

(b) Now return to the \emph{nonhomogeneous} problem with an internal source $S(x,t)$. Motivated by part (a), suppose that $u(x,t)$ can be expanded in terms of the eigenfunctions you found:
\[
u(x,t) = \sum_{n=1}^\infty T_n(t)\,X_n(x).
\]
Explain why it is natural also to expand the source term $S(x,t)$ in the same eigenfunctions,
\[
S(x,t) = \sum_{n=1}^\infty b_n(t)\,X_n(x),
\]
for some time-dependent coefficients $b_n(t)$. How can you compute the functions $b_n(t)$ from $S(x,t)$ using orthogonality?

\emph{Hint:} Recall the orthogonality relation for the eigenfunctions on $(0,\pi)$ with respect to the standard inner product
\[
\langle f,g\rangle = \int_0^\pi f(x)g(x)\,dx.
\]

\medskip

(c) Substitute the expansions
\[
u(x,t)=\sum_{n=1}^\infty T_n(t)X_n(x), \qquad 
S(x,t)=\sum_{n=1}^\infty b_n(t)X_n(x)
\]
into the PDE
\[
u_t = u_{xx} + S(x,t),
\]
and use the fact that $X_n''(x) = -\lambda_n X_n(x)$. Then use orthogonality of the $X_n$ to derive a system of uncoupled first-order ordinary differential equations for the time-dependent coefficients $T_n(t)$.

Write down the ODE satisfied by $T_n(t)$ in terms of $\lambda_n$ and $b_n(t)$, and write the initial condition for $T_n(0)$ in terms of the initial data $u(x,0)$.

\emph{Hint:} Multiply both sides of the PDE (after substituting the series) by $X_m(x)$, integrate from $0$ to $\pi$, and use orthogonality to isolate a single index $m$.

\medskip

(d) We now examine a specific internal heat source
\[
S(x,t) = e^{-t}\sin x.
\]
Notice that this is already written as a single eigenfunction of the spatial operator. Use your work in part (c) to solve the full boundary value problem
\[
\begin{cases}
u_t = u_{xx} + e^{-t}\sin x, & 0<x<\pi,\ t>0,\\[4pt]
u(0,t)=0,\quad u(\pi,t)=0, & t>0,\\[4pt]
u(x,0)=0, & 0<x<\pi.
\end{cases}
\]

\begin{enumerate}
\item[(i)] Determine the coefficients $b_n(t)$ for this particular forcing $S(x,t)$.
\item[(ii)] Show that the system of ODEs for $T_n(t)$ simplifies dramatically, and identify which modes $n$ are actually forced.
\item[(iii)] Solve the resulting first-order ODE(s) for $T_n(t)$ with the given initial data and write the final expression for $u(x,t)$.
\end{enumerate}

\emph{Hint:} For the nontrivial mode, you will encounter an ODE of the form
\[
T'(t) + \lambda T(t) = e^{-t}, \qquad T(0)=0,
\]
which you can solve by an integrating factor.

\medskip

(e) Explore two extensions of this example.

\begin{enumerate}
\item[(i)] Suppose the source is \emph{time-independent}: $S(x,t)=S(x)$ only. Use your general formula from part (c) to describe qualitatively what you expect for the long-time behavior of $u(x,t)$ as $t\to\infty$. Do you expect a steady-state solution? Why?

\item[(ii)] Suppose instead that the boundary conditions are changed to \emph{Neumann} conditions
\[
u_x(0,t)=0,\qquad u_x(\pi,t)=0,
\]
while keeping the same PDE and source $S(x,t)=e^{-t}\sin x$. How would the eigenvalue problem and eigenfunctions change? In outline, how would the Fourier method for this nonhomogeneous Neumann problem differ from what you did above?
\end{enumerate}

\end{problem}

% ===== Example 6: Nonhomogeneous Forcing in a Heat Equation Boundary Value Problem (full solution) =====
\begin{problem}[Nonhomogeneous Forcing in a Heat Equation Boundary Value Problem]
Consider the initial–boundary value problem
\[
\begin{cases}
u_t = u_{xx} + e^{-t}\sin x, & 0<x<\pi,\ t>0,\\[4pt]
u(0,t)=0,\quad u(\pi,t)=0, & t>0,\\[4pt]
u(x,0)=0, & 0<x<\pi.
\end{cases}
\]
Use the Fourier (eigenfunction) method to find the temperature $u(x,t)$.
\end{problem}

\begin{solution}
We solve this nonhomogeneous heat equation by expanding in the eigenfunctions of the associated homogeneous spatial operator. This illustrates the general principle of the Fourier method for boundary value problems: we diagonalize the spatial operator using its eigenfunctions, expand both the solution and the forcing in that basis, and obtain decoupled ordinary differential equations in time for the Fourier coefficients.

\medskip

\noindent\textbf{1. Spatial eigenvalue problem and eigenfunctions.}

We first recall the eigenfunctions of the one-dimensional Laplacian with homogeneous Dirichlet boundary conditions on $(0,\pi)$. The associated eigenvalue problem is
\[
X''(x) + \lambda X(x)=0,\qquad X(0)=0,\quad X(\pi)=0.
\]
For $\lambda>0$, the general solution is $X(x)=A\cos(\sqrt{\lambda}\,x)+B\sin(\sqrt{\lambda}\,x)$. The condition $X(0)=0$ forces $A=0$, so $X(x)=B\sin(\sqrt{\lambda}\,x)$. The condition $X(\pi)=0$ then yields
\[
B\sin(\sqrt{\lambda}\,\pi)=0.
\]
To obtain nontrivial eigenfunctions ($B\neq 0$) we must have $\sin(\sqrt{\lambda}\,\pi)=0$, hence
\[
\sqrt{\lambda} = n,\qquad n=1,2,3,\dots,
\]
so that
\[
\lambda_n = n^2,\qquad X_n(x)=\sin(nx).
\]
These eigenfunctions form an orthogonal basis of $L^2(0,\pi)$ with respect to the usual inner product
\[
\langle f,g\rangle = \int_0^\pi f(x)g(x)\,dx.
\]

\medskip

\noindent\textbf{2. Eigenfunction expansion of $u$ and of the forcing.}

Because the spatial operator $u\mapsto u_{xx}$ with Dirichlet boundary conditions has eigenfunctions $\sin(nx)$, it is natural to look for a solution $u(x,t)$ of the form
\[
u(x,t) = \sum_{n=1}^\infty T_n(t)\,\sin(nx),
\]
where the time-dependent coefficients $T_n(t)$ are to be determined.

We also expand the forcing term in the same basis. Our source is
\[
S(x,t) = e^{-t}\sin x.
\]
But $\sin x$ is exactly the first eigenfunction $X_1(x)$, so we immediately see that
\[
S(x,t) = e^{-t}\sin x = b_1(t)\sin x + \sum_{n=2}^\infty b_n(t)\sin(nx),
\]
with
\[
b_1(t)=e^{-t}, \qquad b_n(t)=0 \ \text{for }n\ge 2.
\]
Equivalently, the only nonzero Fourier coefficient of $S(x,t)$ is its first sine coefficient.

\medskip

\noindent\textbf{3. Deriving ODEs for the Fourier coefficients.}

We now substitute the eigenfunction expansion for $u$ into the PDE. Compute
\[
u_t(x,t) = \sum_{n=1}^\infty T_n'(t)\,\sin(nx),
\]
and
\[
u_{xx}(x,t) = \sum_{n=1}^\infty T_n(t)\,\frac{d^2}{dx^2}\sin(nx)
= \sum_{n=1}^\infty T_n(t)\,(-n^2)\sin(nx)
= -\sum_{n=1}^\infty n^2 T_n(t)\,\sin(nx).
\]
The PDE
\[
u_t = u_{xx} + e^{-t}\sin x
\]
becomes
\[
\sum_{n=1}^\infty T_n'(t)\,\sin(nx)
=
-\sum_{n=1}^\infty n^2 T_n(t)\,\sin(nx)
+ e^{-t}\sin x.
\]
We regard $e^{-t}\sin x$ as $e^{-t}\sin x + 0\cdot \sin(2x)+0\cdot \sin(3x)+\cdots$, that is,
\[
e^{-t}\sin x = \sum_{n=1}^\infty b_n(t)\,\sin(nx),
\]
with $b_1(t)=e^{-t}$ and $b_n(t)=0$ for $n\ge 2$.

Thus the PDE can be written as
\[
\sum_{n=1}^\infty T_n'(t)\,\sin(nx)
=
-\sum_{n=1}^\infty n^2 T_n(t)\,\sin(nx)
+\sum_{n=1}^\infty b_n(t)\,\sin(nx).
\]
Because the sine functions form an orthogonal basis, equality of these series for all $x$ implies equality of the coefficients of each $\sin(nx)$:
\[
T_n'(t) = -n^2 T_n(t) + b_n(t), \qquad n=1,2,\dots.
\]
In our case,
\[
b_1(t)=e^{-t},\qquad b_n(t)=0\ \text{for }n\ge 2,
\]
so
\[
\begin{cases}
T_1'(t) = -1^2 T_1(t) + e^{-t},\\[2pt]
T_n'(t) = -n^2 T_n(t), & n\ge 2.
\end{cases}
\]

The initial condition $u(x,0)=0$ gives
\[
u(x,0) = \sum_{n=1}^\infty T_n(0)\,\sin(nx) = 0.
\]
Since $\{\sin(nx)\}$ is a basis, all coefficients must vanish:
\[
T_n(0)=0,\qquad n=1,2,\dots.
\]

\medskip

\noindent\textbf{4. Solving the ODEs for $T_n(t)$.}

For $n\ge 2$ the ODEs are homogeneous:
\[
T_n'(t) = -n^2 T_n(t),\qquad T_n(0)=0.
\]
The unique solution is
\[
T_n(t)\equiv 0,\qquad n\ge 2,
\]
since the general solution is $T_n(t)=C_n e^{-n^2 t}$ and the initial condition forces $C_n=0$.

For $n=1$ we have the forced first-order linear ODE
\[
T_1'(t) + T_1(t) = e^{-t},\qquad T_1(0)=0.
\]
We solve this by using an integrating factor. The integrating factor is $e^{\int 1\,dt}=e^{t}$, and multiplying the equation by $e^{t}$ gives
\[
e^{t}T_1'(t) + e^{t}T_1(t) = e^{t}\,e^{-t} = 1.
\]
The left-hand side is the derivative of $e^{t}T_1(t)$:
\[
\frac{d}{dt}\bigl(e^t T_1(t)\bigr) = 1.
\]
Integrating from $0$ to $t$ yields
\[
e^{t}T_1(t) - e^{0}T_1(0) = \int_0^t 1\,ds = t.
\]
Since $T_1(0)=0$, we obtain
\[
e^{t}T_1(t) = t,\qquad\text{so}\qquad
T_1(t) = t e^{-t}.
\]

In summary,
\[
T_1(t) = t e^{-t},\qquad T_n(t) = 0\ \text{for }n\ge 2.
\]

\medskip

\noindent\textbf{5. Assembling the solution.}

Returning to the eigenfunction expansion
\[
u(x,t) = \sum_{n=1}^\infty T_n(t)\,\sin(nx),
\]
we see that only the first term survives:
\[
u(x,t) = T_1(t)\sin x = t e^{-t}\sin x.
\]

We may check quickly that this formula satisfies all conditions.

First, the PDE:
\[
u_t = \frac{\partial}{\partial t}\bigl(t e^{-t}\sin x\bigr)
= (e^{-t} - t e^{-t})\sin x
= (1-t)e^{-t}\sin x,
\]
and
\[
u_{xx} = \frac{\partial^2}{\partial x^2}\bigl(t e^{-t}\sin x\bigr)
= t e^{-t}(-\sin x) = -t e^{-t}\sin x.
\]
Therefore
\[
u_{xx} + e^{-t}\sin x
= -t e^{-t}\sin x + e^{-t}\sin x
= (1-t)e^{-t}\sin x
= u_t,
\]
so the PDE is satisfied.

Next, the boundary conditions:
\[
u(0,t) = t e^{-t}\sin 0 = 0,\qquad
u(\pi,t) = t e^{-t}\sin \pi = 0,
\]
so the Dirichlet conditions hold. Finally, the initial condition:
\[
u(x,0) = 0\cdot e^{0}\sin x = 0,
\]
which matches $u(x,0)=0$.

\medskip

\noindent\textbf{6. Conceptual remarks.}

This example illustrates the central ideas of the Fourier method for boundary value problems with nonhomogeneous forcing:

\begin{itemize}
\item We first solve the spatial eigenvalue problem associated with the homogeneous operator and boundary conditions, obtaining an orthogonal basis of eigenfunctions (here, $\sin(nx)$).
\item We expand both the unknown solution $u(x,t)$ and the given forcing term $S(x,t)$ in this eigenbasis.
\item Orthogonality reduces the PDE to a family of decoupled first-order linear ODEs in time for the Fourier coefficients $T_n(t)$, with nonhomogeneous terms given by the Fourier coefficients of $S$.
\item In the present problem, only a single spatial mode is forced, so only one ODE is nontrivial, and the final solution is a single time-dependent Fourier mode.
\end{itemize}

In more complicated problems, the same procedure leads to a system of ODEs that can be solved by integrating factors or other standard ODE methods. The principle remains the same: the eigenfunctions encode the boundary conditions and diagonalize the spatial operator, allowing us to treat time evolution and forcing mode by mode.

\end{solution}

\section{Case Study: Burgers' Equation (*)}
% --- Narrative plan (auto-generated) ---
% This section uses Burgers’ equation as a unifying case study to explore several core ideas in partial differential equations, including nonlinear advection, diffusion, shock formation, and the interplay between analytic and numerical methods. Burgers’ equation is simple enough to be written on a single line, yet rich enough to display many of the qualitative behaviors encountered in fluid dynamics, traffic flow, and conservation laws, such as steepening waves, shock fronts, and smoothing by viscosity. By examining both the inviscid and viscous forms, we see how adding a small diffusive term changes the nature of solutions and resolves singularities.
%
% The techniques we develop here connect to many other parts of applied mathematics. The inviscid equation highlights the method of characteristics and connects directly to ordinary differential equations and dynamical systems along characteristic curves. The viscous equation can be transformed into the heat equation via the Hopf–Cole transform, which leads naturally to Fourier analysis, convolution with the heat kernel, and, in more advanced treatments, connections with complex analysis through integral representations. Throughout this section we emphasize how these tools, first encountered in simpler settings, come together in a coherent way around a single nonlinear PDE model.
%
% Our goal is to let you rediscover standard techniques through guided problems rather than only reading formal solutions. By starting from linear transport and diffusion equations and gradually adding nonlinearity and viscosity, you will see how methods are adapted and extended. This case study prepares you for more sophisticated conservation laws and nonlinear PDEs in fluid mechanics, as well as for numerical methods that must cope with sharp gradients and discontinuities.

% ===== Example 1: Warm-Up: Linear Transport and the Method of Characteristics (inquiry-based) =====
\begin{problem}[Warm-Up: Linear Transport and the Method of Characteristics]
A basic model for advection of a scalar quantity along a one-dimensional medium is the \emph{linear transport equation}
\[
u_t + c\,u_x = 0,
\]
where $u(t,x)$ is the concentration (or temperature, density, etc.), and $c$ is a constant velocity. In words, the model says that $u$ is simply carried along with the flow, without diffusion, reaction, or sources. Before we confront nonlinear transport such as Burgers' equation, we will use this example to introduce and practice the method of characteristics. The goal is to see how the partial differential equation encodes the translation of the initial profile and how that motion is represented geometrically by families of curves in the $(t,x)$-plane.

Consider the Cauchy problem on the whole line:
\[
\begin{cases}
u_t + c\,u_x = 0, & t>0,\; x\in\mathbb{R},\\[0.3em]
u(0,x) = u_0(x), & x\in\mathbb{R},
\end{cases}
\]
where $c$ is a fixed real constant and $u_0:\mathbb{R}\to\mathbb{R}$ is a given initial profile.

\smallskip

(a) The method of characteristics seeks curves in the $(t,x)$-plane along which the partial differential equation reduces to an ordinary differential equation. Let a characteristic curve be parameterized by $s\mapsto (t(s),x(s))$, and let $u(s):=u(t(s),x(s))$ be the value of the solution along this curve.

\quad(i) Use the chain rule to express $\dfrac{du}{ds}$ in terms of $u_t$, $u_x$, $t'(s)$, and $x'(s)$.

\quad(ii) We would like to choose $t'(s)$ and $x'(s)$ so that the PDE implies $\dfrac{du}{ds}=0$, that is, $u$ is constant along each characteristic. Show that if you pick $t'(s)=1$ and $x'(s)=c$, then the condition $u_t + c\,u_x =0$ indeed gives $\dfrac{du}{ds}=0$.

\quad(iii) Solve the resulting system of ordinary differential equations for the characteristic curves:
\[
\frac{dt}{ds}=1, \qquad \frac{dx}{ds}=c.
\]
Describe the family of characteristic curves in the $(t,x)$-plane (for example, are they straight lines, what is their slope, and what parameter labels which curve we are on?).

\emph{Hint:} One convenient way is to eliminate the parameter $s$ and find a direct relationship between $t$ and $x$ along a characteristic.

\smallskip

(b) Use your description from part (a) to write an explicit formula for all characteristic curves passing through the initial line $t=0$. More concretely:

\quad(i) Let $\xi\in\mathbb{R}$ denote the $x$--coordinate where a characteristic intersects the initial line $t=0$. Express $x$ at a later time $t$ along this characteristic in terms of $\xi$, $c$, and $t$.

\quad(ii) Argue that we can label each characteristic by its intersection point $\xi$ at $t=0$, and that along this characteristic, the solution satisfies
\[
u(t,x(t)) = u(0,\xi)=u_0(\xi).
\]

\quad(iii) Use the relation between $x$, $t$, and $\xi$ to eliminate $\xi$ and obtain a formula for $u(t,x)$ directly in terms of $u_0$ and the variables $t$ and $x$.

\emph{Hint:} Solve your formula from part (b)(i) for $\xi$ as a function of $(t,x)$, then substitute into $u_0(\xi)$.

\smallskip

(c) Let us now explore how different initial profiles move under this transport equation.

\quad(i) Take
\[
u_0(x) = H(x) := \begin{cases}
0, & x<0,\\
1, & x\ge 0,
\end{cases}
\]
the \emph{Heaviside step} profile. Using your formula from part (b), write an explicit expression for $u(t,x)$ in this case. Carefully interpret how the discontinuity moves in the $(t,x)$-plane.

\quad(ii) Take instead a smooth ``bump'' profile
\[
u_0(x) = e^{-x^2}.
\]
Again, write down $u(t,x)$ and describe in words what happens to the shape and position of the bump over time.

\quad(iii) Finally, consider an oscillatory profile
\[
u_0(x) = \cos(kx), \quad k>0.
\]
Compute $u(t,x)$ and simplify it as much as possible. Is the frequency of oscillations changing with time, or only their phase? How can you see this in your formula?

\emph{Hint:} For (iii), use the cosine addition formula.

\smallskip

(d) In this part you will consolidate the geometric and analytic points of view.

\quad(i) Sketch a few characteristic lines $x-ct=\text{constant}$ in the $(t,x)$-plane. On the line $t=0$, sketch your favorite initial profile $u_0(x)$ (for example, the bump $e^{-x^2}$). Use arrows along characteristics to indicate how the values of $u$ are transported.

\quad(ii) Explain in words why the solution you found in part (b) can be described as follows: ``the entire initial profile is translated rigidly at constant speed $c$ without any change in shape.''

\quad(iii) Show that if $u_0$ is continuous and bounded, then for each fixed $t>0$ the solution $u(t,\cdot)$ is also continuous and bounded, and
\[
\|u(t,\cdot)\|_{L^\infty(\mathbb{R})} = \|u_0\|_{L^\infty(\mathbb{R})}.
\]
Why does this make sense physically for a pure transport equation?

\emph{Hint:} Use your explicit formula for $u(t,x)$ and basic properties of translations of functions.

\smallskip

(e) This simple linear model is a stepping stone toward understanding the nonlinear Burgers' equation
\[
u_t + u\,u_x = 0,
\]
which we will study later in this chapter.

\quad(i) In the linear model, the characteristic curves are straight lines $x-ct=\text{constant}$ with a fixed slope $1/c$ (or vertical if $c=0$). For Burgers' equation, the characteristic speed is not constant but depends on $u$ itself. Based on your current understanding, describe informally what you expect to change in the geometric picture of characteristics when going from $u_t + c u_x = 0$ to $u_t + u u_x =0$.

\quad(ii) Suppose in the linear case we take $c<0$ instead of $c>0$. How does this change the direction of motion of the initial profile? How does this show up in your explicit formula and in the picture of characteristics?

\emph{Hint:} Think about whether characteristics tilt to the left or to the right as $t$ increases, and whether the sign of $c$ affects the \emph{magnitude} or only the \emph{direction} of the translation.
\end{problem}

% ===== Example 1: Warm-Up: Linear Transport and the Method of Characteristics (full solution) =====
\begin{problem}[Warm-Up: Linear Transport and the Method of Characteristics]
Consider the linear transport equation with constant velocity $c\in\mathbb{R}$:
\[
\begin{cases}
u_t + c\,u_x = 0, & t>0,\; x\in\mathbb{R},\\[0.3em]
u(0,x)=u_0(x), & x\in\mathbb{R},
\end{cases}
\]
where $u_0:\mathbb{R}\to\mathbb{R}$ is a given function.

\begin{enumerate}
\item Use the method of characteristics to derive an explicit formula for the solution $u(t,x)$ in terms of $u_0$.
\item Apply your formula to the following initial data:
  \begin{enumerate}
  \item The Heaviside step $u_0(x)=H(x)$.
  \item The Gaussian bump $u_0(x)=e^{-x^2}$.
  \item The oscillatory profile $u_0(x)=\cos(kx)$ with $k>0$.
  \end{enumerate}
  In each case, describe briefly how the profile evolves in time.
\item Show that if $u_0$ is bounded and continuous, then for each $t>0$ the function $u(t,\cdot)$ is also bounded and continuous, and
\[
\|u(t,\cdot)\|_{L^\infty(\mathbb{R})} = \|u_0\|_{L^\infty(\mathbb{R})}.
\]
\end{enumerate}
Briefly indicate how this example illustrates the method of characteristics that will be used later for Burgers' equation $u_t + u u_x=0$.
\end{problem}

\begin{solution}
We solve the transport equation
\[
u_t + c\,u_x = 0
\]
by the method of characteristics. The central idea of this method is to reduce the partial differential equation to a family of ordinary differential equations along special curves in the $(t,x)$-plane, called characteristic curves.

\medskip

\textbf{1. Derivation of the characteristic equations and solution formula.}

Let a characteristic curve be parameterized by a variable $s\mapsto (t(s),x(s))$, and consider the function $u$ along this curve:
\[
U(s) := u(t(s),x(s)).
\]
By the chain rule,
\[
\frac{dU}{ds} = u_t(t(s),x(s))\,t'(s) + u_x(t(s),x(s))\,x'(s).
\]
We wish to arrange that $\dfrac{dU}{ds}=0$, so that $u$ is constant along each characteristic. Comparing with the PDE $u_t + c\,u_x = 0$, it is natural to choose
\[
t'(s) = 1, \qquad x'(s) = c.
\]
With this choice, we obtain
\[
\frac{dU}{ds}
= u_t(t(s),x(s))\cdot 1 + u_x(t(s),x(s))\cdot c
= u_t + c\,u_x = 0.
\]
Thus $U(s)$ is constant along any solution of this system. The characteristic curves therefore satisfy the system of ordinary differential equations
\[
\frac{dt}{ds} = 1, \qquad \frac{dx}{ds} = c.
\]

Integrating these, we find
\[
t(s) = s + C_1, \qquad x(s) = cs + C_2,
\]
where $C_1$ and $C_2$ are constants. Eliminating $s$ between these two equations, we obtain
\[
x - c t = C_2 - c C_1 = \text{constant}.
\]
Thus the characteristics in the $(t,x)$-plane are straight lines of slope $c$ in the $(s,x)$ representation, or more usefully, lines of the form
\[
x - c t = \text{constant},
\]
when viewed in the $(t,x)$-plane. Each such line is a characteristic along which $u$ remains constant.

To connect this with the initial condition at time $t=0$, we label a characteristic by its intersection with the initial line. Let $\xi\in\mathbb{R}$ denote the point where the characteristic meets the line $t=0$:
\[
t = 0,\quad x = \xi.
\]
Along the characteristic starting from $(0,\xi)$, the relation $x - c t = \text{constant}$ yields
\[
x - c t = \xi.
\]
Equivalently,
\[
x(t) = \xi + c t.
\]
The value of $u$ remains constant along this characteristic, so for all $t\ge 0$,
\[
u(t,x(t)) = u(0,\xi) = u_0(\xi).
\]
Now we fix $(t,x)$ and identify which characteristic passes through this point. The line $x - ct = \text{constant}$ passing through $(t,x)$ has constant equal to $x-ct$, so its intersection with $t=0$ occurs at the point $(0,\xi)$ where
\[
\xi = x - c t.
\]
Therefore the point $(t,x)$ lies on the characteristic that originated from $(0,x-ct)$, and along that characteristic the value of $u$ is $u_0(x-ct)$. Hence
\[
u(t,x) = u_0(x - c t).
\]
This is the explicit solution formula obtained by the method of characteristics.

We can summarize the result:

\emph{For the Cauchy problem}
\[
u_t + c\,u_x = 0, \quad u(0,x) = u_0(x),
\]
\emph{the solution is given by}
\[
u(t,x) = u_0(x - c t), \quad t\ge 0,\; x\in\mathbb{R}.
\]

Analytically, the PDE has been reduced to a simple translation of the initial data. Geometrically, each characteristic line $x-ct=\text{constant}$ carries the value of $u$ from the initial line $t=0$ forward in time without change.

\medskip

\textbf{2. Examples of evolving profiles.}

We now apply the formula $u(t,x)=u_0(x-ct)$ to the three specified initial profiles.

\smallskip

\emph{(a) Heaviside step $u_0(x)=H(x)$.}

Recall that
\[
H(x) = \begin{cases}
0, & x<0,\\[0.2em]
1, & x\ge 0.
\end{cases}
\]
Then the solution is
\[
u(t,x) = u_0(x - c t) = H(x - c t).
\]
By the definition of $H$, this equals
\[
u(t,x) = 
\begin{cases}
0, & x - c t < 0 \;\;(\text{i.e. } x < c t),\\[0.2em]
1, & x - c t \ge 0 \;\;(\text{i.e. } x \ge c t).
\end{cases}
\]
Thus the discontinuity (the jump from $0$ to $1$) occurs at $x = c t$ and moves with constant speed $c$. If $c>0$, the step moves to the right; if $c<0$, it moves to the left. The height of the jump and the values on either side are unchanged.

\smallskip

\emph{(b) Gaussian bump $u_0(x)=e^{-x^2}$.}

In this case the solution is
\[
u(t,x) = u_0(x-ct) = e^{-(x-ct)^2}.
\]
The shape of the function in $x$ is exactly the same Gaussian bump, but its center has moved from $x=0$ to $x=ct$. There is no spreading or deformation of the profile; it is simply translated to the right if $c>0$ or to the left if $c<0$.

\smallskip

\emph{(c) Oscillatory profile $u_0(x)=\cos(kx)$.}

We obtain
\[
u(t,x) = u_0(x-ct) = \cos\big(k(x - c t)\big).
\]
Using the trigonometric identity
\[
\cos(\alpha - \beta) = \cos\alpha\cos\beta + \sin\alpha\sin\beta,
\]
we can write
\[
u(t,x) = \cos(kx - kct)
= \cos(kx)\cos(kct) + \sin(kx)\,\sin(kct).
\]
For each fixed time $t$, this is a cosine wave in $x$ with the same spatial frequency $k$ and the same amplitude $1$. The effect of time is only to change the phase by $kct$, that is, to shift the pattern in space. Equivalently, it is a rigid translation of the wave to the right (if $c>0$) with constant speed $c$. The wavelength and amplitude remain unchanged.

\medskip

\textbf{3. Preservation of boundedness and the $L^\infty$ norm.}

Assume that $u_0$ is bounded and continuous on $\mathbb{R}$. Then the solution is
\[
u(t,x) = u_0(x-ct).
\]

First, continuity is preserved. For any fixed $t>0$, the map $x\mapsto x-ct$ is a continuous bijection from $\mathbb{R}$ to $\mathbb{R}$. The composition of a continuous function $u_0$ with a continuous map is continuous, hence $x\mapsto u_0(x-ct)$ is continuous. Therefore $u(t,\cdot)$ is continuous for each $t>0$.

Second, boundedness and the supremum norm are preserved. Since $u_0$ is bounded, there exists $M\ge 0$ such that $|u_0(x)|\le M$ for all $x\in\mathbb{R}$, so
\[
|u(t,x)| = |u_0(x-ct)| \le M
\]
for all $x\in\mathbb{R}$. Thus $u(t,\cdot)$ is bounded, and
\[
\|u(t,\cdot)\|_{L^\infty(\mathbb{R})}
= \sup_{x\in\mathbb{R}} |u(t,x)|
= \sup_{x\in\mathbb{R}} |u_0(x-ct)|.
\]
But as $x$ ranges over $\mathbb{R}$, the argument $x-ct$ also ranges over $\mathbb{R}$. Therefore
\[
\sup_{x\in\mathbb{R}} |u_0(x-ct)|
= \sup_{y\in\mathbb{R}} |u_0(y)|
= \|u_0\|_{L^\infty(\mathbb{R})}.
\]
Hence the $L^\infty$ norm is exactly conserved:
\[
\|u(t,\cdot)\|_{L^\infty(\mathbb{R})} = \|u_0\|_{L^\infty(\mathbb{R})}, \quad t\ge 0.
\]

From a physical standpoint, this expresses the idea that the transport equation models pure advection without sources, sinks, or diffusion. The magnitude of the quantity being transported does not grow or decay; it is merely rearranged in space by a rigid translation.

\medskip

\textbf{4. Connection with Burgers' equation.}

This example illustrates the core mechanism of the method of characteristics. We identified characteristic curves in the $(t,x)$-plane along which the solution is constant, reduced the PDE to an ODE system, and then used the initial condition to parametrize the characteristics and obtain an explicit solution formula.

In the linear transport equation, the characteristic speed $c$ is constant, so the characteristics are straight parallel lines $x-ct=\text{constant}$ and never intersect. In the nonlinear Burgers' equation
\[
u_t + u\,u_x = 0,
\]
the characteristic speed is the solution itself, so the characteristic equations become
\[
\frac{dt}{ds} = 1, \qquad \frac{dx}{ds} = u,\qquad \frac{du}{ds} = 0.
\]
Along each characteristic the value of $u$ still remains constant, but now the slope of each line in the $(t,x)$-plane depends on that constant value. As a result, characteristics can converge and intersect, leading to the formation of shocks and more intricate behavior. The linear case worked out here is therefore an essential warm-up: it shows how characteristics encode the geometry of transport in the simplest setting, preparing us to analyze these more complex phenomena in Burgers' equation.
\end{solution}

% ===== Example 2: Inviscid Burgers’ Equation and Shock Formation (inquiry-based) =====
\begin{problem}[Inviscid Burgers’ Equation and Shock Formation]
Burgers’ equation is a simple nonlinear model that already captures many phenomena of hyperbolic conservation laws, such as wave steepening and shock formation. In this problem we study the \emph{inviscid} Burgers’ equation, where the advecting velocity equals the solution itself. Starting from smooth initial data, we will follow characteristics, see how they can intersect in finite time, and interpret this as the breakdown of a classical solution and the onset of a shock. The goal is to understand both the mathematics (characteristics, loss of invertibility) and the physical picture (wave steepening and breaking).

Consider the inviscid Burgers’ equation on the real line
\[
u_t + u\,u_x = 0, \qquad x \in \mathbb{R},\ t > 0,
\]
with smooth initial data
\[
u(x,0) = u_0(x), \qquad x \in \mathbb{R}.
\]

\smallskip
(a) \textbf{Characteristics and constancy along them.}  

Think of the equation as an advection equation with “velocity” $u(x,t)$:
\[
u_t + a(x,t)\,u_x = 0 \quad \text{with} \quad a(x,t) = u(x,t).
\]
We seek characteristic curves $t \mapsto x(t)$ along which $u$ is constant.

\begin{enumerate}
\item[(i)] Write down the system of ordinary differential equations for $x(t)$ and $u(t)$ that encodes the idea “move with the flow so that $u$ does not change along the path.”
\item[(ii)] Show from this system that along each characteristic,
\[
\frac{d}{dt}u(x(t),t) = 0,
\]
that is, $u$ is constant on each characteristic curve.
\end{enumerate}
Hint: For (ii), apply the chain rule to $u(x(t),t)$ and then use the PDE to simplify.

\smallskip
(b) \textbf{Using initial data to parametrize characteristics.}  

We now label each characteristic by the point $\xi$ where it starts from the initial line $t=0$. That is, we consider
\[
x(0) = \xi, \qquad u(x(0),0) = u_0(\xi).
\]

\begin{enumerate}
\item[(i)] Using your ODE system from part (a), solve for $u(t)$ along a characteristic that starts at $(\xi,0)$. Express $u(t)$ in terms of $u_0(\xi)$.
\item[(ii)] Solve for the characteristic curve $x(t;\xi)$ that starts at $x(0)=\xi$, using the fact that $u$ is constant along it. Show that
\[
x(t;\xi) = \xi + t\,u_0(\xi).
\]
\item[(iii)] Explain why, as long as we can solve for $\xi$ in terms of $x$ and $t$ (that is, as long as the mapping $\xi \mapsto x(t;\xi)$ is invertible), we can write the solution in the form
\[
u(x,t) = u_0(\xi(x,t)).
\]
\end{enumerate}
% Hint: You may find it useful to think of $\xi$ as a Lagrangian (particle) label, and $(x,t)$ as Eulerian coordinates.

\smallskip
(c) \textbf{When do characteristics intersect? Condition for shock formation.}  

The characteristics are the curves $x(t;\xi)$ in the $(x,t)$-plane. If two distinct values $\xi_1 \neq \xi_2$ lead to the same point $(x,t)$, the solution formula $u(x,t) = u_0(\xi)$ becomes multi-valued, indicating a breakdown of the classical solution.

\begin{enumerate}
\item[(i)] Show that the map $\xi \mapsto x(t;\xi)$ is strictly increasing if and only if
\[
\frac{\partial x}{\partial \xi}(t;\xi) > 0 \quad \text{for all }\xi.
\]
Compute $\dfrac{\partial x}{\partial \xi}(t;\xi)$ for
\[
x(t;\xi) = \xi + t\,u_0(\xi).
\]
\item[(ii)] Using your expression, derive the condition for the first time $t>0$ when the map $\xi \mapsto x(t;\xi)$ ceases to be one-to-one. Show that this happens when, for some $\xi$,
\[
\frac{\partial x}{\partial \xi}(t;\xi) = 0.
\]
\item[(iii)] Assume $u_0$ is smooth and that its derivative attains a negative minimum:
\[
m := \min_{\xi \in \mathbb{R}} u_0'(\xi) < 0.
\]
Using the condition from (ii), argue that the earliest possible such time is
\[
t_s = -\frac{1}{m} = -\frac{1}{\displaystyle\min_{\xi} u_0'(\xi)}.
\]
Interpret $t_s$ as the \emph{shock formation time}.
\end{enumerate}
Hint: You should get $\partial x/\partial \xi = 1 + t\,u_0'(\xi)$.

\smallskip
(d) \textbf{A concrete example: computing the shock time and location.}  

Consider the specific initial condition
\[
u_0(x) = -x.
\]

\begin{enumerate}
\item[(i)] Compute $u_0'(x)$ and the minimum value of $u_0'(x)$ over $\mathbb{R}$.
\item[(ii)] Use your formula for $t_s$ from part (c) to find the shock time for this initial condition.
\item[(iii)] Write down the explicit formula for the characteristics $x(t;\xi)$ for this $u_0$, and sketch a family of characteristic curves in the $(x,t)$-plane. From your sketch, identify the point $(x_s,t_s)$ where characteristics first intersect.
\item[(iv)] Using your expressions, describe qualitatively what happens to the shape of the graph $x \mapsto u(x,t)$ as $t$ approaches $t_s$ from below. How does this relate to the idea of “wave steepening”?
\end{enumerate}
% Hint: For this example, you should find $x(t;\xi) = \xi(1-t)$.

\smallskip
(e) \textbf{Extensions and “what if” questions.}

\begin{enumerate}
\item[(i)] Suppose instead that $u_0'(x) \ge 0$ for all $x$ (for example, $u_0(x) = x$). Use the formula $\partial x/\partial \xi = 1 + t u_0'(\xi)$ to argue that characteristics never intersect for $t>0$. What does this say about the long-time behavior of the solution, in contrast to the compressive case where $u_0'$ is negative somewhere?
\item[(ii)] (Conceptual) The inviscid Burgers’ equation is often “regularized” by adding viscosity:
\[
u_t + u\,u_x = \nu\,u_{xx}, \qquad \nu > 0.
\]
Based on your understanding of shock formation, briefly explain why adding the term $\nu u_{xx}$ prevents the appearance of a multi-valued solution, and instead leads to a sharp but smooth transition layer (a “viscous shock”) as $\nu \to 0^+$. You do not need to compute any explicit solutions; just describe the expected behavior qualitatively.
\end{enumerate}

\end{problem}

% ===== Example 2: Inviscid Burgers’ Equation and Shock Formation (full solution) =====
\begin{problem}[Inviscid Burgers’ Equation and Shock Formation]
Consider the inviscid Burgers’ equation
\[
u_t + u\,u_x = 0, \qquad x \in \mathbb{R},\ t>0,
\]
with smooth initial data $u(x,0) = u_0(x)$.

\begin{enumerate}
\item[(a)] Use the method of characteristics to show that along a characteristic curve starting from $(\xi,0)$, one has
\[
u(x(t;\xi),t) = u_0(\xi), \qquad x(t;\xi) = \xi + t\,u_0(\xi).
\]
\item[(b)] Compute $\dfrac{\partial x}{\partial \xi}(t;\xi)$ and show that the classical solution remains single‐valued as long as $\dfrac{\partial x}{\partial \xi}(t;\xi)>0$ for all $\xi$. Assuming
\[
m := \min_{\xi \in \mathbb{R}} u_0'(\xi) < 0,
\]
prove that the earliest time $t_s$ at which characteristics intersect is
\[
t_s = -\frac{1}{m} = -\frac{1}{\displaystyle\min_{\xi} u_0'(\xi)}.
\]
\item[(c)] For the specific initial data $u_0(x) = -x$:
\begin{enumerate}
\item[(i)] Compute $u_0'(x)$ and the shock time $t_s$.
\item[(ii)] Find $x(t;\xi)$ explicitly and determine the point $(x_s,t_s)$ where characteristics first intersect.
\item[(iii)] Describe qualitatively how the graph $x \mapsto u(x,t)$ behaves as $t \uparrow t_s$, and interpret this as wave steepening leading to shock formation.
\end{enumerate}
\end{enumerate}
\end{problem}

\begin{solution}
We analyze the inviscid Burgers’ equation
\[
u_t + u\,u_x = 0
\]
with smooth initial data $u(x,0) = u_0(x)$ using the method of characteristics. This equation is a nonlinear conservation law; the central idea is to move along the flow determined by $u$ itself so that the solution becomes constant along suitable curves in the $(x,t)$-plane.

\medskip
\noindent\textbf{(a) Characteristics and explicit form.}

We seek curves $t \mapsto x(t)$ for which the value of $u$ stays constant along the path. Consider a parametrization of a characteristic as $t \mapsto x(t;\xi)$, where $\xi$ labels the point where the curve meets the initial line $t=0$. Along such a curve, define
\[
U(t) := u(x(t;\xi),t).
\]
By the chain rule,
\[
\frac{dU}{dt} = u_t(x(t;\xi),t) + u_x(x(t;\xi),t)\,\frac{dx}{dt}.
\]
We now choose the characteristic velocity to be the advecting velocity from the PDE:
\[
\frac{dx}{dt} = u(x(t;\xi),t) = U(t).
\]
Substituting this into the expression for $dU/dt$ and using the PDE $u_t + u u_x = 0$, we obtain
\[
\frac{dU}{dt} 
= u_t + u_x\,\frac{dx}{dt}
= u_t + u_x\,u
= u_t + u\,u_x
= 0.
\]
Therefore, along each characteristic,
\[
\frac{d}{dt}u(x(t;\xi),t) = 0,
\]
so $u$ is constant along the characteristic. The constant value is determined by the initial condition at $t=0$.

On the initial line we have
\[
x(0;\xi) = \xi, \qquad u(x(0;\xi),0) = u_0(\xi).
\]
Since $dU/dt=0$ and $U(0) = u_0(\xi)$, we conclude
\[
u(x(t;\xi),t) = U(t) = U(0) = u_0(\xi)
\]
for all $t$ for which the characteristic is defined and $u$ remains smooth.

To find the characteristic paths $x(t;\xi)$, we use the equation
\[
\frac{dx}{dt} = u(x(t;\xi),t) = u_0(\xi),
\]
because $u$ is constant along each characteristic. This is now a simple linear ordinary differential equation in $t$:
\[
\frac{dx}{dt} = u_0(\xi), \qquad x(0;\xi) = \xi.
\]
Integrating, we find
\[
x(t;\xi) = \xi + t\,u_0(\xi).
\]
Thus the characteristic curves are straight lines in the $(x,t)$-plane with slope $u_0(\xi)$, and the solution along each such line is given by
\[
u(x(t;\xi),t) = u_0(\xi).
\]

As long as the mapping $\xi \mapsto x(t;\xi)$ can be inverted to solve for $\xi$ as a function of $(x,t)$, we can write the solution implicitly as
\[
u(x,t) = u_0(\xi(x,t)),
\]
where $\xi(x,t)$ is determined from the relation $x = \xi + t\,u_0(\xi)$.

\medskip
\noindent\textbf{(b) Intersection of characteristics and shock time.}

The characteristic map at fixed time $t$ is
\[
X_t : \xi \mapsto x(t;\xi) = \xi + t\,u_0(\xi).
\]
A classical (single-valued) solution exists as long as $X_t$ is one-to-one, so that each point $(x,t)$ comes from a unique initial label $\xi$. For a smooth mapping from $\mathbb{R}$ to $\mathbb{R}$, a sufficient condition for injectivity is that the derivative never vanishes and does not change sign. We compute
\[
\frac{\partial x}{\partial \xi}(t;\xi)
= \frac{\partial}{\partial \xi}\bigl(\xi + t\,u_0(\xi)\bigr)
= 1 + t\,u_0'(\xi).
\]

If $\partial x/\partial \xi>0$ for all $\xi$, then the map is strictly increasing and thus injective. In that case, the implicit formula
\[
x = \xi + t\,u_0(\xi), \qquad u(x,t) = u_0(\xi)
\]
defines a smooth classical solution.

Loss of injectivity first occurs when there exists some $\xi$ and $t>0$ such that
\[
\frac{\partial x}{\partial \xi}(t;\xi) = 0,
\]
because then two nearby characteristics become tangent and the mapping starts to fold. Using our expression,
\[
\frac{\partial x}{\partial \xi}(t;\xi) = 1 + t\,u_0'(\xi),
\]
the condition $\partial x/\partial \xi = 0$ becomes
\[
1 + t\,u_0'(\xi) = 0
\quad\Longleftrightarrow\quad
t = -\frac{1}{u_0'(\xi)}
\]
for those $\xi$ where $u_0'(\xi) < 0$.

Assume that $u_0$ is smooth and that its derivative attains a negative minimum
\[
m := \min_{\xi \in \mathbb{R}} u_0'(\xi) < 0.
\]
For each fixed $\xi$ with $u_0'(\xi) < 0$, the time at which the corresponding characteristic derivative vanishes is
\[
t(\xi) = -\frac{1}{u_0'(\xi)} > 0.
\]
The earliest time at which \emph{any} characteristic family loses injectivity is then
\[
t_s = \inf_{\{\xi: u_0'(\xi)<0\}} t(\xi)
= \inf_{\{\xi: u_0'(\xi)<0\}} \left(-\frac{1}{u_0'(\xi)}\right).
\]
Since $u_0'(\xi) \ge m$ for all $\xi$ and $m<0$, the function $-1/u_0'(\xi)$ is minimized when $u_0'(\xi)$ is as small as possible, that is, when $u_0'(\xi) = m$. Hence
\[
t_s = -\frac{1}{m}
= -\frac{1}{\displaystyle\min_{\xi} u_0'(\xi)} > 0.
\]
We call $t_s$ the \emph{shock formation time}. At $t = t_s$, the characteristic curves first develop tangencies and then intersections, and the formally constructed solution $u(x,t) = u_0(\xi(x,t))$ becomes multi-valued. In the theory of conservation laws, this signals the breakdown of the classical solution and the need to pass to weak (and then entropy) solutions, which develop a discontinuity (a shock) at or shortly after $t_s$.

\medskip
\noindent\textbf{(c) Example: $u_0(x) = -x$. Wave steepening and shock.}

We now specialize to the initial condition
\[
u_0(x) = -x.
\]

\smallskip
\emph{(i) Derivative and shock time.}

We have
\[
u_0'(x) = -1
\]
for all $x \in \mathbb{R}$. Thus
\[
m = \min_{x\in\mathbb{R}} u_0'(x) = -1.
\]
Using the general formula for the shock time,
\[
t_s = -\frac{1}{m} = -\frac{1}{-1} = 1.
\]
Therefore, the solution remains a smooth classical solution for $0 \le t < 1$, and characteristics first intersect at time $t=1$.

\smallskip
\emph{(ii) Characteristics and intersection point.}

For this $u_0$, the characteristic equation becomes
\[
x(t;\xi) = \xi + t\,u_0(\xi) = \xi + t(-\xi) = \xi(1 - t).
\]
Along such a characteristic,
\[
u(x(t;\xi),t) = u_0(\xi) = -\xi.
\]

The family of characteristic curves is therefore
\[
x = \xi(1-t), \quad t \ge 0, \quad \xi \in \mathbb{R}.
\]
For $0 \le t < 1$, each line is straight with slope in the $x$–$t$ plane determined by $u_0(\xi)$, and the factor $(1-t)$ simply compresses the $x$-coordinate. As $t \uparrow 1$, we see that
\[
x(1;\xi) = \xi(1-1) = 0
\]
for \emph{every} $\xi$. Thus all characteristics meet at the single point
\[
(x_s, t_s) = (0, 1).
\]
This is the first time and place where the mapping $\xi \mapsto x(t;\xi)$ fails to be one-to-one: for $t<1$,
\[
\frac{\partial x}{\partial \xi}(t;\xi) = 1 + t\,u_0'(\xi) = 1 + t(-1) = 1 - t > 0,
\]
so the map is strictly increasing; at $t=1$, the derivative vanishes everywhere.

\smallskip
\emph{(iii) Shape of $u(x,t)$ and wave steepening.}

For $0 \le t < 1$, we can invert the relation $x = \xi(1-t)$ to obtain $\xi = x/(1-t)$. Then the solution can be written explicitly as
\[
u(x,t) = u_0\!\left(\frac{x}{1-t}\right)
= -\frac{x}{1-t}.
\]
Thus, for any fixed $t \in [0,1)$, the profile $x \mapsto u(x,t)$ is a straight line with slope
\[
\frac{\partial u}{\partial x}(x,t) = -\frac{1}{1-t},
\]
which is negative and whose magnitude grows without bound as $t \uparrow 1$:
\[
\left|\frac{\partial u}{\partial x}(x,t)\right|
= \frac{1}{1-t} \to \infty \quad \text{as } t \to 1^-.
\]
This means the wave profile becomes increasingly steep. Graphically, the line $u(x,t)$ pivots and steepens, and at $t=1$ the slope becomes infinite, corresponding to vertical tangents in the $(x,u)$-plane. In the characteristic picture, this is the same as the straight characteristic lines collapsing onto $(0,1)$.

This phenomenon is called \emph{wave steepening}: regions where $u_0'(x)$ is negative are “compressive,” causing characteristics to converge and the gradient $u_x$ to blow up in finite time. In the full theory of conservation laws, the physically relevant solution beyond $t_s$ includes a discontinuity (a shock) at $x=0$, $t\ge1$, rather than a multi-valued profile.

\medskip
\noindent\textbf{Connection to the Burgers’ equation case study.}

This example encapsulates the key ideas discussed in the case study on Burgers’ equation. The method of characteristics reduces the nonlinear PDE to a family of ODEs, revealing that $u$ is transported along curves whose speed depends on $u$ itself. Because different parts of the wave travel at different speeds, compressive regions (where $u_0'$ is negative) produce intersecting characteristics in finite time. The explicit formula
\[
t_s = -\frac{1}{\min u_0'}
\]
quantifies the shock formation time for smooth data. For the simple initial profile $u_0(x)=-x$, we can see explicitly how the solution steepens and how characteristics collapse, which makes this a canonical example for understanding nonlinear self-advection, finite-time gradient blow-up, and the onset of shocks in conservation laws.

\end{solution}

% ===== Example 3: Viscous Burgers’ Equation and the Hopf–Cole Transform (inquiry-based) =====
\begin{problem}[Viscous Burgers’ Equation and the Hopf–Cole Transform]
The scalar Burgers’ equation is a classical toy model for nonlinear waves, traffic flow, and turbulence. In its inviscid form it behaves like a nonlinear transport equation and can form shocks in finite time. If we add a small viscosity, the equation becomes parabolic: nonlinear steepening now competes with diffusive smoothing. In this problem you will discover the Hopf–Cole transform, a logarithmic change of variables that converts the nonlinear viscous Burgers’ equation into the linear heat equation, and then use it to build solutions.

Consider the \emph{viscous Burgers’ equation} on the real line
\[
u_t + u\,u_x \;=\; \nu\,u_{xx}, \qquad t>0,\ x\in\mathbb{R},\quad \nu>0.
\]

\smallskip
\noindent
(a) \textbf{From inviscid to viscous Burgers.}
Recall that the inviscid Burgers’ equation can be written in conservation form as
\[
u_t + \left(\frac{u^2}{2}\right)_x = 0.
\]
Explain briefly (you may appeal to the method of characteristics) why solutions of the inviscid Burgers’ equation with smooth initial data can develop shocks in finite time. Then show that the viscous Burgers’ equation can be written in the conservation–diffusion form
\[
u_t + \left(\frac{u^2}{2}\right)_x = \nu\,u_{xx}.
\]
Why does the added term $\nu u_{xx}$ have a smoothing, or regularizing, effect on solutions?

\medskip
\noindent
(b) \textbf{Guessing a transform: from $u$ to $\phi$.}
Our goal is to transform the nonlinear equation for $u$ into a linear equation for a new unknown. Consider the change of variables
\[
u(t,x) = -\,2\nu\,\frac{\phi_x(t,x)}{\phi(t,x)},
\]
where $\phi(t,x)$ is assumed to be positive and smooth.

\begin{itemize}
\item[(i)] Compute $u_x$ and $u_{xx}$ in terms of $\phi$ and its derivatives.
\item[(ii)] Compute $u_t$ in terms of $\phi$ and its derivatives.
\item[(iii)] Substitute these expressions into the viscous Burgers’ equation and simplify.
\end{itemize}
Show that, under this substitution, $u$ satisfies the viscous Burgers’ equation if and only if $\phi$ satisfies a \emph{linear} partial differential equation. Identify this equation.

\emph{Hint:} It is much easier to work with $w = \ln \phi$ first. Note that
\[
\frac{\phi_x}{\phi} = w_x,\qquad \frac{\phi_t}{\phi} = w_t,
\]
and express $u$, $u_x$, $u_{xx}$, and $u_t$ in terms of $w$ and its derivatives before simplifying.

\medskip
\noindent
(c) \textbf{Relating initial data for $u$ and for $\phi$.}
Suppose we are given initial data
\[
u(0,x) = u_0(x),
\]
where $u_0$ is a smooth function that decays sufficiently fast as $|x|\to\infty$.

\begin{itemize}
\item[(i)] Using the relation $u = -2\nu \phi_x/\phi$, derive an ordinary differential equation in $x$ for the initial profile $\phi_0(x) := \phi(0,x)$ in terms of $u_0(x)$.
\item[(ii)] Solve this ODE to express $\phi_0(x)$ in terms of $u_0(x)$, up to a multiplicative constant.
\item[(iii)] Argue why this multiplicative constant does not affect the corresponding solution $u(t,x)$.
\end{itemize}

\emph{Hint:} Your ODE should be first order and separable. You may find it helpful to integrate from a fixed reference point, such as $x=0$.

\medskip
\noindent
(d) \textbf{Solving the linear problem and transforming back.}
In part (b) you should have found that $\phi$ satisfies the heat equation
\[
\phi_t = \nu\,\phi_{xx}
\]
with initial data $\phi(0,x) = \phi_0(x)$ obtained in part (c).

\begin{itemize}
\item[(i)] Recall (or look up) the heat kernel on $\mathbb{R}$,
\[
G_\nu(t,x) = \frac{1}{\sqrt{4\pi \nu t}}\exp\!\left(-\frac{x^2}{4\nu t}\right), \qquad t>0.
\]
Write $\phi(t,x)$ as a convolution of $G_\nu$ with $\phi_0$.

\item[(ii)] Use $u = -2\nu\,\phi_x/\phi$ to express $u(t,x)$ in terms of $\phi_0$ and the heat kernel. You may leave your answer in the form
\[
u(t,x) = -2\nu\,\partial_x \ln \Phi(t,x),
\]
for a suitable integral expression $\Phi(t,x)$.

\item[(iii)] Now obtain an explicit example by \emph{choosing} a simple initial profile for $\phi$:
\[
\phi_0(x) = 1 + e^{-x^2}.
\]
First compute the corresponding initial velocity $u_0(x)$ using the relation
\[
u_0(x) = -2\nu\,\frac{\phi_0'(x)}{\phi_0(x)}.
\]
Then solve the heat equation with this initial data. (You may use without proof that the convolution of two Gaussian functions is again a Gaussian function with an explicitly computable variance.) Finally, compute $u(t,x)$ from the formula $u = -2\nu\,\phi_x/\phi$.
\end{itemize}

\emph{Hint:} For the convolution step, you will need to compute
\[
\bigl(G_\nu(t,\cdot)*e^{-(\cdot)^2}\bigr)(x) = \int_{-\infty}^{\infty}
\frac{1}{\sqrt{4\pi \nu t}}\exp\!\left(-\frac{(x-y)^2}{4\nu t}\right)e^{-y^2}\,dy.
\]
Rewrite the exponent as a quadratic polynomial in $y$ and complete the square.

\medskip
\noindent
(e) \textbf{Extensions and limits.}

\begin{itemize}
\item[(i)] Consider the \emph{inviscid} Burgers’ equation
\[
u_t + u\,u_x = 0
\]
with the same initial data $u(0,x) = u_0(x)$ obtained from $\phi_0(x) = 1 + e^{-x^2}$. Use the method of characteristics (you do not need to give full details) to explain qualitatively what you expect the solution to look like as $t$ increases. How does this compare to the viscous solution you found in part (d)(iii)?

\item[(ii)] Suppose instead that the spatial domain is the $2\pi$-periodic interval and that $u$ is $2\pi$-periodic in $x$. How would you adapt the Hopf–Cole transform and the solution method? In particular, what replaces the heat kernel representation of $\phi$?

\item[(iii)] (Conceptual question.) Based on your formulas, what do you expect to happen in the limit $\nu\to 0^+$ for fixed $t>0$? How might this limit be related to shock formation in the inviscid Burgers’ equation?
\end{itemize}
\end{problem}

% ===== Example 3: Viscous Burgers’ Equation and the Hopf–Cole Transform (full solution) =====
\begin{problem}[Viscous Burgers’ Equation and the Hopf–Cole Transform]
Consider the viscous Burgers’ equation on $\mathbb{R}$,
\[
u_t + u\,u_x = \nu\,u_{xx}, \qquad t>0,\ x\in\mathbb{R},\quad \nu>0.
\]
\begin{enumerate}
\item[(a)] Show that the change of variables
\[
u(t,x) = -\,2\nu\,\frac{\phi_x(t,x)}{\phi(t,x)}
\]
transforms the viscous Burgers’ equation into the linear heat equation
\[
\phi_t = \nu\,\phi_{xx}.
\]
\item[(b)] Given smooth initial data $u(0,x)=u_0(x)$ decaying sufficiently fast at infinity, express the corresponding initial data $\phi(0,x)=\phi_0(x)$ in terms of $u_0(x)$, up to a multiplicative constant. Then use the heat kernel
\[
G_\nu(t,x) = \frac{1}{\sqrt{4\pi \nu t}}\exp\!\left(-\frac{x^2}{4\nu t}\right)
\]
to obtain an integral representation of $u(t,x)$.

\item[(c)] As a concrete example, take
\[
\phi_0(x) = 1 + e^{-x^2}.
\]
Compute the induced initial velocity
\[
u_0(x) = -\,2\nu\,\frac{\phi_0'(x)}{\phi_0(x)},
\]
solve the heat equation for $\phi(t,x)$ with this initial data, and then compute $u(t,x)$. Briefly explain how this viscous solution illustrates the competition between nonlinear steepening and diffusive smoothing, in comparison with the inviscid Burgers’ equation.
\end{enumerate}
\end{problem}

\begin{solution}
We study the scalar viscous Burgers’ equation
\[
u_t + u\,u_x = \nu\,u_{xx},
\]
which is a nonlinear conservation law with an added diffusive term. The central idea is the Hopf–Cole transform, which converts this nonlinear parabolic equation into the linear heat equation.

\medskip
\noindent\textbf{(a) The Hopf–Cole transform and the heat equation.}
We introduce a new function $\phi(t,x)$, assumed positive and smooth, by
\[
u(t,x) = -\,2\nu\,\frac{\phi_x(t,x)}{\phi(t,x)}.
\]
It is convenient to write $\phi = e^{w}$, so that $w = \ln\phi$ and
\[
\frac{\phi_x}{\phi} = w_x,\qquad \frac{\phi_t}{\phi} = w_t.
\]
Then the transformation becomes
\[
u = -2\nu\,w_x.
\]
Differentiating with respect to $x$ and $t$ yields
\[
u_x = -2\nu\,w_{xx},\qquad
u_{xx} = -2\nu\,w_{xxx},\qquad
u_t = -2\nu\,w_{xt}.
\]
Substituting these expressions into the viscous Burgers’ equation gives
\[
-2\nu\,w_{xt} + \bigl(-2\nu\,w_x\bigr)\bigl(-2\nu\,w_{xx}\bigr)
= \nu\bigl(-2\nu\,w_{xxx}\bigr).
\]
We now simplify:
\[
-2\nu\,w_{xt} + 4\nu^2 w_x w_{xx} = -2\nu^2 w_{xxx}.
\]
Divide both sides by $-2\nu$:
\[
w_{xt} - 2\nu w_x w_{xx} = \nu w_{xxx}.
\]
On the other hand, if we compute the equation satisfied by $w$ under the assumption that $\phi$ solves the heat equation $\phi_t = \nu\phi_{xx}$, we obtain
\[
\frac{\phi_t}{\phi} = \frac{\nu\phi_{xx}}{\phi}
\quad\Longrightarrow\quad
w_t = \nu\,\frac{\phi_{xx}}{\phi}.
\]
Using $\phi = e^w$, we have
\[
\phi_x = \phi w_x,\qquad
\phi_{xx} = \phi(w_{xx} + (w_x)^2).
\]
Hence
\[
w_t = \nu\,(w_{xx} + (w_x)^2).
\]
Differentiating this relation with respect to $x$ yields
\[
w_{xt} = \nu(w_{xxx} + 2 w_x w_{xx}).
\]
Rewriting,
\[
w_{xt} - 2\nu w_x w_{xx} = \nu w_{xxx},
\]
which is exactly the relation obtained by substituting the transform into the viscous Burgers’ equation.

Thus:

\smallskip
\begin{itemize}
\item If $\phi$ satisfies the heat equation $\phi_t = \nu\phi_{xx}$, then $w=\ln\phi$ satisfies $w_t = \nu(w_{xx} + (w_x)^2)$, and consequently $u=-2\nu w_x$ satisfies $u_t + u u_x = \nu u_{xx}$.
\item Conversely, if $u$ solves viscous Burgers and can be written as $u = -2\nu\,\phi_x/\phi$ with $\phi>0$, then $w=\ln\phi$ satisfies the above quasi-linear equation, which in turn is equivalent to $\phi$ satisfying the heat equation.
\end{itemize}

Therefore the Hopf–Cole transform $u = -2\nu \phi_x/\phi$ linearizes the viscous Burgers’ equation into the heat equation
\[
\phi_t = \nu\,\phi_{xx}.
\]

\medskip
\noindent\textbf{(b) Initial data and integral representation.}
We are given $u(0,x)=u_0(x)$, a smooth, decaying function. At time $t=0$ the relation $u = -2\nu \phi_x/\phi$ becomes
\[
u_0(x) = -\,2\nu\,\frac{\phi_0'(x)}{\phi_0(x)},
\qquad \phi_0(x):=\phi(0,x).
\]
This is a first-order linear ordinary differential equation in $x$ for $\phi_0$:
\[
\frac{\phi_0'(x)}{\phi_0(x)} = -\frac{1}{2\nu}u_0(x).
\]
Separating variables and integrating from $0$ to $x$,
\[
\int_{0}^{x} \frac{\phi_0'(y)}{\phi_0(y)}\,dy
= -\frac{1}{2\nu}\int_{0}^{x} u_0(y)\,dy.
\]
The left-hand side is $\ln\
hand side is $\ln\phi_0(x)-\ln\phi_0(0)$, so we obtain
\[
\ln\frac{\phi_0(x)}{\phi_0(0)}
= -\frac{1}{2\nu}\int_{0}^{x} u_0(y)\,dy.
\]
Exponentiating gives
\[
\phi_0(x) = \phi_0(0)\,\exp\!\left(-\frac{1}{2\nu}\int_{0}^{x} u_0(y)\,dy\right)
= C \exp\!\left(-\frac{1}{2\nu}\int_{0}^{x} u_0(y)\,dy\right),
\]
where $C>0$ is an arbitrary constant.

This constant has no effect on $u$, because
\[
u(t,x) = -2\nu\,\frac{\phi_x(t,x)}{\phi(t,x)}
\]
is invariant under the scaling $\phi \mapsto C\phi$ (both numerator and denominator are multiplied by $C$).

Since $\phi$ satisfies the heat equation with initial data $\phi_0$, the solution is given by convolution with the heat kernel,
\[
\phi(t,x) = \bigl(G_\nu(t,\cdot)*\phi_0\bigr)(x)
= \int_{\mathbb{R}} G_\nu(t,x-y)\,\phi_0(y)\,dy,
\]
where
\[
G_\nu(t,x) = \frac{1}{\sqrt{4\pi\nu t}}\exp\!\left(-\frac{x^2}{4\nu t}\right).
\]
Thus
\[
u(t,x) = -2\nu\,\frac{\partial_x\phi(t,x)}{\phi(t,x)}
= -2\nu\,\partial_x \ln\Phi(t,x),
\]
where
\[
\Phi(t,x) := \int_{\mathbb{R}} G_\nu(t,x-y)\,\phi_0(y)\,dy.
\]
Equivalently,
\[
u(t,x)
= -2\nu\,\frac{\displaystyle\int_{\mathbb{R}} \partial_x G_\nu(t,x-y)\,\phi_0(y)\,dy}
{\displaystyle\int_{\mathbb{R}} G_\nu(t,x-y)\,\phi_0(y)\,dy}.
\]

\medskip
\noindent\textbf{(c) Explicit example with $\phi_0(x)=1+e^{-x^2}$.}

\smallskip
\noindent\emph{Initial velocity.}
Here
\[
\phi_0(x) = 1 + e^{-x^2},\qquad
\phi_0'(x) = -2x\,e^{-x^2},
\]
so
\[
u_0(x) = -2\nu\,\frac{\phi_0'(x)}{\phi_0(x)}
= -2\nu\,\frac{-2x\,e^{-x^2}}{1+e^{-x^2}}
= \frac{4\nu x\,e^{-x^2}}{1+e^{-x^2}}.
\]

\smallskip
\noindent\emph{Solving the heat equation for $\phi$.}
By linearity,
\[
\phi(t,x) = \bigl(G_\nu(t,\cdot)*\phi_0\bigr)(x)
= \bigl(G_\nu(t,\cdot)*1\bigr)(x)
 + \bigl(G_\nu(t,\cdot)*e^{-(\cdot)^2}\bigr)(x).
\]
The first term is simply $1$, because $G_\nu$ has total mass $1$. For the second term,
\[
\bigl(G_\nu(t,\cdot)*e^{-(\cdot)^2}\bigr)(x)
= \int_{-\infty}^{\infty}
\frac{1}{\sqrt{4\pi \nu t}}
\exp\!\left(-\frac{(x-y)^2}{4\nu t}\right)e^{-y^2}\,dy.
\]
Completing the square in the exponent and evaluating the Gaussian integral (or using the standard formula for the evolution of a Gaussian under the heat equation) yields
\[
\bigl(G_\nu(t,\cdot)*e^{-(\cdot)^2}\bigr)(x)
= \frac{1}{\sqrt{1+4\nu t}}
\exp\!\left(-\frac{x^2}{1+4\nu t}\right).
\]
Hence
\[
\phi(t,x)
= 1 + \frac{1}{\sqrt{1+4\nu t}}
\exp\!\left(-\frac{x^2}{1+4\nu t}\right).
\]

\smallskip
\noindent\emph{Computing $u(t,x)$.}
Differentiate $\phi$ with respect to $x$:
\[
\phi_x(t,x)
= \frac{1}{\sqrt{1+4\nu t}}\,
\frac{d}{dx}\exp\!\left(-\frac{x^2}{1+4\nu t}\right)
= -\,\frac{2x}{(1+4\nu t)^{3/2}}
\exp\!\left(-\frac{x^2}{1+4\nu t}\right).
\]
Therefore
\[
u(t,x)
= -2\nu\,\frac{\phi_x(t,x)}{\phi(t,x)}
= \frac{4\nu x}{(1+4\nu t)^{3/2}}\,
\frac{\exp\!\left(-\frac{x^2}{1+4\nu t}\right)}
{1 + \dfrac{1}{\sqrt{1+4\nu t}}
\exp\!\left(-\frac{x^2}{1+4\nu t}\right)}.
\]
This is a smooth, globally defined solution of the viscous Burgers’ equation for all $t>0$.

\smallskip
\noindent\emph{Qualitative behavior.}
The Gaussian component in $\phi(t,x)$ spreads and decreases in amplitude on the diffusive length scale $\sqrt{\nu t}$, and this spreading is inherited by $u(t,x)$ through the logarithmic derivative. The nonlinear term $u\,u_x$ tends to steepen gradients and transport the profile, while the viscous term $\nu u_{xx}$ spreads and smooths it. The explicit formula for $u(t,x)$ remains bounded with gradients that decay in time, illustrating how viscosity prevents shock formation and enforces smoothing, in contrast with the inviscid Burgers’ equation, where the same initial velocity profile would typically develop a shock in finite time.

\end{solution}

% ===== Example 4: Traveling Wave and Shock Profile Solutions (inquiry-based) =====
\begin{problem}[Traveling Wave and Shock Profile Solutions]
In this problem we study traveling wave solutions of the \emph{viscous Burgers' equation}
\[
u_t + \frac{1}{2}(u^2)_x = \nu\,u_{xx}, \qquad \nu>0,
\]
which is a simple model for a scalar conservation law with dissipation. We are interested in waves that move with constant speed and connect two different constant states at infinity. These solutions model a ``smoothed'' shock wave whose steepness depends on the viscosity $\nu$ and whose speed is determined by a conservation-law condition (the Rankine--Hugoniot jump condition).

We will assume a traveling wave ansatz and reduce the partial differential equation to an ordinary differential equation. We will then integrate this ODE explicitly and interpret the resulting profile.

\smallskip

Let $u(x,t)$ solve
\[
u_t + \frac{1}{2}(u^2)_x = \nu\,u_{xx}.
\]
We look for traveling wave solutions of the form
\[
u(x,t) = U(\xi), \qquad \xi = x - st,
\]
that satisfy the asymptotic conditions
\[
\lim_{\xi\to -\infty} U(\xi) = u_-, 
\qquad 
\lim_{\xi\to +\infty} U(\xi) = u_+,
\]
for given constants $u_-$ and $u_+$ with $u_- \neq u_+$, and some wave speed $s$ to be determined.

\begin{enumerate}[(a)]
\item (Setting up the traveling wave ODE)  
Compute $u_t$, $u_x$, and $u_{xx}$ in terms of $U$ and its derivatives with respect to $\xi$. Substitute the ansatz $u(x,t)=U(x-st)$ into Burgers' equation and simplify to obtain an ordinary differential equation for $U(\xi)$.

State your final ODE in the form
\[
\nu U''(\xi) + (s - U(\xi))\,U'(\xi) = 0.
\]

% Hint: Use the chain rule: $u_t = -s\,U'(\xi)$, $u_x = U'(\xi)$, $u_{xx} = U''(\xi)$.

\item (First integration and the role of the far-field states)  
The ODE from part (a) is autonomous (it does not depend explicitly on $\xi$), so it can be integrated once.

\begin{enumerate}
\item Regard $U'(\xi)$ as a function of $U$ and use the chain rule $U'' = \dfrac{dU'}{d\xi} = \dfrac{dU'}{dU}U'$ to rewrite the second-order ODE as a first-order ODE for $p(U) := U'(\xi)$ as a function of $U$.

\item Solve this first-order ODE to obtain an expression of the form
\[
U'(\xi) = \frac{1}{2\nu}\bigl((U(\xi)-s)^2 - C\bigr),
\]
for some constant $C$.

\end{enumerate}

\textit{Hint:} After substituting $U'' = p'(U)U'$, divide by $U'$ (assuming $U'$ is not identically zero) to reduce to a first-order linear ODE for $p(U)$.

\item (Determining the wave speed: Rankine--Hugoniot condition)  
Use the asymptotic conditions at $\xi\to\pm\infty$ to determine both the constant $C$ and the wave speed $s$ in terms of $u_-$ and $u_+$.

\begin{enumerate}
\item Explain why
\[
\lim_{\xi\to\pm\infty} U'(\xi) = 0.
\]

\item Evaluate the first-integral expression for $U'(\xi)$ as $\xi\to\pm\infty$ and show that $C = (u_\pm - s)^2$.

\item By equating the two expressions for $C$ corresponding to $u_-$ and $u_+$, derive a formula for $s$ in terms of $u_-$ and $u_+$. Show that
\[
s = \frac{u_- + u_+}{2}.
\]

\end{enumerate}

\textit{Hint:} At $\xi\to\pm\infty$, both $U(\xi)$ and $U'(\xi)$ should approach constant limits; use these to evaluate the constant of integration from part (b).

\item (Solving for the traveling wave profile)  
With the value of $s$ from part (c), rewrite the first-integral equation in a factorized form involving $(U-u_-)$ and $(U-u_+)$, and then solve explicitly for $U(\xi)$.

\begin{enumerate}
\item Show that, with $s = (u_- + u_+)/2$, the first-integral can be written as
\[
U'(\xi) = \frac{1}{2\nu}\,(U(\xi) - u_-)\,(U(\xi) - u_+).
\]

\item Rewrite this as a separable ODE and integrate to obtain an implicit formula for $U(\xi)$ of the form
\[
\frac{U(\xi)-u_-}{U(\xi)-u_+} = C\,e^{\alpha \xi},
\]
for suitable constants $C$ and $\alpha$ depending on $u_\pm$ and $\nu$.

\item Choose a convenient shift of the coordinate $\xi$ (for instance, by requiring $U(0) = s$) to determine $C$, and manipulate the exponential expression into a hyperbolic tangent form. Show that one can write the solution as
\[
U(\xi)
= \frac{u_- + u_+}{2}
  - \frac{u_- - u_+}{2}
    \tanh\!\left(
      \frac{(u_- - u_+)\,\xi}{4\nu}
    \right),
\]
for the case $u_- > u_+$.

\end{enumerate}

\textit{Hint:} For the last step, recall the identity
\[
\tanh\left(\frac{y}{2}\right) 
= \frac{1 - e^{-y}}{1 + e^{-y}},
\]
and try to rearrange your exponential expression into the form of a ratio similar to the right-hand side.

\item (Interpretation and extensions)  

\begin{enumerate}
\item Describe in words the qualitative shape of the profile $U(\xi)$ when $u_- > u_+$. What happens to the profile as $\nu \to 0^+$? How does the ``thickness'' of the transition layer scale with $\nu$?

\item What changes in your analysis, if any, when $u_- < u_+$ (that is, when the left state is smaller than the right state)? What does the traveling wave solution look like in that case?

\item (Optional extension) Burgers' equation without viscosity is
\[
u_t + \frac{1}{2}(u^2)_x = 0.
\]
Based on your computations above, argue informally that in the inviscid limit $\nu\to 0^+$, the viscous traveling wave solution converges to a discontinuous shock moving with speed $s = (u_- + u_+)/2$. How does this speed relate to the Rankine--Hugoniot condition for the conservation law?
\end{enumerate}

\end{enumerate}
\end{problem}

% ===== Example 4: Traveling Wave and Shock Profile Solutions (full solution) =====
\begin{problem}[Traveling Wave and Shock Profile Solutions]
Consider the viscous Burgers' equation
\[
u_t + \frac{1}{2}(u^2)_x = \nu\,u_{xx}, \qquad \nu>0.
\]
Seek traveling wave solutions of the form $u(x,t) = U(\xi)$ with $\xi = x - st$ that satisfy
\[
\lim_{\xi\to -\infty} U(\xi) = u_-, \qquad \lim_{\xi\to +\infty} U(\xi) = u_+,
\]
for given constants $u_\pm$ with $u_- \neq u_+$.  

\begin{enumerate}[(i)]
\item Derive the ordinary differential equation for $U(\xi)$ and integrate it once to obtain a first-integral.
\item Use the far-field conditions to determine the wave speed $s$ in terms of $u_-$ and $u_+$, and show that $s = (u_- + u_+)/2$.
\item Show that $U$ satisfies a separable first-order ODE and solve it explicitly to obtain the traveling wave profile. For $u_- > u_+$, show that
\[
U(\xi)
= \frac{u_- + u_+}{2}
  - \frac{u_- - u_+}{2}
    \tanh\!\left(
      \frac{(u_- - u_+)\,\xi}{4\nu}
    \right).
\]
\item Briefly describe what happens to this profile as $\nu \to 0^+$ and relate the wave speed to the Rankine--Hugoniot condition for the inviscid conservation law $u_t + \tfrac12 (u^2)_x = 0$.
\end{enumerate}
\end{problem}

\begin{solution}
We are studying a model conservation law with diffusion,
\[
u_t + \frac{1}{2}(u^2)_x = \nu\,u_{xx},
\]
and we look for traveling waves of the form $u(x,t) = U(\xi)$, where $\xi = x - st$ and $s$ is a constant speed to be determined. The profile $U$ is required to connect two distinct constant states $u_-$ and $u_+$ at spatial infinity.

\medskip

\noindent\textbf{(i) Deriving and integrating the ODE.}
Using the chain rule with $\xi = x - st$, we compute
\[
u_t = \frac{\partial U}{\partial \xi}\,\frac{\partial \xi}{\partial t}
= -s\,U'(\xi), 
\quad
u_x = U'(\xi),
\quad
u_{xx} = U''(\xi),
\]
where primes denote derivatives with respect to $\xi$. Also,
\[
(u^2)_x = (U(\xi)^2)_x = \frac{\mathrm{d}}{\mathrm{d}\xi}(U(\xi)^2)\,\frac{\partial \xi}{\partial x}
= 2U(\xi)U'(\xi).
\]
Substituting into Burgers' equation gives
\[
-s\,U'(\xi) + \frac{1}{2}\cdot 2U(\xi)U'(\xi) = \nu\,U''(\xi),
\]
that is,
\[
-s\,U' + U\,U' = \nu\,U''.
\]
Rearranging, we obtain the second-order ordinary differential equation
\[
\nu U''(\xi) + (s - U(\xi))\,U'(\xi) = 0.
\]

This ODE is autonomous (it does not depend explicitly on $\xi$), so we integrate it once. Introduce $p(U) := U'(\xi)$ and use the chain rule
\[
U''(\xi) = \frac{\mathrm{d}U'}{\mathrm{d}\xi} 
= \frac{\mathrm{d}p}{\mathrm{d}U}\,\frac{\mathrm{d}U}{\mathrm{d}\xi}
= p'(U)\,p(U).
\]
Substituting $U'' = p'(U)p(U)$ and $U' = p(U)$ into the ODE gives
\[
\nu\,p'(U)\,p(U) + (s - U)\,p(U) = 0.
\]
Assuming the profile is nontrivial so that $p(U)\not\equiv 0$, we may divide by $p(U)$ to obtain the first-order linear ODE
\[
\nu\,p'(U) + s - U = 0.
\]
Solving for $p'(U)$,
\[
p'(U) = \frac{U - s}{\nu}.
\]
Integrating with respect to $U$ yields
\[
p(U) = \int \frac{U - s}{\nu}\,\mathrm{d}U
= \frac{1}{2\nu}(U - s)^2 + C_1,
\]
where $C_1$ is a constant of integration. Returning to the original notation $p(U) = U'(\xi)$, we have the first-integral
\[
U'(\xi) = \frac{1}{2\nu}(U(\xi) - s)^2 + C_1.
\]
It will be convenient to denote $C := -2\nu C_1$, so we can rewrite this as
\[
U'(\xi) = \frac{1}{2\nu}\left[(U(\xi) - s)^2 - C\right],
\]
which is the desired first-integral form.

\medskip

\noindent\textbf{(ii) Determining the wave speed via the far-field states.}
We are given that
\[
\lim_{\xi\to -\infty}U(\xi) = u_-,
\qquad
\lim_{\xi\to +\infty}U(\xi) = u_+,
\]
with $u_- \neq u_+$. Because the profile is smooth and connects two constants, the derivative $U'(\xi)$ tends to zero at both infinities:
\[
\lim_{\xi\to\pm\infty} U'(\xi) = 0.
\]
Intuitively, far from the transition region the solution has settled down to a constant, so its spatial derivative vanishes.

We use this information in the first-integral. As $\xi\to -\infty$, we have $U(\xi)\to u_-$ and $U'(\xi)\to 0$, so
\[
0 = \lim_{\xi\to -\infty} U'(\xi)
= \lim_{\xi\to -\infty} \frac{1}{2\nu}\Bigl[(U(\xi) - s)^2 - C\Bigr]
= \frac{1}{2\nu}\Bigl[(u_- - s)^2 - C\Bigr].
\]
Thus
\[
C = (u_- - s)^2.
\]
Similarly, as $\xi\to +\infty$,
\[
0 = \lim_{\xi\to +\infty} U'(\xi)
= \frac{1}{2\nu}\Bigl[(u_+ - s)^2 - C\Bigr],
\]
so
\[
C = (u_+ - s)^2.
\]
Equating the two expressions for $C$ gives
\[
(u_- - s)^2 = (u_+ - s)^2.
\]
Taking square roots produces two possibilities:
\[
u_- - s = u_+ - s
\quad\text{or}\quad
u_- - s = -(u_+ - s).
\]
The first possibility $u_- - s = u_+ - s$ would imply $u_- = u_+$, which contradicts $u_- \neq u_+$. Therefore we must have
\[
u_- - s = -(u_+ - s),
\]
which simplifies to
\[
u_- - s = -u_+ + s
\quad\Longrightarrow\quad
2s = u_- + u_+.
\]
Thus the wave speed is
\[
s = \frac{u_- + u_+}{2}.
\]

This is precisely the Rankine--Hugoniot velocity for the conservation law $u_t + \tfrac12 (u^2)_x = 0$, because in general the shock speed $s$ satisfies
\[
s = \frac{f(u_+) - f(u_-)}{u_+ - u_-}
\quad\text{with}\quad f(u) = \frac{u^2}{2}.
\]
Evaluating,
\[
s = \frac{\tfrac12 u_+^2 - \tfrac12 u_-^2}{u_+ - u_-}
= \frac{(u_+ - u_-)(u_+ + u_-)}{2(u_+ - u_-)}
= \frac{u_- + u_+}{2},
\]
in agreement with our computation.

\medskip

\noindent\textbf{(iii) Separable first-order ODE and explicit profile.}
Now that $s$ is known, we can simplify the first-integral. From $C = (u_- - s)^2 = (u_+ - s)^2$ and $s = (u_- + u_+)/2$, it is straightforward to check that
\[
(u - s)^2 - C = (u - u_-)(u - u_+)
\]
for any real $u$. Indeed,
\[
(u - s)^2 - (u_- - s)^2 = (u - u_-)(u - u_+),
\]
which can be verified by direct algebra or by factoring the left-hand side as a difference of squares. Substituting this into our first-integral,
\[
U'(\xi) = \frac{1}{2\nu}\bigl((U(\xi) - s)^2 - C\bigr)
= \frac{1}{2\nu}\,(U(\xi) - u_-)(U(\xi) - u_+).
\]
Thus $U$ satisfies the first-order autonomous ODE
\[
U'(\xi) = \frac{1}{2\nu}\,(U(\xi) - u_-)(U(\xi) - u_+).
\]

This equation is separable. Assuming $U(\xi)$ stays between $u_-$ and $u_+$ (which will be the case for the monotone shock profile), we may write
\[
\frac{\mathrm{d}U}{(U - u_-)(U - u_+)} = \frac{\mathrm{d}\xi}{2\nu}.
\]
We integrate both sides. Using partial fractions,
\[
\frac{1}{(U - u_-)(U - u_+)}
= \frac{1}{u_- - u_+}\left(\frac{1}{U - u_-} - \frac{1}{U - u_+}\right).
\]
Integrating with respect to $U$ gives
\[
\int \frac{\mathrm{d}U}{(U - u_-)(U - u_+)}
= \frac{1}{u_- - u_+}\ln\left|\frac{U - u_-}{U - u_+}\right| + C_2,
\]
where $C_2$ is a constant of integration. On the right-hand side, integrating with respect to $\xi$ yields
\[
\int \frac{\mathrm{d}\xi}{2\nu} = \frac{\xi}{2\nu} + C_3,
\]
for another constant $C_3$. We may combine the constants into a single constant $C_4$. Dropping the absolute values (since we know the sign of the numerator and denominator from the ordering of $u_-$ and $u_+$ and the monotonicity of the profile), we obtain
\[
\frac{1}{u_- - u_+}\ln\left(\frac{U(\xi) - u_-}{U(\xi) - u_+}\right)
= \frac{\xi}{2\nu} + C_4.
\]
Multiplying by $(u_- - u_+)$ gives
\[
\ln\left(\frac{U(\xi) - u_-}{U(\xi) - u_+}\right)
= \frac{(u_- - u_+)\,\xi}{2\nu} + C_5,
\]
with $C_5 = (u_- - u_+)C_4$. Exponentiating both sides,
\[
\frac{U(\xi) - u_-}{U(\xi) - u_+}
= C\,\exp\!\left(\frac{(u_- - u_+)\,\xi}{2\nu}\right),
\]
where $C = e^{C_5}$ is a nonzero constant.

Next we choose a convenient origin for $\xi$ to fix $C$. It is natural to center the wave by requiring that the midpoint value $s = (u_- + u_+)/2$ occur at $\xi = 0$:
\[
U(0) = s.
\]
Substituting $\xi = 0$ and $U(0) = s$ into the implicit formula gives
\[
\frac{s - u_-}{s - u_+} = C.
\]
Since $s = (u_- + u_+)/2$, we have
\[
s - u_- = \frac{u_- + u_+}{2} - u_- = \frac{u_+ - u_-}{2},
\qquad
s - u_+ = \frac{u_- + u_+}{2} - u_+ = \frac{u_- - u_+}{2}.
\]
Hence
\[
C = \frac{s - u_-}{s - u_+}
= \frac{\tfrac{u_+ - u_-}{2}}{\tfrac{u_- - u_+}{2}} = -1.
\]
Therefore the implicit relation simplifies to
\[
\frac{U(\xi) - u_-}{U(\xi) - u_+}
= -\exp\!\left(\frac{(u_- - u_+)\,\xi}{2\nu}\right).
\]

To express $U$ in a more transparent form, let
\[
\Delta := u_- - u_+ > 0.
\]
Then
\[
\frac{U(\xi) - u_-}{U(\xi) - u_+}
= -\exp\!\left(\frac{\Delta\,\xi}{2\nu}\right).
\]
Solving for $U(\xi)$, write
\[
U(\xi) - u_- = -e^{\Delta\xi/(2\nu)}\bigl(U(\xi) - u_+\bigr),
\]
so
\[
U(\xi)\bigl(1 + e^{\Delta\xi/(2\nu)}\bigr)
= u_- + u_+\,e^{\Delta\xi/(2\nu)}.
\]
Thus
\[
U(\xi)
= \frac{u_- + u_+\,e^{\Delta\xi/(2\nu)}}{1 + e^{\Delta\xi/(2\nu)}}.
\]
This is already a legitimate explicit formula. We can simplify it further to
\[
U(\xi)
= \frac{u_- + u_+}{2}
  - \frac{u_- - u_+}{2}
    \tanh\!\left(\frac{(u_- - u_+)\,\xi}{4\nu}\right).
\]

To see this, write
\[
U(\xi)
= \frac{u_- + u_+\,e^{\Delta\xi/(2\nu)}}{1 + e^{\Delta\xi/(2\nu)}}
= u_+ + \frac{u_- - u_+}{1 + e^{\Delta\xi/(2\nu)}}.
\]
Set
\[
z := e^{-\Delta\xi/(2\nu)}.
\]
Then
\[
\frac{1}{1 + e^{\Delta\xi/(2\nu)}}
= \frac{z}{1 + z}.
\]
Recall the identity
\[
\tanh\!\left(\frac{y}{2}\right)
= \frac{1 - e^{-y}}{1 + e^{-y}},
\]
which, with $y = \Delta\xi/(2\nu)$, gives
\[
\tanh\!\left(\frac{\Delta\xi}{4\nu}\right)
= \frac{1 - e^{-\Delta\xi/(2\nu)}}{1 + e^{-\Delta\xi/(2\nu)}}
= \frac{1 - z}{1 + z}.
\]
Solving for $\dfrac{z}{1+z}$,
\[
\frac{z}{1+z}
= \frac{1 - \tanh\!\left(\dfrac{\Delta\xi}{4\nu}\right)}{2}.
\]
Hence
\[
U(\xi)
= u_+ + (u_- - u_+)\,\frac{1}{2}
   \Bigl[1 - \tanh\!\Bigl(\frac{\Delta\xi}{4\nu}\Bigr)\Bigr]
= \frac{u_- + u_+}{2}
  - \frac{\Delta}{2}\,\tanh\!\Bigl(\frac{\Delta\xi}{4\nu}\Bigr),
\]
and since $\Delta = u_- - u_+$, we obtain
\[
U(\xi)
= \frac{u_- + u_+}{2}
  - \frac{u_- - u_+}{2}
    \tanh\!\left(\frac{(u_- - u_+)\,\xi}{4\nu}\right),
\]
which is the claimed explicit traveling wave profile for $u_- > u_+$.

\medskip

\noindent\textbf{(iv) Inviscid limit and Rankine--Hugoniot speed.}
For $u_- > u_+$, the function $U(\xi)$ is a smooth, monotone decreasing transition from $u_-$ (as $\xi\to -\infty$) to $u_+$ (as $\xi\to +\infty$). The transition occurs over a characteristic length scale given by the argument of the hyperbolic tangent,
\[
\frac{(u_- - u_+)\,\xi}{4\nu} = O(1)
\quad\Longrightarrow\quad
|\xi| = O\!\left(\frac{\nu}{|u_- - u_+|}\right).
\]
Thus, as $\nu \to 0^+$, the thickness of the transition layer shrinks like $O(\nu)$, and $U(\xi)$ approaches a discontinuous step function:
\[
U(\xi) \to
\begin{cases}
u_-, & \xi < 0,\\[4pt]
u_+, & \xi > 0,
\end{cases}
\]
i.e., a shock located at $\xi = 0$ (or, in $(x,t)$ variables, at $x = st$).

The corresponding solution of the inviscid conservation law
\[
u_t + \frac{1}{2}(u^2)_x = 0
\]
is a propagating shock joining $u_-$ to $u_+$. The speed $s$ of this shock is given by the Rankine--Hugoniot condition
\[
s = \frac{f(u_+) - f(u_-)}{u_+ - u_-},
\quad f(u) = \frac{u^2}{2},
\]
which yields
\[
s = \frac{\tfrac12 u_+^2 - \tfrac12 u_-^2}{u_+ - u_-}
= \frac{u_- + u_+}{2},
\]
exactly the speed obtained from the traveling wave analysis. Thus, in the limit $\nu\to 0^+$, the viscous shock profile converges to the entropy-satisfying inviscid shock moving at the Rankine--Hugoniot speed.

\end{solution}

% ===== Example 5: Boundary-Value Problems and Numerical Exploration for Burgers’ Equation (inquiry-based) =====
\begin{problem}[Boundary-Value Problems and Numerical Exploration for Burgers’ Equation]
In this case study we look at viscous Burgers’ equation on a finite spatial interval with imposed inflow and outflow conditions. This is a simple model for one-dimensional flow in a narrow channel driven by a difference in boundary velocities. The equation combines nonlinear advection, which tends to steepen gradients and form shock-like transitions, with viscosity, which tends to smooth them out. Our goal is to (i) set up a boundary-value problem, (ii) derive a simple finite difference scheme, (iii) understand a basic stability constraint, and (iv) predict qualitatively what numerical solutions should look like when we vary the viscosity and the grid.

Consider viscous Burgers’ equation
\[
u_t + u\,u_x = \nu\,u_{xx}, \qquad 0<x<1,\quad t>0,
\]
with viscosity $\nu>0$. We impose Dirichlet boundary conditions
\[
u(0,t) = 1,\qquad u(1,t)=0,\qquad t>0,
\]
and an initial condition $u(x,0)=u_0(x)$ with $0\le u_0(x)\le 1$. You may think of $u$ as a scalar velocity, with fluid entering from the left at speed $1$ and exiting to the right at speed $0$.

\smallskip

(a) Rewrite Burgers’ equation in conservative form by introducing an appropriate flux function $f(u)$ so that the equation becomes
\[
u_t + \bigl(f(u)\bigr)_x = \nu\,u_{xx}.
\]
What is $f(u)$? Explain briefly why this conservative form is natural if we think of $u$ as a transported quantity subject to diffusion.

\medskip

(b) We now discretize the problem on a uniform grid $x_j = j\,\Delta x$ for $j=0,1,\dots,N$, where $\Delta x = 1/N$. Let $u_j^n$ be an approximation to $u(x_j,t^n)$ at time $t^n = n\,\Delta t$.

\begin{itemize}
    \item[(i)] For the diffusion term $u_{xx}$, propose a standard second-order centered finite difference at interior grid points. Write your formula explicitly.
    \item[(ii)] For the nonlinear advection term $u\,u_x$, notice that in our problem the solution is expected to stay between $0$ and $1$, so the characteristic velocity $u$ is nonnegative. Use this to motivate a first-order upwind approximation of the form
    \[
    (u\,u_x)(x_j,t^n) \approx u_j^n \,\frac{u_j^n - u_{j-1}^n}{\Delta x},
    \]
    and explain (in words) why using a backward difference in $x$ makes sense when $u\ge 0$.
\end{itemize}

Using these ideas, assemble a fully explicit finite difference scheme for the interior points $j=1,\dots,N-1$ of the form
\[
u_j^{n+1} = u_j^n + \text{(convection contribution)} + \text{(diffusion contribution)}.
\]
Write the scheme out completely in terms of $u_{j-1}^n, u_j^n, u_{j+1}^n$, and the parameters $\Delta t, \Delta x, \nu$.

\emph{Hint:} Start from $u_t = -u\,u_x + \nu u_{xx}$, approximate $u_t$ by a forward difference in time, then plug in your spatial approximations.

\medskip

(c) We incorporate the boundary conditions directly into the discrete system by setting
\[
u_0^n = 1,\qquad u_N^n = 0 \qquad \text{for all } n\ge 0.
\]
Explain how these fixed boundary values enter the update formula for $u_1^{n+1}$ and $u_{N-1}^{n+1}$. Write the explicit update for $u_1^{n+1}$, and indicate which terms come from $u_0^n$.

\emph{Hint:} Your interior formula for $u_j^{n+1}$ is valid for $j=1,\dots,N-1$. When $j=1$, the value $u_{j-1}^n$ is simply $u_0^n$, which is known from the boundary condition.

\medskip

(d) To understand stability, linearize the scheme around a constant state $u\equiv U$ with $0 < U \le 1$. The linearized partial differential equation is
\[
v_t + U\,v_x = \nu\,v_{xx},
\]
where $v$ is a small perturbation. The corresponding linearized finite difference scheme (using the same stencils as in part (b)) has the form
\[
v_j^{n+1} = v_j^n - C\,(v_j^n - v_{j-1}^n) + \mu\,(v_{j+1}^n - 2v_j^n + v_{j-1}^n),
\]
for suitable dimensionless parameters $C$ and $\mu$.

\begin{itemize}
    \item[(i)] Identify $C$ and $\mu$ in terms of $U$, $\nu$, $\Delta t$, and $\Delta x$.
    \item[(ii)] Rewrite this update rule in the form
    \[
    v_j^{n+1} = a\,v_{j-1}^n + b\,v_j^n + c\,v_{j+1}^n,
    \]
    and express $a,b,c$ in terms of $C$ and $\mu$.
    \item[(iii)] A very robust (though somewhat conservative) way to guarantee stability in the maximum norm is to demand $a,b,c\ge 0$ and $a+b+c=1$, so that the new value is a convex combination of the three neighboring values. Use these conditions to derive an inequality relating $C$ and $\mu$.
\end{itemize}

Finally, rewrite your inequality as a time-step restriction of the form
\[
\Delta t \;\le\; \frac{1}{\dfrac{U}{\Delta x} \;+\; \dfrac{2\nu}{\Delta x^2}}.
\]
Explain how this restriction changes when the viscosity $\nu$ is very small, and when the grid is very fine (that is, when $\Delta x$ is very small).

\emph{Hint:} Verify that $a+b+c=1$ automatically, and focus on the nonnegativity of each coefficient.

\medskip

(e) Conceptual exploration.

\begin{itemize}
    \item[(i)] Suppose you run your scheme with a moderate viscosity, say $\nu=10^{-2}$, and start from the initial condition $u_0(x)\equiv 0$. Based on the continuous equation and on your discrete scheme, describe qualitatively what you expect the solution $u(x,t)$ to look like for large $t$. In particular, what kind of spatial profile do you expect between $x=0$ and $x=1$? Where will the gradients be largest, and how does the thickness of the transition layer depend on $\nu$?
    \item[(ii)] Now imagine decreasing $\nu$ by a factor of $10$ while keeping the same grid and time step (still satisfying your stability condition), and then, alternatively, refining the grid $\Delta x$ while keeping $\nu$ fixed. In each case, explain qualitatively how the numerical solution changes. When might you start to see numerical artifacts such as undershoots, overshoots, or excessive smearing near the steep gradient region?
\end{itemize}

\emph{Hint:} Think about the competition between physical diffusion (controlled by $\nu$) and numerical diffusion or dispersion (introduced by the discretization and finite grid resolution). How does each one affect the sharpness of a shock-like transition?
\end{problem}

% ===== Example 5: Boundary-Value Problems and Numerical Exploration for Burgers’ Equation (full solution) =====
\begin{problem}[Boundary-Value Problems and Numerical Exploration for Burgers’ Equation]
Consider viscous Burgers’ equation
\[
u_t + u\,u_x = \nu\,u_{xx}, \qquad 0<x<1,\ t>0,
\]
with viscosity $\nu>0$, Dirichlet boundary conditions
\[
u(0,t) = 1,\quad u(1,t)=0,\quad t>0,
\]
and an initial condition $u(x,0)=u_0(x)$ with $0\le u_0(x)\le 1$.

\begin{enumerate}
    \item Rewrite the equation in conservative form $u_t + (f(u))_x = \nu u_{xx}$ and identify $f(u)$.
    \item Discretize the problem on a uniform grid $x_j=j\Delta x$ ($j=0,\dots,N$), $\Delta x=1/N$, with time step $\Delta t$, using:
    \begin{itemize}
        \item forward Euler in time for $u_t$,
        \item a second-order centered difference for $u_{xx}$ at interior points,
        \item a first-order upwind approximation $u\,u_x \approx u_j^n (u_j^n - u_{j-1}^n)/\Delta x$ for the advection term (assuming $0\le u\le 1$).
    \end{itemize}
    Derive the explicit finite difference scheme for interior nodes $j=1,\dots,N-1$, and indicate how the Dirichlet boundary conditions enter at $j=1$ and $j=N-1$.
    \item Linearize the scheme about a constant state $u\equiv U$ with $0<U\le 1$ to obtain a scheme for perturbations $v$ of the form
    \[
    v_j^{n+1} = v_j^n - C\,(v_j^n - v_{j-1}^n) + \mu\,(v_{j+1}^n - 2v_j^n + v_{j-1}^n).
    \]
    Express $C$ and $\mu$ in terms of $U,\nu,\Delta t,\Delta x$, rewrite the update as
    \[
    v_j^{n+1} = a\,v_{j-1}^n + b\,v_j^n + c\,v_{j+1}^n,
    \]
    and derive a sufficient stability condition by requiring $a,b,c\ge 0$ and $a+b+c=1$. Show that this yields the time-step restriction
    \[
    \Delta t \;\le\; \frac{1}{\dfrac{U}{\Delta x} \;+\; \dfrac{2\nu}{\Delta x^2}}.
    \]
    \item Briefly discuss the qualitative behavior of the numerical solution for large times, and how it depends on the viscosity $\nu$ and the grid spacing $\Delta x$. In particular, comment on the thickness of the shock-like transition between $x=0$ and $x=1$, and on how under-resolved steep gradients can lead to numerical artifacts.
\end{enumerate}
\end{problem}

\begin{solution}
We proceed step by step, highlighting the interaction between the partial differential equation, the boundary conditions, and the numerical discretization.

\medskip

\noindent\textbf{(1) Conservative form.}
Starting from
\[
u_t + u\,u_x = \nu\,u_{xx},
\]
we recognize that $u\,u_x$ is the $x$-derivative of $u^2/2$. Indeed,
\[
\frac{d}{dx}\left(\frac{u^2}{2}\right) = u\,u_x.
\]
Thus we may rewrite the equation in conservation form
\[
u_t + \bigl(f(u)\bigr)_x = \nu\,u_{xx},
\]
with flux
\[
f(u) = \frac{u^2}{2}.
\]

In this form, the left-hand side expresses a local conservation law for the quantity $u$, transported by the nonlinear flux $f(u)$. The right-hand side adds a diffusive term, representing viscous smoothing. When we impose boundary conditions at $x=0$ and $x=1$, we are effectively fixing the inflow and outflow values of $u$ (and hence controlling the boundary fluxes) while allowing the interior to adjust dynamically.

\medskip

\noindent\textbf{(2) Finite difference discretization.}
We introduce a uniform spatial grid
\[
x_j = j\,\Delta x,\qquad j=0,1,\dots,N,\qquad \Delta x = \frac{1}{N},
\]
and discrete time levels
\[
t^n = n\,\Delta t,\qquad n=0,1,2,\dots.
\]
We denote by $u_j^n$ an approximation to $u(x_j,t^n)$.

We first rewrite the partial differential equation as
\[
u_t = -u\,u_x + \nu\,u_{xx}.
\]
We approximate $u_t$ at time level $t^n$ by a forward difference,
\[
u_t(x_j,t^n) \approx \frac{u_j^{n+1} - u_j^n}{\Delta t}.
\]

For the diffusion term, at an interior point $x_j$ with $1\le j\le N-1$ we use the standard centered second difference,
\[
u_{xx}(x_j,t^n) \approx \frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\Delta x^2}.
\]

For the nonlinear advection term $u\,u_x$, we use that in our setting the solution is expected to satisfy $0\le u\le 1$ for all $x$ and $t$. The advection velocity $u$ is thus nonnegative, so characteristics move from left to right. For an upwind discretization of $u_x$ in this situation, we use a backward difference,
\[
u_x(x_j,t^n) \approx \frac{u_j^n - u_{j-1}^n}{\Delta x}.
\]
Multiplying by $u_j^n$ gives the approximation
\[
(u\,u_x)(x_j,t^n) \approx u_j^n\,\frac{u_j^n - u_{j-1}^n}{\Delta x}.
\]
Using a backward difference is natural: when information propagates from left to right, the value at $x_j$ is determined by what has happened at points $x_{j-1}$, $x_{j-2}$, and so on; the backward stencil looks ``into the wind'' and thereby adds numerical damping that stabilizes the scheme.

Putting these pieces together, the discrete version of $u_t = -u\,u_x + \nu u_{xx}$ at an interior point is
\[
\frac{u_j^{n+1} - u_j^n}{\Delta t}
= - u_j^n\,\frac{u_j^n - u_{j-1}^n}{\Delta x}
+ \nu\,\frac{u_{j+1}^n - 2u_j^n + u_{j-1}^n}{\Delta x^2},
\]
for $j=1,\dots,N-1$.

Solving for $u_j^{n+1}$ we obtain the explicit finite difference scheme
\begin{equation}\label{eq:explicit-scheme}
u_j^{n+1}
= u_j^n
- \frac{\Delta t}{\Delta x}\,u_j^n\,(u_j^n - u_{j-1}^n)
+ \nu\,\frac{\Delta t}{\Delta x^2}\,\bigl(u_{j+1}^n - 2u_j^n + u_{j-1}^n\bigr),
\qquad j=1,\dots,N-1.
\end{equation}
This is an explicit scheme combining a first-order upwind approximation of nonlinear advection with a second-order centered approximation of diffusion.

\medskip

\noindent\textbf{Boundary conditions in the scheme.}
The Dirichlet boundary conditions
\[
u(0,t) = 1,\qquad u(1,t) = 0
\]
are imposed discretely as
\[
u_0^n = 1,\qquad u_N^n = 0\qquad\text{for all }n.
\]
These values are held fixed at every time step and enter into the update of the neighboring interior points.

For example, for $j=1$, the scheme \eqref{eq:explicit-scheme} becomes
\[
u_1^{n+1}
= u_1^n
- \frac{\Delta t}{\Delta x}\,u_1^n\,(u_1^n - u_0^n)
+ \nu\,\frac{\Delta t}{\Delta x^2}\,\bigl(u_2^n - 2u_1^n + u_0^n\bigr),
\]
and here $u_0^n=1$ is substituted from the boundary condition. Thus the left boundary value explicitly influences the update at the first interior point. An analogous statement holds at $j=N-1$, where $u_N^n=0$ enters the scheme.

\medskip

\noindent\textbf{(3) Linearization and stability via convex combinations.}
To study stability, we linearize about a constant state $u\equiv U$ with $0<U\le 1$. We write
\[
u_j^n = U + v_j^n,
\]
where $v_j^n$ is a small perturbation. Substituting into \eqref{eq:explicit-scheme
} and discarding terms that are quadratic in $v$, we obtain
\[
U + v_j^{n+1}
= U + v_j^n
- \frac{\Delta t}{\Delta x}\,U\,(v_j^n - v_{j-1}^n)
+ \nu\,\frac{\Delta t}{\Delta x^2}\,\bigl(v_{j+1}^n - 2v_j^n + v_{j-1}^n\bigr).
\]
The constant $U$ cancels from both sides, leaving
\[
v_j^{n+1}
= v_j^n
- \frac{U\Delta t}{\Delta x}\,(v_j^n - v_{j-1}^n)
+ \nu\,\frac{\Delta t}{\Delta x^2}\,\bigl(v_{j+1}^n - 2v_j^n + v_{j-1}^n\bigr).
\]

Comparing with the given linearized form,
\[
v_j^{n+1} = v_j^n - C\,(v_j^n - v_{j-1}^n) + \mu\,(v_{j+1}^n - 2v_j^n + v_{j-1}^n),
\]
we can read off
\[
C = \frac{U\,\Delta t}{\Delta x}, \qquad
\mu = \frac{\nu\,\Delta t}{\Delta x^2}.
\]

\medskip

\noindent\emph{Rewriting as a three-point stencil.}
We expand
\begin{align*}
v_j^{n+1}
&= v_j^n - C\,(v_j^n - v_{j-1}^n)
   + \mu\,(v_{j+1}^n - 2v_j^n + v_{j-1}^n) \\
&= v_j^n - C v_j^n + C v_{j-1}^n
   + \mu v_{j+1}^n - 2\mu v_j^n + \mu v_{j-1}^n \\
&= (C+\mu)\,v_{j-1}^n
   + \bigl(1 - C - 2\mu\bigr)\,v_j^n
   + \mu\,v_{j+1}^n.
\end{align*}
Thus
\[
a = C+\mu,\qquad b = 1 - C - 2\mu,\qquad c = \mu.
\]
One readily checks
\[
a + b + c = (C+\mu) + (1 - C - 2\mu) + \mu = 1.
\]

\medskip

\noindent\emph{Convex-combination stability condition.}
A sufficient condition for stability in the maximum norm is that
\[
a\ge 0,\quad b\ge 0,\quad c\ge 0,\quad a+b+c=1,
\]
so that $v_j^{n+1}$ is a convex combination of neighboring values.

We already have $a+b+c=1$. The nonnegativity conditions are:
\begin{itemize}
    \item $c = \mu \ge 0$, which holds automatically since $\nu,\Delta t,\Delta x^2>0$;
    \item $a = C+\mu \ge 0$, which holds automatically since $U>0$;
    \item $b = 1 - C - 2\mu \ge 0$, which gives the only nontrivial constraint:
    \[
    C + 2\mu \le 1.
    \]
\end{itemize}
In terms of the original parameters,
\[
C + 2\mu
= \frac{U\,\Delta t}{\Delta x} + 2\,\frac{\nu\,\Delta t}{\Delta x^2}
= \Delta t\left(\frac{U}{\Delta x} + \frac{2\nu}{\Delta x^2}\right) \le 1.
\]
Thus a sufficient stability condition is
\[
\boxed{\;
\Delta t \;\le\; \frac{1}{\dfrac{U}{\Delta x} + \dfrac{2\nu}{\Delta x^2}}
\;}
\]
as claimed.

\medskip

\noindent\emph{Behavior for small $\nu$ and small $\Delta x$.}
\begin{itemize}
    \item If $\nu$ is very small, the diffusive term contributes little to the restriction and the condition is dominated by advection:
    \[
    \Delta t \lesssim \frac{\Delta x}{U}.
    \]
    The time step can then scale linearly with $\Delta x$ (a typical CFL condition for advection-dominated problems).

    \item If the grid is very fine (small $\Delta x$) with fixed $\nu$, the diffusive term
    \[
    \frac{2\nu}{\Delta x^2}
    \]
    becomes large and dominates the bound. Then
    \[
    \Delta t \lesssim \frac{\Delta x^2}{2\nu},
    \]
    which is the classic parabolic time-step restriction: as $\Delta x$ halves, $\Delta t$ must be reduced by roughly a factor of $4$ to maintain stability.
\end{itemize}

\medskip

\noindent\textbf{(4) Qualitative behavior and dependence on $\nu$ and $\Delta x$.}

\medskip

\noindent\emph{(i) Long-time behavior for moderate viscosity.}
Take $\nu = 10^{-2}$ and initial data $u_0(x)\equiv 0$. The left boundary enforces $u(0,t)=1$ and the right boundary enforces $u(1,t)=0$. Over time, the solution is driven from the initial state towards a steady profile $u_\infty(x)$ that solves the stationary boundary-value problem
\[
u\,u_x = \nu\,u_{xx}, \qquad 0<x<1,\qquad u_\infty(0)=1,\quad u_\infty(1)=0.
\]
Qualitatively, this steady solution is
\begin{itemize}
    \item monotone decreasing from $u\approx 1$ near $x=0$ to $u\approx 0$ near $x=1$,
    \item smooth, with a single transition layer (shock-like region) somewhere between $x=0$ and $x=1$,
    \item relatively broad for a moderate viscosity: the gradient is largest in the transition region, whose thickness is of order $\nu$ (up to factors depending on the precise profile).
\end{itemize}
In the discrete scheme, $u_j^n$ will relax towards a numerically steady state with a similar smooth, monotone profile. The largest changes from one grid point to the next occur in the same transition region; away from it the solution will be nearly flat (close to $1$ on the left and $0$ on the right).

\medskip

\noindent\emph{(ii) Effect of decreasing $\nu$ and of refining the grid.}

\smallskip
\noindent\underline{Decreasing $\nu$ (fixed grid, stable $\Delta t$).}
If we reduce $\nu$ by a factor of $10$ while keeping the grid and (stability-respecting) time step the same:
\begin{itemize}
    \item The physical diffusion becomes weaker, so the continuous steady profile develops a sharper transition layer: the region over which $u$ drops from $1$ to $0$ becomes thinner.
    \item On a fixed grid, once the true physical layer becomes thinner than a few mesh cells, the scheme cannot fully resolve it. The numerically computed layer will then be smeared over several grid points, with a thickness controlled more by numerical diffusion (from the upwind advection and discrete diffusion) and grid spacing than by the physical $\nu$.
    \item In this first-order upwind/explicit-diffusion scheme, the main artifact is \emph{excessive smearing}: the shock-like transition will appear too wide compared to the true solution. Strong oscillatory artifacts (undershoots/overshoots) are less likely here because the scheme is monotone under the convex-combination stability condition derived above.
\end{itemize}

\smallskip
\noindent\underline{Refining the grid $\Delta x$ (fixed $\nu$).}
Now keep $\nu$ fixed and decrease $\Delta x$:
\begin{itemize}
    \item The physical transition layer becomes better resolved: with more grid points across the layer, the discrete profile more closely matches the sharpness of the continuous solution.
    \item However, the stability restriction becomes more severe, since for small $\Delta x$ the term $2\nu/\Delta x^2$ dominates. To keep $C+2\mu\le 1$, $\Delta t$ must be reduced roughly like $\Delta x^2$.
    \item If $\Delta x$ is refined \emph{without} a corresponding reduction in $\Delta t$ (so that $C+2\mu>1$), the convex-combination property is lost. Then one may begin to see numerical instabilities, which can manifest as spurious oscillations, undershoots, or overshoots near the steep gradient, and in the worst case as blow-up of the numerical solution.
    \item When the stability condition is respected, refinement typically reduces numerical diffusion, leading to a \emph{sharper} and more accurate representation of the shock-like transition, with less artificial smearing.
\end{itemize}

In summary, the sharpness and accuracy of the numerical transition layer are governed by both the physical viscosity $\nu$ and the effective numerical resolution $\Delta x$ (together with a stable $\Delta t$). Lower viscosity creates steeper true gradients, which require finer grids (and smaller time steps) to be represented faithfully without excessive numerical smearing or instability.

\end{solution}

