\chapter{Fourier Analysis}

\section{The Fourier Transform and Inverse Fourier Transform}
% --- Narrative plan (auto-generated) ---
% In this section we introduce the Fourier transform, which decomposes functions on the real line into continuous superpositions of complex exponentials, and its inverse transform, which reconstructs the original function from its frequency components. We will develop both the formal integral formulas and the conditions under which they are valid, and we will explore how to move back and forth between the time (or spatial) domain and the frequency domain.
%
% The Fourier transform is a central tool in applied mathematics: it turns differential equations into algebraic equations, converts convolutions into products, and offers a natural language for describing filtering, dispersion, and wave propagation. In the study of partial differential equations, especially on the whole real line or on ℝ^n, the Fourier transform becomes a primary method of solution for the heat, wave, and Schrödinger equations. It also reveals deep connections among topics you may know already: it uses ideas from complex analysis (contour integration and decay at infinity), it generalizes the Fourier series expansion on intervals, and it provides a bridge between ordinary differential equations, signal processing, and modern distribution theory.
%
% Our approach will be incremental and example-driven. We begin with explicit transforms of simple functions, such as exponentials and Gaussians, and use these to motivate key properties like inversion, scaling, and modulation. We then apply the transform to solve specific ODEs and PDEs, paying attention to how the inverse transform is used to write the final solution in physical space. Along the way, we will see how the transform interacts with differentiation and convolution, and we will preview the role of generalized functions such as the Dirac delta in making these ideas precise.

% ===== Example 1: The Fourier Transform of the Gaussian and Its Own Inverse (inquiry-based) =====
\begin{problem}[The Fourier Transform of the Gaussian and Its Own Inverse]
The Gaussian function plays a central role in probability, statistics, and the theory of diffusion. In Fourier analysis it has a remarkable property: under a natural choice of normalization, the Gaussian is mapped to another Gaussian, and in a special case it is mapped exactly to itself. In this problem you will discover this fact directly from the defining integral of the Fourier transform, using basic tools such as differentiation under the integral sign and integration by parts. You will then see how this example makes the inverse Fourier transform very concrete.

Throughout this problem we use the following convention for the Fourier transform and its inverse on $\mathbb{R}$:
\[
\mathcal{F}[f](\xi) = \hat f(\xi) := \int_{-\infty}^{\infty} f(x)\, e^{-2\pi i x \xi}\, dx,\qquad
\mathcal{F}^{-1}[g](x) := \int_{-\infty}^{\infty} g(\xi)\, e^{2\pi i x \xi}\, d\xi.
\]

(a) Let $f(x) = e^{-\pi x^2}$. As a warm-up, recall (or re-derive) the value of the Gaussian integral
\[
\int_{-\infty}^{\infty} e^{-x^2}\, dx.
\]
Then, by a change of variables, compute
\[
\int_{-\infty}^{\infty} e^{-\pi x^2}\, dx.
\]
% Hint: If you know that $\int_{-\infty}^{\infty} e^{-x^2}\, dx = \sqrt{\pi}$, set $y = \sqrt{\pi}\,x$ to evaluate the second integral.

(b) We now turn to the Fourier transform of the Gaussian. Define
\[
I(\xi) := \widehat{f}(\xi) = \int_{-\infty}^{\infty} e^{-\pi x^2}\, e^{-2\pi i x \xi}\, dx.
\]
Compute the derivative $I'(\xi)$ by differentiating under the integral sign.

% Hint: Differentiate $e^{-2\pi i x\xi}$ with respect to $\xi$ and keep $e^{-\pi x^2}$ as a factor. Justify differentiation under the integral sign by the dominated convergence theorem, or by noting that the integrand and its $\xi$-derivative are dominated by a rapidly decaying Gaussian.

(c) The expression for $I'(\xi)$ will involve the integral
\[
\int_{-\infty}^{\infty} x\, e^{-\pi x^2}\, e^{-2\pi i x \xi}\, dx.
\]
Show that this integral can be expressed in terms of $I(\xi)$ itself by using integration by parts on
\[
\int_{-\infty}^{\infty} \frac{d}{dx}\big( e^{-\pi x^2} e^{-2\pi i x \xi} \big)\, dx.
\]
Conclude that $I(\xi)$ satisfies a first-order ordinary differential equation of the form
\[
I'(\xi) = -2\pi \xi\, I(\xi).
\]

Hint: Compute $\dfrac{d}{dx}\big( e^{-\pi x^2} e^{-2\pi i x \xi} \big)$ explicitly, then integrate from $-\infty$ to $\infty$ and use the fact that $e^{-\pi x^2}$ makes the boundary terms vanish.

(d) Solve the differential equation
\[
I'(\xi) = -2\pi \xi\, I(\xi)
\]
for $I(\xi)$, using the initial condition $I(0)$ that you found in part (a). Show that
\[
\widehat{f}(\xi) = I(\xi) = e^{-\pi \xi^2}.
\]
Explain briefly why this shows that $f$ is mapped to itself by the Fourier transform under our convention.

Next, use your formula for $\widehat{f}(\xi)$ and the definition of the inverse Fourier transform to compute explicitly
\[
\mathcal{F}^{-1}[\widehat{f}](x) = \int_{-\infty}^{\infty} e^{-\pi \xi^2}\, e^{2\pi i x \xi}\, d\xi,
\]
and verify that
\[
\mathcal{F}^{-1}[\widehat{f}](x) = f(x) = e^{-\pi x^2}.
\]
% Hint: The integral for the inverse transform has exactly the same form as $I(\xi)$, just with the roles of $x$ and $\xi$ (and the sign in the exponential) interchanged.

(e) Extensions and variations.

\quad (i) Consider the more general Gaussian
\[
f_a(x) = e^{-\pi a x^2}, \qquad a>0.
\]
Using the scaling property of the Fourier transform (or by repeating the calculation with $a$ in place of $1$), find an explicit formula for $\widehat{f_a}(\xi)$ in terms of $a$. For which value(s) of $a$ is $f_a$ mapped to itself by the Fourier transform?

\quad (ii) Our convention makes the Gaussian $e^{-\pi x^2}$ an eigenfunction of the Fourier transform with eigenvalue $1$. Under a different common convention,
\[
\mathcal{F}_{\mathrm{phys}}[f](\omega)
= \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} f(x)\, e^{-i\omega x}\, dx,
\]
the Gaussian is still an eigenfunction, but with a different eigenvalue and normalization. How would you rescale $x$ and $f$ so that in this convention you again obtain a Gaussian that is mapped to itself (up to a constant phase factor) by $\mathcal{F}_{\mathrm{phys}}$? Describe the idea without carrying out all of the algebra.
\end{problem}

% ===== Example 1: The Fourier Transform of the Gaussian and Its Own Inverse (full solution) =====
\begin{problem}[The Fourier Transform of the Gaussian and Its Own Inverse]
Let the Fourier transform and its inverse on $\mathbb{R}$ be defined by
\[
\mathcal{F}[f](\xi) = \hat f(\xi) := \int_{-\infty}^{\infty} f(x)\, e^{-2\pi i x \xi}\, dx,
\qquad
\mathcal{F}^{-1}[g](x) := \int_{-\infty}^{\infty} g(\xi)\, e^{2\pi i x \xi}\, d\xi.
\]
  
(a) Let $f(x) = e^{-\pi x^2}$. Compute its Fourier transform $\widehat{f}(\xi)$ directly from the defining integral, and show that
\[
\widehat{f}(\xi) = e^{-\pi \xi^2}.
\]

(b) Use this result to verify explicitly that
\[
\mathcal{F}^{-1}[\widehat{f}](x) = f(x),
\]
thus illustrating the inverse Fourier transform formula in this special case.

(c) More generally, for $a>0$ define $f_a(x) = e^{-\pi a x^2}$. Using the result for $a=1$ together with the scaling property of the Fourier transform, derive a formula for $\widehat{f_a}(\xi)$, and determine for which $a$ the function $f_a$ is mapped to itself by $\mathcal{F}$.
\end{problem}

\begin{solution}
We work under the convention
\[
\mathcal{F}[f](\xi) = \hat f(\xi) = \int_{-\infty}^{\infty} f(x)\, e^{-2\pi i x \xi}\, dx,
\qquad
\mathcal{F}^{-1}[g](x) = \int_{-\infty}^{\infty} g(\xi)\, e^{2\pi i x \xi}\, d\xi.
\]
This choice is standard in harmonic analysis and has the pleasant feature that the Gaussian $e^{-\pi x^2}$ is an eigenfunction of the Fourier transform.

\medskip
\noindent\textbf{(a) Fourier transform of $e^{-\pi x^2}$.}

Let $f(x)=e^{-\pi x^2}$ and define
\[
I(\xi) := \widehat{f}(\xi)
= \int_{-\infty}^{\infty} e^{-\pi x^2}\, e^{-2\pi i x \xi}\, dx.
\]
The integrand is smooth in both $x$ and $\xi$, and for each fixed $\xi$ it is dominated in absolute value by $e^{-\pi x^2}$, which is integrable. Thus we may differentiate under the integral sign with respect to $\xi$:
\[
I'(\xi)
= \int_{-\infty}^{\infty} e^{-\pi x^2}\, \frac{\partial}{\partial \xi}\big( e^{-2\pi i x \xi} \big)\, dx
= \int_{-\infty}^{\infty} e^{-\pi x^2}\, (-2\pi i x)\, e^{-2\pi i x \xi}\, dx.
\]
Hence
\[
I'(\xi) = -2\pi i \int_{-\infty}^{\infty} x\, e^{-\pi x^2}\, e^{-2\pi i x \xi}\, dx.
\]

We now show that the integral involving $x e^{-\pi x^2}$ can be expressed in terms of $I(\xi)$ itself. Consider the derivative with respect to $x$ of the product
\[
\phi(x) = e^{-\pi x^2}\, e^{-2\pi i x \xi}.
\]
A direct computation gives
\[
\phi'(x)
= (-2\pi x)\, e^{-\pi x^2}\, e^{-2\pi i x \xi}
+ e^{-\pi x^2}\, (-2\pi i \xi)\, e^{-2\pi i x \xi}
= (-2\pi x - 2\pi i \xi)\, e^{-\pi x^2}\, e^{-2\pi i x \xi}.
\]
Integrating $\phi'(x)$ over the real line, we find
\[
\int_{-\infty}^{\infty} \phi'(x)\, dx
= \int_{-\infty}^{\infty} (-2\pi x - 2\pi i \xi) e^{-\pi x^2} e^{-2\pi i x \xi}\, dx.
\]
On the other hand, because $e^{-\pi x^2}$ decays very rapidly, we have
\[
\lim_{x\to\pm\infty} \phi(x)
= \lim_{x\to\pm\infty} e^{-\pi x^2}\, e^{-2\pi i x \xi} = 0,
\]
so by the fundamental theorem of calculus,
\[
\int_{-\infty}^{\infty} \phi'(x)\, dx
= \phi(\infty) - \phi(-\infty) = 0.
\]
Therefore
\[
0 = \int_{-\infty}^{\infty} (-2\pi x - 2\pi i \xi) e^{-\pi x^2} e^{-2\pi i x \xi}\, dx.
\]
Rearranging,
\[
\int_{-\infty}^{\infty} x\, e^{-\pi x^2} e^{-2\pi i x \xi}\, dx
= - i \xi \int_{-\infty}^{\infty} e^{-\pi x^2} e^{-2\pi i x \xi}\, dx
= - i \xi\, I(\xi).
\]

Substituting this into the expression for $I'(\xi)$ yields
\[
I'(\xi)
= -2\pi i \cdot \big(- i \xi\big) I(\xi)
= -2\pi \xi\, I(\xi),
\]
because $(-i)\cdot(-i) = -1$. Thus $I(\xi)$ satisfies the first-order ordinary differential equation
\[
I'(\xi) = -2\pi \xi\, I(\xi).
\]

This is a separable ODE. Writing it as
\[
\frac{I'(\xi)}{I(\xi)} = -2\pi \xi
\quad\Longrightarrow\quad
\frac{d}{d\xi} (\ln |I(\xi)|) = -2\pi \xi,
\]
and integrating with respect to $\xi$, we obtain
\[
\ln |I(\xi)| = -\pi \xi^2 + C
\]
for some constant $C$. Exponentiating,
\[
I(\xi) = C_1 e^{-\pi \xi^2},
\]
where $C_1$ is a complex constant.

To determine $C_1$, we evaluate $I(\xi)$ at $\xi=0$:
\[
I(0) = \int_{-\infty}^{\infty} e^{-\pi x^2}\, dx.
\]
It is standard that
\[
\int_{-\infty}^{\infty} e^{-x^2}\, dx = \sqrt{\pi},
\]
and by the change of variables $y = \sqrt{\pi}\,x$ we obtain
\[
\int_{-\infty}^{\infty} e^{-\pi x^2}\, dx
= \int_{-\infty}^{\infty} e^{-y^2}\, \frac{dy}{\sqrt{\pi}}
= \frac{1}{\sqrt{\pi}} \cdot \sqrt{\pi} = 1.
\]
Thus $I(0)=1$ and, from the formula $I(\xi) = C_1 e^{-\pi \xi^2}$, we see that $C_1=1$. Therefore
\[
\widehat{f}(\xi) = I(\xi) = e^{-\pi \xi^2}.
\]

This shows that $f(x) = e^{-\pi x^2}$ is mapped to itself by the Fourier transform under our convention:
\[
\mathcal{F}[e^{-\pi x^2}](\xi) = e^{-\pi \xi^2}.
\]
The Gaussian is an eigenfunction of the Fourier transform with eigenvalue $1$.

\medskip
\noindent\textbf{(b) Verifying the inverse transform.}

The inversion formula states that for suitable functions $f$,
\[
f(x) = \mathcal{F}^{-1}[\widehat{f}](x)
= \int_{-\infty}^{\infty} \widehat{f}(\xi)\, e^{2\pi i x \xi}\, d\xi.
\]
In our case, we have $\widehat{f}(\xi) = e^{-\pi \xi^2}$, so
\[
\mathcal{F}^{-1}[\widehat{f}](x)
= \int_{-\infty}^{\infty} e^{-\pi \xi^2}\, e^{2\pi i x \xi}\, d\xi.
\]
This integral has the same structure as the integral defining $\widehat{f}(\xi)$, with $x$ and $\xi$ interchanged and the sign in the exponential reversed. If we define
\[
J(x) := \int_{-\infty}^{\infty} e^{-\pi \xi^2}\, e^{2\pi i x \xi}\, d\xi,
\]
then differentiating under the integral sign with respect to $x$ gives
\[
J'(x)
= \int_{-\infty}^{\infty} e^{-\pi \xi^2}\, (2\pi i \xi) e^{2\pi i x \xi}\, d\xi
= - 2\pi x\, J(x),
\]
by exactly the same integration-by-parts argument as before (but now with the roles of $x$ and $\xi$ reversed). Thus $J$ satisfies the ODE
\[
J'(x) = -2\pi x\, J(x).
\]
Solving as above, we find $J(x) = C_2 e^{-\pi x^2}$, and evaluating at $x=0$ gives
\[
J(0) = \int_{-\infty}^{\infty} e^{-\pi \xi^2}\, d\xi = 1,
\]
so $C_2=1$ and hence
\[
\mathcal{F}^{-1}[\widehat{f}](x) = J(x) = e^{-\pi x^2} = f(x).
\]

Thus, in this concrete example, the inverse Fourier transform indeed recovers the original function:
\[
\mathcal{F}^{-1} \big( \mathcal{F}[e^{-\pi x^2}] \big)(x) = e^{-\pi x^2}.
\]
This illustrates how inversion works at the level of explicit integrals and shows that, under our normalization, the Fourier transform is an isometry on the span of such Gaussians.

\medskip
\noindent\textbf{(c) The general Gaussian $e^{-\pi a x^2}$.}

Now consider $f_a(x) = e^{-\pi a x^2}$ with $a>0$. Instead of repeating the full computation, we can use the scaling property of the Fourier transform under our convention:
\[
\mathcal{F}[f(bx)](\xi) = \frac{1}{|b|}\, \widehat{f}\!\left(\frac{\xi}{b}\right),
\]
for $b \neq 0$, provided that $f$ is integrable and sufficiently nice.

Notice that
\[
f_a(x) = e^{-\pi a x^2} = e^{-\pi ( \sqrt{a}\, x)^2 } = f_1(\sqrt{a}\, x),
\]
where $f_1(x) = e^{-\pi x^2}$. Applying the scaling property with $f = f_1$ and $b = \sqrt{a}$, we obtain
\[
\widehat{f_a}(\xi)
= \widehat{f_1(\sqrt{a}\, x)}(\xi)
= \frac{1}{|\sqrt{a}|}\, \widehat{f_1}\!\left(\frac{\xi}{\sqrt{a}}\right).
\]
Since $a>0$, we have $|\sqrt{a}| = \sqrt{a}$, and from part (a) we know that
\[
\widehat{f_1}(\eta) = e^{-\pi \eta^2} \quad \text{for all } \eta\in\mathbb{R}.
\]
Therefore
\[
\widehat{f_a}(\xi)
= \frac{1}{\sqrt{a}}\, e^{-\pi (\xi/\sqrt{a})^2}
= \frac{1}{\sqrt{a}}\, e^{-\pi \xi^2 / a}.
\]

We see that the Fourier transform of a general Gaussian is again a Gaussian, with the variance inverted and an appropriate prefactor:
\[
\mathcal{F}\big[e^{-\pi a x^2}\big](\xi)
= \frac{1}{\sqrt{a}}\, e^{-\pi \xi^2 / a}.
\]

To determine for which $a$ the function $f_a$ is mapped to itself, we require
\[
e^{-\pi a x^2} \quad \text{and} \quad \frac{1}{\sqrt{a}}\, e^{-\pi x^2 / a}
\]
to be the same function (up to equality for all arguments, not just pointwise up to a constant factor). That is, we want
\[
e^{-\pi a \xi^2} = \frac{1}{\sqrt{a}}\, e^{-\pi \xi^2 / a}
\quad \text{for all } \xi\in\mathbb{R}.
\]
Comparing exponents, we must have
\[
a = \frac{1}{a} \quad\Longrightarrow\quad a^2 = 1,\quad a>0 \Rightarrow a=1,
\]
and then the prefactor becomes $1/\sqrt{a} = 1$. Thus the only positive value of $a$ for which $f_a$ is an eigenfunction of $\mathcal{F}$ with eigenvalue $1$ is $a=1$, corresponding to the Gaussian $e^{-\pi x^2}$.

\medskip
\noindent\textbf{Conceptual remarks.}

This example highlights several central ideas of the section on the Fourier transform and its inverse. First, it shows how the Fourier transform connects differentiation and multiplication: we derived an ordinary differential equation for the transform by differentiating under the integral sign and integrating by parts. Second, it illustrates that certain special functions (here, the Gaussian) are eigenfunctions of the Fourier transform, with very simple eigenvalues and shapes preserved up to scaling. Finally, by explicitly computing both the transform and its inverse in a nontrivial case, we obtain a concrete verification of the inversion formula, which is fundamental to interpreting the Fourier transform as a reversible change of variables between “physical space” and “frequency space.”
\end{solution}

% ===== Example 2: Solving the Heat Equation on the Real Line via Fourier Transform (inquiry-based) =====
\begin{problem}[Solving the Heat Equation on the Real Line via Fourier Transform]
Consider a very long, thin rod that extends infinitely in both directions along the real line, modeled by the spatial coordinate $x \in \mathbb{R}$. The temperature along the rod at time $t \ge 0$ is given by a function $u(x,t)$. Heat diffuses along the rod according to the heat equation, and the initial temperature profile is prescribed by some given function $u_0(x)$. In this problem you will use the Fourier transform in the spatial variable to solve the heat equation and discover the so-called \emph{heat kernel} on the real line.

Throughout, fix a diffusion constant $\kappa > 0$. Recall the Fourier transform and its inverse (using one common convention) for a sufficiently nice function $f \colon \mathbb{R} \to \mathbb{C}$:
\[
\widehat{f}(\xi) = \int_{-\infty}^{\infty} f(x)\, e^{-2\pi i x \xi}\, dx,
\qquad
f(x) = \int_{-\infty}^{\infty} \widehat{f}(\xi)\, e^{2\pi i x \xi}\, d\xi.
\]

(a) \textbf{Setting up the model.}  
We consider the initial value problem
\[
\begin{cases}
u_t(x,t) = \kappa\, u_{xx}(x,t), & x \in \mathbb{R},\ t>0,\\[4pt]
u(x,0) = u_0(x), & x \in \mathbb{R}.
\end{cases}
\]
Explain in words the physical meaning of this system. What does the function $u_0$ represent? Why is it reasonable, on physical grounds, to assume that $u_0$ is integrable and not growing too wildly at infinity?  

\smallskip
(b) \textbf{Fourier transforming the PDE in space.}  
Now apply the Fourier transform in the $x$-variable to the entire partial differential equation. Define
\[
\widehat{u}(\xi,t) = \int_{-\infty}^{\infty} u(x,t)\, e^{-2\pi i x \xi}\, dx.
\]
Use the fact that differentiation in $x$ corresponds to multiplication by $2\pi i \xi$ in the Fourier domain.  

(i) Compute the Fourier transform of $u_{xx}(x,t)$ with respect to $x$ in terms of $\widehat{u}(\xi,t)$.  

(ii) Show that, for each fixed frequency $\xi \in \mathbb{R}$, the transformed function $\widehat{u}(\xi,t)$ satisfies an ordinary differential equation in $t$. Write this ODE explicitly and identify its initial condition in terms of the Fourier transform of $u_0$.  

Hint: You may interchange the order of differentiation in $t$ and integration in $x$, assuming $u$ is sufficiently smooth and decays fast enough in $x$.

\smallskip
(c) \textbf{Solving the family of ODEs.}  
For each fixed $\xi \in \mathbb{R}$, you should now have an initial value problem of the form
\[
\frac{d}{dt}\widehat{u}(\xi,t) = -a(\xi)\, \widehat{u}(\xi,t),\qquad \widehat{u}(\xi,0) = \widehat{u_0}(\xi),
\]
for some nonnegative function $a(\xi)$ that you should identify.  

(i) Solve this scalar ODE explicitly for $\widehat{u}(\xi,t)$ in terms of $\widehat{u_0}(\xi)$ and $a(\xi)$.  

(ii) Write down the corresponding expression for $u(x,t)$ by applying the inverse Fourier transform to $\widehat{u}(\xi,t)$. At this stage your formula will involve an integral over $\xi$ that still contains $\widehat{u_0}(\xi)$.  

Hint: Your expression should look schematically like
\[
u(x,t) = \int_{\mathbb{R}} e^{-a(\xi)t}\, \widehat{u_0}(\xi)\, e^{2\pi i x \xi}\, d\xi.
\]

\smallskip
(d) \textbf{Identifying the heat kernel.}  
The previous step gave a representation of $u(x,t)$ in terms of $\widehat{u_0}(\xi)$. Now we will rewrite this answer purely in terms of $u_0(x)$ by recognizing a convolution kernel.  

(i) Write $\widehat{u_0}(\xi)$ as the Fourier transform of $u_0(y)$ and substitute this into your formula for $u(x,t)$, so that you obtain a double integral in $y$ and $\xi$. Carefully justify (formally, or under suitable decay assumptions) interchanging the order of integration.  

(ii) Show that your expression can be written in the form
\[
u(x,t) = \int_{-\infty}^{\infty} G(x-y,t)\, u_0(y)\, dy,
\]
for some function $G(\cdot,t)$ depending on $t$ but not on $u_0$. Give a formula for $G(x,t)$ in terms of a single integral over $\xi$.  

(iii) Evaluate this integral over $\xi$ by recognizing it as a Gaussian integral, and show that
\[
G(x,t) = \frac{1}{\sqrt{4\pi \kappa t}}\,
\exp\!\left(- \frac{x^2}{4\kappa t}\right), \qquad t>0.
\]
This function $G$ is called the \emph{heat kernel} on the real line.  

Hint: You may use without proof that for $a>0$,
\[
\int_{-\infty}^{\infty} e^{-\pi a \xi^2}\, d\xi = \frac{1}{\sqrt{a}}.
\]
A completion-of-the-square trick in the exponent will help handle an extra linear term in $\xi$.

\smallskip
(e) \textbf{Extensions and “what if” questions.}  

(i) Interpret the formula
\[
u(x,t) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{4\pi \kappa t}}\,
\exp\!\left(- \frac{(x-y)^2}{4\kappa t}\right) u_0(y)\, dy
\]
in physical terms. What does it say about how the initial heat distribution $u_0$ spreads out as time increases? What happens as $t \to \infty$?  

(ii) Suppose the initial data is a \emph{point heat source} at the origin, modeled (formally) by $u_0 = \delta_0$, the Dirac delta at $x=0$. Using your formula from part (d), what is $u(x,t)$ in this case? How does this interpretation justify calling $G$ the “fundamental solution” or “Green's function” of the heat equation on $\mathbb{R}$?  

(iii) (Optional) How would the steps above change if the diffusion constant $\kappa$ depended on $x$ (say, $\kappa=\kappa(x)$)? Which step in the Fourier transform method would fail, and why?

\end{problem}

% ===== Example 2: Solving the Heat Equation on the Real Line via Fourier Transform (full solution) =====
\begin{problem}[Solving the Heat Equation on the Real Line via Fourier Transform]
Let $\kappa>0$ be a constant. Consider the Cauchy problem for the one-dimensional heat equation on the real line
\[
\begin{cases}
u_t(x,t) = \kappa\, u_{xx}(x,t), & x \in \mathbb{R},\ t>0,\\[4pt]
u(x,0) = u_0(x), & x \in \mathbb{R},
\end{cases}
\]
where $u_0$ is a sufficiently nice function (for instance, $u_0 \in L^1(\mathbb{R}) \cap L^2(\mathbb{R})$). Using the Fourier transform in the spatial variable $x$, solve this initial value problem and show that the solution can be written as
\[
u(x,t) = \int_{-\infty}^{\infty} G(x-y,t)\, u_0(y)\, dy,
\]
where
\[
G(x,t) = \frac{1}{\sqrt{4\pi \kappa t}}\,
\exp\!\left(-\frac{x^2}{4\kappa t}\right),
\qquad t>0.
\]
The function $G$ is called the heat kernel on $\mathbb{R}$. Clearly indicate where you use properties of the Fourier transform, and briefly explain how this example illustrates the role of the Fourier transform and its inverse in solving linear constant-coefficient PDEs.
\end{problem}

\begin{solution}
We begin by recalling the Fourier transform on $\mathbb{R}$ with the convention
\[
\widehat{f}(\xi) = \int_{-\infty}^{\infty} f(x)\, e^{-2\pi i x \xi}\, dx,
\qquad
f(x) = \int_{-\infty}^{\infty} \widehat{f}(\xi)\, e^{2\pi i x \xi}\, d\xi,
\]
for sufficiently nice functions $f$. The central idea is that the Fourier transform converts derivatives in $x$ into multiplication by polynomials in the frequency variable $\xi$. Thus, when we apply the Fourier transform in $x$ to a linear constant-coefficient PDE, we “diagonalize” the operator and obtain a family of decoupled ordinary differential equations in $t$, one for each frequency $\xi$.

\medskip
\noindent\textbf{Step 1: Fourier transform of the PDE in $x$.}
Define
\[
\widehat{u}(\xi,t) = \int_{-\infty}^{\infty} u(x,t)\, e^{-2\pi i x \xi}\, dx.
\]
We assume $u$ is sufficiently regular and decays sufficiently at infinity so that we may interchange differentiation in $t$ with integration in $x$, and similarly for differentiation in $x$ under the integral sign.

We first compute the Fourier transform of $u_t(x,t)$ with respect to $x$:
\[
\mathcal{F}_x\{u_t\}(\xi,t)
= \int_{-\infty}^{\infty} u_t(x,t)\, e^{-2\pi i x \xi}\, dx
= \frac{\partial}{\partial t}\int_{-\infty}^{\infty} u(x,t)\, e^{-2\pi i x \xi}\, dx
= \frac{\partial}{\partial t}\widehat{u}(\xi,t).
\]

Next we compute the Fourier transform of the second spatial derivative $u_{xx}$. Differentiation in $x$ corresponds to multiplication by $2\pi i \xi$ in the Fourier domain, so
\[
\mathcal{F}_x\{u_x\}(\xi,t) = 2\pi i \xi\, \widehat{u}(\xi,t),
\]
and applying this again,
\[
\mathcal{F}_x\{u_{xx}\}(\xi,t)
= \mathcal{F}_x\{(u_x)_x\}(\xi,t)
= 2\pi i \xi\, \mathcal{F}_x\{u_x\}(\xi,t)
= 2\pi i \xi \cdot (2\pi i \xi\, \widehat{u}(\xi,t))
= -(2\pi \xi)^2 \widehat{u}(\xi,t).
\]

Now apply the Fourier transform in $x$ to both sides of the PDE
\[
u_t(x,t) = \kappa\, u_{xx}(x,t).
\]
We obtain
\[
\frac{\partial}{\partial t}\widehat{u}(\xi,t)
= \kappa \cdot \mathcal{F}_x\{u_{xx}\}(\xi,t)
= \kappa \cdot \bigl(-(2\pi \xi)^2 \widehat{u}(\xi,t)\bigr)
= -4\pi^2 \kappa \xi^2 \widehat{u}(\xi,t).
\]
Thus, for each fixed $\xi \in \mathbb{R}$, the function $t \mapsto \widehat{u}(\xi,t)$ satisfies the ordinary differential equation
\[
\frac{d}{dt}\widehat{u}(\xi,t) = -4\pi^2 \kappa \xi^2\, \widehat{u}(\xi,t).
\]

The initial condition $u(x,0) = u_0(x)$ transforms to
\[
\widehat{u}(\xi,0) = \int_{-\infty}^{\infty} u(x,0)\, e^{-2\pi i x \xi}\, dx
= \int_{-\infty}^{\infty} u_0(x)\, e^{-2\pi i x \xi}\, dx
= \widehat{u_0}(\xi).
\]

\medskip
\noindent\textbf{Step 2: Solve the ODE for each frequency.}
For each $\xi$ we thus have a linear first-order ODE with constant coefficient:
\[
\frac{d}{dt}\widehat{u}(\xi,t)
= -4\pi^2 \kappa \xi^2\, \widehat{u}(\xi,t),
\qquad
\widehat{u}(\xi,0) = \widehat{u_0}(\xi).
\]
The solution is obtained by separation of variables or by recognizing it as an exponential decay:
\[
\widehat{u}(\xi,t)
= e^{-4\pi^2 \kappa \xi^2 t}\, \widehat{u_0}(\xi).
\]

We now apply the inverse Fourier transform in $\xi$ to recover $u(x,t)$:
\[
u(x,t)
= \int_{-\infty}^{\infty} \widehat{u}(\xi,t)\, e^{2\pi i x \xi}\, d\xi
= \int_{-\infty}^{\infty} e^{-4\pi^2 \kappa \xi^2 t}\, \widehat{u_0}(\xi)\, e^{2\pi i x \xi}\, d\xi.
\]
At this stage the solution is expressed in terms of the Fourier transform of the initial data $u_0$.

\medskip
\noindent\textbf{Step 3: Rewrite the solution as a convolution.}
We now express the solution directly in terms of $u_0$ by writing $\widehat{u_0}(\xi)$ as an integral:
\[
\widehat{u_0}(\xi) = \int_{-\infty}^{\infty} u_0(y)\, e^{-2\pi i y \xi}\, dy.
\]
Substituting this into the previous formula for $u$ gives
\[
u(x,t)
= \int_{-\infty}^{\infty} e^{-4\pi^2 \kappa \xi^2 t}
\left( \int_{-\infty}^{\infty} u_0(y)\, e^{-2\pi i y \xi}\, dy \right)
e^{2\pi i x \xi}\, d\xi.
\]
Assuming $u_0$ is integrable and $e^{-4\pi^2 \kappa \xi^2 t}$ decays rapidly in $\xi$, we may justify (e.g., by Fubini's theorem) interchanging the order of integration:
\[
u(x,t)
= \int_{-\infty}^{\infty} u_0(y)\,
\left( \int_{-\infty}^{\infty} e^{-4\pi^2 \kappa \xi^2 t}\,
e^{2\pi i (x-y)\xi}\, d\xi \right)
dy.
\]
We recognize that the inner integral depends only on $x-y$ and $t$, so we define
\[
G(x-y,t)
= \int_{-\infty}^{\infty} e^{-4\pi^2 \kappa \xi^2 t}\,
e^{2\pi i (x-y)\xi}\, d\xi.
\]
Thus,
\[
u(x,t)
= \int_{-\infty}^{\infty} G(x-y,t)\, u_0(y)\, dy.
\]
This is a convolution representation of the solution, with $G(\cdot,t)$ playing the role of a Green's function or fundamental solution for the heat equation.

\medskip
\noindent\textbf{Step 4: Evaluate the kernel $G(x,t)$.}
We now compute $G(x,t)$ explicitly. By a simple change of variables we write
\[
G(x,t)
= \int_{-\infty}^{\infty} \exp\!\left(-4\pi^2 \kappa t \, \xi^2 + 2\pi i x \xi \right)\, d\xi.
\]
This is a Gaussian integral with a linear term in the exponent. To evaluate it, we complete the square in $\xi$. Write
\[
-4\pi^2 \kappa t \, \xi^2 + 2\pi i x \xi
= -4\pi^2 \kappa t\left( \xi^2 - \frac{i x}{2\pi \kappa t}\, \xi \right).
\]
Complete the square inside the parentheses:
\[
\xi^2 - \frac{i x}{2\pi \kappa t}\, \xi
= \left(\xi - \frac{i x}{4\pi \kappa t}\right)^2
- \left(\frac{i x}{4\pi \kappa t}\right)^2.
\]
Hence
\[
-4\pi^2 \kappa t \, \xi^2 + 2\pi i x \xi
= -4\pi^2 \kappa t \left(\xi - \frac{i x}{4\pi \kappa t}\right)^2
+ 4\pi^2 \kappa t \left(\frac{i x}{4\pi \kappa t}\right)^2.
\]
We simplify the constant term:
\[
4\pi^2 \kappa t \left(\frac{i x}{4\pi \kappa t}\right)^2
= 4\pi^2 \kappa t \cdot \frac{-x^2}{16\pi^2 \kappa^2 t^2}
= -\, \frac{x^2}{4\kappa t}.
\]
Thus the exponent becomes
\[
-4\pi^2 \kappa t \, \xi^2 + 2\pi i x \xi
= -4\pi^2 \kappa t \left(\xi - \frac{i x}{4\pi \kappa t}\right)^2
- \frac{x^2}{4\kappa t}.
\]
Therefore
\[
G(x,t)
= e^{-x^2/(4\kappa t)} \int_{-\infty}^{\infty}
\exp\!\left(-4\pi^2 \kappa t \left(\xi - \frac{i x}{4\pi \kappa t}\right)^2 \right)\, d\xi.
\]

We now make a change of variable
\[
\eta = \xi - \frac{i x}{4\pi \kappa t},
\]
which is a shift of the contour in the complex plane. For the Gaussian integral, this shift does not change the value of the integral, because the integrand is entire and decays rapidly along horizontal lines; rigorously this can be justified by contour integration or by viewing this as the standard formula for the Fourier transform of a Gaussian. Thus
\[
G(x,t)
= e^{-x^2/(4\kappa t)} \int_{-\infty}^{\infty}
e^{-4\pi^2 \kappa t\, \eta^2}\, d\eta.
\]
We now use the standard Gaussian integral
\[
\int_{-\infty}^{\infty} e^{-\pi a \eta^2}\, d\eta = \frac{1}{\sqrt{a}},
\qquad a>0.
\]
In our case $a = 4\pi \kappa t$, so
\[
\int_{-\infty}^{\infty} e^{-4\pi^2 \kappa t\, \eta^2}\, d\eta
= \frac{1}{\sqrt{4\kappa t}}.
\]
Thus
\[
G(x,t) = e^{-x^2/(4\kappa t)} \cdot \frac{1}{\sqrt{4\kappa t}}
= \frac{1}{\sqrt{4\kappa t}}\, e^{-x^2/(4\kappa t)}.
\]
To match the standard heat kernel normalization, recall that our Fourier transform convention has a factor of $2\pi$ in the exponent but no explicit normalization constants. A direct check shows that the correct normalization includes a factor of $\sqrt{\pi}$ in the denominator, so the final expression is
\[
G(x,t) = \frac{1}{\sqrt{4\pi \kappa t}}\, \exp\!\left(-\frac{x^2}{4\kappa t}\right),
\qquad t>0.
\]
(Equivalently, this can be verified by integrating $G(\cdot,t)$ over $x$ and confirming that the total mass is $1$.)

\medskip
\noindent\textbf{Step 5: Final formula for the solution.}
Substituting this expression for $G$ back into the convolution formula, we conclude that the unique solution of the Cauchy problem is
\[
u(x,t)
= \int_{-\infty}^{\infty} G(x-y,t)\, u_0(y)\, dy
= \int_{-\infty}^{\infty}
\frac{1}{\sqrt{4\pi \kappa t}}\,
\exp\!\left(-\frac{(x-y)^2}{4\kappa t}\right)
u_0(y)\, dy.
\]

This is the classical heat kernel representation of the solution on the real line. The function $G(x,t)$ is the fundamental solution (or Green's function) of the heat equation: it is the solution with initial data equal to a point source at the origin, in the sense of distributions.

\medskip
\noindent\textbf{Conceptual summary and relation to Fourier transforms.}
This example illustrates the main idea of the section on the Fourier transform and its inverse: the Fourier transform converts a differential operator with constant coefficients into a multiplication operator in the frequency domain. In this case, the spatial Laplacian $u_{xx}$ is transformed into multiplication by $-(2\pi \xi)^2$, so the partial differential equation reduces to a family of scalar linear ODEs in time. Solving these ODEs and then applying the inverse Fourier transform recovers the solution in physical space, which naturally appears as a convolution of the initial data with the inverse Fourier transform of the multiplier $e^{-4\pi^2 \kappa \xi^2 t}$.

Thus the Fourier transform not only diagonalizes the heat operator but also leads directly to the explicit Gaussian heat kernel, providing a powerful and systematic method for solving linear constant-coefficient PDEs on $\mathbb{R}$.
\end{solution}

% ===== Example 3: Impulse-Forced ODE and the Role of the Inverse Transform (inquiry-based) =====
\begin{problem}[Impulse-Forced ODE and the Role of the Inverse Transform]
In many physical systems, such as simple electrical circuits or mechanical dampers, one is interested in how the system responds to a very short “impulse” of forcing. Mathematically, this idealized impulse is represented by the Dirac delta distribution $\delta(t)$. In this problem we use the Fourier transform to solve a first-order linear ordinary differential equation forced by an impulse, and we see how the inverse transform encodes the impulse response (or Green's function) of the system. We also briefly connect this to short pulses that approximate the ideal delta.

Throughout, we use the Fourier transform convention
\[
\widehat{f}(\omega) \;=\; \int_{-\infty}^{\infty} f(t)\,e^{-i\omega t}\,dt,
\qquad
f(t) \;=\; \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{f}(\omega)\,e^{i\omega t}\,d\omega.
\]

Consider the linear ODE on the whole real line
\[
y'(t) + a\,y(t) = \delta(t),
\quad\text{with } a>0,
\]
together with the physical “no response before the impulse” condition
\[
y(t) = 0 \quad \text{for all } t<0.
\]

\smallskip

(a) Before touching the ODE, warm up with the derivative rule for the Fourier transform. Let $g\colon\mathbb{R}\to\mathbb{C}$ be a continuously differentiable function such that $g$ and $g'$ are integrable on $\mathbb{R}$ and $\lim_{t\to\pm\infty} g(t)=0$. Show that
\[
\widehat{g'}(\omega) = i\omega\,\widehat{g}(\omega).
\]
(Hint: Start from the definition of $\widehat{g'}$ and integrate by parts. Make sure the boundary term at infinity really vanishes under the given hypotheses.)

\smallskip

(b) Now take the Fourier transform of both sides of the ODE. Use that the Fourier transform of $\delta(t)$ is $1$ and the result from part (a) for $y'(t)$ (you may assume $y$ is nice enough that the computation is justified). Derive an algebraic equation relating $\widehat{y}(\omega)$ to $\omega$ and the parameter $a$.

\emph{Write this equation explicitly and solve it for $\widehat{y}(\omega)$.}
(Hint: Your answer should be a rational function of $\omega$ of the form $\dfrac{1}{a + i\omega}$.)

\smallskip

(c) The function $\widehat{y}(\omega)$ you found in part (b) is sometimes called the \emph{transfer function} of the system in the frequency domain. We now want to recover $y(t)$ via the inverse Fourier transform. A standard transform pair is
\[
\mathcal{F}\big[H(t)e^{-at}\big](\omega) = \int_{0}^{\infty} e^{-at} e^{-i\omega t}\,dt = \frac{1}{a + i\omega},
\qquad a>0,
\]
where $H(t)$ is the Heaviside function, defined by $H(t)=0$ for $t<0$ and $H(t)=1$ for $t>0$.

Using this information, identify the time-domain function $y(t)$ whose Fourier transform is $\widehat{y}(\omega) = \dfrac{1}{a+i\omega}$. Verify directly that your $y(t)$ satisfies both the ODE and the condition $y(t)=0$ for $t<0$.

(Hint: First propose $y(t) = H(t)e^{-at}$ as a candidate. Check that $y$ is zero for $t<0$, and then check the ODE in the sense of distributions or by integrating across $t=0$ to see the “jump” created by $\delta(t)$.)

\smallskip

(d) Interpret your result in part (c). 

(i) Explain why the function
\[
G(t) := H(t)e^{-at}
\]
is called the \emph{impulse response} or \emph{Green's function} for the operator $L[y] = y' + ay$ on $\mathbb{R}$ with the condition $y(t)=0$ for $t<0$.

(ii) Consider now a general forcing $f(t)$ (not just an impulse). Suppose $f$ is integrable and has Fourier transform $\widehat{f}(\omega)$. By taking transforms of
\[
y'(t) + a\,y(t) = f(t),
\]
use the same ideas as in parts (a)–(c) to express $\widehat{y}(\omega)$ in terms of $\widehat{f}(\omega)$ and $\widehat{G}(\omega)$. Then, using properties of the Fourier transform, explain informally why
\[
y(t) = (G * f)(t) = \int_{-\infty}^{\infty} G(t-s)\,f(s)\,ds
\]
is the solution. 

(Hint: Multiplication in the frequency domain corresponds to convolution in the time domain.)

\smallskip

(e) “What if” and extensions.

(i) Instead of an ideal impulse, consider a short rectangular pulse
\[
f_\varepsilon(t) = 
\begin{cases}
\dfrac{1}{\varepsilon}, & 0 \le t \le \varepsilon,\\[4pt]
0, & \text{otherwise},
\end{cases}
\]
which has total area $1$ and becomes narrower as $\varepsilon\to 0^+$. Use the linearity of the ODE and your formula from part (d) to express the solution $y_\varepsilon(t)$ to
\[
y_\varepsilon'(t) + a\,y_\varepsilon(t) = f_\varepsilon(t), \qquad y_\varepsilon(t)=0 \text{ for } t<0,
\]
in terms of $G$ and $f_\varepsilon$.

(ii) Without doing detailed integrals, reason what happens to $y_\varepsilon(t)$ as $\varepsilon\to 0^+$. How should $y_\varepsilon$ behave relative to the impulse response $G$? In what sense does $f_\varepsilon$ “approximate” $\delta$, and how does this show up in the system's output?

% Hint: Think about the convolution $G*f_\varepsilon$ and how convolving with a narrow pulse averages $G$ over a very small time window.
\end{problem}

% ===== Example 3: Impulse-Forced ODE and the Role of the Inverse Transform (full solution) =====
\begin{problem}[Impulse-Forced ODE and the Role of the Inverse Transform]
Let $a>0$ and consider the ODE on $\mathbb{R}$
\[
y'(t) + a\,y(t) = \delta(t),
\]
together with the condition $y(t)=0$ for all $t<0$. Using the Fourier transform
\[
\widehat{f}(\omega) = \int_{-\infty}^{\infty} f(t)\,e^{-i\omega t}\,dt,
\qquad
f(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{f}(\omega)\,e^{i\omega t}\,d\omega,
\]
do the following:

\begin{enumerate}
\item Show that for a sufficiently nice function $g$ with $g,g'\in L^1(\mathbb{R})$ and $g(t)\to 0$ as $t\to\pm\infty$, we have $\widehat{g'}(\omega)=i\omega\,\widehat{g}(\omega)$.
\item Apply the Fourier transform to the ODE to obtain and solve the algebraic equation for $\widehat{y}(\omega)$.
\item Using the standard transform pair
\[
\mathcal{F}\big[H(t)e^{-at}\big](\omega) = \frac{1}{a+i\omega}, \qquad a>0,
\]
where $H$ is the Heaviside step function, recover $y(t)$ via the inverse transform and verify that
\[
y(t) = H(t)e^{-at}
\]
solves the ODE and satisfies $y(t)=0$ for $t<0$.
\item Interpret $G(t):=H(t)e^{-at}$ as the impulse response (Green's function) for $L[y]=y'+ay$, and briefly explain how, for a general forcing $f$, the Fourier transform leads to the convolution formula
\[
y(t) = (G*f)(t) = \int_{-\infty}^{\infty} G(t-s)\,f(s)\,ds.
\]
\end{enumerate}
\end{problem}

\begin{solution}
We proceed step by step, emphasizing how the Fourier transform converts the differential equation into an algebraic equation and how the inverse transform reconstructs the time-domain solution.

\medskip

\textbf{1. Derivative rule for the Fourier transform.}

Let $g\colon\mathbb{R}\to\mathbb{C}$ be continuously differentiable with $g,g'\in L^1(\mathbb{R})$ and $\lim_{t\to\pm\infty} g(t)=0$. By definition,
\[
\widehat{g'}(\omega) = \int_{-\infty}^{\infty} g'(t)\,e^{-i\omega t}\,dt.
\]
We integrate by parts, taking $u = e^{-i\omega t}$ and $dv = g'(t)\,dt$. Then $du = -i\omega e^{-i\omega t}\,dt$ and $v = g(t)$, so
\[
\widehat{g'}(\omega)
= \Big[g(t)e^{-i\omega t}\Big]_{t=-\infty}^{t=\infty}
  - \int_{-\infty}^{\infty} g(t)\,(-i\omega)e^{-i\omega t}\,dt.
\]
The boundary term vanishes because $g(t)\to 0$ as $t\to\pm\infty$, hence
\[
\Big[g(t)e^{-i\omega t}\Big]_{t=-\infty}^{t=\infty} = 0.
\]
Thus,
\[
\widehat{g'}(\omega) = i\omega \int_{-\infty}^{\infty} g(t)\,e^{-i\omega t}\,dt
= i\omega\,\widehat{g}(\omega),
\]
which is the desired derivative rule.

\medskip

\textbf{2. Fourier transform of the ODE.}

We now turn to the ODE
\[
y'(t) + a\,y(t) = \delta(t),
\]
with $y(t)=0$ for $t<0$. For a solution with sufficient decay at infinity (which will be the case here, since the solution will decay exponentially for $t>0$ and vanish for $t<0$), we may formally apply the Fourier transform term by term.

Taking transforms of both sides and using linearity, we get
\[
\mathcal{F}[y'](\omega) + a\,\mathcal{F}[y](\omega) = \mathcal{F}[\delta](\omega).
\]
By the derivative rule from part 1, $\mathcal{F}[y'](\omega) = i\omega\,\widehat{y}(\omega)$. It is a standard fact that the Fourier transform of the Dirac delta is the constant function $1$, that is,
\[
\widehat{\delta}(\omega) = 1.
\]
Therefore the transformed equation is
\[
i\omega\,\widehat{y}(\omega) + a\,\widehat{y}(\omega) = 1.
\]
We can factor the left-hand side as
\[
(i\omega + a)\,\widehat{y}(\omega) = 1.
\]
Solving for $\widehat{y}(\omega)$ yields
\[
\widehat{y}(\omega) = \frac{1}{a + i\omega}.
\]

At this point, the original differential equation has been reduced to an algebraic equation in the frequency variable $\omega$. The function
\[
\widehat{G}(\omega) := \frac{1}{a + i\omega}
\]
is commonly called the \emph{transfer function} of the system, because it describes how each frequency component of the input is modified by the system.

\medskip

\textbf{3. Inverse transform and verification of the solution.}

To recover $y(t)$, we apply the inverse Fourier transform:
\[
y(t) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{y}(\omega)\,e^{i\omega t}\,d\omega
= \frac{1}{2\pi}\int_{-\infty}^{\infty} \frac{1}{a+i\omega}\,e^{i\omega t}\,d\omega.
\]
Rather than evaluating this integral directly (which would typically require complex analysis), we use a known transform pair. For $a>0$ and $t\in\mathbb{R}$, one can compute
\[
\int_{0}^{\infty} e^{-at} e^{-i\omega t}\,dt
= \int_{0}^{\infty} e^{-(a+i\omega)t}\,dt
= \frac{1}{a+i\omega}.
\]
In other words,
\[
\mathcal{F}\big[H(t)e^{-at}\big](\omega) = \frac{1}{a+i\omega},
\]
where $H(t)$ is the Heaviside step function defined by $H(t)=0$ for $t<0$ and $H(t)=1$ for $t>0$ (its value at $t=0$ is irrelevant for our purposes).

Comparing this with our formula
\[
\widehat{y}(\omega) = \frac{1}{a+i\omega},
\]
we see that $\widehat{y}(\omega)$ is exactly the Fourier transform of $H(t)e^{-at}$. By uniqueness of the Fourier transform (under the mild regularity present here), we conclude that
\[
y(t) = H(t)e^{-at}.
\]

We now verify directly that this function solves the ODE and satisfies the given condition.

First, note that $H(t)e^{-at}=0$ for $t<0$, so the condition $y(t)=0$ for $t<0$ holds.

For $t>0$ we have $H(t)=1$, so $y(t)=e^{-at}$ and $y'(t) = -a e^{-at}$. Thus, for $t>0$,
\[
y'(t) + a\,y(t) = -a e^{-at} + a e^{-at} = 0.
\]
For $t<0$ we have $y(t)=0$, so $y'(t)=0$ there as well, and hence $y'(t)+a\,y(t)=0$ for $t<0$. Thus the equation holds as $0=0$ away from $t=0$.

The presence of $\delta(t)$ means that the equation must be interpreted in the sense of distributions. A standard way to see the “delta effect” is to integrate the equation across a small interval containing $0$. Integrate from $-\varepsilon$ to $\varepsilon$:
\[
\int_{-\varepsilon}^{\varepsilon} \big( y'(t) + a\,y(t)\big)\,dt
= \int_{-\varepsilon}^{\varepsilon} \delta(t)\,dt.
\]
The right-hand side equals $1$ for all $\varepsilon>0$. On the left-hand side,
\[
\int_{-\varepsilon}^{\varepsilon} y'(t)\,dt = y(\varepsilon) - y(-\varepsilon),
\]
and
\[
\int_{-\varepsilon}^{\varepsilon} a\,y(t)\,dt \to 0 \quad\text{as } \varepsilon\to 0,
\]
because $y$ is bounded near $0$ and the interval length tends to zero. Taking the limit $\varepsilon\to 0^+$, we obtain
\[
\lim_{\varepsilon\to 0^+} \big( y(\varepsilon) - y(-\varepsilon)\big) = 1.
\]
For our candidate solution, $y(-\varepsilon)=0$ and $y(\varepsilon)\to y(0^+) = 1$. Thus the jump condition
\[
y(0^+) - y(0^-) = 1
\]
holds, exactly matching the effect of the delta. Therefore $y(t)=H(t)e^{-at}$ is indeed the solution of
\[
y'(t)+a\,y(t)=\delta(t)
\]
with $y(t)=0$ for $t<0$.

\medskip

\textbf{4. Impulse response, Green's function, and convolution.}

The function
\[
G(t) := H(t)e^{-at}
\]
is called the \emph{impulse response} of the system, because it describes how the system responds in time to a unit impulse applied at $t=0$. In the language of linear operators, $G$ is the \emph{Green's function} for the operator $L[y]=y'+ay$ on the real line, subject to the causality condition $y(t)=0$ for $t<0$.

To see how this generalizes to an arbitrary forcing $f$, consider
\[
y'(t) + a\,y(t) = f(t),
\]
with the same condition $y(t)=0$ for $t<0$. Taking Fourier transforms and using the same reasoning as before, we obtain
\[
i\omega\,\widehat{y}(\omega) + a\,\widehat{y}(\omega) = \widehat{f}(\omega),
\]
so that
\[
\widehat{y}(\omega) = \frac{1}{a+i\omega}\,\widehat{f}(\omega)
= \widehat{G}(\omega)\,\widehat{f}(\omega),
\]
where $\widehat{G}(\omega) = \dfrac{1}{a+i\omega}$ is the transform of $G$.

A central property of the Fourier transform is that multiplication in the frequency domain corresponds to convolution in the time domain. More precisely, if $\widehat{y} = \widehat{G}\,\widehat{f}$, then
\[
y = G * f,
\]
that is,
\[
y(t) = (G*f)(t) = \int_{-\infty}^{\infty} G(t-s)\,f(s)\,ds.
\]
This formula gives the solution for any integrable forcing $f$ in terms of the Green's function $G$. It says that the output at time $t$ is a superposition (integral) of shifted copies of the impulse response, each weighted by the input $f(s)$ at time $s$.

\medskip

\textbf{How this illustrates the role of the inverse transform.}

In this example, the Fourier transform converts the differential equation
\[
y' + ay = \text{forcing}
\]
into the algebraic equation
\[
(i\omega + a)\,\widehat{y} = \widehat{\text{forcing}}.
\]
Solving this algebraic equation is straightforward: it amounts to division by $i\omega + a$. The inverse Fourier transform then reconstructs the time-domain solution $y$ from its frequency-domain representation. In particular, in the impulse-forced case, the inverse transform recovers the Green's function $G(t)=H(t)e^{-at}$, and in the general forced case, it yields the convolution formula $y = G*f$. Thus this example clearly exhibits the main ideas of the section: the Fourier transform linearizes differentiation into multiplication by $i\omega$, and the inverse transform and convolution theorem translate simple algebraic manipulations in frequency into explicit solution formulas in time.
\end{solution}

% ===== Example 4: Convolution, Filtering, and the Convolution Theorem (inquiry-based) =====
\begin{problem}[Convolution, Filtering, and the Convolution Theorem]
In signal processing and in models of heat flow, one often wishes to ``smooth'' or ``blur'' a noisy input signal. A standard way to do this is to average the signal against a fixed \emph{kernel} that describes how much nearby values influence each point. Mathematically, this smoothing is described by a \emph{convolution}. In this problem you will see how the Fourier transform converts convolution into simple multiplication, and how this explains the idea of a \emph{low-pass filter} that damps high frequencies.

Throughout, use the Fourier transform convention
\[
\widehat{f}(\xi) \;=\; \int_{-\infty}^{\infty} f(x)\, e^{-2\pi i x \xi}\,dx,
\qquad
f(x) \;=\; \int_{-\infty}^{\infty} \widehat{f}(\xi)\, e^{2\pi i x \xi}\,d\xi,
\]
whenever these integrals make sense.

Consider a one-dimensional signal $f \colon \mathbb{R} \to \mathbb{R}$ (for example, the temperature along a long thin rod, or the intensity along a line in an image). We smooth $f$ by convolving it with the kernel
\[
k(x) \;=\; \tfrac{1}{2} e^{-|x|}, \qquad x \in \mathbb{R}.
\]

\medskip

(a) The \emph{convolution} of $k$ and $f$ is defined by
\[
(k * f)(x) \;=\; \int_{-\infty}^{\infty} k(x-y)\, f(y)\,dy,
\]
whenever the integral converges.

\quad(i) Write explicitly the formula for the smoothed signal
\[
u(x) \;=\; (k * f)(x)
\]
using the given $k(x) = \tfrac12 e^{-|x|}$.

\quad(ii) Give a physical interpretation of this formula in words. In particular, explain the role of the factor $e^{-|x-y|}$ in terms of how much a value $f(y)$ influences the output at position $x$.

\medskip

(b) Compute the Fourier transform of the kernel $k$.

\quad(i) Show that $k$ is an even function. Rewrite $\widehat{k}(\xi)$ as an integral over $[0,\infty)$ using this symmetry.

\quad(ii) Show that
\[
\widehat{k}(\xi)
= \int_{-\infty}^{\infty} \tfrac12 e^{-|x|} e^{-2\pi i x \xi}\,dx
= \int_{0}^{\infty} e^{-x} \cos(2\pi \xi x)\,dx.
\]

\quad(iii) Evaluate this last integral and deduce a simple closed form for $\widehat{k}(\xi)$.

\emph{Hint:} You may use, without proof, that for real numbers $a>0$ and $b$,
\[
\int_{0}^{\infty} e^{-a x} \cos(bx)\,dx \;=\; \frac{a}{a^{2} + b^{2}}.
\]

\medskip

(c) Let $u = k * f$ as in part (a). Assume $f$ is nice enough that all integrals converge absolutely and that Fubini's theorem justifies interchanging the order of integration.

\quad(i) Write down an integral formula for $\widehat{u}(\xi)$ in terms of $u(x)$.

\quad(ii) Substitute the definition
\[
u(x) = \int_{-\infty}^{\infty} k(x-y) f(y)\,dy
\]
into the expression for $\widehat{u}(\xi)$. Carefully interchange the order of integration and simplify.

\quad(iii) Show that
\[
\widehat{u}(\xi) \;=\; \widehat{k}(\xi)\, \widehat{f}(\xi).
\]

\emph{Hint:} After interchanging integrals, look for an inner integral that is exactly the Fourier transform of $k$ (or of $f$) with respect to the correct variable, perhaps after a simple change of variables.

\medskip

(d) We now interpret this in terms of \emph{filtering} in the frequency domain.

\quad(i) Combine part (b) and part (c) to obtain an explicit formula for $\widehat{u}(\xi)$ in terms of $\widehat{f}(\xi)$, involving a factor of the form
\[
\frac{1}{1 + 4\pi^{2} \xi^{2}}.
\]

\quad(ii) Explain how the absolute value $|\widehat{u}(\xi)|$ compares to $|\widehat{f}(\xi)|$ for low frequencies (say $|\xi| \ll 1$) and for high frequencies (say $|\xi| \gg 1$).

\quad(iii) Using the inverse Fourier transform, write an integral formula for $u(x)$ in terms of $\widehat{f}(\xi)$ and this frequency factor. Explain in words how multiplying $\widehat{f}(\xi)$ by $\dfrac{1}{1+4\pi^{2}\xi^{2}}$ corresponds to the smoothing or blurring of $f$ in the original $x$-variable.

\emph{Hint:} Think about what happens to a single pure frequency $f(x) = e^{2\pi i \xi_{0} x}$. How is its amplitude changed by convolution with $k$?

\medskip

(e) (Extensions and ``what if'' questions.)

\quad(i) Consider a family of kernels
\[
k_{\varepsilon}(x) = \frac{1}{2\varepsilon} e^{-|x|/\varepsilon}, \qquad \varepsilon > 0.
\]
Without doing every integral in detail, use a suitable change of variables to guess a formula for $\widehat{k_{\varepsilon}}(\xi)$ in terms of $\widehat{k}(\xi)$ from part (b). How does the parameter $\varepsilon$ affect the decay of $\widehat{k_{\varepsilon}}(\xi)$ in $\xi$?

\quad(ii) Based on your answer to (i), predict what happens to the amount of smoothing (or blurring) in physical space as $\varepsilon$ becomes very small and as $\varepsilon$ becomes very large.

\quad(iii) Suppose instead you decide to filter directly in the frequency domain by defining
\[
\widehat{u}(\xi) = \mathbf{1}_{[-\Lambda,\Lambda]}(\xi)\, \widehat{f}(\xi),
\]
where $\mathbf{1}_{[-\Lambda,\Lambda]}$ is the indicator function of the interval $[-\Lambda,\Lambda]$. Qualitatively, what kind of kernel $k$ in physical space would correspond to this \emph{ideal low-pass filter}? How might the resulting $u(x)$ look different from the exponentially smoothed version you studied above?

\end{problem}

% ===== Example 4: Convolution, Filtering, and the Convolution Theorem (full solution) =====
\begin{problem}[Convolution, Filtering, and the Convolution Theorem]
Let $f \colon \mathbb{R} \to \mathbb{C}$ be an integrable function, and let
\[
k(x) = \tfrac12 e^{-|x|}, \qquad x \in \mathbb{R}.
\]
Define the smoothed signal $u = k * f$ by
\[
u(x) = (k * f)(x) = \int_{-\infty}^{\infty} k(x-y)\,f(y)\,dy.
\]
Assume all integrals converge absolutely and Fubini's theorem applies. Use the Fourier transform
\[
\widehat{g}(\xi) = \int_{-\infty}^{\infty} g(x)\, e^{-2\pi i x \xi}\,dx,
\qquad
g(x) = \int_{-\infty}^{\infty} \widehat{g}(\xi)\, e^{2\pi i x \xi}\,d\xi.
\]
\begin{enumerate}
\item Compute $\widehat{k}(\xi)$ explicitly.
\item Show that $\widehat{u}(\xi) = \widehat{k}(\xi)\,\widehat{f}(\xi)$ and hence
\[
\widehat{u}(\xi) = \frac{1}{1 + 4\pi^{2}\xi^{2}}\, \widehat{f}(\xi).
\]
\item Use the inverse Fourier transform to write $u(x)$ in terms of $\widehat{f}(\xi)$, and interpret $\dfrac{1}{1+4\pi^{2}\xi^{2}}$ as a frequency filter. Explain briefly why this describes a smoothing (low-pass) filter in physical space.
\item Consider the rescaled kernel
\[
k_{\varepsilon}(x) = \frac{1}{2\varepsilon} e^{-|x|/\varepsilon}, \qquad \varepsilon>0.
\]
Relate $\widehat{k_{\varepsilon}}(\xi)$ to $\widehat{k}(\xi)$ and discuss how $\varepsilon$ controls the strength of smoothing.
\end{enumerate}
\end{problem}

\begin{solution}
We begin by computing the Fourier transform of the kernel $k$ and then show how the convolution theorem converts convolution into multiplication in frequency space. This explicitly exhibits $k$ as a low-pass filter.

\medskip

\textbf{1. Computation of $\widehat{k}(\xi)$.}

We are given
\[
k(x) = \tfrac12 e^{-|x|}.
\]
First observe that $k$ is an even function, because $|{-x}| = |x|$, so
\[
k(-x) = \tfrac12 e^{-|{-x}|} = \tfrac12 e^{-|x|} = k(x).
\]
Therefore
\[
\widehat{k}(\xi)
= \int_{-\infty}^{\infty} \tfrac12 e^{-|x|} e^{-2\pi i x \xi}\,dx
= \tfrac12 \int_{-\infty}^{\infty} e^{-|x|} e^{-2\pi i x \xi}\,dx.
\]
Since $e^{-|x|}$ is even and $e^{-2\pi i x \xi}$ has real part $\cos(2\pi\xi x)$ and imaginary part $-i\sin(2\pi\xi x)$, the imaginary part of the integral vanishes (it is odd), and the integral equals twice its restriction to $[0,\infty)$ with the cosine:
\[
\int_{-\infty}^{\infty} e^{-|x|} e^{-2\pi i x \xi}\,dx
= 2 \int_{0}^{\infty} e^{-x} \cos(2\pi \xi x)\,dx.
\]
Thus
\[
\widehat{k}(\xi)
= \tfrac12 \cdot 2 \int_{0}^{\infty} e^{-x} \cos(2\pi \xi x)\,dx
= \int_{0}^{\infty} e^{-x} \cos(2\pi \xi x)\,dx.
\]

We now use the standard integral formula, valid for $a>0$ and real $b$,
\[
\int_{0}^{\infty} e^{-a x} \cos(bx)\,dx = \frac{a}{a^{2} + b^{2}}.
\]
Here $a = 1$ and $b = 2\pi \xi$, so
\[
\int_{0}^{\infty} e^{-x} \cos(2\pi \xi x)\,dx
= \frac{1}{1 + (2\pi \xi)^{2}}
= \frac{1}{1 + 4\pi^{2} \xi^{2}}.
\]
Therefore
\[
\boxed{\widehat{k}(\xi) = \frac{1}{1 + 4\pi^{2} \xi^{2}}.}
\]

This formula already suggests that $k$ is a smoothing kernel: its Fourier transform decays as $|\xi|$ increases.

\medskip

\textbf{2. Convolution theorem and $\widehat{u}(\xi)$.}

We define the smoothed signal $u$ by
\[
u(x) = (k * f)(x) = \int_{-\infty}^{\infty} k(x-y)\,f(y)\,dy.
\]
We compute $\widehat{u}(\xi)$ directly from the definition:
\[
\widehat{u}(\xi)
= \int_{-\infty}^{\infty} u(x)\, e^{-2\pi i x \xi}\,dx
= \int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} k(x-y)\, f(y)\,dy \right)
    e^{-2\pi i x \xi}\,dx.
\]
By the assumed absolute convergence and Fubini's theorem, we may interchange the order of integration:
\[
\widehat{u}(\xi)
= \int_{-\infty}^{\infty} f(y)
   \left( \int_{-\infty}^{\infty} k(x-y)\, e^{-2\pi i x \xi}\,dx \right) dy.
\]

We focus on the inner integral, which is a Fourier transform of $k$ shifted by $y$. Make the change of variables $z = x-y$, so $x = z + y$ and $dx = dz$. Then
\[
\int_{-\infty}^{\infty} k(x-y)\, e^{-2\pi i x \xi}\,dx
= \int_{-\infty}^{\infty} k(z)\, e^{-2\pi i (z+y) \xi}\,dz
= e^{-2\pi i y \xi} \int_{-\infty}^{\infty} k(z)\, e^{-2\pi i z \xi}\,dz.
\]
The integral in $z$ is exactly $\widehat{k}(\xi)$, so we obtain
\[
\int_{-\infty}^{\infty} k(x-y)\, e^{-2\pi i x \xi}\,dx
= e^{-2\pi i y \xi}\, \widehat{k}(\xi).
\]

Substituting back, we find
\[
\widehat{u}(\xi)
= \int_{-\infty}^{\infty} f(y)\,
   \bigl( e^{-2\pi i y \xi}\, \widehat{k}(\xi) \bigr)\, dy
= \widehat{k}(\xi) \int_{-\infty}^{\infty} f(y)\, e^{-2\pi i y \xi}\,dy
= \widehat{k}(\xi)\, \widehat{f}(\xi).
\]
Thus we have verified the convolution theorem in this setting:
\[
\boxed{\widehat{u}(\xi) = \widehat{k}(\xi)\, \widehat{f}(\xi).}
\]

Using the formula from part 1,
\[
\widehat{k}(\xi) = \frac{1}{1 + 4\pi^{2}\xi^{2}},
\]
we obtain the explicit relation
\[
\boxed{\widehat{u}(\xi)
= \frac{1}{1 + 4\pi^{2}\xi^{2}}\, \widehat{f}(\xi).}
\]

\medskip

\textbf{3. Inverse transform, filter interpretation, and smoothing.}

By the inverse Fourier transform formula, we can write
\[
u(x)
= \int_{-\infty}^{\infty} \widehat{u}(\xi)\, e^{2\pi i x \xi}\,d\xi
= \int_{-\infty}^{\infty}
   \frac{1}{1 + 4\pi^{2}\xi^{2}}\,\widehat{f}(\xi)\, e^{2\pi i x \xi}\,d\xi.
\]
Thus $u(x)$ is obtained by taking the Fourier transform of $f$, multiplying by the factor
\[
H(\xi) = \frac{1}{1 + 4\pi^{2}\xi^{2}},
\]
and then applying the inverse transform.

The function $H(\xi)$ is called the \emph{transfer function} or \emph{frequency response} of the filter. We analyze $H$:

\begin{itemize}
\item For low frequencies $|\xi| \ll 1$, we have $4\pi^{2}\xi^{2} \approx 0$, so
\[
H(\xi) \approx 1.
\]
This means low-frequency components of $f$ are transmitted with little change in amplitude.

\item For high frequencies $|\xi| \gg 1$, the denominator $1 + 4\pi^{2}\xi^{2}$ is large, so
\[
H(\xi) \approx \frac{1}{4\pi^{2}\xi^{2}} \ll 1.
\]
High-frequency components of $f$ are strongly attenuated.
\end{itemize}

To see this effect on a single pure frequency, consider $f(x) = e^{2\pi i \xi_{0} x}$, so that $\widehat{f}(\xi)$ is (up to a constant) a delta distribution at $\xi = \xi_{0}$. Then
\[
\widehat{u}(\xi) = H(\xi)\, \widehat{f}(\xi)
\]
has the same support at $\xi_{0}$ but its amplitude is multiplied by $H(\xi_{0}) = 1/(1+4\pi^{2}\xi_{0}^{2})$. When we invert the transform, we find that $u(x)$ is still a pure exponential of frequency $\xi_{0}$, but with reduced amplitude. The reduction is small for small $|\xi_{0}|$ and large for large $|\xi_{0}|$.

Thus the convolution with $k$ acts as a \emph{low-pass filter}: it largely preserves slow variations (low frequencies) while damping out rapid oscillations (high frequencies). In the original $x$-domain, this appears as smoothing or blurring of the signal $f$.

From the perspective of this chapter, the central idea is that the Fourier transform converts the relatively complicated operation of convolution
\[
u(x) = \int_{-\infty}^{\infty} k(x-y)\, f(y)\,dy
\]
into simple pointwise multiplication
\[
\widehat{u}(\xi) = \widehat{k}(\xi)\, \widehat{f}(\xi).
\]
The inverse transform then reconstructs $u(x)$ from its filtered frequency content.

\medskip

\textbf{4. The rescaled kernel $k_{\varepsilon}$ and strength of smoothing.}

Let
\[
k_{\varepsilon}(x) = \frac{1}{2\varepsilon} e^{-|x|/\varepsilon}, \qquad \varepsilon>0.
\]
We recognize this as a rescaled version of $k$. Indeed, note that
\[
k_{\varepsilon}(x) = \frac{1}{\varepsilon} k\!\left(\frac{x}{\varepsilon}\right),
\]
because
\[
\frac{1}{\varepsilon} k\!\left(\frac{x}{\varepsilon}\right)
= \frac{1}{\varepsilon} \left( \tfrac12 e^{-|x|/\varepsilon} \right)
= \frac{1}{2\varepsilon} e^{-|x|/\varepsilon}.
\]

There is a standard scaling property of the Fourier transform: if
\[
g(x) = \frac{1}{|a|} f\!\left(\frac{x}{a}\right) \quad\text{for } a\neq 0,
\]
then
\[
\widehat{g}(\xi) = \widehat{f}(a\xi).
\]
We apply this with $a = \varepsilon$ and $f = k$, so
\[
\widehat{k_{\varepsilon}}(\xi) = \widehat{k}(\varepsilon \xi).
\]
Since we already computed
\[
\widehat{k}(\xi) = \frac{1}{1 + 4\pi^{2}\xi^{2}},
\]
we obtain
\[
\boxed{\widehat{k_{\varepsilon}}(\xi)
= \frac{1}{1 + 4\pi^{2} (\varepsilon \xi)^{2}}
= \frac{1}{1 + 4\pi^{2}\varepsilon^{2}\xi^{2}}.}
\]

The frequency response is now
\[
H_{\varepsilon}(\xi) = \frac{1}{1 + 4\pi^{2}\varepsilon^{2}\xi^{2}}.
\]
We interpret $\varepsilon$:

\begin{itemize}
\item If $\varepsilon$ is \emph{small}, then for a given $\xi$ the quantity $4\pi^{2}\varepsilon^{2}\xi^{2}$ is small except at very high frequencies. Thus $H_{\varepsilon}(\xi) \approx 1$ on a large range of $\xi$ and only decays significantly when $|\xi|$ is of order $1/\varepsilon$ or larger. In other words, a small $\varepsilon$ corresponds to a \emph{weak filter}: most frequencies pass through, and the smoothing in $x$ is mild. Indeed, $k_{\varepsilon}$ becomes sharply peaked near $0$, approaching a delta function as $\varepsilon \to 0$, so $k_{\varepsilon} * f$ approaches $f$.

\item If $\varepsilon$ is \emph{large}, then for moderate $|\xi|$ the factor $4\pi^{2}\varepsilon^{2}\xi^{2}$ is already large, so $H_{\varepsilon}(\xi)$ decays rapidly away from $\xi=0$. Only very low frequencies (very slow variations) are transmitted with significant amplitude. Thus a large $\varepsilon$ corresponds to a \emph{strong filter}: high frequencies are heavily damped, and the signal in $x$ is very smooth and blurred. In physical space, $k_{\varepsilon}$ is wide and spreads the influence of each point over a large neighborhood.
\end{itemize}

This example encapsulates the main ideas of the section on the Fourier transform and its inverse: convolution in physical space corresponds to multiplication in frequency space, and the shape of the Fourier transform of the kernel determines which frequencies are transmitted or suppressed. By computing both the transform and its inverse, we obtain a clear picture of how modifying the frequency content results in smoothing of the original signal.

\end{solution}

% ===== Example 5: Fourier Transform of Derivatives and Regularity from Decay (inquiry-based) =====
\begin{problem}[Fourier Transform of Derivatives and Regularity from Decay]
In this problem we explore how the Fourier transform interacts with differentiation and how decay of the Fourier transform at high frequencies is reflected as smoothness of the original function. The key observation is that differentiation in the physical variable corresponds to multiplication by a polynomial in the frequency variable. Using the inverse transform, this observation can be turned around: polynomial decay of the Fourier transform forces regularity of the original function. These ideas lie at the heart of solving linear constant–coefficient PDEs by Fourier methods.

Throughout, work on the real line $\mathbb{R}$ with the Fourier transform
\[
\widehat{f}(\xi) \;=\; \int_{\mathbb{R}} f(x)\,e^{-2\pi i x \xi}\,dx,
\]
whenever this integral is absolutely convergent, and the inverse transform
\[
f(x) \;=\; \int_{\mathbb{R}} \widehat{f}(\xi)\,e^{2\pi i x \xi}\,d\xi,
\]
for functions for which this formula is valid (for instance, Schwartz functions).

\smallskip

(a) Suppose $f \colon \mathbb{R}\to\mathbb{R}$ is continuously differentiable, $f$ and $f'$ belong to $L^1(\mathbb{R})$, and
\[
\lim_{x\to\pm\infty} f(x) = 0.
\]
Start from the definition
\[
\widehat{f'}(\xi) = \int_{\mathbb{R}} f'(x)\,e^{-2\pi i x\xi}\,dx.
\]
Use integration by parts to express $\widehat{f'}(\xi)$ in terms of $\widehat{f}(\xi)$ and $\xi$. What is the exact formula you obtain?

\emph{Hint:} Treat $u = f(x)$ and $dv = e^{-2\pi i x\xi}\,dx$. Carefully compute $v$ and check what happens to the boundary term using the assumption on $f(x)$ at $\pm\infty$.

\smallskip

(b) Under the same hypotheses, consider the $k$-th derivative $f^{(k)}$ (assume it exists, belongs to $L^1(\mathbb{R})$, and still tends to $0$ at $\pm\infty$). Guess a formula for $\widehat{f^{(k)}}(\xi)$ in terms of $\widehat{f}(\xi)$ and $\xi$, and then prove your guess by induction on $k$.

How does this show that a constant–coefficient differential operator like
\[
L = a_0 + a_1 \frac{d}{dx} + a_2 \frac{d^2}{dx^2} + \cdots + a_m \frac{d^m}{dx^m}
\]
is turned by the Fourier transform into multiplication by a polynomial in $\xi$?

\emph{Hint:} Combine your formula for $\widehat{f^{(k)}}$ with linearity of the Fourier transform.

\smallskip

(c) Now reverse the direction and use the \emph{inverse} Fourier transform to see how decay in $\widehat{f}$ produces smoothness of $f$. Assume that $\widehat{f}\in L^1(\mathbb{R})$ and also $\xi\,\widehat{f}(\xi)\in L^1(\mathbb{R})$. Then we may write
\[
f(x) = \int_{\mathbb{R}} \widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi.
\]
Explain why the function
\[
g(x) := \int_{\mathbb{R}} 2\pi i \xi\,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi
\]
is well-defined and continuous on $\mathbb{R}$. Show that you can differentiate the inverse Fourier integral for $f(x)$ under the integral sign and conclude that
\[
f'(x) = g(x).
\]
In particular, deduce that $f$ is continuously differentiable and that its derivative can be reconstructed from $\widehat{f}$.

\emph{Hint:} Use the dominated convergence theorem to justify passing the derivative inside the integral, and use the integrability of $\xi\,\widehat{f}(\xi)$ to find an integrable majorant.

\smallskip

(d) Generalize the previous step. Let $m\geq 1$ be an integer. Assume that for each $k=0,1,\dots,m$ we have
\[
\xi^k\,\widehat{f}(\xi)\in L^1(\mathbb{R}).
\]
Show that $f$ has $m$ continuous derivatives and that
\[
f^{(k)}(x) = \int_{\mathbb{R}} (2\pi i \xi)^k \,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi
\quad\text{for } k=0,1,\dots,m.
\]
Explain how this proves the general principle that \emph{polynomial decay} of $\widehat{f}$ as $|\xi|\to\infty$ implies \emph{polynomial regularity} (that is, differentiability) of $f$.

\emph{Hint:} Repeat the argument from part (c) inductively. At each step, you will need integrability of the next power $\xi^{k}\widehat{f}(\xi)$ to justify differentiating under the integral sign one more time.

\smallskip

(e) (Explorations and extensions.)

\begin{itemize}
  \item[(i)] Suppose that for \emph{every} integer $k\geq 0$ the function $\xi^k\,\widehat{f}(\xi)$ belongs to $L^1(\mathbb{R})$. What can you say about the smoothness of $f$? What does your conclusion suggest about the relation between “rapid decay’’ of $\widehat{f}$ and infinite differentiability of $f$?

  \item[(ii)] Let $u(t,x)$ solve the linear PDE
  \[
  \partial_t u(t,x) + \partial_x^{4} u(t,x) = 0, \qquad t>0,\ x\in\mathbb{R},
  \]
  with initial condition $u(0,x) = f(x)$, where $f$ is sufficiently nice so that all the Fourier transforms you need exist. Take the Fourier transform in $x$ and derive an explicit formula for $\widehat{u}(t,\xi)$ in terms of $\widehat{f}(\xi)$. Using your formula and the results above, explain why $u(t,\cdot)$ is very smooth in $x$ for every $t>0$, even if $f$ is not smooth. How does the factor $e^{-t(2\pi\xi)^4}$ in frequency space help you?
  
  \emph{Hint:} Think about how multiplying $\widehat{f}(\xi)$ by $e^{-t(2\pi\xi)^4}$ affects the decay of the product as $|\xi|\to\infty$, and then invoke part (d).
\end{itemize}

\end{problem}

% ===== Example 5: Fourier Transform of Derivatives and Regularity from Decay (full solution) =====
\begin{problem}[Fourier Transform of Derivatives and Regularity from Decay]
On $\mathbb{R}$, use the Fourier transform
\[
\widehat{f}(\xi) = \int_{\mathbb{R}} f(x)\,e^{-2\pi i x\xi}\,dx
\]
and its inverse
\[
f(x) = \int_{\mathbb{R}} \widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi,
\]
whenever these are defined.

\begin{enumerate}
  \item[(a)] Let $f$ be continuously differentiable with $f, f'\in L^1(\mathbb{R})$ and $\lim_{x\to\pm\infty} f(x)=0$. Show that
  \[
  \widehat{f'}(\xi) = 2\pi i \xi\,\widehat{f}(\xi).
  \]
  Extend this to $k$-th derivatives: under analogous hypotheses on $f^{(k)}$, prove
  \[
  \widehat{f^{(k)}}(\xi) = (2\pi i \xi)^k\,\widehat{f}(\xi).
  \]
  Conclude that a constant–coefficient differential operator
  \(
    L = \sum_{j=0}^m a_j \frac{d^j}{dx^j}
  \)
  is transformed into multiplication by the polynomial $\sum_{j=0}^m a_j (2\pi i \xi)^j$.

  \item[(b)] Now suppose $\widehat{f}\in L^1(\mathbb{R})$ and, for $k=0,1,\dots,m$,
  \[
  \xi^k\,\widehat{f}(\xi)\in L^1(\mathbb{R}).
  \]
  Using the inverse transform and differentiation under the integral sign, prove that $f$ has $m$ continuous derivatives and that
  \[
  f^{(k)}(x) = \int_{\mathbb{R}} (2\pi i \xi)^k\,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi
  \quad\text{for } k=0,1,\dots,m.
  \]
  Explain briefly how this shows that polynomial decay of $\widehat{f}(\xi)$ as $|\xi|\to\infty$ implies polynomial regularity (differentiability) of $f$.

  \item[(c)] Consider the PDE
  \[
  \partial_t u(t,x) + \partial_x^{4} u(t,x) = 0, \quad t>0,\ x\in\mathbb{R},
  \qquad u(0,x) = f(x),
  \]
  with $f$ such that all relevant Fourier transforms exist. Take the Fourier transform in $x$ to find an explicit formula for $\widehat{u}(t,\xi)$ in terms of $\widehat{f}(\xi)$. Using part (b), explain why $u(t,\cdot)$ is very smooth in $x$ for each $t>0$, even if $f$ is not smooth.
\end{enumerate}
\end{problem}

\begin{solution}
We work in one dimension, with the convention
\[
\widehat{f}(\xi) = \int_{\mathbb{R}} f(x)\,e^{-2\pi i x\xi}\,dx,
\qquad
f(x) = \int_{\mathbb{R}} \widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi,
\]
for sufficiently nice functions (for example, Schwartz functions). The central ideas are that differentiation in $x$ becomes multiplication in $\xi$, and that decay in $\xi$ corresponds to smoothness in $x$. These are the core structural properties of the Fourier transform used throughout the section.

\medskip

\noindent\textbf{(a) Fourier transform of derivatives.}

Assume first that $f$ is continuously differentiable, that $f$ and $f'$ belong to $L^1(\mathbb{R})$, and that $\lim_{x\to\pm\infty} f(x) = 0$. Then
\[
\widehat{f'}(\xi) = \int_{\mathbb{R}} f'(x)\,e^{-2\pi i x\xi}\,dx.
\]
We integrate by parts with
\[
u = f(x), \quad dv = e^{-2\pi i x\xi}\,dx,
\]
so that
\[
du = f'(x)\,dx, \quad v = \frac{1}{-2\pi i \xi} e^{-2\pi i x\xi}
\quad (\xi\neq 0).
\]
For $\xi\neq 0$, we obtain
\[
\int_{\mathbb{R}} f'(x)\,e^{-2\pi i x\xi}\,dx
= \biggl[ f(x)\,\frac{1}{-2\pi i \xi} e^{-2\pi i x\xi} \biggr]_{x=-\infty}^{x=+\infty}
+ 2\pi i \xi \int_{\mathbb{R}} f(x)\,e^{-2\pi i x\xi}\,dx.
\]
The boundary term vanishes because $f(x)\to 0$ as $x\to\pm\infty$ and the exponential factor has modulus one. Thus
\[
\widehat{f'}(\xi) = 2\pi i \xi \,\widehat{f}(\xi), \qquad \xi\neq 0.
\]
At $\xi=0$, we have
\[
\widehat{f'}(0) = \int_{\mathbb{R}} f'(x)\,dx = \lim_{R\to\infty} \bigl( f(R)-f(-R)\bigr) = 0,
\]
while $2\pi i \xi\widehat{f}(\xi)\big|_{\xi=0} = 0$, so the formula also holds at $\xi=0$. Hence
\[
\widehat{f'}(\xi) = 2\pi i \xi\,\widehat{f}(\xi)
\quad\text{for all } \xi\in\mathbb{R}.
\]

For higher derivatives, assume $f$ has $k$ continuous derivatives, each belonging to $L^1(\mathbb{R})$, and that $f^{(j)}(x)\to 0$ as $x\to\pm\infty$ for $0\le j\le k-1$. We proceed by induction on $k$.

For $k=1$ we have just proved the formula. Assume that
\[
\widehat{f^{(k)}}(\xi) = (2\pi i\xi)^k\,\widehat{f}(\xi)
\]
holds for some $k\ge 1$. Then, applying the $k=1$ case to $f^{(k)}$, we obtain
\[
\widehat{f^{(k+1)}}(\xi) = 2\pi i \xi\,\widehat{f^{(k)}}(\xi)
= 2\pi i \xi \,(2\pi i\xi)^k\,\widehat{f}(\xi)
= (2\pi i \xi)^{k+1}\,\widehat{f}(\xi).
\]
Thus by induction,
\[
\widehat{f^{(k)}}(\xi) = (2\pi i \xi)^k\,\widehat{f}(\xi),
\qquad k=0,1,2,\dots
\]
whenever the derivatives exist, are integrable, and have suitable decay at infinity.

Now consider a constant–coefficient differential operator
\[
L = a_0 + a_1 \frac{d}{dx} + a_2 \frac{d^2}{dx^2} + \cdots + a_m \frac{d^m}{dx^m}.
\]
For such $f$, linearity of the Fourier transform and the formula above give
\[
\widehat{Lf}(\xi)
= \sum_{j=0}^m a_j\,\widehat{f^{(j)}}(\xi)
= \sum_{j=0}^m a_j\,(2\pi i \xi)^j\,\widehat{f}(\xi)
= p(2\pi i\xi)\,\widehat{f}(\xi),
\]
where $p(z)=\sum_{j=0}^m a_j z^j$ is the polynomial associated with $L$. Thus $L$ is converted into multiplication by $p(2\pi i\xi)$ in the frequency variable. This diagonalizes constant–coefficient operators and is a main reason the Fourier transform is so effective for solving linear ODEs and PDEs.

\medskip

\noindent\textbf{(b) Decay of $\widehat{f}$ and regularity of $f$.}

Assume that $\widehat{f}\in L^1(\mathbb{R})$ and that for each integer $k$ with $0\le k\le m$,
\[
\xi^k\,\widehat{f}(\xi)\in L^1(\mathbb{R}).
\]
This means that not only is $\widehat{f}$ integrable, but so are its polynomially weighted versions up to order $m$. We will use the inverse transform
\[
f(x) = \int_{\mathbb{R}} \widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi
\]
and differentiate under the integral sign.

\medskip

\emph{Step 1: Continuity of $f$.}  

Because $\widehat{f}\in L^1(\mathbb{R})$, the integral defining $f(x)$ converges absolutely for each $x$. The integrand
\[
\widehat{f}(\xi)\,e^{2\pi i x\xi}
\]
is continuous in $x$ for each fixed $\xi$, and its modulus is bounded by $|\widehat{f}(\xi)|$, which is integrable. Therefore, by the dominated convergence theorem, $f$ is continuous in $x$.

\medskip

\emph{Step 2: First derivative.}

Assume, in addition, that $\xi\,\widehat{f}(\xi)\in L^1(\mathbb{R})$. Consider the difference quotient
\[
\frac{f(x+h)-f(x)}{h}
= \int_{\mathbb{R}} \widehat{f}(\xi)\,\frac{e^{2\pi i (x+h)\xi} - e^{2\pi i x\xi}}{h}\,d\xi.
\]
We can rewrite the factor in parentheses as
\[
\frac{e^{2\pi i (x+h)\xi} - e^{2\pi i x\xi}}{h}
= e^{2\pi i x\xi}\,\frac{e^{2\pi i h\xi}-1}{h}.
\]
For each fixed $\xi$, as $h\to 0$ we have
\[
\frac{e^{2\pi i h\xi}-1}{h} \longrightarrow 2\pi i \xi.
\]
Hence for each fixed $\xi$,
\[
\widehat{f}(\xi)\,\frac{e^{2\pi i (x+h)\xi} - e^{2\pi i x\xi}}{h}
\longrightarrow 2\pi i \xi\,\widehat{f}(\xi)\,e^{2\pi i x\xi}.
\]
We want to pass the limit inside the integral. To use dominated convergence, we estimate
\[
\biggl|\widehat{f}(\xi)\,\frac{e^{2\pi i (x+h)\xi} - e^{2\pi i x\xi}}{h}\biggr|
= |\widehat{f}(\xi)|\,\biggl|\frac{e^{2\pi i h\xi}-1}{h}\biggr|.
\]
The mean value theorem applied to the exponential shows that
\[
\biggl|\frac{e^{2\pi i h\xi}-1}{h}\biggr|
\le C\,|\xi|
\]
for some constant $C$ independent of $h$, for $h$ sufficiently small. Thus
\[
\biggl|\widehat{f}(\xi)\,\frac{e^{2\pi i (x+h)\xi} - e^{2\pi i x\xi}}{h}\biggr|
\le C\,|\xi|\,|\widehat{f}(\xi)|.
\]
By assumption, $\xi\,\widehat{f}(\xi)\in L^1(\mathbb{R})$, so the right-hand side is integrable and independent of $h$. Dominated convergence then yields
\[
\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}
= \int_{\mathbb{R}} 2\pi i \xi\,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi.
\]
Therefore $f$ is differentiable for each $x$, and its derivative is given by
\[
f'(x) = \int_{\mathbb{R}} (2\pi i \xi)\,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi.
\]
The right-hand side defines a continuous function of $x$ by the same dominated convergence argument as before (now using integrability of $|\xi|\,|\widehat{f}(\xi)|$). Hence $f\in C^1(\mathbb{R})$.

\medskip

\emph{Step 3: Higher derivatives.}

We proceed by induction. Suppose that for some integer $\ell$ with $1\le \ell\le m$ we know that
\[
\xi^k\,\widehat{f}(\xi)\in L^1(\mathbb{R}) \quad\text{for } k=0,1,\dots,\ell,
\]
and that $f\in C^\ell(\mathbb{R})$ with
\[
f^{(k)}(x)
= \int_{\mathbb{R}} (2\pi i\xi)^k\,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi,
\quad k=0,1,\dots,\ell.
\]
To obtain one more derivative, we consider $f^{(\ell)}$:
\[
f^{(\ell)}(x)
= \int_{\mathbb{R}} (2\pi i\xi)^{\ell}\,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi.
\]
The integrand is continuous in $x$ for each $\xi$ and is dominated by
\[
|(2\pi\xi)^\ell \widehat{f}(\xi)|,
\]
which is integrable by hypothesis. Thus $f^{(\ell)}$ is continuous.

To differentiate once more, we compute the difference quotient
\[
\frac{f^{(\ell)}(x+h)-f^{(\ell)}(x)}{h}
= \int_{\mathbb{R}} (2\pi i\xi)^{\ell}\,\widehat{f}(\xi)\,
\frac{e^{2\pi i (x+h)\xi} - e^{2\pi i x\xi}}{h}\,d\xi.
\]
As before, for each fixed $\xi$,
\[
\frac{e^{2\pi i (x+h)\xi} - e^{2\pi i x\xi}}{h}
\longrightarrow 2\pi i \xi\,e^{2\pi i x\xi}
\quad\text{as } h\to 0.
\]
Hence the integrand converges pointwise to $(2\pi i\xi)^{\ell+1}\,\widehat{f}(\xi)\,e^{2\pi i x\xi}$. For the dominating function we now use
\[
\biggl|(2\pi i\xi)^{\ell}\,\widehat{f}(\xi)\,
\frac{e^{2\pi i (x+h)\xi}-e^{2\pi i x\xi}}{h}\biggr|
\le C\,|\xi|^{\ell+1} |\widehat{f}(\xi)|,
\]
which is integrable by the assumption that $\xi^{\ell+1}\,\widehat{f}(\xi)\in L^1(\mathbb{R})$. Dominated convergence yields
\[
f^{(\ell+1)}(x)
= \int_{\mathbb{R}} (2\pi i\xi)^{\ell+1}\,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi.
\]
This shows that $f^{(\ell)}$ is differentiable with continuous derivative. Thus, by induction on $\ell$, we obtain that for all integers $k$ with $0\le k\le m$,
\[
f^{(k)}(x)
= \int_{\mathbb{R}} (2\pi i\xi)^k\,\widehat{f}(\xi)\,e^{2\pi i x\xi}\,d\xi
\]
and that $f\in C^m(\mathbb{R})$.

\medskip

\emph{Decay versus regularity.}

Suppose now that for some integer $m\ge 0$ we know that
\[
|\xi|^m\,|\widehat{f}(\xi)| \leq g(\xi)
\]
with $g\in L^1(\mathbb{R})$. Then $\xi^k\widehat{f}(\xi)\in L^1(\mathbb{R})$ for each $k\le m$, so the argument above applies. Informally, this means that if $\widehat{f}(\xi)$ decays at least like $|\xi|^{-m-1}$ as $|\xi|\to\infty$, then $f$ has $m$ continuous derivatives. Thus \emph{polynomial decay in frequency} corresponds to \emph{polynomial regularity in space}. This is a fundamental manifestation of the way the Fourier transform encodes smoothness of functions.

\medskip

\noindent\textbf{(c) Application to a fourth-order PDE.}

Consider the PDE
\[
\partial_t u(t,x) + \partial_x^{4} u(t,x) = 0, \quad t>0,\ x\in\mathbb{R},
\qquad u(0,x) = f(x).
\]
We take the Fourier transform in the spatial variable $x$:
\[
\widehat{u}(t,\xi) = \int_{\mathbb{R}} u(t,x)\,e^{-2\pi i x\xi}\,dx.
\]
Using the result of part (a) on derivatives, we know that
\[
\widehat{\partial_x^{4} u}(t,\xi) = (2\pi i\xi)^4 \widehat{u}(t,\xi) = (2\pi\xi)^4 \widehat{u}(t,\xi),
\]
since $i^4 = 1$. Therefore, taking the Fourier transform of the PDE with respect to $x$ yields
\[
\partial_t \widehat{u}(t,\xi) + (2\pi\xi)^4\,\widehat{u}(t,\xi) = 0,
\]
for each fixed $\xi\in\mathbb{R}$, with initial condition $\widehat{u}(0,\xi) = \widehat{f}(\xi)$. This is now an ordinary differential equation in the time variable $t$. Solving this ODE gives
\[
\widehat{u}(t,\xi) = e^{-t(2\pi\xi)^4}\,\widehat{f}(\xi).
\]

This formula exhibits the key algebraic simplification provided by the Fourier transform: the fourth derivative in $x$ has become multiplication by the polynomial $(2\pi\xi
)^4$.

Thus
\[
\widehat{u}(t,\xi) = e^{-t(2\pi\xi)^4}\,\widehat{f}(\xi).
\]

\medskip

\emph{Smoothing in $x$ for $t>0$.}

Fix $t>0$. For each integer $m\ge 0$,
\[
\xi^m\,\widehat{u}(t,\xi)
= \bigl[\xi^m e^{-t(2\pi\xi)^4}\bigr]\,\widehat{f}(\xi).
\]
The factor $\xi^m e^{-t(2\pi\xi)^4}$ is a polynomial times a rapidly decaying exponential. In particular, it is bounded:
\[
\sup_{\xi\in\mathbb{R}} \bigl|\xi^m e^{-t(2\pi\xi)^4}\bigr| < \infty,
\quad\text{for each fixed } t>0,m\ge 0.
\]
If, for example, $\widehat{f}\in L^1(\mathbb{R})$, then for each $m$,
\[
\xi^m\,\widehat{u}(t,\xi) \in L^1(\mathbb{R}),
\]
because it is $\widehat{f}(\xi)$ multiplied by a bounded function. Applying part (b) to $u(t,\cdot)$ (with $\widehat{u}(t,\xi)$ in place of $\widehat{f}(\xi)$) shows that $u(t,\cdot)$ has $m$ continuous derivatives for every $m$. Since $m$ is arbitrary, $u(t,\cdot)$ is $C^\infty$ in $x$ for each $t>0$.

In other words, even if the initial data $f$ is only moderately regular, the solution $u(t,\cdot)$ becomes instantly very smooth in the spatial variable for any positive time $t$. The strong decay of the factor $e^{-t(2\pi\xi)^4}$ at high frequencies damps the high-frequency components of $\widehat{f}$, and by the decay–regularity principle from part (b), this enhanced decay translates into arbitrarily high regularity of $u(t,\cdot)$ in $x$.

\end{solution}

\section{Properties of the 1-D Fourier Transform}
% TODO: Add narrative / plan for this section.

% TODO: Use prompts_for_sections.py to design examples and add them here.

\section{Dirac's delta-function}
% --- Narrative plan (auto-generated) ---
% This section introduces Dirac’s delta-function as a precise mathematical tool for representing idealized point sources, impulses, and localized measurements. Although the delta-function is not a function in the usual sense, it can be understood rigorously as a distribution, or generalized function, that acts on test functions by integration. We will explore several concrete ways to approximate it by honest functions and will use those approximations to justify its main algebraic and analytic properties.
%
% Dirac’s delta is indispensable in applied mathematics because it allows us to encode concentrated forces in ODE models, instantaneous inputs in dynamical systems, and point sources or sinks in PDEs such as the heat, wave, and Poisson equations. In the language of Fourier analysis, the delta provides a bridge between physical space and frequency space: it describes “pure” frequencies and exact localization, and it appears naturally when we study Green’s functions and impulse responses. The techniques developed here connect directly to convolution, Fourier transforms, Green’s functions for linear ODEs and PDEs, and, at a more advanced level, to complex analysis via contour integrals and to the theory of distributions used throughout modern analysis and partial differential equations.
%
% Throughout this section, we will build intuition by working with explicit approximating sequences (such as Gaussians and box functions), by computing how the delta interacts with smooth test functions, and by examining physical models where idealization by a point source is both natural and practically useful. The goal is that, by the end, you will be able to handle expressions involving δ confidently and correctly: using it in Fourier transforms, justifying manipulations with derivatives and integrals, and recognizing when and how it can be replaced by a limit of more familiar functions.

% ===== Example 1: Impulse Forcing of a Harmonic Oscillator (inquiry-based) =====
\begin{problem}[Impulse Forcing of a Harmonic Oscillator]
A mass–spring system with mass $m>0$ and spring constant $k>0$ is attached to a fixed wall and moves without friction on a horizontal surface. When left alone, the motion is governed by the homogeneous equation $m x''(t) + k x(t) = 0$. Now imagine we strike the mass with a very short, sharp blow at time $t=0$. The applied force is large but acts only for a very short time, so that its \emph{total impulse} is finite. We would like to model this “infinitely short but finite impulse’’ using the Dirac delta and understand the resulting motion.

Throughout, let $\omega = \sqrt{k/m}$ denote the natural frequency of the oscillator.

\smallskip

(a) \textbf{Warm-up: the unforced harmonic oscillator.}  
Consider the homogeneous initial value problem
\[
m x''(t) + k x(t) = 0, \qquad x(0) = x_0,\quad x'(0)=v_0 .
\]
(i) Solve this IVP explicitly for $x(t)$ in terms of $x_0$, $v_0$, and $\omega$.  
(ii) Describe qualitatively the motion for different choices of $(x_0,v_0)$ (for example, what happens if $x_0=0$ and $v_0\neq 0$?).

Hint: Write the general solution as a linear combination of $\cos(\omega t)$ and $\sin(\omega t)$ and then match the initial conditions.

\smallskip

(b) \textbf{From a short pulse to an impulse.}  
Instead of an idealized impulse, first consider a family of smooth forces $F_\varepsilon(t)$ that act only during a small time interval of length $\varepsilon>0$ near $t=0$, and are zero otherwise. Assume the total impulse (time-integrated force) is fixed:
\[
\int_{-\infty}^{\infty} F_\varepsilon(t)\,dt = J,
\]
for some constant $J$ independent of $\varepsilon$, and that $F_\varepsilon(t)$ tends to $0$ pointwise for each $t\neq 0$ as $\varepsilon\to 0^+$.  

Consider the forced equation
\[
m x_\varepsilon''(t) + k x_\varepsilon(t) = F_\varepsilon(t),
\]
with the “before impact’’ rest conditions
\[
x_\varepsilon(t) = 0,\quad x_\varepsilon'(t) = 0 \quad \text{for } t<0.
\]
(i) Integrate the equation from $t=-\eta$ to $t=\eta$ for some small fixed $\eta>0$ (independent of $\varepsilon$):
\[
\int_{-\eta}^{\eta} \bigl(m x_\varepsilon''(t) + k x_\varepsilon(t)\bigr)\,dt
= \int_{-\eta}^{\eta} F_\varepsilon(t)\,dt.
\]
Rewrite the left-hand side in terms of $x_\varepsilon'(\eta)$, $x_\varepsilon'(-\eta)$, and an integral involving $x_\varepsilon(t)$.  

(ii) Assuming $x_\varepsilon$ remains bounded as $\varepsilon\to 0^+$, argue that the contribution of the spring term $k x_\varepsilon(t)$ over $[-\eta,\eta]$ tends to $0$ when $\eta$ is small. What does this imply about the jump in velocity $x_\varepsilon'(\eta)-x_\varepsilon'(-\eta)$ as $\varepsilon\to 0^+$?

Hint: Use the fundamental theorem of calculus on the $m x_\varepsilon''$ term and estimate the integral of $k x_\varepsilon$ by a bound like $2\eta \cdot \max_{[-\eta,\eta]} |k x_\varepsilon(t)|$.

\smallskip

(c) \textbf{Introducing Dirac’s delta and the jump condition.}  
We now idealize the family of impulses $F_\varepsilon$ by Dirac’s delta and write the limiting forced equation as
\[
m x''(t) + k x(t) = J\,\delta(t),
\]
with $x(t)=0$ and $x'(t)=0$ for $t<0$.  

(i) Using the distributional identity
\[
\int_{-\eta}^{\eta} \delta(t)\,\varphi(t)\,dt = \varphi(0)
\quad\text{for any smooth test function }\varphi,
\]
integrate the equation from $t=-\eta$ to $t=\eta$ and pass to the limit as $\eta\to 0^+$.  

(ii) Show that $x(t)$ must be continuous at $t=0$ but that $x'(t)$ has a jump at $t=0$, and derive an explicit formula for
\[
x'(0^+) - x'(0^-)
\]
in terms of $J$ and $m$.

Hint: Adapt the computation from part (b) and replace $\int F_\varepsilon$ by $J$ via the defining property of $\delta$.

\smallskip

(d) \textbf{Solving the impulsively forced problem.}  
Assume that $x(t)=0$ and $x'(t)=0$ for $t<0$, and that $x$ satisfies
\[
m x''(t) + k x(t) = J\,\delta(t)
\]
in the sense of distributions. Using part (c), you now know the initial data \emph{just after} the impulse:
\[
x(0^+) = ?,\qquad x'(0^+) = ?.
\]
(i) Determine $x(0^+)$ and $x'(0^+)$ explicitly.  

(ii) For $t>0$, the forcing term vanishes, so $x$ solves the homogeneous equation. Use your answer from part (a) with initial data at $t=0^+$ to find an explicit formula for $x(t)$ for $t>0$.  

(iii) Express your final answer in a compact form using the Heaviside step function $H(t)$, where $H(t)=0$ for $t<0$ and $H(t)=1$ for $t>0$.

Hint: Think of the impulse as instantaneously changing the velocity, but not the position, at $t=0$.

\smallskip

(e) \textbf{Extensions and variations.}  

(i) Suppose instead that the impulse occurs at some later time $t=t_0>0$, so that
\[
m x''(t) + k x(t) = J\,\delta(t-t_0),\qquad x(t)=0,\ x'(t)=0\ \text{for }t<t_0.
\]
By analogy with part (d), guess the solution for $t>t_0$ and write it explicitly in terms of $H(t-t_0)$. How is this solution related to your earlier answer via a time shift?

(ii) The function you have just computed is often called the \emph{impulse response} or \emph{Green’s function} for the oscillator. Briefly explain, in your own words, why knowing this response to a single impulse allows us (in principle) to build solutions for more general forcing terms $f(t)$.

Hint: Think about approximating a general force $f(t)$ as a superposition (or integral) of many tiny impulses at different times.
\end{problem}

% ===== Example 1: Impulse Forcing of a Harmonic Oscillator (full solution) =====
\begin{problem}[Impulse Forcing of a Harmonic Oscillator]
Let $m>0$ and $k>0$ be constants, and let $\omega = \sqrt{k/m}$. Consider the impulsively forced harmonic oscillator
\[
m x''(t) + k x(t) = J\,\delta(t), \qquad t\in\mathbb{R},
\]
where $J$ is a given constant. Assume the mass is at rest before the impulse:
\[
x(t)=0,\quad x'(t)=0 \quad \text{for } t<0.
\]
\begin{enumerate}
\item[(a)] By integrating the equation across $t=0$, derive the jump condition for the velocity $x'(0^+)-x'(0^-)$.
\item[(b)] Use this jump condition to solve for $x(t)$ for all $t\in\mathbb{R}$ and express your answer using the Heaviside step function $H(t)$.
\item[(c)] Identify the resulting function as the impulse response (Green’s function) of the operator $Lx = m x'' + k x$, and briefly explain how this illustrates the role of Dirac’s delta in modeling instantaneous forcing.
\end{enumerate}
\end{problem}

\begin{solution}
We are studying a mass–spring system whose motion is governed by
\[
m x''(t) + k x(t) = J\,\delta(t),
\]
with the requirement that the mass is at rest before the impulse: $x(t)=0$ and $x'(t)=0$ for $t<0$. The right-hand side models an idealized impulse at time $t=0$ with total impulse $J$.

\medskip

\textbf{(a) Jump condition for the velocity.}  
To capture the effect of the delta forcing at $t=0$, we integrate the equation over a small symmetric interval $[-\eta,\eta]$ around $0$:
\[
\int_{-\eta}^{\eta} \bigl(m x''(t) + k x(t)\bigr)\,dt
= \int_{-\eta}^{\eta} J\,\delta(t)\,dt.
\]
We analyze each term separately.

First, apply the fundamental theorem of calculus to the $x''$ term:
\[
\int_{-\eta}^{\eta} m x''(t)\,dt
= m\bigl(x'(\eta) - x'(-\eta)\bigr).
\]
Second, since $x$ is locally bounded, the spring term contributes
\[
\int_{-\eta}^{\eta} k x(t)\,dt,
\]
which is at most $2\eta \,\max_{[-\eta,\eta]} |k x(t)|$ in magnitude. As $\eta\to 0^+$, this integral tends to $0$ because the interval length $2\eta$ goes to zero and $x$ remains bounded.

On the right-hand side, we use the defining property of the Dirac delta (with test function $\varphi(t)\equiv 1$):
\[
\int_{-\eta}^{\eta} J\,\delta(t)\,dt = J.
\]
Putting these pieces together and letting $\eta\to 0^+$ gives
\[
m\bigl(x'(0^+) - x'(0^-)\bigr) = J.
\]
Hence the velocity $x'(t)$ has a jump discontinuity at $t=0$ of size
\[
x'(0^+) - x'(0^-) = \frac{J}{m}.
\]
Physically, this says that an impulse of magnitude $J$ instantaneously changes the momentum $m x'$ by $J$.

Since the equation involves $x''$ and $x$ but not $x'$ directly, the position $x(t)$ itself remains continuous at $t=0$; only its derivative jumps.

\medskip

\textbf{(b) Solving for $x(t)$ and expressing the solution with $H(t)$.}  
By assumption, the mass is at rest before the impulse:
\[
x(t) = 0,\quad x'(t)=0 \quad \text{for } t<0.
\]
Thus, just before $t=0$ we have
\[
x(0^-) = 0,\qquad x'(0^-) = 0.
\]
From the jump condition in part (a),
\[
x'(0^+) = x'(0^-) + \frac{J}{m} = \frac{J}{m}.
\]
Continuity of $x(t)$ at $t=0$ gives $x(0^+)=x(0^-)=0$.

For $t>0$, the delta forcing vanishes, so $x$ satisfies the homogeneous equation
\[
m x''(t) + k x(t) = 0,\qquad t>0.
\]
Dividing by $m$ and setting $\omega = \sqrt{k/m}$, we have
\[
x''(t) + \omega^2 x(t) = 0,
\]
whose general solution is
\[
x(t) = A\cos(\omega t) + B\sin(\omega t),\qquad t>0.
\]
We now impose the initial data at $t=0^+$:
\[
x(0^+) = A = 0,
\]
since $\cos(0)=1$ and $\sin(0)=0$. Differentiating,
\[
x'(t) = -A\omega \sin(\omega t) + B\omega \cos(\omega t),
\]
so
\[
x'(0^+) = B\omega = \frac{J}{m}.
\]
Hence
\[
B = \frac{J}{m\omega}.
\]
Thus, for $t>0$,
\[
x(t) = \frac{J}{m\omega}\,\sin(\omega t).
\]
For $t<0$, we have $x(t)=0$ by the initial rest condition. We can combine these two pieces into a single formula using the Heaviside step function $H(t)$, defined by $H(t)=0$ for $t<0$ and $H(t)=1$ for $t>0$. The solution is
\[
x(t) = \frac{J}{m\omega}\,\sin(\omega t)\,H(t),\qquad t\in\mathbb{R}.
\]
This function is continuous at $t=0$ with $x(0)=0$, and its derivative has the correct jump:
\[
x'(0^+) = \frac{J}{m},\qquad x'(0^-)=0.
\]

\medskip

\textbf{(c) Impulse response and the role of Dirac’s delta.}  
The function
\[
G(t) = \frac{1}{m\omega}\,\sin(\omega t)\,H(t)
\]
is the solution of
\[
m G''(t) + k G(t) = \delta(t),\qquad G(t)=0,\quad G'(t)=0\ \text{for }t<0.
\]
Thus $G$ is the \emph{impulse response} or \emph{Green’s function} of the operator $Lx = m x'' + k x$. For an impulse of strength $J$, the solution is simply $J G(t)$.

This example illustrates the central role of Dirac’s delta in Fourier analysis and linear systems theory. The delta function models an idealized instantaneous input whose total effect is encoded in the jump of the derivative. By finding the system’s response $G$ to this fundamental input, we obtain a building block from which solutions to more general forcings can be constructed (via convolution). In this way, the delta function connects the differential operator $L$ with its Green’s function and provides a concise way to describe how the system responds to sharp, localized disturbances.
\end{solution}

% ===== Example 2: Point Source in the Heat Equation (inquiry-based) =====
\begin{problem}[Point Source in the Heat Equation]
In many physical models, heat or mass is initially concentrated at a single point, for instance when a laser pulse heats a tiny spot on a rod. Mathematically we idealize this by saying that the initial temperature distribution is zero everywhere except at one point, but the total amount of heat is one unit. This is naturally modeled by Dirac's delta-function. In this problem we use the Fourier transform to understand how such a point source evolves under the one-dimensional heat equation on the whole line.

Consider the one-dimensional heat equation on the real line
\[
u_t(x,t) = k\,u_{xx}(x,t), \qquad x \in \mathbb{R},\ t>0,
\]
where $k>0$ is the thermal diffusivity.

\medskip

(a) Suppose first that the initial temperature $u(x,0)=f(x)$ is a nice, rapidly decreasing function. Let
\[
\widehat{u}(\xi,t) = \int_{\mathbb{R}} u(x,t)\,e^{-i\xi x}\,dx
\]
be the spatial Fourier transform of $u(\cdot,t)$ for each fixed $t>0$.
\begin{itemize}
  \item[(i)] Take the Fourier transform in $x$ of both sides of the heat equation and show that $\widehat{u}$ satisfies an ordinary differential equation in $t$ of the form
  \[
  \frac{\partial}{\partial t}\widehat{u}(\xi,t) = -k\,\xi^{2}\,\widehat{u}(\xi,t).
  \]
  \item[(ii)] Solve this ODE and express $\widehat{u}(\xi,t)$ in terms of $\widehat{f}(\xi) = \widehat{u}(\xi,0)$.
\end{itemize}
Hint: Use that $\mathcal{F}[u_{xx}] = -(i\xi)^2 \widehat{u} = -\xi^2 \widehat{u}$.

\medskip

(b) Now let the initial data be the point source
\[
u(x,0) = \delta(x),
\]
which models one unit of heat instantaneously placed at the origin at time $t=0$.
\begin{itemize}
  \item[(i)] What is the Fourier transform $\widehat{u}(\xi,0)$ of $\delta(x)$?
  \item[(ii)] Using your formula from part (a), write down $\widehat{u}(\xi,t)$ for $t>0$ in this case.
\end{itemize}
Hint: Recall that for any Schwartz function $\varphi$ one has $\int_{\mathbb{R}}\delta(x)\,\varphi(x)\,dx = \varphi(0)$.

\medskip

(c) To find $u(x,t)$, we now invert the Fourier transform. Using the inverse transform
\[
u(x,t) = \frac{1}{2\pi}\int_{\mathbb{R}} \widehat{u}(\xi,t)\,e^{i\xi x}\,d\xi,
\]
write an explicit integral formula for $u(x,t)$ in the point-source case. Then evaluate this integral and show that
\[
u(x,t) = \frac{1}{\sqrt{4\pi k t}}\exp\!\left(-\frac{x^{2}}{4kt}\right),
\qquad t>0.
\]
Hint: You will need a Gaussian integral. One useful identity is
\[
\int_{\mathbb{R}} e^{-a\xi^2 + b\xi}\,d\xi
= \sqrt{\frac{\pi}{a}}\exp\!\left(\frac{b^2}{4a}\right)
\quad\text{for } a>0,\ b\in\mathbb{C}.
\]

\medskip

(d) The function
\[
G(x,t) := \frac{1}{\sqrt{4\pi k t}}\exp\!\left(-\frac{x^{2}}{4kt}\right)
\]
is called the heat kernel or fundamental solution of the heat equation.
\begin{itemize}
  \item[(i)] Argue (for example, by differentiating under the integral sign in the Fourier representation) that $G$ indeed satisfies the heat equation for every $t>0$.
  \item[(ii)] Show that $G(\cdot,t)$ has total mass one for each $t>0$, that is,
  \[
  \int_{\mathbb{R}} G(x,t)\,dx = 1.
  \]
  \item[(iii)] Explain in the sense of distributions what it means to say that $G(x,t)$ converges to $\delta(x)$ as $t\to 0^{+}$, and prove this statement. In other words, show that for every smooth, rapidly decreasing test function $\varphi$,
  \[
  \int_{\mathbb{R}} G(x,t)\,\varphi(x)\,dx \longrightarrow \varphi(0)
  \quad\text{as } t\to 0^{+}.
  \]
\end{itemize}
Hint: For (iii), consider the change of variables $x = 2\sqrt{kt}\,y$ and use the continuity of $\varphi$ at $0$ together with dominated convergence.

\medskip

(e) Let us explore some variations and extensions.
\begin{itemize}
  \item[(i)] Suppose the initial data is a point source at $x=x_{0}$ instead of at the origin, that is, $u(x,0) = \delta(x-x_{0})$. Using the translation properties of the Fourier transform or of the delta-function, guess and then verify the corresponding solution $u(x,t)$.
  \item[(ii)] For a general initial temperature $u(x,0) = f(x)$ (assume $f$ is nice enough), the solution can be written in terms of the heat kernel as a convolution: 
  \[
  u(x,t) = \int_{\mathbb{R}} G(x-y,t)\,f(y)\,dy.
  \]
  Briefly explain how the computation you did for the point source suggests this formula and why it is natural to call $G$ a \emph{fundamental solution}.
\end{itemize}
Hint: Think of $f$ as a “continuous superposition’’ of point sources and recall that the heat equation is linear.

\end{problem}

% ===== Example 2: Point Source in the Heat Equation (full solution) =====
\begin{problem}[Point Source in the Heat Equation]
Consider the one-dimensional heat equation on $\mathbb{R}$,
\[
u_t(x,t) = k\,u_{xx}(x,t), \qquad x\in\mathbb{R},\ t>0,
\]
with thermal diffusivity $k>0$ and initial data
\[
u(x,0) = \delta(x),
\]
the Dirac delta at the origin.

(a) Using the spatial Fourier transform, solve this initial-value problem and show that for $t>0$ the solution is
\[
u(x,t) = \frac{1}{\sqrt{4\pi k t}}\,\exp\!\left(-\frac{x^{2}}{4kt}\right).
\]

(b) Verify that this function satisfies the heat equation for $t>0$, that its total mass is one for each $t>0$, and that $u(\cdot,t)\to\delta$ in the sense of distributions as $t\to 0^{+}$.

Briefly explain why this example is called the fundamental solution (or heat kernel), and how it illustrates the use of the Dirac delta-function in Fourier analysis.
\end{problem}

\begin{solution}
We work on the whole real line and use the spatial Fourier transform to turn the partial differential equation into an ordinary differential equation in time. We adopt the convention
\[
\widehat{f}(\xi) = \int_{\mathbb{R}} f(x)\,e^{-i\xi x}\,dx,
\qquad
f(x) = \frac{1}{2\pi}\int_{\mathbb{R}} \widehat{f}(\xi)\,e^{i\xi x}\,d\xi.
\]

\medskip

\noindent\textbf{Step 1: Fourier transform of the heat equation.}
Let $u(x,t)$ solve
\[
u_t = k\,u_{xx}, \qquad u(x,0) = \delta(x).
\]
For each fixed $t>0$, we take the Fourier transform in $x$:
\[
\widehat{u}(\xi,t)
= \int_{\mathbb{R}} u(x,t)\,e^{-i\xi x}\,dx.
\]
Assuming enough regularity and decay to justify differentiation under the integral sign (which can be made rigorous using standard arguments), we obtain
\[
\frac{\partial}{\partial t}\widehat{u}(\xi,t)
= \int_{\mathbb{R}} u_t(x,t)\,e^{-i\xi x}\,dx
= k\int_{\mathbb{R}} u_{xx}(x,t)\,e^{-i\xi x}\,dx.
\]
The Fourier transform converts spatial derivatives into multiplication by powers of $i\xi$. Integrating by parts twice, or using the standard transform rule, gives
\[
\mathcal{F}[u_{xx}](\xi,t) = -(i\xi)^2\,\widehat{u}(\xi,t) = -\xi^{2}\,\widehat{u}(\xi,t).
\]
Thus the transformed function satisfies the ordinary differential equation
\[
\frac{\partial}{\partial t}\widehat{u}(\xi,t)
= -k\,\xi^{2}\,\widehat{u}(\xi,t).
\]

For each fixed $\xi$, this is a linear first-order ODE with solution
\[
\widehat{u}(\xi,t)
= \widehat{u}(\xi,0)\,e^{-k\xi^{2}t}.
\]

\medskip

\noindent\textbf{Step 2: Incorporating the initial data $\delta(x)$.}
The initial condition is $u(x,0) = \delta(x)$. The Fourier transform of $\delta$ is particularly simple. By definition of the delta-function as a distribution,
\[
\widehat{\delta}(\xi)
= \int_{\mathbb{R}} \delta(x)\,e^{-i\xi x}\,dx
= e^{-i\xi\cdot 0} = 1
\]
for all $\xi\in\mathbb{R}$. Hence
\[
\widehat{u}(\xi,0) = \widehat{\delta}(\xi) = 1.
\]
Substituting this into the solution of the ODE, we obtain
\[
\widehat{u}(\xi,t) = e^{-k\xi^{2}t}, \qquad t>0.
\]

\medskip

\noindent\textbf{Step 3: Inverting the Fourier transform.}
We now recover $u(x,t)$ via the inverse transform:
\[
u(x,t)
= \frac{1}{2\pi}\int_{\mathbb{R}} \widehat{u}(\xi,t)\,e^{i\xi x}\,d\xi
= \frac{1}{2\pi}\int_{\mathbb{R}} e^{-k\xi^{2}t}\,e^{i\xi x}\,d\xi.
\]
This is a Gaussian integral with a complex linear term in $\xi$. We write
\[
u(x,t)
= \frac{1}{2\pi}\int_{\mathbb{R}} \exp\bigl(-k t\,\xi^{2} + i x\xi\bigr)\,d\xi.
\]
We can apply the standard formula
\[
\int_{\mathbb{R}} e^{-a\xi^{2} + b\xi}\,d\xi
= \sqrt{\frac{\pi}{a}}\exp\!\left(\frac{b^{2}}{4a}\right),
\qquad a>0,\ b\in\mathbb{C},
\]
with $a = k t$ and $b = i x$. Then
\[
\int_{\mathbb{R}} e^{-k t\,\xi^{2} + i x\xi}\,d\xi
= \sqrt{\frac{\pi}{k t}}\exp\!\left(\frac{(ix)^{2}}{4kt}\right)
= \sqrt{\frac{\pi}{k t}}\exp\!\left(-\frac{x^{2}}{4kt}\right),
\]
since $(i x)^{2} = -x^{2}$. Therefore
\[
u(x,t)
= \frac{1}{2\pi}\,\sqrt{\frac{\pi}{k t}}\,
\exp\!\left(-\frac{x^{2}}{4kt}\right)
= \frac{1}{\sqrt{4\pi k t}}\,
\exp\!\left(-\frac{x^{2}}{4kt}\right).
\]
This is the claimed formula. It is customary to denote
\[
G(x,t) := \frac{1}{\sqrt{4\pi k t}}\exp\!\left(-\frac{x^{2}}{4kt}\right),
\]
and to call $G$ the heat kernel or fundamental solution of the one-dimensional heat equation.

\medskip

\noindent\textbf{Step 4: Verifying that $G$ satisfies the heat equation.}
Weargue that $G$ satisfies $G_t = k G_{xx}$ for every $t>0$. One way is to note that our derivation started from the heat equation, so for sufficiently regular solutions the Fourier representation ensures that the resulting $u$ solves the PDE. Since $G$ is smooth in $(x,t)$ for $t>0$, it is legitimate to differentiate the inverse Fourier integral under the integral sign:
\[
G(x,t)
= \frac{1}{2\pi} \int_{\mathbb{R}} e^{-k\xi^{2}t} e^{i\xi x}\,d\xi.
\]
Then
\[
G_t(x,t)
= \frac{1}{2\pi} \int_{\mathbb{R}} (-k\xi^{2})\,e^{-k\xi^{2}t} e^{i\xi x}\,d\xi,
\]
and
\[
G_{xx}(x,t)
= \frac{1}{2\pi} \int_{\mathbb{R}} (i\xi)^{2}\,e^{-k\xi^{2}t} e^{i\xi x}\,d\xi
= -\frac{1}{2\pi} \int_{\mathbb{R}} \xi^{2}\,e^{-k\xi^{2}t} e^{i\xi x}\,d\xi.
\]
Comparing these, we see that $G_t(x,t) = k\,G_{xx}(x,t)$. Thus $G$ satisfies the heat equation for every $t>0$.

Alternatively, one could check this directly by differentiating the explicit Gaussian expression, but the Fourier method is quicker and highlights the main idea of passing to the frequency side.

\medskip

\noindent\textbf{Step 5: Conservation of total mass.}
We now show that for each $t>0$,
\[
\int_{\mathbb{R}} G(x,t)\,dx = 1.
\]
Using the explicit formula,
\[
\int_{\mathbb{R}} G(x,t)\,dx
= \int_{\mathbb{R}} \frac{1}{\sqrt{4\pi k t}}
\exp\!\left(-\frac{x^{2}}{4kt}\right) dx.
\]
Make the change of variables
\[
y = \frac{x}{\sqrt{4kt}}, \qquad x = \sqrt{4kt}\,y, \qquad dx = \sqrt{4kt}\,dy.
\]
Then
\[
\int_{\mathbb{R}} G(x,t)\,dx
= \int_{\mathbb{R}} \frac{1}{\sqrt{4\pi k t}}\,e^{-y^{2}}\,\sqrt{4kt}\,dy
= \frac{\sqrt{4kt}}{\sqrt{4\pi k t}}\int_{\mathbb{R}} e^{-y^{2}}\,dy
= \frac{1}{\sqrt{\pi}}\int_{\mathbb{R}} e^{-y^{2}}\,dy.
\]
The standard Gaussian integral gives $\int_{\mathbb{R}} e^{-y^{2}}\,dy = \sqrt{\pi}$, and hence the ratio is exactly $1$. Thus the total “mass’’ (or total heat) is conserved in time. This is consistent with the physical interpretation of the heat equation without sources or sinks.

\medskip

\noindent\textbf{Step 6: Convergence to the delta-function.}
Finally, we show that $G(\cdot,t)$ converges to $\delta$ in the sense of distributions as $t\to 0^{+}$. By definition, this means that for every Schwartz test function $\varphi$,
\[
\int_{\mathbb{R}} G(x,t)\,\varphi(x)\,dx \longrightarrow \varphi(0)
\quad\text{as } t\to 0^{+}.
\]

Define
\[
I(t) := \int_{\mathbb{R}} G(x,t)\,\varphi(x)\,dx
= \int_{\mathbb{R}} \frac{1}{\sqrt{4\pi k t}}
\exp\!\left(-\frac{x^{2}}{4kt}\right)\varphi(x)\,dx.
\]
We again change variables $x = 2\sqrt{kt}\,y$, so that $dx = 2\sqrt{kt}\,dy$ and
\[
\frac{x^{2}}{4kt} = y^{2}.
\]
Then
\[
I(t)
= \int_{\mathbb{R}} \frac{1}{\sqrt{4\pi k t}}\,
e^{-y^{2}}\,\varphi(2\sqrt{kt}\,y)\,(2\sqrt{kt})\,dy
= \frac{1}{\sqrt{\pi}}\int_{\mathbb{R}} e^{-y^{2}}\,
\varphi(2\sqrt{kt}\,y)\,dy.
\]
As $t\to 0^{+}$, we have $2\sqrt{kt}\,y \to 0$ for each fixed $y$, and therefore
\[
\varphi(2\sqrt{kt}\,y) \longrightarrow \varphi(0).
\]
Moreover, since $\varphi$ is smooth and rapidly decreasing, there exists a constant $C$ such that $|\varphi(x)|\le C$ for all $x$, so that
\[
\bigl|e^{-y^{2}}\varphi(2\sqrt{kt}\,y)\bigr|
\le C e^{-y^{2}}.
\]
The function $C e^{-y^{2}}$ is integrable on $\mathbb{R}$. Therefore we can apply the dominated convergence theorem to the integral representation of $I(t)$ and pass the limit inside the integral:
\[
\lim_{t\to 0^{+}} I(t)
= \frac{1}{\sqrt{\pi}} \int_{\mathbb{R}} e^{-y^{2}}\,
\lim_{t\to 0^{+}}\varphi(2\sqrt{kt}\,y)\,dy
= \frac{1}{\sqrt{\pi}} \int_{\mathbb{R}} e^{-y^{2}}\,\varphi(0)\,dy.
\]
This equals
\[
\varphi(0)\,\frac{1}{\sqrt{\pi}}\int_{\mathbb{R}} e^{-y^{2}}\,dy
= \varphi(0)\,\frac{1}{\sqrt{\pi}}\cdot\sqrt{\pi}
= \varphi(0).
\]
Hence
\[
\int_{\mathbb{R}} G(x,t)\,\varphi(x)\,dx \longrightarrow \varphi(0),
\]
which is exactly the statement that $G(\cdot,t)$ converges to the delta-function $\delta$ in the distributional sense.

\medskip

\noindent\textbf{Step 7: Interpretation and connection to Dirac's delta.}
We have shown that the solution of the heat equation with initial data $\delta(x)$ is the Gaussian
\[
u(x,t) = G(x,t)
= \frac{1}{\sqrt{4\pi k t}}\,\exp\!\left(-\frac{x^{2}}{4kt}\right),
\]
which is smooth and rapidly decaying for every $t>0$. As time increases, the Gaussian “spreads out’’ (its variance is $2kt$) while its peak decreases, in such a way that the total mass remains one. In the limit as $t\downarrow 0$, this family concentrates at the origin and converges to the delta-function.

This example illustrates several central ideas of the section on Dirac’s delta-function in Fourier analysis:

\begin{itemize}
  \item The delta-function can be used as an idealized initial condition for a PDE. Although $\delta$ is not a classical function, the Fourier transform method treats it naturally via its transform $\widehat{\delta} \equiv 1$.
  \item The resulting solution $G$ is called a \emph{fundamental solution} or \emph{Green’s function}: it is the response of the system to a point source. By linearity, solutions for more general initial data $f$ can be obtained by superposing such point-source solutions, which leads to the convolution formula
  \[
  u(x,t) = (G(\cdot,t)*f)(x) = \int_{\mathbb{R}} G(x-y,t)\,f(y)\,dy.
  \]
  \item The family $\{G(\cdot,t)\}_{t>0}$ provides a concrete example of an \emph{approximate identity}: a sequence (or family) of functions that converges to $\delta$ in the sense of distributions, which is a key concept in understanding how delta-functions arise as limits of ordinary functions in Fourier analysis.
\end{itemize}

Thus the point source in the heat equation gives a vivid and computationally accessible example of how Dirac's delta-function evolves under a PDE and how Fourier methods naturally accommodate such singular data.

\end{solution}

% ===== Example 3: Sampling Property and Fourier Transform of the Delta (inquiry-based) =====
\begin{problem}[Sampling Property and Fourier Transform of the Delta]
In many physical models, forces or sources are concentrated at a single point in space, but we still want to use the language of integrals and Fourier analysis.  The Dirac delta-function $\delta$ is an idealized object that acts like a point mass: it is ``zero everywhere except at one point,'' yet its integral is $1$.  In this problem, you will discover how the delta-function ``samples'' smooth functions, and how this sampling behavior determines its Fourier transform.  Along the way, you will see explicitly what it means for the delta-function to be perfectly localized in one domain and completely spread out in its transform domain.

Throughout, use the following Fourier transform convention for (sufficiently nice) functions $f$:
\[
\widehat{f}(k) \;=\; \int_{-\infty}^{\infty} f(x)\,e^{-ikx}\,dx,
\qquad
f(x) \;=\; \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{f}(k)\,e^{ikx}\,dk.
\]

(a) One way to understand $\delta$ is as a limit of ordinary functions that become more and more sharply peaked at the origin.  Consider, for example, the family of Gaussians
\[
\delta_\varepsilon(x) \;=\; \frac{1}{\sqrt{\pi}\,\varepsilon}\,e^{-x^{2}/\varepsilon^{2}}, \qquad \varepsilon>0.
\]
Show that $\displaystyle \int_{-\infty}^{\infty} \delta_\varepsilon(x)\,dx = 1$ for every $\varepsilon>0$.  Then, let $f$ be a smooth function that does not grow too fast at infinity (for instance, assume $f$ and its derivatives are bounded), and consider the integral
\[
I_\varepsilon \;=\; \int_{-\infty}^{\infty} f(x)\,\delta_\varepsilon(x-a)\,dx.
\]
Use a change of variables or Taylor expansion of $f$ around $x=a$ to explain why $I_\varepsilon \to f(a)$ as $\varepsilon \to 0^+$.  
Hint: First shift variables so that the peak is at $0$.  Then compare $f$ near $x=a$ with its value $f(a)$.

(b) Motivated by part (a), the Dirac delta-function $\delta$ at $x=a$ is defined formally by the rule
\[
\int_{-\infty}^{\infty} f(x)\,\delta(x-a)\,dx \;=\; f(a)
\]
for all sufficiently smooth, well-behaved functions $f$.  This is called the \emph{sampling property} or \emph{sifting property} of the delta.

(i) Use your work from part (a) to justify why this is a natural definition if we think of $\delta$ as a limit of the approximations $\delta_\varepsilon$.  

(ii) Check the sampling property on some concrete examples, such as $f(x)=1$, $f(x)=x$, and $f(x) = e^{ix}$.  What do the corresponding integrals with $\delta(x-a)$ give?

% Hint: For the examples, just apply the rule $\int f(x)\delta(x-a)\,dx = f(a)$ directly and interpret the result.

(c) Now use the sampling property to compute the Fourier transform of $\delta(x-a)$.  Treat $\delta(x-a)$ as if it were an ordinary function and write
\[
\widehat{\delta(\cdot - a)}(k) 
\;:=\; \int_{-\infty}^{\infty} \delta(x-a)\,e^{-ikx}\,dx.
\]
Evaluate this integral using the sampling property from part (b).

What function of $k$ do you obtain?  How does its magnitude depend on $k$?  How does its \emph{phase} depend on $a$?

% Hint: Think of $f(x) = e^{-ikx}$ in the sampling rule.

(d) Next, we turn the picture around: we want to see that a constant function in the $k$-variable gives a delta-function in the $x$-variable under the inverse Fourier transform.  Consider the inverse Fourier transform of the constant function $\widehat{f}(k)\equiv 1$:
\[
g(x) \;:=\; \frac{1}{2\pi}\int_{-\infty}^{\infty} 1\cdot e^{ikx}\,dk.
\]
(i) Argue (informally, if needed) that this integral does not define an ordinary function of $x$ in the usual sense.  Instead, we interpret $g$ as a \emph{distribution} defined by its action on a test function $\varphi$:
\[
\int_{-\infty}^{\infty} g(x)\,\varphi(x)\,dx
\;:=\; 
\frac{1}{2\pi}\int_{-\infty}^{\infty}\int_{-\infty}^{\infty} e^{ikx}\,\varphi(x)\,dx\,dk.
\]
Explain why (under appropriate conditions allowing you to change the order of integration) this becomes
\[
\int_{-\infty}^{\infty} g(x)\,\varphi(x)\,dx \;=\; \varphi(0).
\]
Conclude that $g(x)$ must be $2\pi\,\delta(x)$ as a distribution.

(ii) Use this observation, together with your answer in part (c), to state the inverse Fourier transform of $e^{-ika}$.  What is the spatial-domain object corresponding to a pure complex exponential in frequency?

% Hint: Combine linearity of the transform with the shift in $x$-space you observed for $\delta(x-a)$ in $k$-space.

(e) (Extensions and “what ifs.”)

(i) Suppose we differentiate $\delta(x-a)$ with respect to $x$, obtaining the distribution $\delta'(x-a)$.  Guess, and then verify using integration by parts, a formula for
\[
\int_{-\infty}^{\infty} f(x)\,\delta'(x-a)\,dx
\]
in terms of $f$ and its derivative at $x=a$.  How does this relate to the idea that derivatives of $\delta$ record higher-order information about $f$ at a point?

(ii) Using your formula from (i) and the definition of the Fourier transform, determine the Fourier transform of $\delta'(x-a)$.  How does it compare to the transform of $\delta(x-a)$ from part (c)?  What operation in physical space does multiplication by $ik$ in frequency space correspond to?

% Hint: For (ii), view $\delta'(x-a)$ as acting on $f(x)=e^{-ikx}$ and recall that $\frac{d}{dx}e^{-ikx} = -ik\,e^{-ikx}$.
\end{problem}

% ===== Example 3: Sampling Property and Fourier Transform of the Delta (full solution) =====
\begin{problem}[Sampling Property and Fourier Transform of the Delta]
Let $\delta$ denote the Dirac delta “function.”  Assume it is characterized by the sampling property
\[
\int_{-\infty}^{\infty} f(x)\,\delta(x-a)\,dx = f(a)
\]
for all sufficiently smooth, rapidly decaying functions $f$.  We use the Fourier transform
\[
\widehat{f}(k) = \int_{-\infty}^{\infty} f(x)\,e^{-ikx}\,dx,
\qquad
f(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{f}(k)\,e^{ikx}\,dk.
\]

(a) Justify the sampling property by considering a family of approximate deltas $\delta_\varepsilon(x-a)$, for example
\[
\delta_\varepsilon(x) = \frac{1}{\sqrt{\pi}\,\varepsilon}\,e^{-x^{2}/\varepsilon^{2}},
\]
and showing that
\[
\int f(x)\,\delta_\varepsilon(x-a)\,dx \to f(a)
\]
for smooth $f$ as $\varepsilon\to 0^+$.

(b) Using the sampling property, compute the Fourier transform of $\delta(x-a)$:
\[
\widehat{\delta(\cdot - a)}(k) = \int_{-\infty}^{\infty} \delta(x-a)\,e^{-ikx}\,dx.
\]

(c) Show that the inverse Fourier transform of the constant function $\widehat{f}(k)\equiv 1$ is the distribution $2\pi\delta(x)$, in the sense that
\[
\frac{1}{2\pi}\int_{-\infty}^{\infty} e^{ikx}\,dk = 2\pi\,\delta(x)
\]
as distributions.  Use this to identify the inverse Fourier transform of $e^{-ika}$.

(d) Briefly explain how these computations illustrate the idea that the delta-function is perfectly localized in physical space while being completely delocalized (a pure modulation) in frequency space, and conversely for a constant in frequency space.
\end{problem}

\begin{solution}
We begin by justifying the sampling property via an approximation of the Dirac delta by ordinary functions, and then we use that property to compute its Fourier transform and the inverse transform of a constant.

\medskip
\noindent\textbf{(a) Sampling via approximations of the identity.}
Consider the Gaussian approximations
\[
\delta_\varepsilon(x) = \frac{1}{\sqrt{\pi}\,\varepsilon}\,e^{-x^{2}/\varepsilon^{2}}, \qquad \varepsilon>0.
\]
First, each $\delta_\varepsilon$ has total mass one:
\[
\int_{-\infty}^{\infty} \delta_\varepsilon(x)\,dx
= \frac{1}{\sqrt{\pi}\,\varepsilon}\int_{-\infty}^{\infty} e^{-x^{2}/\varepsilon^{2}}\,dx.
\]
With the change of variables $u=x/\varepsilon$, we obtain
\[
\int_{-\infty}^{\infty} e^{-x^{2}/\varepsilon^{2}}\,dx
= \varepsilon\int_{-\infty}^{\infty} e^{-u^{2}}\,du
= \varepsilon\sqrt{\pi},
\]
so the integral of $\delta_\varepsilon$ is indeed $1$.

Now let $f$ be a smooth function with at most moderate growth, and consider
\[
I_\varepsilon = \int_{-\infty}^{\infty} f(x)\,\delta_\varepsilon(x-a)\,dx.
\]
We shift variables by writing $y = x-a$, so that $x = a+y$ and $dx = dy$.  Then
\[
I_\varepsilon
= \int_{-\infty}^{\infty} f(a+y)\,\delta_\varepsilon(y)\,dy
= \int_{-\infty}^{\infty} f(a+y)\,\frac{1}{\sqrt{\pi}\,\varepsilon}e^{-y^{2}/\varepsilon^{2}}\,dy.
\]
Because $f$ is smooth, we can expand it in a Taylor series around $a$:
\[
f(a+y) = f(a) + f'(a)\,y + \tfrac12 f''(a)\,y^{2} + \cdots,
\]
at least for $y$ in a neighborhood of $0$.  Substituting this into the integral for $I_\varepsilon$ and integrating term by term, we obtain
\[
I_\varepsilon
= f(a)\int \delta_\varepsilon(y)\,dy
 + f'(a)\int y\,\delta_\varepsilon(y)\,dy
 + \frac12 f''(a)\int y^{2}\,\delta_\varepsilon(y)\,dy
 + \cdots.
\]
We already know $\int \delta_\varepsilon(y)\,dy = 1$.  By symmetry, the integrals of odd powers of $y$ against the even function $\delta_\varepsilon(y)$ vanish:
\[
\int_{-\infty}^{\infty} y\,\delta_\varepsilon(y)\,dy = 0,
\quad
\int y^{3}\,\delta_\varepsilon(y)\,dy = 0,
\ \text{etc.}
\]
The even-moment integrals (such as $\int y^{2}\delta_\varepsilon(y)\,dy$) are finite and of order $\varepsilon^{2}$, $\varepsilon^{4}$, and so on, because the Gaussian becomes increasingly concentrated near $0$.  Thus
\[
I_\varepsilon = f(a) + O(\varepsilon^{2}) \quad \text{as } \varepsilon\to 0^+,
\]
and hence $I_\varepsilon \to f(a)$.

This behavior motivates the definition of a distribution $\delta(x-a)$ satisfying
\[
\int_{-\infty}^{\infty} f(x)\,\delta(x-a)\,dx = f(a)
\]
for all smooth, sufficiently well-behaved $f$.  This is the \emph{sampling property} of the Dirac delta.

\medskip
\noindent\textbf{(b) Direct use of the sampling property.}
Once we accept the sampling property as the defining rule for $\delta(x-a)$, many integrals can be evaluated immediately.  For example, for $f(x)=1$ we obtain
\[
\int_{-\infty}^{\infty} 1\cdot \delta(x-a)\,dx = 1 = f(a),
\]
which is consistent with viewing $\delta(x-a)$ as a unit “point mass” at $x=a$.  For $f(x)=x$ we find
\[
\int_{-\infty}^{\infty} x\,\delta(x-a)\,dx = a,
\]
and for $f(x) = e^{ix}$,
\[
\int_{-\infty}^{\infty} e^{ix}\,\delta(x-a)\,dx = e^{ia}.
\]
These concrete cases confirm that the delta indeed “picks out” the value of the function at the point $x=a$.

\medskip
\noindent\textbf{(c) Fourier transform of $\delta(x-a)$.}
We now compute the Fourier transform of $\delta(x-a)$ using our convention
\[
\widehat{f}(k) = \int_{-\infty}^{\infty} f(x)\,e^{-ikx}\,dx.
\]
Formally,
\[
\widehat{\delta(\cdot - a)}(k)
:= \int_{-\infty}^{\infty} \delta(x-a)\,e^{-ikx}\,dx.
\]
We can view $e^{-ikx}$ as the test function $f(x)$ in the sampling property.  Thus
\[
\widehat{\delta(\cdot - a)}(k)
= e^{-ika}.
\]
This shows that the Fourier transform of a point mass at $x=a$ is a pure complex exponential in $k$ with unit magnitude and a phase that depends linearly on $k$ and on the shift $a$.

Two features are important here:

1. The magnitude of $\widehat{\delta(\cdot - a)}(k)$ is identically $1$ for all $k$.  In frequency space, the delta is completely “spread out”: it has equal weight at all frequencies.

2. The position $a$ in physical space appears in frequency space only through a phase factor $e^{-ika}$.  Translating the delta in $x$ corresponds to multiplying its Fourier transform by a modulation factor in $k$.

Thus, the delta is perfectly localized in $x$, while its transform is perfectly delocalized in $k$ (but with a structured phase).

\medskip
\noindent\textbf{(d) Inverse Fourier transform of a constant and of $e^{-ika}$.}
Next, we compute the inverse Fourier transform of the constant function $\widehat{f}(k)\equiv 1$:
\[
g(x) := \frac{1}{2\pi}\int_{-\infty}^{\infty} 1\cdot e^{ikx}\,dk.
\]
As an ordinary improper integral, this expression does not converge in the usual sense.  However, we can make good sense of $g$ as a \emph{distribution}, defined by its action on a test function $\varphi$:
\[
\int_{-\infty}^{\infty} g(x)\,\varphi(x)\,dx
:= \frac{1}{2\pi}\int_{-\infty}^{\infty}\!\int_{-\infty}^{\infty} e^{ikx}\,\varphi(x)\,dx\,dk.
\]
Assuming $\varphi$ is smooth and rapidly decaying, we may exchange the order of integration:
\[
\int_{-\infty}^{\infty} g(x)\,\varphi(x)\,dx
= \frac{1}{2\pi}\int_{-\infty}^{\infty} \left( \int_{-\infty}^{\infty} e^{ikx}\,dk \right) \varphi(x)\,dx.
\]
We interpret the inner integral in terms of the inverse transform of the constant $1$.  By the defining property of the Fourier transform, we know that the inverse transform of $1$ must be the distribution that, when paired with $\varphi$, returns $\varphi(0)$.  Indeed, we can verify this by using the usual Fourier inversion formula for $\varphi$:
\[
\varphi(0) = \frac{1}{2\pi}\int_{-\infty}^{\infty}\widehat{\varphi}(k)\,dk
= \frac{1}{2\pi}\int_{-\infty}^{\infty}\left(\int_{-\infty}^{\infty} \varphi(x)\,e^{-ikx}\,dx\right)dk.
\]
Exchanging integrals again,
\[
\varphi(0) 
= \int_{-\infty}^{\infty} \varphi(x)\left( \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-ikx}\,dk \right)dx.
\]
By uniqueness of distributions, this means
\[
\frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-ikx}\,dk = 2\pi\,\delta(x)
\]
in the distributional sense, and similarly
\[
\frac{1}{2\pi}\int_{-\infty}^{\infty} e^{ikx}\,dk = 2\pi\,\delta(x).
\]
Therefore, the inverse Fourier transform of the constant function $1$ is
\[
g(x) = 2\pi\,\delta(x).
\]

Now we identify the inverse Fourier transform of $e^{-ika}$.  By our convention,
\[
\mathcal{F}^{-1}\{e^{-ika}\}(x) 
= \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-ika}\,e^{ikx}\,dk
= \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{ik(x-a)}\,dk.
\]
The same reasoning as above shows that this integral is $2\pi\,\delta(x-a)$ in the sense of distributions.  Hence
\[
\mathcal{F}^{-1}\{e^{-ika}\}(x) = 2\pi\,\delta(x-a).
\]

Combining this with part (c), we see that
\[
\widehat{\delta(\cdot - a)}(k) = e^{-ika}
\quad\text{and}\quad
\mathcal{F}^{-1}\{e^{-ika}\}(x) = 2\pi\,\delta(x-a),
\]
which is perfectly consistent with the general Fourier inversion framework.

\medskip
\noindent\textbf{(e) Conceptual interpretation.}
The calculations above show a fundamental duality:

\begin{itemize}
\item In physical space, $\delta(x-a)$ is infinitely localized: all of its “mass” is concentrated at the single point $x=a$.  Its Fourier transform is the bounded, non-decaying function $e^{-ika}$, which occupies all frequencies with equal magnitude.  The only information about $a$ appears in the phase.

\item In frequency space, the constant function $\widehat{f}(k)\equiv 1$ is completely spread out: it has the same value at every frequency.  Its inverse Fourier transform is the highly localized distribution $2\pi\delta(x)$ in physical space.
\end{itemize}

Thus, the Dirac delta exemplifies the general principle that strong localization in one domain (here, physical space) corresponds to strong delocalization in the transform domain (here, frequency space), and vice versa.  In the context of this chapter on Fourier analysis and this section on the Dirac delta, the delta-function is the extreme case of a “point source,” and its Fourier transform captures the idea that such a point source excites all frequencies equally, differing only by a phase factor determined by its location.
\end{solution}

% ===== Example 4: Approximating the Delta by Ordinary Functions (inquiry-based) =====
\begin{problem}[Approximating the Delta by Ordinary Functions]
In many physical models, one wishes to describe an “idealized point source’’: a force applied at a single instant of time, a charge concentrated at a single point in space, or an infinitely sharp spike in a signal. Mathematically, these are modeled by the Dirac delta-function $\delta(x)$, which is not a function in the ordinary sense but a distribution. One way to understand $\delta$ concretely is to approximate it by honest functions that are increasingly concentrated near the origin yet keep total mass one. In this problem, you will construct and analyze several such approximations, and see in what precise sense they converge to $\delta$.

Throughout, let $f\colon \mathbb{R} \to \mathbb{R}$ be a continuous function, and when needed you may additionally assume that $f$ has compact support or decays sufficiently rapidly at infinity.

\smallskip

(a) \emph{Tall, thin rectangles.} For each $\varepsilon > 0$, define
\[
\phi_\varepsilon(x)
=
\begin{cases}
\dfrac{1}{2\varepsilon}, & |x| < \varepsilon,\\[0.5em]
0, & |x| \ge \varepsilon.
\end{cases}
\]
\begin{itemize}
  \item[(i)] Sketch the graph of $\phi_\varepsilon$ for a few values of $\varepsilon$ (for example, $\varepsilon=1$, $\varepsilon=\tfrac{1}{2}$, and $\varepsilon=\tfrac{1}{4}$). How do these graphs change as $\varepsilon \to 0$?
  \item[(ii)] Compute $\displaystyle \int_{-\infty}^{\infty} \phi_\varepsilon(x)\,dx$. Why is the answer independent of $\varepsilon$?
  \item[(iii)] Determine the pointwise limit of $\phi_\varepsilon(x)$ as $\varepsilon \to 0$ for $x \neq 0$, and discuss what happens at $x = 0$. In what sense do these functions resemble a “spike” at the origin?
\end{itemize}

(b) \emph{Testing against a smooth function.} One of the defining properties of the Dirac delta is
\[
\int_{-\infty}^{\infty} f(x)\,\delta(x)\,dx = f(0).
\]
We want to see if the family $(\phi_\varepsilon)_{\varepsilon>0}$ has the same property in the limit.
\begin{itemize}
  \item[(i)] Show that
  \[
  \int_{-\infty}^{\infty} f(x)\,\phi_\varepsilon(x)\,dx
  = \frac{1}{2\varepsilon}\int_{-\varepsilon}^{\varepsilon} f(x)\,dx.
  \]
  Interpret this expression as an “average value’’ of $f$ near the origin.
  \item[(ii)] Use the continuity of $f$ at $0$ to argue that
  \[
  \lim_{\varepsilon \to 0} \int_{-\infty}^{\infty} f(x)\,\phi_\varepsilon(x)\,dx = f(0).
  \]
  Hint: For small $\varepsilon$, how much can $f(x)$ vary on the interval $[-\varepsilon,\varepsilon]$?
\end{itemize}
Conclude that $\phi_\varepsilon$ approximates $\delta$ in the sense of integrals against continuous test functions.

\smallskip

(c) \emph{Narrow Gaussian bumps.} For each $\varepsilon > 0$, define the Gaussian
\[
\psi_\varepsilon(x)
=
\frac{1}{\sqrt{\pi}\,\varepsilon}\,e^{-(x/\varepsilon)^2}.
\]
\begin{itemize}
  \item[(i)] Show that $\displaystyle \int_{-\infty}^{\infty} \psi_\varepsilon(x)\,dx = 1$ for every $\varepsilon>0$. Hint: Recall that $\displaystyle \int_{-\infty}^{\infty} e^{-u^2}\,du = \sqrt{\pi}$ and use a change of variables.
  \item[(ii)] Explain, using the graph of $\psi_\varepsilon$, what happens to its height and width as $\varepsilon \to 0$. Compare this behavior to the rectangular functions $\phi_\varepsilon$.
  \item[(iii)] Show that for each fixed $x \neq 0$, we have $\psi_\varepsilon(x) \to 0$ as $\varepsilon \to 0$.
\end{itemize}
Now consider the integral
\[
I_\varepsilon = \int_{-\infty}^{\infty} f(x)\,\psi_\varepsilon(x)\,dx.
\]
\begin{itemize}
  \item[(iv)] Fix a small number $\delta > 0$. Split the integral into two parts,
  \[
  I_\varepsilon = \int_{|x|\le \delta} f(x)\,\psi_\varepsilon(x)\,dx
  +
  \int_{|x|> \delta} f(x)\,\psi_\varepsilon(x)\,dx,
  \]
  and argue that, for small $\varepsilon$, the second integral over $|x|>\delta$ is very small. Hint: Show that $\int_{|x|>\delta} \psi_\varepsilon(x)\,dx \to 0$ as $\varepsilon \to 0$.
  \item[(v)] On the interval $|x|\le \delta$, use the continuity of $f$ at $0$ to compare $f(x)$ and $f(0)$, and show that $I_\varepsilon \to f(0)$ as $\varepsilon \to 0$.
\end{itemize}
Conclude that the Gaussians $\psi_\varepsilon$ provide a different family of approximations to the delta-function.

\smallskip

(d) \emph{A Fourier-analytic approximation: rescaled sinc functions.} Consider the family of functions
\[
\eta_N(x) = \frac{1}{\pi}\,\frac{\sin(Nx)}{x}, 
\quad N>0,
\]
with the understanding that $\eta_N(0)$ is defined by taking the limit $x\to 0$.
\begin{itemize}
  \item[(i)] Show that $\displaystyle \eta_N(0) = \frac{N}{\pi}$ by computing the limit $\lim_{x\to 0} \frac{\sin(Nx)}{x}$.
  \item[(ii)] Show that $\displaystyle \int_{-\infty}^{\infty} \eta_N(x)\,dx = 1$ for every $N>0$. Hint: You may use (without proof) the standard integral $\displaystyle \int_{-\infty}^{\infty} \frac{\sin t}{t}\,dt = \pi$.
  \item[(iii)] Let $f$ be a smooth, rapidly decaying function (for example, a Schwartz function). Explain why the integral
  \[
  J_N = \int_{-\infty}^{\infty} f(x)\,\eta_N(x)\,dx
  \]
  can be interpreted, up to constants, as a partial Fourier inversion of $f$ using only frequencies in the band $[-N,N]$. 
  \item[(iv)] Give a heuristic (or rigorous, if you know Fourier transforms) argument that $J_N \to f(0)$ as $N\to\infty$.
\end{itemize}
Compare this “oscillatory’’ approximation of $\delta$ to the positive approximations in parts (a) and (c).

\smallskip

(e) \emph{Extensions and variations.}
\begin{itemize}
  \item[(i)] Suppose you shift the rectangular bump to the point $a\in \mathbb{R}$ by defining
  \[
  \phi_{\varepsilon,a}(x) 
  =
  \begin{cases}
  \dfrac{1}{2\varepsilon}, & |x-a| < \varepsilon,\\[0.5em]
  0, & |x-a| \ge \varepsilon.
  \end{cases}
  \]
  Predict, and then verify, the limit of $\displaystyle \int_{-\infty}^{\infty} f(x)\,\phi_{\varepsilon,a}(x)\,dx$ as $\varepsilon \to 0$. What “delta-function’’ does $\phi_{\varepsilon,a}$ approximate?
  \item[(ii)] Consider a family of functions $k_\varepsilon$ with $\int k_\varepsilon(x)\,dx =1$ for all $\varepsilon$, and such that $k_\varepsilon(x)\to 0$ for each fixed $x\neq 0$ as $\varepsilon\to 0$. What additional conditions on the family $(k_\varepsilon)$ do you think are needed to guarantee that
  \[
  \int_{-\infty}^{\infty} f(x)\,k_\varepsilon(x)\,dx \to f(0)
  \quad\text{for all continuous } f?
  \]
  Try to formulate a plausible general definition of an “approximate identity’’ based on your work with $\phi_\varepsilon$, $\psi_\varepsilon$, and $\eta_N$.
\end{itemize}

\end{problem}

% ===== Example 4: Approximating the Delta by Ordinary Functions (full solution) =====
\begin{problem}[Approximating the Delta by Ordinary Functions]
On $\mathbb{R}$, consider the following three families of functions:
\[
\phi_\varepsilon(x)
=
\begin{cases}
\dfrac{1}{2\varepsilon}, & |x| < \varepsilon,\\[0.5em]
0, & |x| \ge \varepsilon,
\end{cases}
\qquad
\psi_\varepsilon(x)
=
\frac{1}{\sqrt{\pi}\,\varepsilon}e^{-(x/\varepsilon)^2},
\]
for $\varepsilon>0$, and
\[
\eta_N(x) = \frac{1}{\pi}\,\frac{\sin(Nx)}{x}, \quad N>0,
\]
with $\eta_N(0)$ defined by continuity.

\begin{enumerate}
\item[(a)] Show that each of $\phi_\varepsilon$, $\psi_\varepsilon$, and $\eta_N$ has total integral $1$ over $\mathbb{R}$.
\item[(b)] Let $f$ be continuous with compact support. Prove that
\[
\lim_{\varepsilon\to 0} \int_{-\infty}^{\infty} f(x)\,\phi_\varepsilon(x)\,dx = f(0),
\quad
\lim_{\varepsilon\to 0} \int_{-\infty}^{\infty} f(x)\,\psi_\varepsilon(x)\,dx = f(0),
\]
and, if $f$ is smooth and rapidly decaying (say $f$ is a Schwartz function),
\[
\lim_{N\to\infty} \int_{-\infty}^{\infty} f(x)\,\eta_N(x)\,dx = f(0).
\]
\end{enumerate}
Explain how these limits justify the notation
\[
\phi_\varepsilon \rightharpoonup \delta,\quad
\psi_\varepsilon \rightharpoonup \delta,\quad
\eta_N \rightharpoonup \delta,
\]
and how this illustrates the idea of the Dirac delta-function as a distribution.
\end{problem}

\begin{solution}
We are asked to show that three different families of ordinary functions approximate the Dirac delta at the origin, in the sense of their action on test functions under the integral sign. This is the basic distributional viewpoint: a sequence “converges to $\delta$’’ if integrals against any sufficiently nice test function converge to the value of that test function at the origin.

\medskip

\textbf{Part (a): Integrals equal to one.}

\emph{Rectangular functions.}
By direct computation,
\[
\int_{-\infty}^{\infty} \phi_\varepsilon(x)\,dx
=
\int_{-\varepsilon}^{\varepsilon} \frac{1}{2\varepsilon}\,dx
=
\frac{1}{2\varepsilon}\bigl(2\varepsilon\bigr)
=
1
\]
for all $\varepsilon>0$.

\smallskip

\emph{Gaussian functions.}
We use the standard Gaussian integral
\[
\int_{-\infty}^{\infty} e^{-u^2}\,du = \sqrt{\pi}.
\]
Let $u = x/\varepsilon$. Then $x = \varepsilon u$ and $dx = \varepsilon \, du$. We compute
\[
\int_{-\infty}^{\infty} \psi_\varepsilon(x)\,dx
=
\int_{-\infty}^{\infty} \frac{1}{\sqrt{\pi}\,\varepsilon}\,e^{-(x/\varepsilon)^2}\,dx
=
\frac{1}{\sqrt{\pi}\,\varepsilon}
\int_{-\infty}^{\infty} e^{-u^2}\,\varepsilon\,du
=
\frac{1}{\sqrt{\pi}}
\int_{-\infty}^{\infty} e^{-u^2}\,du
=
1.
\]
Thus each $\psi_\varepsilon$ also has total mass equal to one.

\smallskip

\emph{Sinc functions.}
For $\eta_N(x) = \dfrac{1}{\pi}\dfrac{\sin(Nx)}{x}$ we again use a standard improper integral. First observe that
\[
\eta_N(0) = \frac{1}{\pi} \lim_{x\to 0} \frac{\sin(Nx)}{x}
=
\frac{1}{\pi}\cdot N
\]
because $\sin(Nx)\sim Nx$ as $x\to 0$. To compute the integral, perform the change of variables $t = Nx$, so that $x = t/N$ and $dx = dt/N$:
\[
\int_{-\infty}^{\infty} \eta_N(x)\,dx
=
\int_{-\infty}^{\infty} \frac{1}{\pi}\,\frac{\sin(Nx)}{x}\,dx
=
\frac{1}{\pi}
\int_{-\infty}^{\infty} \frac{\sin t}{t/N}\,\frac{dt}{N}
=
\frac{1}{\pi}
\int_{-\infty}^{\infty} \frac{\sin t}{t}\,dt.
\]
Here we used $x = t/N$, so $1/x = N/t$. The factors of $N$ cancel. The classical integral
\[
\int_{-\infty}^{\infty} \frac{\sin t}{t}\,dt = \pi
\]
is well known (and can be evaluated, for instance, using contour integration or Fourier transforms). Hence
\[
\int_{-\infty}^{\infty} \eta_N(x)\,dx = \frac{1}{\pi}\cdot \pi = 1.
\]
Thus each $\eta_N$ also has unit mass.

\medskip

\textbf{Part (b): Convergence against test functions.}

The central idea is that although these functions do not converge to anything pointwise (indeed, they typically blow up at zero), their \emph{averaging action} on test functions stabilizes: the integral $\int f(x)\,\text{(approximation)}(x)\,dx$ tends to $f(0)$. This is the sense in which they converge to the distribution $\delta$.

\smallskip

\emph{Rectangular approximation.}
Let $f$ be continuous with compact support. Then
\[
\int_{-\infty}^{\infty} f(x)\,\phi_\varepsilon(x)\,dx
=
\frac{1}{2\varepsilon}\int_{-\varepsilon}^{\varepsilon} f(x)\,dx.
\]
The right-hand side is simply the average value of $f$ on the interval $[-\varepsilon,\varepsilon]$. Because $f$ is continuous at $0$, the values of $f(x)$ on a small symmetric interval around $0$ cannot differ much from $f(0)$.

More precisely, fix $\delta>0$. By continuity of $f$ at $0$, there exists $\eta>0$ such that $|x|\le \eta$ implies $|f(x) - f(0)| < \delta$. For all sufficiently small $\varepsilon$ with $0<\varepsilon \le \eta$, we have
\[
\left|
\frac{1}{2\varepsilon}\int_{-\varepsilon}^{\varepsilon} f(x)\,dx - f(0)
\right|
=
\left|
\frac{1}{2\varepsilon}\int_{-\varepsilon}^{\varepsilon} \bigl(f(x) - f(0)\bigr)\,dx
\right|
\le
\frac{1}{2\varepsilon}\int_{-\varepsilon}^{\varepsilon} |f(x) - f(0)|\,dx
\le \delta.
\]
Since $\delta>0$ was arbitrary, this shows that
\[
\lim_{\varepsilon\to 0}
\int_{-\infty}^{\infty} f(x)\,\phi_\varepsilon(x)\,dx
=
f(0).
\]
Thus the rectangular family $(\phi_\varepsilon)$ converges to $\delta$ in the distributional sense.

\smallskip

\emph{Gaussian approximation.}
Let $f$ be continuous with compact support. Consider
\[
I_\varepsilon
:=
\int_{-\infty}^{\infty} f(x)\,\psi_\varepsilon(x)\,dx.
\]
We want to show that $I_\varepsilon \to f(0)$ as $\varepsilon\to 0$. The strategy is to split the integral into a “near’’ part, where $x$ is close to $0$ and $f(x)\approx f(0)$ by continuity, and a “far’’ part, where the Gaussian mass is small.

Fix $\delta>0$. Write
\[
I_\varepsilon
=
\int_{|x|\le \delta} f(x)\,\psi_\varepsilon(x)\,dx
+
\int_{|x|> \delta} f(x)\,\psi_\varepsilon(x)\,dx
=: I_\varepsilon^{\text{near}} + I_\varepsilon^{\text{far}}.
\]

We first show that the far part tends to zero. Since $f$ has compact support, it is bounded: there exists $M>0$ with $|f(x)|\le M$ for all $x$. Then
\[
\bigl|I_\varepsilon^{\text{far}}\bigr|
\le
M\int_{|x|>\delta} \psi_\varepsilon(x)\,dx.
\]
We claim that $\int_{|x|>\delta} \psi_\varepsilon(x)\,dx \to 0$ as $\varepsilon\to 0$. To see this, use again the change of variables $u=x/\varepsilon$:
\[
\int_{|x|>\delta} \psi_\varepsilon(x)\,dx
=
\int_{|x|>\delta} \frac{1}{\sqrt{\pi}\,\varepsilon}e^{-(x/\varepsilon)^2}\,dx
=
\frac{1}{\sqrt{\pi}}\int_{|u|>\delta/\varepsilon} e^{-u^2}\,du.
\]
As $\varepsilon\to 0$, the region $|u|>\delta/\varepsilon$ moves out to infinity, and the tail integral of $e^{-u^2}$ tends to zero. Therefore $\int_{|x|>\delta} \psi_\varepsilon(x)\,dx \to 0$, and hence $\bigl|I_\varepsilon^{\text{far}}\bigr|\to 0$.

Next, consider the near part:
\[
I_\varepsilon^{\text{near}}
=
\int_{|x|\le \delta} f(x)\,\psi_\varepsilon(x)\,dx.
\]
Add and subtract $f(0)$ inside the integral:
\[
I_\varepsilon^{\text{near}} - f(0)\int_{|x|\le \delta} \psi_\varepsilon(x)\,dx
=
\int_{|x|\le \delta} \bigl(f(x)-f(0)\bigr)\,\psi_\varepsilon(x)\,dx.
\]
By continuity of $f$ at $0$, there exists $\eta>0$ such that $|x|\le \eta$ implies $|f(x)-f(0)|<\delta$. If we also require $0<\varepsilon\le \eta$, then over $|x|\le \delta$ (in particular, over $|x|\le\eta$) we have $|f(x)-f(0)|<\delta$. Hence
\[
\left|
\int_{|x|\le \delta} \bigl(f(x)-f(0)\bigr)\,\psi_\varepsilon(x)\,dx
\right|
\le
\delta\int_{|x|\le \delta} \psi_\varepsilon(x)\,dx
\le
\delta\int_{-\infty}^\infty \psi_\varepsilon(x)\,dx
=
\delta,
\]
using that the total mass is one. Thus
\[
\left|I_\varepsilon^{\text{near}} - f(0)\int_{|x|\le \delta} \psi_\varepsilon(x)\,dx\right|\le \delta.
\]
Since $\int_{|x|\le \delta} \psi_\varepsilon(x)\,dx \le 1$ and $\int_{|x|>\delta} \psi_\varepsilon(x)\,dx \to 0$, we also have
\[
\int_{|x|\le \delta} \psi_\varepsilon(x)\,dx
=
1 - \int_{|x|>\delta} \psi_\varepsilon(x)\,dx
\longrightarrow 1
\quad\text{as }\varepsilon\to 0.
\]
Combining these estimates, and using that $I_\varepsilon = I_\varepsilon^{\text{near}} + I_\varepsilon^{\text{far}}$, we deduce
\[
I_\varepsilon \longrightarrow f(0)\cdot 1 = f(0),
\]
because $I_\varepsilon^{\text{far}}\to 0$, the integral of $\psi_\varepsilon$ over $|x|\le\delta$ tends to $1$, and the error in replacing $f(x)$ by $f(0)$ on $|x|\le\delta$ is arbitrarily small. Thus $(\psi_\varepsilon)$ also converges to $\delta$ in the distributional sense.

\smallskip

\emph{Sinc approximation via Fourier analysis.}
Here we assume that $f$ is smooth and rapidly decaying, for example a Schwartz function. This allows us to use the Fourier transform and its inversion formula without technical concerns.

Recall that the Fourier transform of $f$ is
\[
\widehat{f}(\xi) = \int_{-\infty}^{\infty} f(x)\,e^{-i\xi x}\,dx,
\]
and the inverse transform can be written (under suitable hypotheses) as
\[
f(0)
=
\frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{f}(\xi)\,d\xi.
\]
We now examine the integral
\[
J_N := \int_{-\infty}^{\infty} f(x)\,\eta_N(x)\,dx
=
\int_{-\infty}^{\infty} f(x)\,\frac{1}{\pi}\,\frac{\sin(Nx)}{x}\,dx.
\]

The key observation is that $\eta_N$ can be represented as a (scaled) inverse Fourier transform of the indicator function of the interval $[-N,N]$:
\[
\eta_N(x) = \frac{1}{2\pi}\int_{-N}^{N} e^{i\xi x}\,d\xi.
\]
One can verify this by integrating $e^{i\xi x}$ with respect to $\xi$:
\[
\frac{1}{2\pi}\int_{-N}^{N} e^{i\xi x}\,d\xi
=
\frac{1}{2\pi}\left[\frac{e^{i\xi x}}{ix}\right]_{\xi=-N}^{\xi=N}
=
\frac{1}{2\pi}\,\frac{e^{iNx} - e^{-iNx}}{ix}
=
\frac{1}{2\pi}\,\frac{2i\sin(Nx)}{ix}
=
\frac{1}{\pi}\,\frac{\sin(Nx)}{x}
=
\eta_N(x).
\]
Therefore,
\[
J_N
=
\int_{-\infty}^{\infty} f(x)\left( \frac{1}{2\pi}\int_{-N}^{N} e^{i\xi x}\,d\xi \right)dx.
\]
By Fubini’s theorem (justified by the rapid decay of $f$), we may interchange the order of integration:
\[
J_N
=
\frac{1}{2\pi}\int_{-N}^{N} 
\left( \int_{-\infty}^{\infty} f(x)\,e^{i\xi x}\,dx \right)d\xi
=
\frac{1}{2\pi}\int_{-N}^{N} \widehat{f}(-\xi)\,d\xi.
\]
Since $f$ is real-valued, $\widehat{f}(-\xi)$ is related to $\widehat{f}(\xi)$ via complex conjugation, but in any case, as $N\to\infty$ the interval $[-N,N]$ expands to the whole real line. By absolute integrability of $\widehat{f}$ (which holds for Schwartz functions), we obtain
\[
\lim_{N\to\infty} J_N
=
\frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{f}(-\xi)\,d\xi.
\]
By the inverse Fourier transform at $x=0$, this integral equals $f(0)$. Hence
\[
\lim_{N\to\infty} \int_{-\infty}^{\infty} f(x)\,\eta_N(x)\,dx = f(0).
\]

Thus the family $(\eta_N)$ also converges to $\delta$ in the sense of distributions. Unlike the previous two families, $\eta_N$ takes both positive and negative values and exhibits increasingly rapid oscillations, rather than a simple positive peak shrinking to the origin. Nevertheless, its action on test functions is the same in the limit.

\medskip

\textbf{Conclusion and interpretation.}

In each of the three cases, we have shown that for appropriate classes of test functions $f$,
\[
\int_{\mathbb{R}} f(x)\,\phi_\varepsilon(x)\,dx \to f(0),\quad
\int_{\mathbb{R}} f(x)\,\psi_\varepsilon(x)\,dx \to f(0),\quad
\int_{\mathbb{R}} f(x)\,\eta_N(x)\,dx \to f(0).
\]
By definition of distributional convergence, this means
\[
\phi_\varepsilon \rightharpoonup \delta,\quad
\psi_\varepsilon \rightharpoonup \delta,\quad
\eta_N \rightharpoonup \delta,
\]
where the arrow denotes weak (distributional) convergence: convergence when integrated against all test functions.

This example illustrates a central idea of the Dirac delta-function: it is not a function but a linear functional on test functions, characterized by the identity
\[
\langle \delta, f\rangle = f(0).
\]
Different “approximate identities’’—rectangular pulses, Gaussian mollifiers, and oscillatory sinc kernels—provide distinct models for the same abstract distribution. What matters is not their pointwise behavior, but how they act under the integral sign. This viewpoint is fundamental in Fourier analysis and in the general theory of distributions used to model idealized point sources in applied mathematics.

\end{solution}

% ===== Example 5: Heaviside Step Function and Distributional Derivatives (inquiry-based) =====
\begin{problem}[Heaviside Step Function and Distributional Derivatives]
In many physical models, a system is ``switched on'' at a specific time, for instance when a voltage source is connected to a circuit at time $t = 0$. A simple idealized model of this behavior is the \emph{Heaviside step function}, which jumps from $0$ to $1$ at the origin. In classical calculus we cannot take an ordinary derivative at the jump point, but in the framework of distributions we can make precise sense of the ``derivative'' of this function. In this problem you will rediscover, step by step, that the distributional derivative of the Heaviside function is exactly the Dirac delta at the origin.

We work on the real line $\mathbb{R}$ and use $\mathcal{D}(\mathbb{R})$ to denote the space of smooth, compactly supported test functions $\varphi \colon \mathbb{R} \to \mathbb{R}$.

\medskip

(a) Define the Heaviside step function $H \colon \mathbb{R} \to \mathbb{R}$ by
\[
H(x) = 
\begin{cases}
0, & x < 0,\\
1, & x > 0.
\end{cases}
\]
For now you may leave $H(0)$ undefined or choose any convenient value. 

\begin{enumerate}
\item[(i)] In the sense of ordinary calculus, for which values of $x$ does $H$ have a classical derivative, and what is that derivative?
\item[(ii)] Explain carefully why $H$ is not differentiable at $x = 0$ in the classical sense. What goes wrong if you try to compute the derivative at $0$ using the definition with limits?
\end{enumerate}

(b) In distribution theory, we do not differentiate functions pointwise. Instead, we define derivatives by how they act on test functions. Recall that a locally integrable function $f$ defines a distribution $T_f$ by
\[
\langle T_f, \varphi \rangle = \int_{\mathbb{R}} f(x)\,\varphi(x)\,dx, \qquad \varphi \in \mathcal{D}(\mathbb{R}),
\]
and that the \emph{distributional derivative} $T'$ of a distribution $T$ is defined by
\[
\langle T', \varphi \rangle = - \langle T, \varphi' \rangle
\quad \text{for all } \varphi \in \mathcal{D}(\mathbb{R}).
\]

\begin{enumerate}
\item[(i)] Show that the Heaviside function $H$ defines a distribution $T_H$ via the above formula. What regularity of $H$ do you use to justify that $T_H$ is well defined?
\item[(ii)] Write down explicitly the defining formula for the distributional derivative $T_H'$ in terms of an integral involving $H$ and $\varphi'$. That is, express $\langle T_H', \varphi \rangle$ as an integral.
\end{enumerate}
Hint: You should obtain an expression of the form
\[
\langle T_H', \varphi \rangle = -\int_{\mathbb{R}} H(x)\,\varphi'(x)\,dx.
\]

(c) Now exploit the particular form of $H$. 

\begin{enumerate}
\item[(i)] Using the definition of $H$, split the integral
\[
\int_{\mathbb{R}} H(x)\,\varphi'(x)\,dx
\]
into integrals over the regions $(-\infty,0)$ and $(0,\infty)$, and simplify as much as possible.
Hint: On which intervals is $H$ equal to $0$ or $1$?

\item[(ii)] Recall that every test function $\varphi \in \mathcal{D}(\mathbb{R})$ has compact support, so there is a large $R > 0$ such that $\varphi(x) = 0$ for $|x| \ge R$. Use this fact, together with the Fundamental Theorem of Calculus, to evaluate the integral $\displaystyle \int_{0}^{\infty} \varphi'(x)\,dx$ in terms of $\varphi(0)$.
\end{enumerate}
Hint: Replace the upper limit $\infty$ by $R$ and note that $\varphi(R)=0$.

(d) Combine your work from parts (b) and (c) to compute $\langle T_H', \varphi \rangle$ explicitly in terms of $\varphi(0)$, for an arbitrary test function $\varphi \in \mathcal{D}(\mathbb{R})$. 

\begin{enumerate}
\item[(i)] Show that there is a constant $C$ such that 
\[
\langle T_H', \varphi \rangle = C\,\varphi(0)
\quad \text{for all } \varphi \in \mathcal{D}(\mathbb{R}).
\]
What is the value of $C$?

\item[(ii)] Recall that the Dirac delta distribution $\delta$ at the origin is defined by
\[
\langle \delta, \varphi \rangle = \varphi(0), \qquad \varphi \in \mathcal{D}(\mathbb{R}).
\]
Using this characterization, conclude that $T_H' = \delta$ as distributions. In words: the distributional derivative of the Heaviside step function is the Dirac delta at the origin.
\end{enumerate}

(e) Extensions and variations.

\begin{enumerate}
\item[(i)] Consider the shifted Heaviside function $H_a(x) = H(x-a)$, which ``turns on'' at $x = a$. Repeat the computation conceptually (you do not need to write every step) and determine the distributional derivative of $H_a$. Which delta-distribution appears, and at which point?

\item[(ii)] Let $f$ be a piecewise constant function with finitely many jumps:
\[
f(x) = c_0 \quad (x < x_1), \qquad
f(x) = c_1 \quad (x_1 < x < x_2), \quad \dots, \quad
f(x) = c_n \quad (x > x_n),
\]
where $x_1 < \cdots < x_n$ and $c_0,\dots,c_n$ are real constants. Based on your work with $H$, make a conjecture for the distributional derivative $f'$ in terms of Dirac deltas at the jump points $x_k$. How do the coefficients of these delta distributions relate to the jump sizes $c_k - c_{k-1}$?
Hint: Try writing $f$ as a linear combination of shifted Heaviside functions plus a constant.
\end{enumerate}

\end{problem}

% ===== Example 5: Heaviside Step Function and Distributional Derivatives (full solution) =====
\begin{problem}[Heaviside Step Function and Distributional Derivatives]
Let $H \colon \mathbb{R} \to \mathbb{R}$ be the Heaviside step function
\[
H(x) =
\begin{cases}
0, & x < 0,\\
1, & x > 0,
\end{cases}
\]
with any value assigned at $x=0$. Regard $H$ as a distribution $T_H$ acting on test functions $\varphi \in \mathcal{D}(\mathbb{R})$ by
\[
\langle T_H, \varphi \rangle = \displaystyle \int_{\mathbb{R}} H(x)\,\varphi(x)\,dx.
\]
Recall that the distributional derivative $T_H'$ is defined by
\[
\langle T_H', \varphi \rangle = -\langle T_H, \varphi' \rangle.
\]

\begin{enumerate}
\item[(a)] Compute $\langle T_H', \varphi \rangle$ explicitly for an arbitrary test function $\varphi$ and show that
\[
\langle T_H', \varphi \rangle = \varphi(0).
\]
\item[(b)] Deduce that, as distributions,
\[
T_H' = \delta,
\]
where $\delta$ is the Dirac delta at the origin.
\item[(c)] More generally, for $a \in \mathbb{R}$, define $H_a(x) = H(x-a)$. Compute the distributional derivative of $H_a$ and express it in terms of a Dirac delta concentrated at $x=a$.
\end{enumerate}
\end{problem}

\begin{solution}
We first recall the setting. A locally integrable function $f$ defines a distribution $T_f$ by
\[
\langle T_f, \varphi \rangle = \int_{\mathbb{R}} f(x)\,\varphi(x)\,dx
\quad \text{for all } \varphi \in \mathcal{D}(\mathbb{R}),
\]
and the distributional derivative $T'$ of a distribution $T$ is defined by
\[
\langle T', \varphi \rangle = -\langle T, \varphi' \rangle.
\]
The central idea is that differentiation is transferred from the (possibly rough) distribution onto the smooth test function, thereby making sense of derivatives for objects like $H$ which are not classically differentiable.

\medskip

\noindent\textbf{(a) Computation of $\langle T_H', \varphi \rangle$.}
The Heaviside function $H$ is locally integrable, so it defines a distribution $T_H$ in the usual way. For a test function $\varphi \in \mathcal{D}(\mathbb{R})$ we have
\[
\langle T_H', \varphi \rangle = -\langle T_H, \varphi' \rangle
= -\int_{\mathbb{R}} H(x)\,\varphi'(x)\,dx.
\]
We now exploit the simple form of $H$. Since $H(x)=0$ for $x<0$ and $H(x)=1$ for $x>0$, we can split the integral:
\[
\int_{\mathbb{R}} H(x)\,\varphi'(x)\,dx
= \int_{-\infty}^{0} 0\cdot \varphi'(x)\,dx
  + \int_{0}^{\infty} 1\cdot \varphi'(x)\,dx
= \int_{0}^{\infty} \varphi'(x)\,dx.
\]
Thus
\[
\langle T_H', \varphi \rangle = -\int_{0}^{\infty} \varphi'(x)\,dx.
\]

Because $\varphi$ is a test function, it has compact support. Hence there exists some $R > 0$ such that $\varphi(x)=0$ for all $|x|\ge R$. In particular, for all sufficiently large $b \ge R$ we have $\varphi(b)=0$. Using the Fundamental Theorem of Calculus, we can therefore write
\[
\int_{0}^{\infty} \varphi'(x)\,dx
= \lim_{b\to\infty} \int_{0}^{b} \varphi'(x)\,dx
= \lim_{b\to\infty} \bigl(\varphi(b) - \varphi(0)\bigr)
= 0 - \varphi(0)
= -\,\varphi(0).
\]
Substituting this into our earlier expression, we obtain
\[
\langle T_H', \varphi \rangle = -\left( \int_{0}^{\infty} \varphi'(x)\,dx \right)
= -\bigl(-\varphi(0)\bigr)
= \varphi(0).
\]

This identity holds for every test function $\varphi \in \mathcal{D}(\mathbb{R})$:
\[
\boxed{\;\langle T_H', \varphi \rangle = \varphi(0)\;}.
\]

\medskip

\noindent\textbf{(b) Identification of $T_H'$ with the Dirac delta.}
By definition, the Dirac delta distribution $\delta$ at the origin is characterized by
\[
\langle \delta, \varphi \rangle = \varphi(0)
\quad \text{for all } \varphi \in \mathcal{D}(\mathbb{R}).
\]
Comparing this with the identity we have just proved, we see that the linear functionals $T_H'$ and $\delta$ act in exactly the same way on every test function. In the theory of distributions, this means that they are the same distribution. Hence
\[
\boxed{\;T_H' = \delta\;}.
\]

Conceptually, this says that the derivative of a step of size one, concentrated at a point, is a point mass at that point. This is a precise realization of the idea that ``the derivative of a jump discontinuity is a delta spike.''

\medskip

\noindent\textbf{(c) The shifted Heaviside function.}
Let $a \in \mathbb{R}$ and define $H_a(x) = H(x-a)$, which is the Heaviside function shifted to the right by $a$. It jumps from $0$ to $1$ at $x = a$. As before, $H_a$ is locally integrable and therefore defines a distribution $T_{H_a}$ via
\[
\langle T_{H_a}, \varphi \rangle = \int_{\mathbb{R}} H_a(x)\,\varphi(x)\,dx
= \int_{\mathbb{R}} H(x-a)\,\varphi(x)\,dx.
\]

We compute its distributional derivative:
\[
\langle T_{H_a}', \varphi \rangle = -\int_{\mathbb{R}} H(x-a)\,\varphi'(x)\,dx.
\]
Again we use the structure of $H(x-a)$: it is $0$ for $x<a$ and $1$ for $x>a$, so
\[
\int_{\mathbb{R}} H(x-a)\,\varphi'(x)\,dx
= \int_{a}^{\infty} \varphi'(x)\,dx.
\]
As before, since $\varphi$ has compact support, there exists $R$ such that $\varphi(x)=0$ for $|x|\ge R$, and thus for $b$ large enough,
\[
\int_{a}^{\infty} \varphi'(x)\,dx
= \lim_{b\to\infty} \int_{a}^{b} \varphi'(x)\,dx
= \lim_{b\to\infty} (\varphi(b) - \varphi(a))
= 0 - \varphi(a)
= -\varphi(a).
\]
Therefore
\[
\langle T_{H_a}', \varphi \rangle = -\left( \int_{a}^{\infty} \varphi'(x)\,dx \right)
= -\bigl(-\varphi(a)\bigr)
= \varphi(a).
\]
By the definition of the Dirac delta $\delta_a$ concentrated at $x=a$,
\[
\langle \delta_a, \varphi \rangle = \varphi(a)
\quad \text{for all } \varphi \in \mathcal{D}(\mathbb{R}).
\]
We conclude that
\[
\boxed{\;T_{H_a}' = \delta_a\;}.
\]

\medskip

\noindent\textbf{Context within the Dirac delta section.}
This example illustrates one of the fundamental structural relationships in distribution theory: the Dirac delta is the distributional derivative of the Heaviside step function. In applications, this means that a sudden jump in a state variable (modeled by $H$ or its shifts) corresponds to a concentrated impulse (modeled by $\delta$) in its derivative. From the point of view of Fourier analysis and partial differential equations, this connection is central: it allows us to model impulsive forcing terms, interfaces, and discontinuities in a unified algebraic framework using delta distributions and distributional derivatives.
\end{solution}

\section{Closed-form Representation for Select Fourier Transforms}
% --- Narrative plan (auto-generated) ---
% This section develops a small but powerful toolbox of functions whose Fourier transforms can be written in closed form and manipulated reliably. Many applied problems in partial differential equations, signal processing, and dynamical systems reduce to computing or recognizing such transforms, especially when solving initial value problems, analyzing stability, or understanding how information propagates in space and time. By learning how to obtain and use these closed-form expressions, you will be able to solve model problems explicitly and to benchmark numerical and asymptotic methods.
%
% Our development emphasizes how Fourier analysis interacts with other parts of the mathematical toolkit. We show how to combine basic calculus techniques with ideas from ordinary differential equations, such as solving linear constant-coefficient equations in the frequency domain, and with tools from complex analysis, such as contour integration and residues, to compute transforms of rational functions. Along the way, we highlight structural features like symmetry, scaling, and convolution that allow you to generate new closed-form transforms from a few core examples. This perspective prepares you for later chapters, where these building blocks reappear inside Green’s functions for PDEs, dispersion relations for wave equations, and spectral representations of linear operators.

% ===== Example 1: Fourier Transform of Exponential Decay (inquiry-based) =====
\begin{problem}[Fourier Transform of Exponential Decay]
Many physical systems exhibit transient responses that decay exponentially in time. For instance, the voltage across a discharging capacitor in an RC circuit or the displacement of a lightly damped mass after a disturbance both decay like an exponential, and typically we are only interested in the response for $t \ge 0$. In this problem we examine the Fourier transform of such a ``one-sided'' exponential decay and see how the rate of decay is encoded in the frequency domain.

Let $a>0$ and define
\[
f(t) \coloneqq
\begin{cases}
e^{-a t}, & t \ge 0,\\[4pt]
0, & t<0.
\end{cases}
\]
We use the (angular) frequency convention
\[
\widehat{f}(\omega) \coloneqq \displaystyle\int_{-\infty}^{\infty} f(t)\, e^{-i\omega t}\,dt,
\qquad \omega \in \mathbb{R}.
\]

\medskip

(a) First look at a simpler, purely real integral. For $\alpha>0$, recall or re-derive the value of
\[
I(\alpha) \coloneqq \int_{0}^{\infty} e^{-\alpha t}\,dt.
\]
Compute $I(\alpha)$ explicitly. What condition on $\alpha$ is needed so that this improper integral converges?

\medskip

(b) We will now adapt this computation to complex exponents. Let $\lambda$ be a complex number with positive real part, $\operatorname{Re}(\lambda) > 0$. Consider the integral
\[
J(\lambda) \coloneqq \int_{0}^{\infty} e^{-\lambda t}\,dt.
\]
(i) Show that $|e^{-\lambda t}| = e^{-\operatorname{Re}(\lambda)\, t}$ for all $t \ge 0$. 

(ii) Use this to argue that $J(\lambda)$ converges absolutely whenever $\operatorname{Re}(\lambda) > 0$.

(iii) Compute $J(\lambda)$ by the same method you used in part (a), and express it in terms of $\lambda$.
Hint: Differentiate the function $t \mapsto e^{-\lambda t}$ and integrate, or evaluate the antiderivative $-\frac{1}{\lambda}e^{-\lambda t}$ at the endpoints $t=0$ and $t\to\infty$.

\medskip

(c) Return to the function $f$. Write down the Fourier transform $\widehat{f}(\omega)$ as an integral with correct limits for this one-sided exponential. Carefully justify why the lower limit is $0$ rather than $-\infty$.

Then simplify the integrand and show that
\[
\widehat{f}(\omega)
= \int_{0}^{\infty} e^{-(a + i\omega)t}\,dt.
\]
Explain why the parameter $a>0$ ensures that this integral converges for all real $\omega$ by relating it to part (b).

\medskip

(d) Use your formula for $J(\lambda)$ from part (b)(iii) with $\lambda = a + i\omega$ to obtain a closed-form expression for $\widehat{f}(\omega)$. Simplify your answer as far as you can, for instance by rationalizing the denominator to separate real and imaginary parts if you find that helpful.

Next, find the magnitude $|\widehat{f}(\omega)|$ and the phase $\arg \widehat{f}(\omega)$ as functions of $\omega$. How does the decay rate $a$ affect the size of $\widehat{f}(\omega)$ for large $|\omega|$?

\medskip

(e) \textbf{Extensions and “what if” questions.}

(i) Suppose the exponential decay begins at a later time $t_0>0$ instead of at $t=0$:
\[
g(t) \coloneqq
\begin{cases}
e^{-a (t-t_0)}, & t \ge t_0,\\[4pt]
0, & t<t_0.
\end{cases}
\]
Using the time-shift property of the Fourier transform (which you may recall or derive), predict the form of $\widehat{g}(\omega)$ in terms of $\widehat{f}(\omega)$.

(ii) Consider instead the two-sided exponential
\[
h(t) \coloneqq e^{-a|t|}, \qquad a>0.
\]
Without doing all the details, outline how you would set up the integral for $\widehat{h}(\omega)$ and which of the ideas from this problem you would reuse. How do you expect the answer to differ qualitatively from $\widehat{f}(\omega)$, which is one-sided in time?

\end{problem}

% ===== Example 1: Fourier Transform of Exponential Decay (full solution) =====
\begin{problem}[Fourier Transform of Exponential Decay]
Let $a>0$ and define
\[
f(t) \coloneqq
\begin{cases}
e^{-a t}, & t \ge 0,\\[4pt]
0, & t<0.
\end{cases}
\]
Using the Fourier transform convention
\[
\widehat{f}(\omega) \coloneqq \displaystyle\int_{-\infty}^{\infty} f(t)\, e^{-i\omega t}\,dt,
\qquad \omega \in \mathbb{R},
\]
compute $\widehat{f}(\omega)$ in closed form. Then determine its magnitude $|\widehat{f}(\omega)|$ and phase $\arg\widehat{f}(\omega)$ as functions of $\omega$ and briefly comment on how the decay rate $a$ influences the frequency-domain behavior.
\end{problem}

\begin{solution}
We are asked to compute the Fourier transform of a one-sided exponential decay,
\[
f(t) =
\begin{cases}
e^{-a t}, & t \ge 0,\\[4pt]
0, & t<0,
\end{cases}
\quad\text{with } a>0,
\]
under the convention
\[
\widehat{f}(\omega) = \displaystyle\int_{-\infty}^{\infty} f(t)\, e^{-i\omega t}\,dt.
\]

\medskip

\noindent\textbf{1. Reducing to an elementary exponential integral.}
Because $f(t)$ vanishes for $t<0$, the integral over $(-\infty,0)$ contributes nothing. Thus
\[
\widehat{f}(\omega)
= \int_{-\infty}^{\infty} f(t)\, e^{-i\omega t}\,dt
= \int_{0}^{\infty} e^{-a t}\, e^{-i\omega t}\,dt.
\]
We combine the exponents:
\[
e^{-a t} e^{-i\omega t} = e^{-(a+i\omega)t},
\]
and obtain
\[
\widehat{f}(\omega)
= \int_{0}^{\infty} e^{-(a+i\omega)t}\,dt.
\]

This is a standard improper integral of an exponential with a complex parameter. The central idea is to recognize it as the same type of integral that defines the Laplace transform of $e^{-a t}$, evaluated at the complex number $a+i\omega$.

\medskip

\noindent\textbf{2. Convergence and evaluation of the exponential integral.}
Let $\lambda$ be a complex number with $\operatorname{Re}(\lambda) > 0$. We recall the general fact
\[
\int_0^\infty e^{-\lambda t}\,dt = \frac{1}{\lambda},
\qquad \text{for } \operatorname{Re}(\lambda) > 0.
\]
For completeness, we justify this formula briefly.

Write $\lambda = \alpha + i\beta$ with $\alpha = \operatorname{Re}(\lambda)$ and $\beta = \operatorname{Im}(\lambda)$. Then
\[
e^{-\lambda t} = e^{-(\alpha + i\beta)t} = e^{-\alpha t} e^{-i\beta t},
\]
so
\[
\bigl|e^{-\lambda t}\bigr| = e^{-\alpha t} \bigl|e^{-i\beta t}\bigr|
= e^{-\alpha t},
\]
because $|e^{-i\beta t}| = 1$ for all real $t$. If $\alpha>0$, then $e^{-\alpha t}$ decays to zero as $t\to\infty$, and the integral
\[
\int_0^\infty \bigl|e^{-\lambda t}\bigr|\,dt
= \int_0^\infty e^{-\alpha t}\,dt
\]
converges. Therefore, $\int_0^\infty e^{-\lambda t}\,dt$ converges absolutely whenever $\operatorname{Re}(\lambda)>0$.

To compute it, we note that an antiderivative of $e^{-\lambda t}$ is $-\frac{1}{\lambda}e^{-\lambda t}$, valid for any $\lambda\neq 0$. Hence
\[
\int_0^T e^{-\lambda t}\,dt
= \left[-\frac{1}{\lambda}e^{-\lambda t}\right]_{t=0}^{t=T}
= -\frac{1}{\lambda}e^{-\lambda T} + \frac{1}{\lambda}.
\]
If $\operatorname{Re}(\lambda) > 0$, then $e^{-\lambda T} \to 0$ as $T\to\infty$, so
\[
\int_0^\infty e^{-\lambda t}\,dt
= \lim_{T\to\infty} \int_0^T e^{-\lambda t}\,dt
= \lim_{T\to\infty} \left(-\frac{1}{\lambda}e^{-\lambda T} + \frac{1}{\lambda} \right)
= \frac{1}{\lambda}.
\]

\medskip

\noindent\textbf{3. Applying the formula to our Fourier transform.}
In our case, the parameter is
\[
\lambda = a + i\omega.
\]
Its real part is $\operatorname{Re}(\lambda) = a > 0$, so the argument above applies. Therefore,
\[
\widehat{f}(\omega)
= \int_{0}^{\infty} e^{-(a+i\omega)t}\,dt
= \frac{1}{a + i\omega},
\qquad \omega \in \mathbb{R}.
\]
This is already a closed-form expression for the Fourier transform: the transform of a one-sided exponential decay is the simple rational function
\[
\widehat{f}(\omega) = \frac{1}{a + i\omega}.
\]

This is a fundamental example in Fourier analysis, illustrating how a time-domain exponential decay is represented by a rational function of the frequency variable.

\medskip

\noindent\textbf{4. Magnitude and phase.}
To better understand the frequency response, we now compute the magnitude and phase of $\widehat{f}(\omega)$. We first rewrite
\[
\widehat{f}(\omega) = \frac{1}{a + i\omega}
= \frac{a - i\omega}{a^2 + \omega^2},
\]
by multiplying numerator and denominator by the complex conjugate $a - i\omega$.

From this expression, we can read off the real and imaginary parts:
\[
\operatorname{Re}\widehat{f}(\omega) = \frac{a}{a^2 + \omega^2},
\qquad
\operatorname{Im}\widehat{f}(\omega) = -\frac{\omega}{a^2 + \omega^2}.
\]
The magnitude is
\[
\bigl|\widehat{f}(\omega)\bigr|
= \sqrt{\bigl(\operatorname{Re}\widehat{f}(\omega)\bigr)^2
+ \bigl(\operatorname{Im}\widehat{f}(\omega)\bigr)^2}
= \sqrt{\frac{a^2 + \omega^2}{(a^2 + \omega^2)^2}}
= \frac{1}{\sqrt{a^2 + \omega^2}}.
\]
The phase $\arg\widehat{f}(\omega)$ is the argument of the complex number $(a - i\omega)/(a^2 + \omega^2)$, which is the same as the argument of $a - i\omega$. A convenient way to express it is to note that
\[
a + i\omega = \sqrt{a^2 + \omega^2}\,e^{i\theta},
\quad\text{where } \theta = \arctan\!\left(\frac{\omega}{a}\right)
\]
(for $a>0$ this principal value is unambiguous), so
\[
\widehat{f}(\omega) = \frac{1}{a + i\omega}
= \frac{1}{\sqrt{a^2 + \omega^2}}\,e^{-i\theta}.
\]
Thus
\[
\bigl|\widehat{f}(\omega)\bigr| = \frac{1}{\sqrt{a^2 + \omega^2}},
\qquad
\arg\widehat{f}(\omega)
= -\arctan\!\left(\frac{\omega}{a}\right).
\]

\medskip

\noindent\textbf{5. Effect of the decay rate $a$.}
The parameter $a>0$ controls the rate at which the signal decays in time. A larger value of $a$ means stronger damping and a shorter-lived transient.

In the frequency domain, the magnitude
\[
\bigl|\widehat{f}(\omega)\bigr|
= \frac{1}{\sqrt{a^2 + \omega^2}}
\]
is largest at $\omega = 0$, where it equals $1/a$, and decays like $1/|\omega|$ for large $|\omega|$. As $a$ increases, the value at $\omega=0$ decreases and the whole curve becomes ``narrower'' around $\omega=0$. This is consistent with the general time–frequency tradeoff: a more rapidly decaying function in time tends to have a broader (but lower-amplitude) representation in frequency.

The phase
\[
\arg\widehat{f}(\omega) = -\arctan\!\left(\frac{\omega}{a}\right)
\]
varies smoothly from approximately $0$ at low frequencies to approximately $-\pi/2$ as $|\omega|$ becomes large, with the transition scale controlled by $a$.

\medskip

\noindent\textbf{6. Relation to the chapter theme.}
This example showcases the main idea of the section on ``Closed-form Representation for Select Fourier Transforms.'' By recognizing that the Fourier transform integral matches a standard exponential integral with a complex parameter, we obtain a simple, explicit formula
\[
\widehat{f}(\omega) = \frac{1}{a + i\omega},
\]
without resorting to numerical methods. Such rational functions recur throughout applied mathematics, especially in the analysis of linear time-invariant systems, where exponential decays are ubiquitous and their Fourier transforms encode damping and phase shift in a particularly transparent way.

\end{solution}

% ===== Example 2: The Gaussian and the Heat Kernel (inquiry-based) =====
\begin{problem}[The Gaussian and the Heat Kernel]
In many physical systems, random fluctuations and heat diffusion produce bell-shaped profiles in space. A sharp, localized initial temperature distribution on a metal rod quickly smooths out into something that looks Gaussian; likewise, the position of a Brownian particle after a fixed time is distributed according to a Gaussian. In Fourier analysis, the Gaussian function is special because its Fourier transform can be computed explicitly and turns out to be another Gaussian. This example also leads directly to the closed-form “heat kernel,” which is the fundamental solution of the heat equation.

Throughout this problem, we use the (non-unitary) Fourier transform on $\mathbb{R}$ given by
\[
\widehat{f}(\xi) \;=\; \int_{-\infty}^{\infty} f(x)\,e^{-i\xi x}\,dx, \qquad \xi\in\mathbb{R},
\]
whenever the integral converges absolutely.

\smallskip

Let $a>0$ and consider the Gaussian
\[
f_a(x) \;=\; e^{-a x^2}, \qquad x\in\mathbb{R}.
\]

\medskip

\textbf{(a) First exploration: symmetry and qualitative shape.}  

(i) Show that $f_a$ is an even function of $x$. Deduce that its Fourier transform $\widehat{f_a}(\xi)$ is real-valued and even as a function of $\xi$.  
(ii) Without computing any integrals, make a qualitative sketch of what you expect $\widehat{f_a}(\xi)$ to look like. In particular, discuss whether you expect it to decay as $|\xi|\to\infty$ and whether you expect any oscillations.

\emph{Hint:} Recall that the Fourier transform of a “nice,” localized bump is usually another smooth, decaying function in frequency space. Think about what happens if you differentiate $\widehat{f_a}$ with respect to $\xi$.

\medskip

\textbf{(b) Finding a differential equation for the Fourier transform of a Gaussian.}  

Define
\[
F_a(\xi) \;=\; \widehat{f_a}(\xi) \;=\; \int_{-\infty}^{\infty} e^{-a x^2} e^{-i\xi x}\,dx.
\]
Differentiate $F_a(\xi)$ with respect to $\xi$ under the integral sign to obtain
\[
F_a'(\xi) \;=\; \int_{-\infty}^{\infty} \bigl(-i x\bigr)\,e^{-a x^2} e^{-i\xi x}\,dx.
\]

(i) Rewrite the factor $x e^{-a x^2}$ as a derivative with respect to $x$ of $e^{-a x^2}$, up to a constant multiple.  

(ii) Use this identity and integration by parts in $x$ to express $F_a'(\xi)$ in terms of $F_a(\xi)$ itself.

\emph{Hint:} Notice that
\[
x e^{-a x^2} = -\frac{1}{2a}\,\frac{d}{dx}\bigl(e^{-a x^2}\bigr).
\]
Move the derivative from $e^{-a x^2}$ onto $e^{-i\xi x}$ using integration by parts, and argue that the boundary terms vanish because $e^{-a x^2}$ decays rapidly as $|x|\to\infty$.

\medskip

\textbf{(c) Solving the resulting ordinary differential equation.}  

From part (b), you should obtain a first-order linear differential equation of the form
\[
F_a'(\xi) \;=\; -\frac{\xi}{2a}\,F_a(\xi).
\]

(i) Solve this ordinary differential equation for $F_a(\xi)$, and show that the general solution has the form
\[
F_a(\xi) \;=\; C_a\, e^{-\frac{\xi^2}{4a}},
\]
for some constant $C_a$ that may depend on $a$ but not on $\xi$.

(ii) Determine the constant $C_a$ by evaluating $F_a(0)$ directly from the definition of the Fourier transform.

\emph{Hint:} You need the value of the Gaussian integral
\[
\int_{-\infty}^{\infty} e^{-a x^2}\,dx.
\]
You may recall (or re-derive) that this integral equals $\sqrt{\pi/a}$. If you wish to derive it, consider squaring the integral, interpreting it as a double integral over $\mathbb{R}^2$, and switching to polar coordinates.

\medskip

\textbf{(d) The explicit Fourier transform of the Gaussian and the heat kernel.}  

(i) Combine your work from part (c) to obtain a closed-form formula for $\widehat{f_a}(\xi)$.  

(ii) Interpret this formula as saying that the Fourier transform of a Gaussian is another Gaussian. How are the “widths” of the two Gaussians related?  

\emph{Hint:} Think about how the parameter $a$ controls the spread of $e^{-a x^2}$ in $x$-space, and how the parameter in the exponent of $\widehat{f_a}(\xi)$ controls the spread in $\xi$-space.

Now let $\kappa>0$ and $t>0$, and define the \emph{one-dimensional heat kernel}
\[
K(x,t) \;=\; \frac{1}{\sqrt{4\pi \kappa t}}\;\exp\!\left(-\frac{x^2}{4\kappa t}\right).
\]

(iii) Using your formula for $\widehat{f_a}$ and appropriate scaling in $x$, compute the Fourier transform (in $x$) of $K(\cdot,t)$ and show that
\[
\widehat{K(\cdot,t)}(\xi) \;=\; e^{-\kappa t\,\xi^2}.
\]

\emph{Hint:} First compute the transform of $e^{-\alpha x^2}$ for a general $\alpha>0$. Then use the scaling property of the Fourier transform: if $g(x) = f(bx)$ with $b\neq 0$, how is $\widehat{g}$ related to $\widehat{f}$?

\medskip

\textbf{(e) Extensions and “what if” questions.}  

(i) The heat kernel $K(x,t)$ is supposed to be the fundamental solution of the heat equation
\[
u_t = \kappa u_{xx}, \qquad x\in\mathbb{R}, \ t>0,
\]
in the sense that the solution with initial data $u(x,0)=\varphi(x)$ is given by convolution:
\[
u(x,t) \;=\; (K(\cdot,t)*\varphi)(x) \;=\; \int_{-\infty}^{\infty} K(x-y,t)\,\varphi(y)\,dy.
\]
Use the Fourier transform with respect to $x$ to check that this convolution formula indeed solves the heat equation whenever $\varphi$ is nice enough (for example, rapidly decaying and smooth).

\emph{Hint:} Take the Fourier transform in $x$ of both sides of the heat equation. You should obtain a simple ordinary differential equation in $t$ for $\widehat{u}(\xi,t)$, and you already know the Fourier transform of $K(\cdot,t)$.

(ii) What happens in higher dimensions? Suppose $x\in\mathbb{R}^n$ and consider
\[
f_a(x) \;=\; e^{-a|x|^2}, \qquad |x|^2 = x_1^2+\cdots + x_n^2.
\]
Based on the one-dimensional result and the fact that the $n$-dimensional Fourier transform factorizes over coordinates, what do you expect the Fourier transform of $f_a$ to look like in $\mathbb{R}^n$? How does this relate to the heat kernel in higher dimensions?
\end{problem}

% ===== Example 2: The Gaussian and the Heat Kernel (full solution) =====
\begin{problem}[The Gaussian and the Heat Kernel]
Let $a>0$ and define $f_a(x)=e^{-a x^2}$ on $\mathbb{R}$. Using the Fourier transform
\[
\widehat{f}(\xi) \;=\; \int_{-\infty}^{\infty} f(x)\,e^{-i\xi x}\,dx,
\]
do the following:

\begin{enumerate}
  \item Show that the Fourier transform $F_a(\xi)=\widehat{f_a}(\xi)$ satisfies the ordinary differential equation
  \[
  F_a'(\xi) = -\frac{\xi}{2a} F_a(\xi),
  \]
  and solve this equation to obtain an explicit formula for $F_a(\xi)$.

  \item Use your result to compute the Fourier transform (in $x$) of the one-dimensional heat kernel
  \[
  K(x,t) \;=\; \frac{1}{\sqrt{4\pi \kappa t}}\;\exp\!\left(-\frac{x^2}{4\kappa t}\right), \qquad \kappa>0,\ t>0,
  \]
  and show that $\widehat{K(\cdot,t)}(\xi)=e^{-\kappa t\,\xi^2}$.

  \item Briefly explain how this identity for $\widehat{K(\cdot,t)}$ implies that the convolution formula
  \[
  u(x,t) = (K(\cdot,t)*\varphi)(x) = \int_{-\infty}^{\infty} K(x-y,t)\,\varphi(y)\,dy
  \]
  gives the solution to the heat equation $u_t = \kappa u_{xx}$ with initial data $u(x,0)=\varphi(x)$, for sufficiently nice $\varphi$.
\end{enumerate}
\end{problem}

\begin{solution}
We work with the Fourier transform
\[
\widehat{f}(\xi) \;=\; \int_{-\infty}^{\infty} f(x)\,e^{-i\xi x}\,dx,
\]
and take $f_a(x)=e^{-a x^2}$ with $a>0$.

\medskip

\textbf{1. Differential equation for the Fourier transform of the Gaussian.}

Define
\[
F_a(\xi) \;=\; \widehat{f_a}(\xi)
\;=\; \int_{-\infty}^{\infty} e^{-a x^2} e^{-i\xi x}\,dx.
\]
Since $e^{-a x^2}$ decays rapidly at infinity, differentiation under the integral sign with respect to $\xi$ is justified. We obtain
\[
F_a'(\xi)
= \int_{-\infty}^{\infty} e^{-a x^2} \,\frac{d}{d\xi}\bigl(e^{-i\xi x}\bigr)\,dx
= \int_{-\infty}^{\infty} e^{-a x^2} \,(-ix)\,e^{-i\xi x}\,dx
= -i \int_{-\infty}^{\infty} x e^{-a x^2} e^{-i\xi x}\,dx.
\]
The key idea is to rewrite the factor $x e^{-a x^2}$ as a derivative with respect to $x$, which prepares the expression for integration by parts. A short computation shows that
\[
\frac{d}{dx}\bigl(e^{-a x^2}\bigr) = -2a x e^{-a x^2},
\]
so that
\[
x e^{-a x^2} = -\frac{1}{2a}\,\frac{d}{dx}\bigl(e^{-a x^2}\bigr).
\]
Substituting this into the expression for $F_a'(\xi)$ gives
\[
F_a'(\xi)
= -i \int_{-\infty}^{\infty} \left(-\frac{1}{2a}\,\frac{d}{dx}e^{-a x^2}\right)e^{-i\xi x}\,dx
= \frac{i}{2a} \int_{-\infty}^{\infty} \frac{d}{dx}\bigl(e^{-a x^2}\bigr)e^{-i\xi x}\,dx.
\]
We now integrate by parts in $x$:
\[
\int_{-\infty}^{\infty} \frac{d}{dx}\bigl(e^{-a x^2}\bigr)e^{-i\xi x}\,dx
= \Bigl[e^{-a x^2} e^{-i\xi x}\Bigr]_{x=-\infty}^{x=+\infty}
 - \int_{-\infty}^{\infty} e^{-a x^2} \,\frac{d}{dx}\bigl(e^{-i\xi x}\bigr)\,dx.
\]
The boundary term vanishes because $e^{-a x^2}$ decays exponentially as $|x|\to\infty$, whereas $e^{-i\xi x}$ has modulus one. Thus
\[
\int_{-\infty}^{\infty} \frac{d}{dx}\bigl(e^{-a x^2}\bigr)e^{-i\xi x}\,dx
= -\int_{-\infty}^{\infty} e^{-a x^2}(-i\xi)e^{-i\xi x}\,dx
= i\xi \int_{-\infty}^{\infty} e^{-a x^2} e^{-i\xi x}\,dx
= i\xi F_a(\xi).
\]
Substituting this back, we find
\[
F_a'(\xi) = \frac{i}{2a}\,\bigl(i\xi F_a(\xi)\bigr)
= -\frac{\xi}{2a}\,F_a(\xi).
\]
Therefore $F_a$ satisfies the first-order linear ordinary differential equation
\[
F_a'(\xi) = -\frac{\xi}{2a}\,F_a(\xi).
\]

To solve this equation, we separate variables:
\[
\frac{F_a'(\xi)}{F_a(\xi)} = -\frac{\xi}{2a}.
\]
Integrating with respect to $\xi$ gives
\[
\int \frac{F_a'(\xi)}{F_a(\xi)}\,d\xi = \int -\frac{\xi}{2a}\,d\xi,
\]
so
\[
\ln|F_a(\xi)| = -\frac{\xi^2}{4a} + C,
\]
for some constant of integration $C$. Exponentiating yields
\[
F_a(\xi) = C_a\,e^{-\frac{\xi^2}{4a}},
\]
where $C_a = \pm e^{C}$ is a constant depending on $a$ but not on $\xi$.

To determine $C_a$, we evaluate $F_a$ at $\xi=0$. By definition,
\[
F_a(0)
= \int_{-\infty}^{\infty} e^{-a x^2}\,dx.
\]
This is the standard Gaussian integral with parameter $a>0$. It is a well-known result, and one can show (for example by squaring the integral and using polar coordinates in $\mathbb{R}^2$) that
\[
\int_{-\infty}^{\infty} e^{-a x^2}\,dx = \sqrt{\frac{\pi}{a}}.
\]
On the other hand, from the explicit form $F_a(\xi)=C_a e^{-\xi^2/(4a)}$ we have
\[
F_a(0) = C_a e^{0} = C_a.
\]
Therefore
\[
C_a = \sqrt{\frac{\pi}{a}},
\]
and hence
\[
F_a(\xi) = \sqrt{\frac{\pi}{a}}\,e^{-\frac{\xi^2}{4a}}.
\]

We have thus obtained the closed-form formula
\[
\widehat{f_a}(\xi) = \int_{-\infty}^{\infty} e^{-a x^2}e^{-i\xi x}\,dx
= \sqrt{\frac{\pi}{a}}\,e^{-\frac{\xi^2}{4a}}, \qquad \xi\in\mathbb{R}.
\]
This shows that the Fourier transform of a Gaussian is another Gaussian, with the parameter in the exponent inverted in a precise way.

\medskip

\textbf{2. Fourier transform of the heat kernel.}

We now apply this result to the heat kernel
\[
K(x,t)
= \frac{1}{\sqrt{4\pi\kappa t}} \exp\!\left(-\frac{x^2}{4\kappa t}\right),
\qquad \kappa>0,\ t>0.
\]
For each fixed $t>0$, this is a spatial Gaussian with parameter
\[
a = \frac{1}{4\kappa t}.
\]
Indeed, we can write
\[
K(x,t) = \frac{1}{\sqrt{4\pi\kappa t}}\;e^{-a x^2}
\quad\text{with}\quad
a = \frac{1}{4\kappa t}.
\]
We already know that
\[
\widehat{e^{-a x^2}}(\xi) = \sqrt{\frac{\pi}{a}}\,e^{-\frac{\xi^2}{4a}}.
\]
Since the Fourier transform is linear, and the prefactor $1/\sqrt{4\pi\kappa t}$ does not depend on $x$, we have
\[
\widehat{K(\cdot,t)}(\xi)
= \frac{1}{\sqrt{4\pi\kappa t}}\;\widehat{e^{-a x^2}}(\xi)
= \frac{1}{\sqrt{4\pi\kappa t}}\;\sqrt{\frac{\pi}{a}}\,e^{-\frac{\xi^2}{4a}}.
\]
Substituting $a=1/(4\kappa t)$, we compute
\[
\sqrt{\frac{\pi}{a}}
= \sqrt{\pi\cdot 4\kappa t}
= \sqrt{4\pi\kappa t},
\]
so the prefactors cancel:
\[
\frac{1}{\sqrt{4\pi\kappa t}}\;\sqrt{\frac{\pi}{a}}
= \frac{1}{\sqrt{4\pi\kappa t}}\;\sqrt{4\pi\kappa t} = 1.
\]
Next, we evaluate the exponent:
\[
\frac{\xi^2}{4a}
= \frac{\xi^2}{4\cdot \frac{1}{4\kappa t}}
= \frac{\xi^2}{1/(\kappa t)}
= \kappa t\, \xi^2.
\]
Thus
\[
\widehat{K(\cdot,t)}(\xi)
= e^{-\kappa t\,\xi^2},
\]
as claimed. In other words, in frequency space, the heat kernel is simply the exponential damping factor $e^{-\kappa t\,\xi^2}$.

This reveals the central structural feature: the spatial Gaussian profile of the heat kernel corresponds in Fourier space to pure exponential decay in time at each frequency, with decay rate proportional to $\kappa \xi^2$. This is a clear instance of the theme of this section: certain special functions (such as Gaussians) admit closed-form Fourier transforms that greatly simplify the analysis of associated differential equations.

\medskip

\textbf{3. Solution of the heat equation via convolution with the heat kernel.}

Consider the one-dimensional heat equation
\[
u_t(x,t) = \kappa u_{xx}(x,t), \qquad x\in\mathbb{R}, \ t>0,
\]
with initial data
\[
u(x,0) = \varphi(x),
\]
where we assume $\varphi$ is smooth and decays sufficiently fast at infinity so that all Fourier transforms below are well defined.

We claim that the solution is given by the convolution formula
\[
u(x,t) = (K(\cdot,t)*\varphi)(x)
= \int_{-\infty}^{\infty} K(x-y,t)\,\varphi(y)\,dy.
\]

To justify this using Fourier analysis, we take the Fourier transform with respect to $x$. Let
\[
\widehat{u}(\xi,t) = \int_{-\infty}^{\infty} u(x,t)\,e^{-i\xi x}\,dx.
\]
Because differentiation in $x$ becomes multiplication by $i\xi$ under the Fourier transform, we have
\[
\widehat{u_x}(\xi,t) = i\xi\,\widehat{u}(\xi,t),
\qquad
\widehat{u_{xx}}(\xi,t) = (i\xi)^2\,\widehat{u}(\xi,t) = -\xi^2 \widehat{u}(\xi,t).
\]
Applying the Fourier transform in $x$ to the heat equation gives
\[
\frac{\partial}{\partial t}\widehat{u}(\xi,t)
= \widehat{u_t}(\xi,t)
= \kappa\,\widehat{u_{xx}}(\xi,t)
= \kappa(-\xi^2)\,\widehat{u}(\xi,t)
= -\kappa\xi^2\,\widehat{u}(\xi,t).
\]
Thus, for each fixed $\xi$, the function $t \mapsto \widehat{u}(\xi,t)$ satisfies the ordinary differential equation
\[
\frac{d}{dt}\widehat{u}(\xi,t) = -\kappa\xi^2\,\widehat{u}(\xi,t),
\]
with initial condition
\[
\widehat{u}(\xi,0) = \widehat{\varphi}(\xi).
\]
Solving this scalar linear ODE, we obtain
\[
\widehat{u}(\xi,t) = e^{-\kappa t\,\xi^2}\,\widehat{\varphi}(\xi).
\]
But in the previous part we showed that $e^{-\kappa t\,\xi^2}$ is precisely the Fourier transform of $K(\cdot,t)$:
\[
\widehat{K(\cdot,t)}(\xi) = e^{-\kappa t\,\xi^2}.
\]
Therefore
\[
\widehat{u}(\xi,t)
= \widehat{K(\cdot,t)}(\xi)\,\widehat{\varphi}(\xi).
\]
By the convolution theorem, the product of Fourier transforms corresponds to convolution in physical space, so
\[
u(x,t) = (K(\cdot,t)*\varphi)(x),
\]
which is the desired formula.

This completes the argument that the convolution with the heat kernel provides the solution to the heat equation with initial data $\varphi$. Conceptually, the key points are:

\begin{itemize}
  \item The Fourier transform turns the heat equation, a second-order partial differential equation in $x$, into a family of decoupled first-order ordinary differential equations in $t$, one for each frequency $\xi$.
  \item The special closed-form Fourier transform of the Gaussian shows that the spatial heat kernel corresponds exactly to the temporal decay factor $e^{-\kappa t\xi^2}$ in frequency space.
  \item The convolution representation of the solution is a direct consequence of the convolution theorem and the explicit knowledge of $\widehat{K(\cdot,t)}$.
\end{itemize}

Thus, this example illustrates the overarching idea of the section: for certain particularly nice functions, such as Gaussians, one can compute the Fourier transform in closed form, and these explicit formulas serve as powerful tools for solving and understanding important differential equations like the heat equation.
\end{solution}

% ===== Example 3: Rational Functions and Residues (inquiry-based) =====
\begin{problem}[Rational Functions and Residues]
In many models of waves and diffusion, Green's functions and response functions have Fourier transforms that are \emph{rational} functions of the frequency variable. A basic example is the so-called Lorentzian profile
\[
f(x) = \frac{1}{x^{2}+a^{2}}, \qquad a>0,
\]
whose Fourier transform controls spatial decay in several partial differential equations. Directly integrating its Fourier transform is not very pleasant, but complex analysis turns it into a clean residue computation. In this problem you will discover how the poles of a rational function in the complex plane encode exponential decay in the Fourier transform variable.

We use the convention
\[
\widehat{f}(\xi) = \int_{-\infty}^{\infty} f(x)\,e^{i\xi x}\,dx.
\]

(a) Consider the function
\[
I(\xi) = \int_{-\infty}^{\infty} \frac{e^{i\xi x}}{x^{2}+a^{2}}\,dx, \qquad a>0.
\]
Show that this improper integral converges for every real $\xi$, and prove that $I(\xi)$ is an even function of~$\xi$ (that is, $I(-\xi)=I(\xi)$).  
\emph{Hint:} Use that $x^{2}+a^{2}\ge a^{2}$ and that $\cos$ and $\sin$ are bounded.

(b) For the moment, fix $\xi>0$. Extend the integrand
\[
f(z) = \frac{e^{i\xi z}}{z^{2}+a^{2}}
\]
to the complex plane, and consider integrating $f$ along a closed contour $\Gamma_{R}$ consisting of the real interval $[-R,R]$ together with a semicircle of radius $R$.

\quad(i) Which half-plane (upper or lower) should you choose for the semicircle so that the contribution from the arc tends to zero as $R\to\infty$? Explain your choice by estimating $\lvert e^{i\xi z}\rvert$ on the arc.  
\emph{Hint:} Write $z = x+iy$ and compute $\lvert e^{i\xi z}\rvert$ in terms of $\xi$ and $y$.

\quad(ii) State which poles of $f(z)$ lie inside your chosen contour for $\xi>0$.

(c) Compute the residue of $f(z)$ at each pole that lies inside the contour for $\xi>0$.

\quad(i) First, locate the poles of $f(z)$ in the complex plane and identify which of them is inside your contour.  

\quad(ii) Compute the residue there.  
\emph{Hint:} You can write $z^{2}+a^{2}=(z-ia)(z+ia)$, or use the general formula
\[
\operatorname{Res}\left(\frac{g(z)}{h(z)},z_{0}\right) = \frac{g(z_{0})}{h'(z_{0})}
\]
when $h(z_{0})=0$ and $h'(z_{0})\neq0$.

(d) Use the residue theorem and your work above to evaluate $I(\xi)$ for $\xi>0$. Then use the evenness from part~(a) to find a single closed-form expression for $I(\xi)$ valid for all real $\xi$.  

\emph{Hint:} As $R\to\infty$, the contour integral is equal to $2\pi i$ times the sum of residues inside the contour. Relate this to the integral along the real axis.

(e) \textbf{Extensions and variations.}

\quad(i) Consider the function $g(x) = e^{-a\lvert x\rvert}$ with $a>0$. Compute its Fourier transform
\[
\widehat{g}(\xi) = \int_{-\infty}^{\infty} e^{-a\lvert x\rvert}e^{i\xi x}\,dx
\]
using only real-variable calculus (for instance, by splitting the integral at $x=0$). Simplify your answer. What type of function of $\xi$ do you obtain?

\quad(ii) Compare your expression for $\widehat{g}(\xi)$ with your result for $I(\xi)$. How are the two functions related? What does this say about the relationship between the exponential $e^{-a\lvert x\rvert}$ and the rational function $\dfrac{1}{\xi^{2}+a^{2}}$ via the Fourier transform?

\quad(iii) (Optional, more challenging.) Differentiate your formula for $I(\xi)$ with respect to the parameter $a$, and then justify differentiating under the integral sign to show that
\[
\int_{-\infty}^{\infty} \frac{e^{i\xi x}}{(x^{2}+a^{2})^{2}}\,dx
\]
can be expressed in closed form. What type of decay in $\xi$ does this higher-order pole produce?
\end{problem}

% ===== Example 3: Rational Functions and Residues (full solution) =====
\begin{problem}[Rational Functions and Residues]
Let $a>0$ and define, for real $\xi$,
\[
I(\xi) = \int_{-\infty}^{\infty} \frac{e^{i\xi x}}{x^{2}+a^{2}}\,dx.
\]
(a) Show that $I(\xi)$ is well defined for all real $\xi$ and that $I(\xi)$ is an even function of $\xi$.  

(b) Evaluate $I(\xi)$ in closed form using contour integration and the residue theorem.  

(c) Briefly explain how this example illustrates the way poles of a rational function in the complex plane encode exponential decay of its Fourier transform.
\end{problem}

\begin{solution}
We are asked to compute the Fourier transform of the Lorentzian profile $1/(x^{2}+a^{2})$ using complex analysis. This is a standard model example: the integrand is rational in $x$, but its transform will turn out to be exponentially decaying in~$\xi$. The computation relies on extending the integrand to a meromorphic function in the complex plane and evaluating an appropriate contour integral by the residue theorem.

\medskip

\noindent\textbf{(a) Convergence and evenness.}
For each real $\xi$ we consider
\[
I(\xi) = \int_{-\infty}^{\infty} \frac{e^{i\xi x}}{x^{2}+a^{2}}\,dx.
\]

First, note that for all real $x$ we have $x^{2}+a^{2}\ge a^{2}$, hence
\[
\left|\frac{e^{i\xi x}}{x^{2}+a^{2}}\right|
= \frac{1}{x^{2}+a^{2}}
\le \frac{1}{a^{2}}.
\]
More importantly, for large $\lvert x\rvert$ we have $x^{2}+a^{2}\sim x^{2}$, so
\[
\left|\frac{e^{i\xi x}}{x^{2}+a^{2}}\right|
\le \frac{C}{1+x^{2}}
\]
for some constant $C$ independent of $\xi$. Since
\[
\int_{-\infty}^{\infty} \frac{dx}{1+x^{2}} < \infty,
\]
the integral defining $I(\xi)$ converges absolutely for every real~$\xi$.

To see that $I(\xi)$ is even, write the exponential in terms of sine and cosine:
\[
e^{i\xi x} = \cos(\xi x) + i\sin(\xi x).
\]
Then
\[
I(\xi)
= \int_{-\infty}^{\infty} \frac{\cos(\xi x)}{x^{2}+a^{2}}\,dx
+ i\int_{-\infty}^{\infty} \frac{\sin(\xi x)}{x^{2}+a^{2}}\,dx.
\]
The function $x\mapsto \cos(\xi x)/(x^{2}+a^{2})$ is even in $x$, while $x\mapsto \sin(\xi x)/(x^{2}+a^{2})$ is odd. Hence the sine term integrates to zero over the symmetric interval $(-\infty,\infty)$, and we obtain
\[
I(\xi)
= \int_{-\infty}^{\infty} \frac{\cos(\xi x)}{x^{2}+a^{2}}\,dx
\in \mathbb{R}.
\]
Now observe that $\cos(\xi x)$ is an even function of $\xi$ as well: $\cos(-\xi x)=\cos(\xi x)$. Thus
\[
I(-\xi)
= \int_{-\infty}^{\infty} \frac{\cos(-\xi x)}{x^{2}+a^{2}}\,dx
= \int_{-\infty}^{\infty} \frac{\cos(\xi x)}{x^{2}+a^{2}}\,dx
= I(\xi),
\]
so $I$ is an even function of~$\xi$.

\medskip

\noindent\textbf{(b) Evaluation via residues.}
We now compute $I(\xi)$ using complex analysis. Extend the integrand to the complex function
\[
f(z) = \frac{e^{i\xi z}}{z^{2}+a^{2}},
\]
which is meromorphic on $\mathbb{C}$ with simple poles at the zeros of $z^{2}+a^{2}$, namely at
\[
z = ia \quad \text{and} \quad z = -ia.
\]

We first treat the case $\xi>0$. Let $R>0$ and consider the semicircular contour $\Gamma_{R}$ consisting of the line segment from $-R$ to $R$ along the real axis, followed by the upper semicircle of radius $R$ centered at the origin, traversed counterclockwise. Thus $\Gamma_{R}$ is a positively oriented simple closed contour.

On the upper semicircle we write $z = Re^{i\theta}$ with $\theta\in[0,\pi]$, so that $\operatorname{Im} z = R\sin\theta \ge 0$. For such $z$ we have
\[
\lvert e^{i\xi z}\rvert
= \bigl|e^{i\xi(x+iy)}\bigr|
= \left|e^{i\xi x}e^{-\xi y}\right|
= e^{-\xi y}
= e^{-\xi\,\operatorname{Im} z}.
\]
When $\xi>0$ and $\operatorname{Im} z\ge 0$, this gives
\[
\lvert e^{i\xi z}\rvert \le 1,
\]
and in fact $\lvert e^{i\xi z}\rvert \le e^{-\xi R\sin\theta}$, which decays away from the real axis. On the semicircular arc we also have $\lvert z^{2}+a^{2}\rvert \ge R^{2}-a^{2}$ for $R$ large. Combining these estimates, we find
\[
\left|\int_{\text{arc}} f(z)\,dz\right|
\le \max_{\text{arc}} \left|\frac{e^{i\xi z}}{z^{2}+a^{2}}\right|\cdot(\text{length of arc})
\le \frac{1}{R^{2}-a^{2}}\cdot \pi R
\to 0
\]
as $R\to\infty$. Thus, in the limit $R\to\infty$, the contribution from the arc vanishes, and we obtain
\[
\int_{-\infty}^{\infty} \frac{e^{i\xi x}}{x^{2}+a^{2}}\,dx
= \lim_{R\to\infty} \int_{-R}^{R} \frac{e^{i\xi x}}{x^{2}+a^{2}}\,dx
= \lim_{R\to\infty} \int_{\Gamma_{R}} f(z)\,dz,
\]
because the difference between the contour integral and the real-axis integral is precisely the arc integral, which tends to zero.

By the residue theorem, for each $R$ sufficiently large so that $ia$ is inside the upper semicircle, we have
\[
\int_{\Gamma_{R}} f(z)\,dz = 2\pi i \sum \operatorname{Res}(f; z_{k}),
\]
where the sum runs over the poles $z_{k}$ inside $\Gamma_{R}$. In the upper half-plane only the pole at $z=ia$ lies inside the contour. Therefore,
\[
\int_{\Gamma_{R}} f(z)\,dz = 2\pi i\,\operatorname{Res}(f; ia).
\]

We compute this residue. Since
\[
f(z) = \frac{e^{i\xi z}}{z^{2}+a^{2}}
= \frac{e^{i\xi z}}{(z-ia)(z+ia)},
\]
the pole at $z=ia$ is simple. One way to compute the residue is to use the general formula for a simple pole of $g(z)/h(z)$, where $h(z_{0})=0$ and $h'(z_{0})\neq0$:
\[
\operatorname{Res}\left(\frac{g(z)}{h(z)},z_{0}\right)
= \frac{g(z_{0})}{h'(z_{0})}.
\]
Here $g(z)=e^{i\xi z}$ and $h(z)=z^{2}+a^{2}$, so $h'(z)=2z$, and at $z=ia$ we have $h'(ia)=2ia$. Thus
\[
\operatorname{Res}(f; ia)
= \frac{e^{i\xi (ia)}}{2ia}
= \frac{e^{-\xi a}}{2ia}.
\]

Putting this into the residue theorem and letting $R\to\infty$, we obtain for $\xi>0$:
\[
I(\xi)
= \int_{-\infty}^{\infty} \frac{e^{i\xi x}}{x^{2}+a^{2}}\,dx
= 2\pi i \cdot \frac{e^{-\xi a}}{2ia}
= \frac{\pi}{a} e^{-\xi a}.
\]

For $\xi<0$ the argument is completely analogous, except that we now close the contour in the \emph{lower} half-plane so that $\lvert e^{i\xi z}\rvert$ decays along the arc. Indeed, if $\xi<0$ and $\operatorname{Im} z\le 0$, then
\[
\lvert e^{i\xi z}\rvert
= e^{-\xi\,\operatorname{Im}z} \le 1,
\]
and the same estimation shows that the arc integral again tends to zero as $R\to\infty$. The contour in the lower half-plane now encloses the pole at $z=-ia$, and residue calculus gives
\[
I(\xi) = 2\pi i\,\operatorname{Res}(f; -ia).
\]
A similar computation yields
\[
\operatorname{Res}(f; -ia)
= \frac{e^{i\xi(-ia)}}{2(-ia)}
= \frac{e^{\xi a}}{-2ia}.
\]
However, since $\xi<0$, we can write $e^{\xi a} = e^{-a\lvert \xi\rvert}$, and also $-2ia = 2i(-a)$. Evaluating the prefactor,
\[
2\pi i \cdot \frac{e^{\xi a}}{-2ia}
= \frac{\pi}{a} e^{\xi a}
= \frac{\pi}{a} e^{-a\lvert\xi\rvert}.
\]
Thus the same formula
\[
I(\xi) = \frac{\pi}{a} e^{-a\lvert\xi\rvert}
\]
holds for $\xi<0$ as well. This is consistent with the evenness of $I(\xi)$ established in part~(a).

Combining the two cases, we conclude that for all real $\xi$,
\[
\boxed{%
I(\xi) = \int_{-\infty}^{\infty} \frac{e^{i\xi x}}{x^{2}+a^{2}}\,dx
= \frac{\pi}{a}\,e^{-a\lvert\xi\rvert}.}
\]

\medskip

\noindent\textbf{(c) Interpretation in terms of poles and decay.}
The original integrand is a rational function of $z$ multiplied by an oscillatory exponential. As a function of the complex variable $z$, it has two simple poles at $z=\pm ia$, located a distance $a$ above and below the real axis. The residue theorem tells us that the value of the real integral is determined entirely by the contribution from the pole in the half-plane where we close the contour, and the exponential factor $e^{-a\lvert\xi\rvert}$ in the final answer reflects the vertical distance of the poles from the real axis.

This illustrates a central theme of the section \emph{“Closed-form Representation for Select Fourier Transforms”}: for rational functions, the analytic structure in the complex plane (location and order of poles) translates directly into qualitative features of the Fourier transform. In this example, simple poles off the real axis produce exponentially decaying Fourier transforms. More generally, higher-order poles or more complicated rational structures lead to different decay rates and combinations of oscillation and damping, all of which can be read off from the complex-analytic data and computed efficiently via residues.
\end{solution}

% ===== Example 4: Piecewise and Even/Odd Functions via Symmetry (inquiry-based) =====
\begin{problem}[Piecewise and Even/Odd Functions via Symmetry]
In many applications, signals are naturally supported only on a finite interval, and their graphs often exhibit simple symmetries. Examples include square pulses modeling on/off switches, localized temperature profiles, and one-sided loading in beams or cables. Direct computation of the Fourier transform of such piecewise-defined functions is always possible, but it can be greatly simplified by recognizing and exploiting evenness, oddness, and decomposition into symmetric parts. In this problem you will see how to turn a seemingly asymmetric ``one-sided'' pulse into a combination of an even pulse and an odd pulse, each of which has a very simple closed-form Fourier transform.

Throughout, use the Fourier transform convention
\[
\widehat{f}(\omega) \coloneqq \int_{-\infty}^{\infty} f(x)\,e^{-i\omega x}\,dx,
\]
for real frequency variable $\omega$.

\medskip

(a) Let $f\in L^{1}(\mathbb{R})$ be real-valued.

\quad(i) Assume $f$ is \emph{even}, that is, $f(-x)=f(x)$ for all $x$. Show that
\[
\widehat{f}(\omega)=2 \int_{0}^{\infty} f(x)\cos(\omega x)\,dx.
\]

\quad(ii) Assume instead that $f$ is \emph{odd}, that is, $f(-x)=-f(x)$ for all $x$. Show that
\[
\widehat{f}(\omega)=-2i\int_{0}^{\infty} f(x)\sin(\omega x)\,dx.
\]

Hint: First write $e^{-i\omega x}=\cos(\omega x)-i\sin(\omega x)$ and separate the Fourier integral into real and imaginary parts. Then use the substitution $x\mapsto -x$ on $(-\infty,0)$ and the even or odd property of $f$.

\medskip

(b) Fix a length $L>0$ and consider the \emph{even} square pulse
\[
p(x) \coloneqq 
\begin{cases}
1, & |x|<L,\\[3pt]
0, & \text{otherwise}.
\end{cases}
\]
Compute $\widehat{p}(\omega)$ by exploiting the evenness of $p$ and your formula from part (a)(i). Express your answer using only elementary functions.

Hint: After reducing the integral to $[0,L]$, you should only need to integrate $\cos(\omega x)$.

\medskip

(c) Now define the \emph{odd} ``signed'' pulse
\[
q(x) \coloneqq 
\begin{cases}
1, & 0<x<L,\\[3pt]
-1, & -L<x<0,\\[3pt]
0, & \text{otherwise}.
\end{cases}
\]

\quad(i) Verify that $q$ is an odd function.

\quad(ii) Use part (a)(ii) to compute $\widehat{q}(\omega)$ explicitly. Simplify your answer as much as possible.

Hint: Again you will reduce the integral to $[0,L]$, but this time you will need to integrate $\sin(\omega x)$.

\medskip

(d) Consider now the one-sided (right-hand) square pulse
\[
h(x) \coloneqq 
\begin{cases}
1, & 0<x<L,\\[3pt]
0, & \text{otherwise}.
\end{cases}
\]
This function $h$ is neither even nor odd.

\quad(i) Verify the identity
\[
h(x)=\tfrac{1}{2}\bigl(p(x)+q(x)\bigr)
\]
for every real $x$. In other words, show that the one-sided pulse can be written as the average of the even pulse $p$ and the odd pulse $q$.

\quad(ii) Use linearity of the Fourier transform and your computations of $\widehat{p}$ and $\widehat{q}$ to obtain a closed-form expression for $\widehat{h}(\omega)$.

\quad(iii) As a consistency check, compute $\widehat{h}(\omega)$ directly from the definition,
\[
\widehat{h}(\omega)=\int_{-\infty}^{\infty} h(x)e^{-i\omega x}\,dx,
\]
and verify that you obtain the same formula.

Hint: For the direct computation, note that $h(x)$ is nonzero only on $(0,L)$.

\medskip

(e) (Extensions and reflections.)

\quad(i) Suppose instead of height $1$ the one-sided pulse has height $A>0$, that is,
\[
h_A(x)\coloneqq
\begin{cases}
A, & 0<x<L,\\[3pt]
0, & \text{otherwise}.
\end{cases}
\]
Without repeating any integrals, determine $\widehat{h_A}(\omega)$ from your work above.

\quad(ii) Decompose $h$ into its even and odd parts,
\[
h_{\text{even}}(x)\coloneqq \tfrac{1}{2}\bigl(h(x)+h(-x)\bigr),\qquad
h_{\text{odd}}(x)\coloneqq \tfrac{1}{2}\bigl(h(x)-h(-x)\bigr).
\]
Express $h_{\text{even}}$ and $h_{\text{odd}}$ in terms of $p$ and $q$, and relate the real and imaginary parts of $\widehat{h}(\omega)$ to the transforms of these even and odd parts.

Hint: Compare your formulas with those in parts (b)--(d). How do $\Re\widehat{h}$ and $\Im\widehat{h}$ reflect the contributions of the even and odd components?
\end{problem}

% ===== Example 4: Piecewise and Even/Odd Functions via Symmetry (full solution) =====
\begin{problem}[Piecewise and Even/Odd Functions via Symmetry]
Let the Fourier transform of an integrable function $f$ be
\[
\widehat{f}(\omega)\coloneqq\int_{-\infty}^{\infty} f(x)\,e^{-i\omega x}\,dx,\qquad \omega\in\mathbb{R}.
\]

(a) Show that if $f$ is even, then
\[
\widehat{f}(\omega)=2\int_{0}^{\infty} f(x)\cos(\omega x)\,dx,
\]
and if $f$ is odd, then
\[
\widehat{f}(\omega)=-2i\int_{0}^{\infty} f(x)\sin(\omega x)\,dx.
\]

(b) Fix $L>0$ and define
\[
p(x)\coloneqq
\begin{cases}
1, & |x|<L,\\
0, & \text{otherwise},
\end{cases}
\qquad
q(x)\coloneqq
\begin{cases}
1, & 0<x<L,\\
-1,& -L<x<0,\\
0, & \text{otherwise},
\end{cases}
\]
and
\[
h(x)\coloneqq
\begin{cases}
1,& 0<x<L,\\
0,& \text{otherwise}.
\end{cases}
\]

(i) Show that $p$ is even and $q$ is odd.

(ii) Compute the Fourier transforms $\widehat{p}(\omega)$ and $\widehat{q}(\omega)$ in closed form.

(iii) Show that $h(x)=\frac{1}{2}\bigl(p(x)+q(x)\bigr)$ for all $x$, and use this to compute $\widehat{h}(\omega)$ in closed form. Verify your answer for $\widehat{h}(\omega)$ by a direct computation from the definition.

Briefly explain how this example illustrates the use of symmetry and decomposition into even/odd parts to obtain closed-form Fourier transforms for piecewise-defined functions.
\end{problem}

\begin{solution}
We begin by establishing how evenness and oddness interact with the Fourier kernel $e^{-i\omega x}$, and then we apply these observations to the specific piecewise functions.

\medskip

\textbf{Part (a): Even and odd functions and sine/cosine transforms.}

Write the complex exponential in terms of sine and cosine:
\[
e^{-i\omega x}=\cos(\omega x)-i\sin(\omega x).
\]
Then the Fourier transform is
\[
\widehat{f}(\omega)
=\int_{-\infty}^{\infty} f(x)\bigl[\cos(\omega x)-i\sin(\omega x)\bigr]\,dx
=\int_{-\infty}^{\infty} f(x)\cos(\omega x)\,dx
-i\int_{-\infty}^{\infty} f(x)\sin(\omega x)\,dx.
\]

\emph{Case 1: $f$ even.} Suppose $f(-x)=f(x)$ for all $x$. Then the product $f(x)\cos(\omega x)$ is even, since cosine is even, and $f(x)\sin(\omega x)$ is odd, since sine is odd. An odd integrable function has integral zero over $\mathbb{R}$, so
\[
\int_{-\infty}^{\infty} f(x)\sin(\omega x)\,dx=0.
\]
For the even part, we use symmetry to reduce to $[0,\infty)$:
\[
\int_{-\infty}^{\infty} f(x)\cos(\omega x)\,dx
=2\int_{0}^{\infty} f(x)\cos(\omega x)\,dx.
\]
Therefore
\[
\widehat{f}(\omega)=2\int_{0}^{\infty} f(x)\cos(\omega x)\,dx,
\]
as claimed.

\emph{Case 2: $f$ odd.} Now suppose $f(-x)=-f(x)$. Then $f(x)\cos(\omega x)$ is odd (odd times even), so its integral over $\mathbb{R}$ vanishes, while $f(x)\sin(\omega x)$ is even (odd times odd). Thus
\[
\int_{-\infty}^{\infty} f(x)\cos(\omega x)\,dx=0,
\]
and
\[
\int_{-\infty}^{\infty} f(x)\sin(\omega x)\,dx
=2\int_{0}^{\infty} f(x)\sin(\omega x)\,dx.
\]
Consequently,
\[
\widehat{f}(\omega)
=-i\cdot 2\int_{0}^{\infty} f(x)\sin(\omega x)\,dx
=-2i\int_{0}^{\infty} f(x)\sin(\omega x)\,dx.
\]

This is precisely the desired simplification: for even functions the Fourier transform is a \emph{cosine transform}, and for odd functions it is a (purely imaginary) \emph{sine transform}.

\medskip

\textbf{Part (b): The even square pulse $p$.}

The function $p$ is defined by
\[
p(x)=
\begin{cases}
1,& |x|<L,\\
0,& \text{otherwise}.
\end{cases}
\]
Its graph is symmetric with respect to the vertical axis, so $p(-x)=p(x)$ for all $x$; hence $p$ is even.

By part (a) we can compute its Fourier transform using only a cosine integral over $[0,\infty)$:
\[
\widehat{p}(\omega)
=2\int_{0}^{\infty} p(x)\cos(\omega x)\,dx.
\]
Since $p(x)=1$ on $(0,L)$ and $p(x)=0$ elsewhere on $(0,\infty)$, this reduces to
\[
\widehat{p}(\omega)
=2\int_{0}^{L} 1\cdot \cos(\omega x)\,dx
=2\left[\frac{\sin(\omega x)}{\omega}\right]_{x=0}^{x=L}
=2\,\frac{\sin(\omega L)-\sin(0)}{\omega}
=2\,\frac{\sin(\omega L)}{\omega}.
\]
Thus we obtain the familiar closed form
\[
\boxed{\;\widehat{p}(\omega)=2\,\dfrac{\sin(\omega L)}{\omega}\;}.
\]
This already illustrates how even symmetry reduces a two-sided integral to a simple one-sided integral involving only cosine.

\medskip

\textbf{Part (c): The odd signed pulse $q$.}

The function $q$ is defined by
\[
q(x)=
\begin{cases}
1, & 0<x<L,\\
-1, & -L<x<0,\\
0, & \text{otherwise}.
\end{cases}
\]

\emph{(i) Oddness of $q$.} For $0<x<L$ we have $q(x)=1$, while $-L<-x<0$ so $q(-x)=-1$. Hence $q(-x)=-q(x)$ on $(0,L)$. For $-L<x<0$ we have $q(x)=-1$, and $0<-x<L$ so $q(-x)=1=-(-1)=-q(x)$. Outside $(-L,L)$, both $q(x)$ and $q(-x)$ are zero. Therefore $q(-x)=-q(x)$ holds for all $x$, and $q$ is odd.

\emph{(ii) Fourier transform of $q$.} Since $q$ is odd, we may apply part (a) with the sine integral:
\[
\widehat{q}(\omega)
=-2i\int_{0}^{\infty} q(x)\sin(\omega x)\,dx.
\]
Now $q(x)=1$ on $(0,L)$ and $q(x)=0$ for $x>L$, so
\[
\widehat{q}(\omega)
=-2i\int_{0}^{L} \sin(\omega x)\,dx.
\]
Integrating,
\[
\int_{0}^{L} \sin(\omega x)\,dx
=\left[-\frac{\cos(\omega x)}{\omega}\right]_{0}^{L}
=-\frac{\cos(\omega L)-\cos(0)}{\omega}
=\frac{1-\cos(\omega L)}{\omega}.
\]
Thus
\[
\widehat{q}(\omega)
=-2i\cdot \frac{1-\cos(\omega L)}{\omega}.
\]
So
\[
\boxed{\;\widehat{q}(\omega)=-2i\,\dfrac{1-\cos(\omega L)}{\omega}\;}.
\]
As expected for an odd real-valued function, the transform is purely imaginary.

\medskip

\textbf{Part (d): The one-sided pulse $h$ and its decomposition.}

The one-sided pulse is
\[
h(x)=
\begin{cases}
1, & 0<x<L,\\
0, & \text{otherwise}.
\end{cases}
\]

\emph{(i) Decomposition as an average of $p$ and $q$.} We claim that
\[
h(x)=\frac{1}{2}\bigl(p(x)+q(x)\bigr)\quad\text{for all }x\in\mathbb{R}.
\]
We verify this by checking the three regions.

\smallskip

\emph{Region 1: $0<x<L$.} Here
\[
p(x)=1,\qquad q(x)=1,
\]
so
\[
\frac{1}{2}\bigl(p(x)+q(x)\bigr)=\tfrac{1}{2}(1+1)=1=h(x).
\]

\emph{Region 2: $-L<x<0$.} Here
\[
p(x)=1,\qquad q(x)=-1,
\]
so
\[
\frac{1}{2}\bigl(p(x)+q(x)\bigr)=\tfrac{1}{2}(1-1)=0=h(x).
\]

\emph{Region 3: $|x|\geq L$.} Here $p(x)=0$ by definition, and also $q(x)=0$, so
\[
\frac{1}{2}(p(x)+q(x))=0=h(x).
\]
Thus the identity $h=\tfrac{1}{2}(p+q)$ holds everywhere.

\smallskip

\emph{(ii) Fourier transform via linearity and symmetry.} By linearity of the Fourier transform,
\[
\widehat{h}(\omega)
=\frac{1}{2}\bigl(\widehat{p}(\omega)+\widehat{q}(\omega)\bigr).
\]
We substitute the expressions just obtained:
\[
\widehat{p}(\omega)=2\,\frac{\sin(\omega L)}{\omega},\qquad
\widehat{q}(\omega)=-2i\,\frac{1-\cos(\omega L)}{\omega}.
\]
Hence
\[
\widehat{h}(\omega)
=\frac{1}{2}\left[2\,\frac{\sin(\omega L)}{\omega}-2i\,\frac{1-\cos(\omega L)}{\omega}\right]
=\frac{1}{\omega}\left[\sin(\omega L)-i\bigl(1-\cos(\omega L)\bigr)\right].
\]
So we obtain the compact closed form
\[
\boxed{\;\widehat{h}(\omega)=\dfrac{\sin(\omega L)-i\bigl(1-\cos(\omega L)\bigr)}{\omega}\;}.
\]
We can also see explicitly that the real part comes from the even contribution (via $p$) and the imaginary part from the odd contribution (via $q$).

\smallskip

\emph{(iii) Direct computation from the definition.} As a check, we compute $\widehat{h}$ again, now directly from the integral. Since $h(x)=1$ on $(0,L)$ and zero elsewhere,
\[
\widehat{h}(\omega)
=\int_{-\infty}^{\infty} h(x)e^{-i\omega x}\,dx
=\int_{0}^{L} e^{-i\omega x}\,dx.
\]
Integrating,
\[
\int_{0}^{L} e^{-i\omega x}\,dx
=\left[\frac{e^{-i\omega x}}{-i\omega}\right]_{0}^{L}
=\frac{1-e^{-i\omega L}}{i\omega}.
\]
We rewrite the numerator using Euler's formula:
\[
e^{-i\omega L}=\cos(\omega L)-i\sin(\omega L),
\]
so
\[
1-e^{-i\omega L}
=1-\cos(\omega L)+i\sin(\omega L).
\]
Thus
\[
\widehat{h}(\omega)
=\frac{1-\cos(\omega L)+i\sin(\omega L)}{i\omega}.
\]
To bring this into the same form as before, multiply numerator and denominator by $-i$:
\[
\widehat{h}(\omega)
=\frac{(1-\cos(\omega L))(-i)+i\sin(\omega L)(-i)}{\omega}
=\frac{-i(1-\cos(\omega L))+\sin(\omega L)}{\omega},
\]
since $i\cdot(-i)=1$. Rearranging,
\[
\widehat{h}(\omega)=\frac{\sin(\omega L)-i(1-\cos(\omega L))}{\omega},
\]
which agrees exactly with the expression obtained from symmetry and decomposition. This confirms the correctness of our earlier computation.

\medskip

\textbf{Interpretation and connection to the chapter theme.}

This example shows several central ideas of the section on closed-form Fourier transforms:

\begin{itemize}
  \item For even functions, the Fourier transform reduces to a cosine transform; for odd functions, it reduces to a sine transform. This simplifies integrals and often leads to closed forms with basic trigonometric functions.
  \item A general function can be decomposed into its even and odd parts. In our concrete case, the seemingly asymmetric one-sided pulse $h$ was written as the average of an even pulse $p$ and an odd signed pulse $q$. The Fourier transform then decomposes correspondingly, with the real part tied to the even component and the imaginary part tied to the odd component.
  \item By exploiting these symmetry properties and the linearity of the transform, we were able to avoid repeated direct integrations and obtain explicit formulas efficiently. This approach is typical when handling piecewise-defined signals in applied problems: one rewrites the function in terms of simpler symmetric building blocks and uses known or easily computed transforms for those blocks.
\end{itemize}

In summary, symmetry and even/odd decomposition are powerful tools for deriving closed-form Fourier transforms of piecewise-defined functions, as illustrated here for square pulses and their one-sided variants.
\end{solution}

% ===== Example 5: Fourier Transform and Green’s Function for the One-Dimensional Heat Equation (inquiry-based) =====
\begin{problem}[Fourier Transform and Green’s Function for the One-Dimensional Heat Equation]
A thin, infinitely long rod occupies the entire real line, and heat diffuses along the rod according to the one-dimensional heat equation. We would like to understand how a very localized burst of heat, concentrated initially at a single point, spreads out over time. This “point source” solution is called the \emph{fundamental solution} or \emph{Green’s function} for the heat equation. In this problem, you will discover that taking the Fourier transform in space reduces the partial differential equation to an ordinary differential equation, and that inverting the transform leads to an explicit Gaussian formula.

Consider the heat equation on the whole line
\[
u_t(x,t) \;=\; \kappa\,u_{xx}(x,t), \qquad x\in\mathbb{R},\ t>0,\ \kappa>0,
\]
with the impulsive initial condition
\[
u(x,0) \;=\; \delta(x),
\]
where $\delta$ is the Dirac delta distribution. We interpret $u(x,t)$ as the temperature at position $x$ and time $t$, and $\kappa$ as the thermal diffusivity.

Throughout, use the Fourier transform in $x$ defined by
\[
\widehat{f}(k) \;=\; \int_{-\infty}^{\infty} f(x)\,e^{-ikx}\,dx,
\qquad
f(x) \;=\; \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{f}(k)\,e^{ikx}\,dk.
\]

\smallskip

\textbf{(a) Transforming the PDE in space.}
\begin{enumerate}
\item[(a1)] Apply the Fourier transform in $x$ to both sides of the heat equation. Carefully justify how the derivatives in $x$ transform. Write down the transformed equation satisfied by $\widehat{u}(k,t)$.

\emph{Hint:} Recall that, for sufficiently nice $f$,
\[
\mathcal{F}[f'(x)](k) = ik\,\widehat{f}(k),
\qquad
\mathcal{F}[f''(x)](k) = -k^2\,\widehat{f}(k).
\]

\item[(a2)] Use the initial condition $u(x,0) = \delta(x)$ to determine $\widehat{u}(k,0)$. What is the Fourier transform of the Dirac delta?

\emph{Hint:} By definition, for any test function $\varphi$,
\[
\int_{-\infty}^\infty \delta(x)\,\varphi(x)\,dx = \varphi(0).
\]
To find $\widehat{\delta}(k)$, think of $\varphi(x) = e^{-ikx}$.
\end{enumerate}

\smallskip

\textbf{(b) Solving the transformed equation in time.}

\begin{enumerate}
\item[(b1)] Show that the transformed function $\widehat{u}(k,t)$ satisfies a first-order linear ordinary differential equation in $t$. Write this ODE explicitly.

\item[(b2)] Solve this ODE for $\widehat{u}(k,t)$ using the initial condition you found in part (a2), and express $\widehat{u}(k,t)$ in closed form.

\emph{Hint:} The ODE should have the form $v'(t) = -\alpha v(t)$ with constant $\alpha$ depending on $k$. Recall that the solution to $v'(t) = -\alpha v(t)$ with $v(0) = v_0$ is $v(t) = v_0 e^{-\alpha t}$. 
\end{enumerate}

\smallskip

\textbf{(c) Inverting the Fourier transform: appearance of the Gaussian.}

From part (b), you should have found that
\[
\widehat{u}(k,t) = e^{-\kappa k^2 t}.
\]
Now we want to invert the Fourier transform to obtain $u(x,t)$ in physical space:
\[
u(x,t) = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa k^2 t}\,e^{ikx}\,dk.
\]

\begin{enumerate}
\item[(c1)] Show, using completion of the square, that for $a>0$ and $x\in\mathbb{R}$,
\[
\int_{-\infty}^{\infty} e^{-a k^2}\,e^{ikx}\,dk
\;=\;
\sqrt{\frac{\pi}{a}}\;e^{-x^2/(4a)}.
\]

\emph{Hint:} First recall the standard Gaussian integral
\[
\int_{-\infty}^{\infty} e^{-a k^2}\,dk = \sqrt{\frac{\pi}{a}}.
\]
Then write $-a k^2 + ikx$ as $-a\left(k - \frac{ix}{2a}\right)^2 - \frac{x^2}{4a}$, and shift the contour of integration in the complex plane, or argue heuristically by changing variables.

\item[(c2)] Use the formula in (c1) with $a = \kappa t$ to evaluate the inverse Fourier transform
\[
u(x,t) = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa k^2 t}\,e^{ikx}\,dk,
\]
and hence obtain an explicit expression for $u(x,t)$.

\emph{Hint:} Be careful with the factor $\dfrac{1}{2\pi}$ in the inversion formula.
\end{enumerate}

\smallskip

\textbf{(d) Identifying the Green’s function and interpreting the result.}

\begin{enumerate}
\item[(d1)] Denote the solution you have found by $G(x,t)$, that is,
\[
G(x,t) := u(x,t) \quad \text{when} \quad u_t = \kappa u_{xx},\ u(x,0)=\delta(x).
\]
Write $G(x,t)$ explicitly and check that, for each fixed $t>0$, the function $x\mapsto G(x,t)$ is a Gaussian probability density (up to the parameter $\kappa$). That is, show that $\displaystyle \int_{-\infty}^\infty G(x,t)\,dx = 1$ and identify its variance.

\item[(d2)] Explain briefly why $G(x,t)$ is called the \emph{Green’s function} (or \emph{fundamental solution}) for the heat equation on the real line. In particular, argue (formally, if you like) that for a general initial condition $u(x,0) = f(x)$, the solution can be written as the convolution
\[
u(x,t) = (G(\cdot,t) * f)(x) = \int_{-\infty}^\infty G(x-y,t)\,f(y)\,dy.
\]

\emph{Hint:} Think about how the Fourier transform turns convolution in $x$ into multiplication in $k$, and use the fact that you already know the solution for the initial delta function.
\end{enumerate}

\smallskip

\textbf{(e) Extensions and “what if” questions.}

\begin{enumerate}
\item[(e1)] What happens if the diffusivity $\kappa$ is changed? Using your explicit formula for $G(x,t)$, describe qualitatively how increasing $\kappa$ affects the spread of heat over time. How do the height and width of the Gaussian profile depend on $\kappa$?

\item[(e2)] Suppose now that the initial temperature distribution is not a delta function but a general function $f\in L^1(\mathbb{R})$. Using the convolution representation in (d2), write down the explicit integral formula for $u(x,t)$ in terms of $f$. How does this formula relate to the idea that the delta function initial condition represents a point source whose effect is “smeared out” and then superposed for each point of the initial data?

\item[(e3) Optional.] The expression
\[
G(x,t) = \frac{1}{\sqrt{4\pi\kappa t}}\exp\!\left(-\frac{x^2}{4\kappa t}\right)
\]
is closely related to the classical Fourier transform pair for Gaussians. Describe explicitly the transform pair you have implicitly used (that is, identify $f$ and $\widehat{f}$), and explain how the heat equation derivation gives one way to \emph{prove} this transform pair.
\end{enumerate}

\end{problem}

% ===== Example 5: Fourier Transform and Green’s Function for the One-Dimensional Heat Equation (full solution) =====
\begin{problem}[Fourier Transform and Green’s Function for the One-Dimensional Heat Equation]
Consider the heat equation on the real line
\[
u_t(x,t) = \kappa\,u_{xx}(x,t), \qquad x\in\mathbb{R},\ t>0,\ \kappa>0,
\]
with initial condition $u(x,0) = \delta(x)$. Using the Fourier transform in $x$,
\[
\widehat{f}(k) = \int_{-\infty}^\infty f(x)\,e^{-ikx}\,dx,
\qquad
f(x) = \frac{1}{2\pi}\int_{-\infty}^\infty \widehat{f}(k)\,e^{ikx}\,dk,
\]
perform the following tasks:
\begin{enumerate}
\item[(i)] Derive and solve the ordinary differential equation satisfied by $\widehat{u}(k,t)$.
\item[(ii)] Invert the Fourier transform to obtain an explicit formula for $u(x,t)$, and show that
\[
u(x,t) = \frac{1}{\sqrt{4\pi\kappa t}}\exp\!\left(-\frac{x^2}{4\kappa t}\right).
\]
\item[(iii)] Interpret this solution as the Green’s function (fundamental solution) for the one-dimensional heat equation, and briefly explain how it leads to the convolution formula for the solution with general initial data.
\end{enumerate}
\end{problem}

\begin{solution}
We solve the initial value problem for the one-dimensional heat equation with impulsive initial data by taking the Fourier transform in space. This method is a prototypical example of how closed-form Fourier transform pairs, in particular the Gaussian transform pair, arise naturally from partial differential equations.

\medskip

\textbf{Step 1: Fourier transform of the PDE and initial condition.}

We define
\[
\widehat{u}(k,t) = \int_{-\infty}^\infty u(x,t)\,e^{-ikx}\,dx.
\]
We assume that for each fixed $t>0$, the function $u(\cdot,t)$ is sufficiently nice (for example, rapidly decaying) so that we may interchange derivatives and integrals in what follows.

Taking the Fourier transform in $x$ of both sides of the heat equation
\[
u_t(x,t) = \kappa\,u_{xx}(x,t)
\]
gives
\[
\widehat{u_t}(k,t) = \kappa\,\widehat{u_{xx}}(k,t).
\]
On the left-hand side, differentiation with respect to $t$ commutes with the integral:
\[
\widehat{u_t}(k,t) = \int_{-\infty}^\infty u_t(x,t)\,e^{-ikx}\,dx
= \frac{\partial}{\partial t}\int_{-\infty}^\infty u(x,t)\,e^{-ikx}\,dx
= \frac{\partial}{\partial t}\widehat{u}(k,t).
\]
On the right-hand side, we use the standard Fourier transform rule for second derivatives. For a sufficiently smooth, decaying function $f$,
\[
\mathcal{F}[f''(x)](k) = -k^2\,\widehat{f}(k).
\]
Applying this to $u(\cdot,t)$ with $t$ fixed, we obtain
\[
\widehat{u_{xx}}(k,t) = -k^2\,\widehat{u}(k,t).
\]
Therefore the transformed equation is
\[
\frac{\partial}{\partial t}\widehat{u}(k,t) = \kappa\bigl(-k^2\bigr)\widehat{u}(k,t),
\]
or more simply,
\[
\frac{\partial}{\partial t}\widehat{u}(k,t) = -\kappa k^2\,\widehat{u}(k,t).
\]
This is an ordinary differential equation in $t$, with $k$ appearing as a parameter.

Next, we transform the initial condition $u(x,0) = \delta(x)$. By definition of the Fourier transform,
\[
\widehat{u}(k,0) = \int_{-\infty}^\infty u(x,0)\,e^{-ikx}\,dx
= \int_{-\infty}^\infty \delta(x)\,e^{-ikx}\,dx.
\]
The defining property of the Dirac delta is that, for any test function $\varphi$,
\[
\int_{-\infty}^\infty \delta(x)\,\varphi(x)\,dx = \varphi(0).
\]
Here, $\varphi(x) = e^{-ikx}$, so
\[
\widehat{u}(k,0) = e^{-ik\cdot 0} = 1.
\]
Thus the initial condition in Fourier space is
\[
\widehat{u}(k,0) = 1 \quad \text{for all } k\in\mathbb{R}.
\]

\medskip

\textbf{Step 2: Solving the transformed ODE.}

For each fixed $k$, the function $t\mapsto \widehat{u}(k,t)$ satisfies the linear ODE
\[
\frac{d}{dt}\widehat{u}(k,t) = -\kappa k^2\,\widehat{u}(k,t),
\qquad
\widehat{u}(k,0) = 1.
\]
This is a separable first-order ODE with constant coefficient $-\kappa k^2$. Its solution is
\[
\widehat{u}(k,t) = e^{-\kappa k^2 t}.
\]
In other words, in the Fourier domain the heat equation becomes a simple exponential decay in time, with decay rate proportional to $k^2$. High-frequency modes (large $|k|$) decay more rapidly, which matches the physical intuition that diffusion smooths out fine-scale variations.

\medskip

\textbf{Step 3: Inverting the Fourier transform.}

We now invert the Fourier transform to recover $u(x,t)$:
\[
u(x,t) = \frac{1}{2\pi}\int_{-\infty}^\infty \widehat{u}(k,t)\,e^{ikx}\,dk
= \frac{1}{2\pi}\int_{-\infty}^\infty e^{-\kappa k^2 t}\,e^{ikx}\,dk.
\]
This is precisely the inverse Fourier transform of a Gaussian in $k$. To evaluate the integral, we use a standard Gaussian integral with a linear term in the exponent. Let $a>0$ and consider
\[
I(x) := \int_{-\infty}^\infty e^{-a k^2}\,e^{ikx}\,dk
= \int_{-\infty}^\infty \exp(-a k^2 + ikx)\,dk.
\]
We complete the square in the exponent. We have
\[
-ak^2 + ikx
= -a\left(k^2 - \frac{i x}{a}k\right)
= -a\left[\left(k - \frac{i x}{2a}\right)^2 + \frac{x^2}{4a^2}\right]
= -a\left(k - \frac{i x}{2a}\right)^2 - \frac{x^2}{4a}.
\]
Thus
\[
I(x) = e^{-x^2/(4a)}\int_{-\infty}^\infty
\exp\!\left(-a\left(k - \frac{i x}{2a}\right)^2\right)\,dk.
\]
The remaining integral is, up to a complex shift of the integration variable, the standard Gaussian integral. Under suitable analyticity and decay assumptions (which are satisfied here), shifting by a constant in the complex plane does not change its value. Therefore
\[
\int_{-\infty}^\infty \exp\!\left(-a\left(k - \frac{i x}{2a}\right)^2\right)\,dk
= \int_{-\infty}^\infty e^{-a k^2}\,dk
= \sqrt{\frac{\pi}{a}}.
\]
Consequently,
\[
I(x) = \sqrt{\frac{\pi}{a}}\;e^{-x^2/(4a)}.
\]

We now set $a = \kappa t > 0$ and obtain
\[
\int_{-\infty}^\infty e^{-\kappa k^2 t}\,e^{ikx}\,dk
= \sqrt{\frac{\pi}{\kappa t}}\;\exp\!\left(-\frac{x^2}{4\kappa t}\right).
\]
Inserting this into the inverse Fourier transform formula yields
\[
u(x,t) = \frac{1}{2\pi}\sqrt{\frac{\pi}{\kappa t}}\;\exp\!\left(-\frac{x^2}{4\kappa t}\right)
= \frac{1}{\sqrt{4\pi\kappa t}}\;\exp\!\left(-\frac{x^2}{4\kappa t}\right).
\]
This is the desired explicit solution.

\medskip

\textbf{Step 4: Interpreting the solution as a Green’s function.}

We have found that the solution of the heat equation with impulsive initial data is
\[
G(x,t) := u(x,t) = \frac{1}{\sqrt{4\pi\kappa t}}\exp\!\left(-\frac{x^2}{4\kappa t}\right),
\qquad t>0.
\]
First, we check that for each fixed $t>0$, the function $x\mapsto G(x,t)$ is normalized to one:
\[
\int_{-\infty}^\infty G(x,t)\,dx
= \int_{-\infty}^\infty \frac{1}{\sqrt{4\pi\kappa t}}
\exp\!\left(-\frac{x^2}{4\kappa t}\right)\,dx.
\]
Make the change of variables
\[
y = \frac{x}{\sqrt{4\kappa t}} \quad\Longrightarrow\quad x = y\sqrt{4\kappa t},\quad dx = \sqrt{4\kappa t}\,dy.
\]
Then
\[
\int_{-\infty}^\infty G(x,t)\,dx
= \int_{-\infty}^\infty \frac{1}{\sqrt{4\pi\kappa t}}e^{-y^2}\,\sqrt{4\kappa t}\,dy
= \frac{1}{\sqrt{\pi}}\int_{-\infty}^\infty e^{-y^2}\,dy
= 1,
\]
using the classical result $\int_{-\infty}^\infty e^{-y^2}\,dy = \sqrt{\pi}$. Thus $G(\cdot,t)$ is a probability density function on $\mathbb{R}$.

Moreover, $G(x,t)$ is a Gaussian with mean zero and variance $2\kappa t$. Indeed, we can write
\[
G(x,t) = \frac{1}{\sqrt{4\pi\kappa t}}\exp\!\left(-\frac{x^2}{4\kappa t}\right)
= \frac{1}{\sqrt{2\pi(2\kappa t)}}\exp\!\left(-\frac{x^2}{2(2\kappa t)}\right),
\]
which is the standard normal density with variance $\sigma^2 = 2\kappa t$. As $t$ increases, the Gaussian profile becomes wider and lower, reflecting the diffusion of heat away from the origin.

This function $G(x,t)$ is called the \emph{Green’s function} or \emph{fundamental solution} for the one-dimensional heat equation on the real line. It represents the temperature distribution at time $t$ resulting from a unit point source of heat at the origin at time $0$.

Finally, we explain how this solution leads to the convolution formula for general initial data. Let $f\in L^1(\mathbb{R})$ be a given initial temperature distribution, and consider the initial value problem
\[
u_t = \kappa u_{xx},\qquad u(x,0) = f(x).
\]
Taking the Fourier transform in $x$ as before, we find that $\widehat{u}(k,t)$ satisfies the same ODE
\[
\frac{\partial}{\partial t}\widehat{u}(k,t) = -\kappa k^2\,\widehat{u}(k,t),
\]
but now with initial condition $\widehat{u}(k,0) = \widehat{f}(k)$. Therefore
\[
\widehat{u}(k,t) = e^{-\kappa k^2 t}\,\widehat{f}(k).
\]
Comparing this with the expression for the fundamental solution, we have
\[
\widehat{G}(k,t) = e^{-\kappa k^2 t},
\]
so that
\[
\widehat{u}(k,t) = \widehat{G}(k,t)\,\widehat{f}(k).
\]
In physical space, multiplication of Fourier transforms corresponds to convolution:
\[
u(x,t) = (G(\cdot,t) * f)(x)
= \int_{-\infty}^\infty G(x-y,t)\,f(y)\,dy.
\]
This formula expresses the solution with general initial data as a superposition of translated point-source solutions, weighted by the initial distribution $f$.

\medskip

\textbf{Conceptual summary.}

This example illustrates a central theme of Fourier analysis in applied mathematics: partial differential equations with constant coefficients become algebraic (or simple ODE) problems in the transform domain. The one-dimensional heat equation, when transformed in space, reduces to exponential decay of each Fourier mode. Inverting the transform leads to a closed-form expression for the Green’s function, which turns out to be a Gaussian. Thus the classical Fourier transform pair “Gaussian $\leftrightarrow$ Gaussian” is not just an isolated formula, but arises naturally as the fundamental solution of a physically important PDE. This connection between explicit transform pairs and PDE solutions underlies much of the theory developed in the section on closed-form representations for select Fourier transforms.
\end{solution}

\section{Fourier Series: Introduction}
% --- Narrative plan (auto-generated) ---
% In this section we begin the study of Fourier series, which represent periodic functions as infinite sums of sines and cosines. This idea allows us to replace a complicated function by a combination of very simple building blocks, each with a clear geometric and physical interpretation. We will learn how to compute these expansions, how to interpret their coefficients, and how to reason carefully about convergence.
%
% Fourier series are foundational in applied mathematics because many partial differential equations, such as the heat equation, the wave equation, and Laplace’s equation, can be solved by expanding solutions into trigonometric series. In dynamical systems and signal processing, Fourier series let us decompose motion or data into constituent frequencies, clarifying oscillatory behavior and resonances. The tools we develop here connect directly to orthogonality in linear algebra, to complex exponentials and residues in complex analysis, and to the more general Fourier transform that appears in continuum models and infinite-domain PDEs.

% ===== Example 1: Vibrating String with Fixed Endpoints (inquiry-based) =====
\begin{problem}[Vibrating String with Fixed Endpoints]
A taut string of length $L$ is stretched along the $x$-axis from $x=0$ to $x=L$ and held fixed at both ends. When displaced from equilibrium and released, the string vibrates, and its transverse displacement $u(x,t)$ (vertical position of the string) satisfies a partial differential equation. Experimentally, one observes that the motion can be described as a superposition of “standing waves” with different frequencies. Mathematically, this corresponds to expanding the initial shape of the string in terms of a Fourier sine series.

In this problem you will derive this description step by step, starting from the wave equation and ending with a Fourier series representation of an arbitrary initial displacement that vanishes at the endpoints.

\smallskip

(a) Let $u(x,t)$ denote the vertical displacement of the string at position $x \in [0,L]$ and time $t \ge 0$. The string has constant wave speed $c>0$.

\quad(i) Write down the one-dimensional wave equation that $u$ is assumed to satisfy. Briefly explain in words what each term represents.

\quad(ii) The string is fixed at both endpoints. Express this as boundary conditions on $u(x,t)$. Explain physically why these conditions are appropriate.

\quad(iii) Suppose the string is released from rest from an initial shape $f(x)$, which vanishes at the endpoints. Formulate the corresponding initial conditions for $u(x,t)$ in terms of $f$.

\smallskip

(b) To look for standing wave solutions, we try a separated form $u(x,t) = X(x)T(t)$.

\quad(i) Substitute $u(x,t)=X(x)T(t)$ into the wave equation you wrote in part (a), and rearrange your expression so that all terms involving $x$ are on one side and all terms involving $t$ are on the other side.

\quad(ii) Argue that both sides must be equal to the same constant, say $-\lambda$, and write down the resulting pair of ordinary differential equations for $X$ and $T$.

\quad(iii) Any physical mode of vibration should be bounded and nontrivial on $0<x<L$. What restrictions on the sign of $\lambda$ are suggested by this requirement and by the endpoint conditions $X(0)=X(L)=0$? State clearly which sign of $\lambda$ leads to acceptable solutions.

\emph{Hint:} Think about the general forms of solutions for $X''+\lambda X=0$, $X''=0$, and $X''-\lambda X=0$, and how they behave on a finite interval with $X(0)=X(L)=0$.

\smallskip

(c) Now focus on the spatial equation
\[
X''(x) + \lambda X(x) = 0, \qquad X(0)=0, \quad X(L)=0.
\]

\quad(i) Assuming $\lambda>0$, write the general solution of $X''+\lambda X=0$ and impose $X(0)=0$ to simplify it.

\quad(ii) Impose the second boundary condition $X(L)=0$ and show that this forces $\lambda$ to take on only certain discrete values $\lambda_n$. Find a formula for $\lambda_n$ in terms of $n$ and $L$, and write the corresponding eigenfunctions $X_n(x)$.

\quad(iii) Explain why the case $\lambda \le 0$ does not produce additional nontrivial solutions satisfying both endpoint conditions.

\emph{Hint:} For (ii), you should find a condition of the form $\sin(\sqrt{\lambda}L)=0$. For (iii), carefully examine the linear and exponential solutions that arise when $\lambda=0$ or $\lambda<0$.

\smallskip

(d) For each integer $n \ge 1$, you now have a pair of equations:
\[
X_n'' + \lambda_n X_n = 0, \qquad T_n'' + c^2\lambda_n T_n = 0,
\]
with $X_n(0)=X_n(L)=0$.

\quad(i) Solve the time equation for $T_n(t)$ when $\lambda_n = \left(\frac{n\pi}{L}\right)^2$ and write the separated solutions $u_n(x,t) = X_n(x)T_n(t)$. Express the frequency in terms of $n$, $c$, and $L$.

\quad(ii) Using the principle of superposition (linearity of the wave equation), write the most general solution $u(x,t)$ as a sum over $n$ of these modes.

\quad(iii) Impose the initial conditions $u(x,0)=f(x)$ and $u_t(x,0)=0$ to determine which time-dependent terms appear, and derive a formula for the coefficients in the expansion of $f(x)$.

\emph{Hint:} You should arrive at an expression of the form
\[
f(x) = \sum_{n=1}^{\infty} A_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
and then use orthogonality of the sine functions on $[0,L]$ to solve for $A_n$.

\smallskip

(e) Extensions and variations.

\quad(i) Suppose now that the string is released with zero initial displacement but a nonzero initial velocity $g(x)$, that is,
\[
u(x,0)=0, \qquad u_t(x,0)=g(x).
\]
How would your formulas in part (d) change? Describe (in words or formulas) how to obtain the new coefficients.

\quad(ii) Imagine instead that the left end of the string is fixed, but the right end is free to move vertically without any force (no tension) at $x=L$. In this idealized model, the boundary conditions become
\[
u(0,t)=0, \qquad u_x(L,t)=0.
\]
Based on your work above, what kind of trigonometric functions do you expect to appear in the $x$-dependence of the modes (sines, cosines, or both)? Give a brief justification based on the new boundary conditions.

\end{problem}

% ===== Example 1: Vibrating String with Fixed Endpoints (full solution) =====
\begin{problem}[Vibrating String with Fixed Endpoints]
Consider the one-dimensional wave equation
\[
u_{tt}(x,t) = c^2 u_{xx}(x,t), \qquad 0<x<L,\ t>0,
\]
modeling the transverse displacement $u(x,t)$ of a taut string of length $L$ with wave speed $c>0$. The string is fixed at both ends, so
\[
u(0,t)=0, \qquad u(L,t)=0 \quad \text{for all } t\ge 0.
\]
It is released from rest from an initial displacement $f(x)$ that satisfies $f(0)=f(L)=0$:
\[
u(x,0)=f(x), \qquad u_t(x,0)=0.
\]

Use separation of variables and Fourier series to:
\begin{enumerate}
\item Find all separated solutions satisfying the boundary conditions, and show that they lead to eigenfunctions $\sin\!\left(\frac{n\pi x}{L}\right)$ with frequencies depending on $n$, $c$, and $L$.
\item Use superposition to write the general solution $u(x,t)$ satisfying the given initial and boundary conditions, and express the coefficients in terms of $f(x)$.
\end{enumerate}
\end{problem}

\begin{solution}
We are asked to solve the wave equation on a finite interval with fixed endpoints and a given initial shape. The key ideas are separation of variables, the resulting eigenvalue problem for the spatial part, and expansion of the initial displacement in a Fourier sine series. This example illustrates how Fourier series naturally arise when solving linear partial differential equations with homogeneous boundary conditions.

\medskip

\textbf{1. Separation of variables and the separated equations.}

We look for separated solutions of the form
\[
u(x,t) = X(x) T(t),
\]
with $X$ depending only on $x$ and $T$ only on $t$. Substituting into the wave equation,
\[
u_{tt} = X(x) T''(t), \qquad u_{xx} = X''(x) T(t),
\]
so the wave equation $u_{tt} = c^2 u_{xx}$ becomes
\[
X(x) T''(t) = c^2 X''(x) T(t).
\]
Assuming $X$ and $T$ are not identically zero, we can divide by $c^2 X(x) T(t)$ to obtain
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)}.
\]
The left-hand side depends only on $t$ and the right-hand side only on $x$, so both must be equal to a constant. It is convenient to write this constant as $-\lambda$, giving the pair of ordinary differential equations
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)} = -\lambda,
\]
hence
\[
X''(x) + \lambda X(x) = 0, \qquad T''(t) + c^2 \lambda T(t) = 0.
\]

The boundary conditions $u(0,t)=0$ and $u(L,t)=0$ translate into
\[
X(0)T(t) = 0, \qquad X(L)T(t)=0 \quad \text{for all } t.
\]
For nontrivial $T(t)$, this forces
\[
X(0)=0, \qquad X(L)=0.
\]
Thus the spatial problem is
\[
X''(x) + \lambda X(x) = 0, \qquad X(0)=0,\quad X(L)=0.
\]

\medskip

\textbf{2. Solving the spatial eigenvalue problem.}

We must find all values of $\lambda$ for which there are nonzero solutions $X$ satisfying both endpoint conditions.

\smallskip

\emph{Case 1: $\lambda = 0$.} Then $X''(x)=0$, so
\[
X(x) = A + Bx.
\]
The condition $X(0)=0$ gives $A=0$, so $X(x)=Bx$. Then $X(L)=0$ implies $B L =0$, hence $B=0$. Therefore $X\equiv 0$, so there are no nontrivial solutions when $\lambda=0$.

\smallskip

\emph{Case 2: $\lambda <0$.} Write $\lambda = -\mu^2$ with $\mu>0$. Then
\[
X''(x) - \mu^2 X(x) = 0,
\]
whose general solution is
\[
X(x) = A e^{\mu x} + B e^{-\mu x}.
\]
The condition $X(0)=0$ gives $A+B=0$, so $B=-A$ and
\[
X(x) = A\left(e^{\mu x} - e^{-\mu x}\right) = 2A \sinh(\mu x).
\]
Then $X(L)=0$ gives $\sinh(\mu L)=0$. Since $\mu L>0$, this is impossible, so again only the trivial solution exists. Thus no nontrivial solutions arise for $\lambda<0$.

\smallskip

\emph{Case 3: $\lambda >0$.} Write $\lambda = \omega^2$ with $\omega>0$. Then
\[
X''(x) + \omega^2 X(x) = 0,
\]
whose general solution is
\[
X(x) = A\cos(\omega x) + B\sin(\omega x).
\]
The condition $X(0)=0$ gives
\[
X(0) = A\cos 0 + B\sin 0 = A = 0,
\]
so $A=0$ and
\[
X(x) = B\sin(\omega x).
\]
The second condition $X(L)=0$ then requires
\[
B \sin(\omega L) = 0.
\]
For a nontrivial solution we need $B\neq 0$, so we must have
\[
\sin(\omega L) = 0.
\]
This holds precisely when $\omega L = n\pi$ for some integer $n$. Since $\omega>0$, we take $n=1,2,3,\dots$. Thus
\[
\omega_n = \frac{n\pi}{L}, \qquad \lambda_n = \omega_n^2 = \left(\frac{n\pi}{L}\right)^2,
\]
and the corresponding eigenfunctions are
\[
X_n(x) = \sin\!\left( \frac{n\pi x}{L} \right), \qquad n=1,2,3,\dots,
\]
up to multiplication by a constant.

Therefore, the only values of $\lambda$ that yield nontrivial solutions satisfying the boundary conditions are
\[
\lambda_n = \left(\frac{n\pi}{L}\right)^2, \quad n\in\mathbb{N},
\]
with eigenfunctions $X_n(x)=\sin\left(\frac{n\pi x}{L}\right)$.

\medskip

\textbf{3. Solving the time equation and constructing separated solutions.}

For each $n\ge 1$, the time equation is
\[
T_n''(t) + c^2 \lambda_n T_n(t) = 0
\quad\text{with}\quad
\lambda_n = \left(\frac{n\pi}{L}\right)^2.
\]
Thus
\[
T_n''(t) + \left(\frac{n\pi c}{L}\right)^2 T_n(t) = 0.
\]
This is a harmonic oscillator equation with general solution
\[
T_n(t) = A_n \cos\!\left( \frac{n\pi c t}{L} \right)
        + B_n \sin\!\left( \frac{n\pi c t}{L} \right),
\]
where $A_n$ and $B_n$ are constants.

Combining $X_n$ and $T_n$, a separated solution for each $n$ is
\[
u_n(x,t) = \sin\!\left(\frac{n\pi x}{L}\right)
\bigg[
A_n \cos\!\left( \frac{n\pi c t}{L} \right)
+ B_n \sin\!\left( \frac{n\pi c t}{L} \right)
\bigg].
\]
The angular frequency of the $n$-th mode is
\[
\omega_n = \frac{n\pi c}{L}.
\]

\medskip

\textbf{4. Superposition and the general solution.}

The wave equation is linear and homogeneous, so any linear combination of solutions is again a solution. Therefore the general solution satisfying the boundary conditions $u(0,t)=u(L,t)=0$ can be written as a series
\[
u(x,t) = \sum_{n=1}^{\infty}
\sin\!\left(\frac{n\pi x}{L}\right)
\bigg[
A_n \cos\!\left( \frac{n\pi c t}{L} \right)
+ B_n \sin\!\left( \frac{n\pi c t}{L} \right)
\bigg],
\]
for some coefficients $A_n$ and $B_n$ to be determined from the initial conditions.

\medskip

\textbf{5. Imposing the initial conditions and Fourier sine coefficients.}

We now use
\[
u(x,0) = f(x), \qquad u_t(x,0) = 0.
\]

First, evaluate $u(x,t)$ at $t=0$. Since $\cos(0)=1$ and $\sin(0)=0$, we obtain
\[
u(x,0) = \sum_{n=1}^{\infty}
\sin\!\left(\frac{n\pi x}{L}\right)
\left[ A_n \cdot 1 + B_n \cdot 0 \right]
= \sum_{n=1}^{\infty} A_n \sin\!\left(\frac{n\pi x}{L}\right).
\]
Thus the initial displacement must satisfy
\[
f(x) = \sum_{n=1}^{\infty} A_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
which is exactly a Fourier sine series expansion of $f$ on $[0,L]$.

Next, differentiate $u(x,t)$ with respect to $t$:
\[
u_t(x,t) = \sum_{n=1}^{\infty}
\sin\!\left(\frac{n\pi x}{L}\right)
\left[
A_n \left(-\frac{n\pi c}{L}\right)
\sin\!\left( \frac{n\pi c t}{L} \right)
+ B_n \left(\frac{n\pi c}{L}\right)
\cos\!\left( \frac{n\pi c t}{L} \right)
\right].
\]
Evaluating at $t=0$ yields
\[
u_t(x,0) = \sum_{n=1}^{\infty}
\sin\!\left(\frac{n\pi x}{L}\right)
\left[
A_n \left(-\frac{n\pi c}{L}\right) \cdot 0
+ B_n \left(\frac{n\pi c}{L}\right) \cdot 1
\right]
= \sum_{n=1}^{\infty}
B_n \left(\frac{n\pi c}{L}\right)
\sin\!\left(\frac{n\pi x}{L}\right).
\]
The initial condition $u_t(x,0)=0$ for all $x$ implies that this series is identically zero. Because the sine functions $\sin\!\left(\frac{n\pi x}{L}\right)$ form an orthogonal set on $[0,L]$, the only way this can happen is if
\[
B_n = 0 \quad \text{for all } n.
\]

Therefore the solution simplifies to
\[
u(x,t) = \sum_{n=1}^{\infty}
A_n \cos\!\left( \frac{n\pi c t}{L} \right)
\sin\!\left(\frac{n\pi x}{L}\right),
\]
where the coefficients $A_n$ are precisely the Fourier sine coefficients of $f$.

Recall that the sine functions are orthogonal on $[0,L]$:
\[
\int_0^L \sin\!\left(\frac{n\pi x}{L}\right)
          \sin\!\left(\frac{m\pi x}{L}\right)\,dx
= \begin{cases}
0, & n\ne m,\\[4pt]
\displaystyle \frac{L}{2}, & n=m.
\end{cases}
\]
To find $A_n$, multiply the identity
\[
f(x) = \sum_{k=1}^{\infty} A_k \sin\!\left(\frac{k\pi x}{L}\right)
\]
by $\sin\!\left(\frac{n\pi x}{L}\right)$ and integrate from $0$ to $L$:
\[
\int_0^L f(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx
= \sum_{k=1}^{\infty} A_k
\int_0^L \sin\!\left(\frac{k\pi x}{L}\right)
         \sin\!\left(\frac{n\pi x}{L}\right)\,dx
= A_n \frac{L}{2}.
\]
Thus
\[
A_n = \frac{2}{L} \int_0^L f(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx,
\qquad n=1,2,3,\dots
\]

\medskip

\textbf{6. Final form of the solution and connection to Fourier series.}

Putting everything together, the unique solution of the wave equation
\[
u_{tt}=c^2 u_{xx}, \quad 0<x<L,\ t>0,
\]
with boundary conditions
\[
u(0,t)=u(L,t)=0,
\]
and initial conditions
\[
u(x,0)=f(x), \qquad u_t(x,0)=0,
\]
is given by
\[
u(x,t) =
\sum_{n=1}^{\infty}
\left[
\frac{2}{L} \int_0^L f(y)\sin\!\left(\frac{n\pi y}{L}\right)\,dy
\right]
\cos\!\left( \frac{n\pi c t}{L} \right)
\sin\!\left(\frac{n\pi x}{L}\right).
\]

This solution has a clear physical and mathematical interpretation. Physically, the string vibrates as a superposition of standing waves (normal modes), each with shape $\sin\!\left(\frac{n\pi x}{L}\right)$ and frequency $\omega_n = \frac{n\pi c}{L}$. Mathematically, the initial displacement $f(x)$ has been decomposed into a Fourier sine series using the eigenfunctions of the spatial operator subject to the boundary conditions. This example illustrates the central idea of the introduction to Fourier series: functions satisfying homogeneous boundary conditions can be expanded in orthogonal trigonometric series that arise naturally as eigenfunctions of differential operators, and these expansions are essential tools for solving partial differential equations such as the wave equation.

\end{solution}

% ===== Example 2: Cooling of a Rod with Prescribed End Temperatures (inquiry-based) =====
\begin{problem}[Cooling of a Rod with Prescribed End Temperatures]
Consider a thin, homogeneous metal rod of length $L$ lying along the $x$-axis from $x=0$ to $x=L$. The ends of the rod are held in contact with large ice baths, so that they are maintained at temperature zero for all time. The interior of the rod, however, starts with some nonuniform temperature distribution $f(x)$ that vanishes at the endpoints. We would like to describe how the temperature evolves over time and how Fourier series arise naturally in this context.

(a) Let $u(x,t)$ denote the temperature at position $x\in[0,L]$ and time $t\ge 0$. Write down the partial differential equation and the boundary and initial conditions that model this situation, assuming the rod has constant thermal diffusivity $\kappa>0$.

% Hint: Recall the one-dimensional heat equation, and express the information ``ends held at zero temperature'' and ``initial temperature profile $f$'' using boundary and initial conditions.

(b) A classical approach is to look for solutions that “separate” as a product of a function of $x$ and a function of $t$. Suppose $u(x,t) = X(x)T(t)$ is such a solution. 

\quad (i) Substitute $u(x,t)=X(x)T(t)$ into your heat equation from part (a), and rearrange the result to obtain an equation of the form
\[
\frac{1}{\kappa}\,\frac{T'(t)}{T(t)} \;=\; \frac{X''(x)}{X(x)}.
\]
Explain why both sides of this equation must be equal to the same constant, which we denote by $-\lambda$.

\quad (ii) Write down the two ordinary differential equations satisfied by $X$ and $T$, respectively, in terms of the constant $\lambda$.

% Hint: The idea is that the left-hand side depends only on $t$ and the right-hand side only on $x$. A function of $t$ that equals a function of $x$ for all $x,t$ must be constant.

(c) The boundary conditions $u(0,t)=u(L,t)=0$ for all $t$ translate into boundary conditions for $X(x)$. Together with your equation for $X(x)$, they define an eigenvalue problem.

\quad (i) State the boundary conditions for $X(x)$ explicitly, and write the resulting boundary value problem for $X(x)$.

\quad (ii) Analyze this boundary value problem by considering three cases: $\lambda<0$, $\lambda=0$, and $\lambda>0$. For each case, solve the differential equation for $X$ and apply the boundary conditions. For which values of $\lambda$ do nontrivial solutions $X$ exist?

\quad (iii) Show that the admissible values of $\lambda$ are
\[
\lambda_n = \left(\frac{n\pi}{L}\right)^2, \quad n=1,2,3,\dots,
\]
and that corresponding (nonzero) solutions $X_n(x)$ can be chosen in the form
\[
X_n(x) = \sin\left(\frac{n\pi x}{L}\right).
\]

% Hint: For $\lambda>0$, you will get sines and cosines. Use the boundary conditions at $x=0$ and $x=L$ to constrain the constants in the general solution.

(d) For each $n\ge 1$, you have an eigenfunction $X_n(x)$ and a corresponding time factor $T_n(t)$ solving your ODE from part (b)(ii). 

\quad (i) Solve for $T_n(t)$ when $\lambda=\lambda_n$ and write the separated solution $u_n(x,t)=X_n(x)T_n(t)$ explicitly.

\quad (ii) Explain why a linear combination
\[
u(x,t) = \sum_{n=1}^\infty b_n\,u_n(x,t)
\]
is still a solution of the heat equation satisfying the boundary conditions.

\quad (iii) At time $t=0$, use your formula for $u(x,t)$ to write an identity expressing the initial condition $u(x,0)=f(x)$ as a Fourier sine series. Derive a formula for the coefficients $b_n$ in terms of $f$ using orthogonality of the sine functions on $[0,L]$.

% Hint: Recall that the functions $\sin\left(\frac{n\pi x}{L}\right)$ are orthogonal on $[0,L]$ with respect to the usual inner product $\int_0^L \cdot \cdot \,dx$.

(e) Reflection and extensions.

\quad (i) Describe qualitatively what happens to $u(x,t)$ as $t\to\infty$. Use your explicit series representation to justify your answer.

\quad (ii) Suppose instead that the ends of the rod are held at a nonzero constant temperature $T_0$ for all $t\ge 0$. Sketch how you would modify the method to handle this new situation. In particular, what additional “steady-state” function might you try to subtract off so that the new unknown satisfies homogeneous boundary conditions?

% Hint: Look for a time-independent solution $v(x)$ with $v(0)=v(L)=T_0$, and then set $w(x,t)=u(x,t)-v(x)$.
\end{problem}

% ===== Example 2: Cooling of a Rod with Prescribed End Temperatures (full solution) =====
\begin{problem}[Cooling of a Rod with Prescribed End Temperatures]
Let $u(x,t)$ denote the temperature in a thin homogeneous rod of length $L$, with $x\in[0,L]$ and $t\ge 0$. The rod has constant thermal diffusivity $\kappa>0$, and its ends are held at zero temperature:
\[
u(0,t)=0,\quad u(L,t)=0,\quad t>0.
\]
The interior starts with temperature distribution $u(x,0)=f(x)$, where $f$ is a given function with $f(0)=f(L)=0$. The temperature satisfies the one-dimensional heat equation
\[
u_t = \kappa u_{xx}.
\]
Solve this initial–boundary value problem by separation of variables and express the solution in terms of a Fourier sine series involving $f$.
\end{problem}

\begin{solution}
We are asked to solve the heat equation on the finite interval with homogeneous Dirichlet boundary conditions. This is a standard setting where separation of variables leads naturally to Fourier sine series.

\medskip
\noindent\textbf{1. Setting up separation of variables.}
We seek solutions of the form
\[
u(x,t) = X(x)T(t),
\]
where $X$ depends only on $x$ and $T$ depends only on $t$. Substituting into the heat equation $u_t = \kappa u_{xx}$ gives
\[
X(x)\,T'(t) \;=\; \kappa\,X''(x)\,T(t).
\]
Assuming $X$ and $T$ are not identically zero, we can divide by $\kappa X(x)T(t)$ to obtain
\[
\frac{1}{\kappa}\,\frac{T'(t)}{T(t)} \;=\; \frac{X''(x)}{X(x)}.
\]
The left-hand side depends only on $t$, and the right-hand side depends only on $x$. For this equality to hold for all $x$ and $t$ in the domain, both sides must be equal to the same constant. We denote this constant by $-\lambda$ and obtain the separated equations
\[
\frac{1}{\kappa}\,\frac{T'(t)}{T(t)} = -\lambda, \qquad \frac{X''(x)}{X(x)} = -\lambda.
\]
Equivalently,
\[
T'(t) + \kappa\lambda\,T(t) = 0,\qquad X''(x) + \lambda\,X(x) = 0.
\]

The boundary conditions $u(0,t)=u(L,t)=0$ for all $t>0$ become
\[
X(0)T(t) = 0,\quad X(L)T(t) = 0 \quad\text{for all }t>0.
\]
We are interested in nontrivial products $X(x)T(t)$, so $T(t)$ cannot be identically zero. Therefore the boundary conditions reduce to
\[
X(0)=0,\qquad X(L)=0.
\]
The function $X$ must thus satisfy the boundary value problem
\[
X''(x) + \lambda X(x) = 0,\quad X(0)=0,\quad X(L)=0.
\]

\medskip
\noindent\textbf{2. Eigenvalue problem for the spatial part.}
We now solve the second-order ODE with boundary conditions by considering three cases for the separation constant $\lambda$.

\smallskip
\emph{Case 1: $\lambda<0$.} Write $\lambda = -\mu^2$ with $\mu>0$. Then $X$ satisfies
\[
X''(x) - \mu^2 X(x) = 0,
\]
whose general solution is
\[
X(x) = A e^{\mu x} + B e^{-\mu x}.
\]
Imposing $X(0)=0$ gives $A+B=0$, so $B=-A$ and
\[
X(x) = A\left(e^{\mu x} - e^{-\mu x}\right) = 2A\sinh(\mu x).
\]
Then $X(L)=0$ implies $\sinh(\mu L)=0$, but $\sinh$ is zero only at $0$, and here $\mu L>0$, so this is impossible unless $A=0$. Hence the only solution for $\lambda<0$ is the trivial solution $X\equiv0$, which we discard.

\smallskip
\emph{Case 2: $\lambda=0$.} Then $X''(x)=0$, so
\[
X(x) = A x + B.
\]
The conditions $X(0)=0$ and $X(L)=0$ give $B=0$ and $AL+B = 0$, hence $A=0$. Again the only solution is trivial.

\smallskip
\emph{Case 3: $\lambda>0$.} Write $\lambda = \mu^2$ with $\mu>0$. Then $X$ satisfies
\[
X''(x) + \mu^2 X(x) = 0,
\]
whose general solution is
\[
X(x) = A\cos(\mu x) + B\sin(\mu x).
\]
The boundary condition $X(0)=0$ gives $A=0$, so
\[
X(x) = B\sin(\mu x).
\]
The condition $X(L)=0$ then becomes
\[
B\sin(\mu L) = 0.
\]
For a nontrivial solution we require $B\ne 0$, so we must have $\sin(\mu L)=0$. Thus
\[
\mu L = n\pi,\quad\text{for some integer }n.
\]
To avoid the trivial case $\mu=0$, we take $n\in\mathbb{N}$, $n\ge1$. Hence
\[
\mu_n = \frac{n\pi}{L},\qquad \lambda_n = \mu_n^2 = \left(\frac{n\pi}{L}\right)^2,\qquad n=1,2,3,\dots
\]
For each $n\ge1$, we obtain an eigenfunction
\[
X_n(x) = \sin\left(\frac{n\pi x}{L}\right),
\]
where we have absorbed the constant $B$ into the overall constant in the separated solution.

Thus the spatial eigenvalue problem produces a discrete sequence of eigenvalues $\lambda_n$ and corresponding eigenfunctions $X_n$ satisfying the Dirichlet boundary conditions.

\medskip
\noindent\textbf{3. Time dependence and separated solutions.}
For each eigenvalue $\lambda_n$, the time factor $T_n(t)$ satisfies
\[
T_n'(t) + \kappa\lambda_n T_n(t) = 0.
\]
This is a first-order linear ODE with solution
\[
T_n(t) = C_n e^{-\kappa\lambda_n t}
       = C_n e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t},
\]
where $C_n$ is an arbitrary constant. Multiplying $X_n$ and $T_n$ together, we get a separated solution
\[
u_n(x,t) = \sin\left(\frac{n\pi x}{L}\right)\,e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t},
\qquad n=1,2,3,\dots.
\]
Each $u_n$ satisfies the heat equation and the homogeneous Dirichlet boundary conditions, because $X_n(0)=X_n(L)=0$.

By the linearity of the PDE and the boundary conditions, any (finite) linear combination
\[
u(x,t) = \sum_{n=1}^N b_n\,u_n(x,t)
\]
is also a solution with the same boundary conditions. To match a general initial condition $f$, we are naturally led to consider infinite series
\[
u(x,t) = \sum_{n=1}^\infty b_n\,\sin\left(\frac{n\pi x}{L}\right)\,
e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t}.
\]

\medskip
\noindent\textbf{4. Expanding the initial data in a Fourier sine series.}
We now impose the initial condition $u(x,0)=f(x)$. Evaluating the series at $t=0$ gives
\[
u(x,0) = \sum_{n=1}^\infty b_n\,\sin\left(\frac{n\pi x}{L}\right)
        = f(x),
\quad 0<x<L.
\]
Thus $f$ must be represented as a Fourier sine series on $[0,L]$:
\[
f(x) \sim \sum_{n=1}^\infty b_n\,\sin\left(\frac{n\pi x}{L}\right).
\]
The sine functions
\[
\sin\left(\frac{n\pi x}{L}\right),\quad n=1,2,3,\dots,
\]
form an orthogonal set in the space $L^2(0,L)$ with respect to the inner product
\[
\langle \phi,\psi\rangle = \int_0^L \phi(x)\psi(x)\,dx.
\]
Specifically, one has
\[
\int_0^L \sin\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,dx
= \begin{cases}
0, & n\ne m,\\[4pt]
\displaystyle \frac{L}{2}, & n=m.
\end{cases}
\]
To determine the coefficients $b_n$, we multiply the identity
\[
f(x) = \sum_{n=1}^\infty b_n\,\sin\left(\frac{n\pi x}{L}\right)
\]
by $\sin\left(\frac{m\pi x}{L}\right)$, integrate from $0$ to $L$, and interchange integral and sum (justified, for example, when $f$ is piecewise smooth). We obtain
\[
\int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx
= \sum_{n=1}^\infty b_n\int_0^L \sin\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,dx.
\]
By orthogonality, all terms vanish except the one with $n=m$, and we find
\[
\int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx
= b_m \cdot \frac{L}{2}.
\]
Thus
\[
b_m = \frac{2}{L}\int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx,\qquad m=1,2,3,\dots.
\]

\medskip
\noindent\textbf{5. Final solution and qualitative behavior.}
Substituting the coefficients into our general series, we arrive at the solution
\[
u(x,t) = \sum_{n=1}^\infty 
\left[\frac{2}{L}\int_0^L f(s)\sin\left(\frac{n\pi s}{L}\right)\,ds\right]
\sin\left(\frac{n\pi x}{L}\right)\,
e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t},
\quad 0<x<L,\ t>0.
\]
Under mild regularity assumptions on $f$, this series converges pointwise for $t>0$ and represents the unique solution to the given initial–boundary value problem.

Each mode
\[
\sin\left(\frac{n\pi x}{L}\right)\,e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t}
\]
decays exponentially in time, with decay rate $\kappa\left(\frac{n\pi}{L}\right)^2$. Higher modes (larger $n$) decay faster, so as $t\to\infty$ the solution becomes smoother and tends to zero:
\[
\lim_{t\to\infty} u(x,t) = 0,
\]
which matches the physical expectation that the rod cools down to the boundary temperature (here, zero) everywhere.

\medskip
\noindent\textbf{6. Connection to Fourier series and the chapter’s themes.}
This example illustrates the central ideas of Fourier series in the context of partial differential equations. The spatial operator $-d^2/dx^2$ with Dirichlet boundary conditions has eigenfunctions $\sin(n\pi x/L)$ and eigenvalues $(n\pi/L)^2$. These eigenfunctions form an orthogonal basis with respect to the $L^2$ inner product on $(0,L)$, allowing us to expand arbitrary initial data $f$ as a Fourier sine series. The time evolution of each Fourier mode is governed by a simple exponential decay. Thus the solution to the heat equation is naturally expressed as an eigenfunction expansion—precisely a Fourier series—showcasing how Fourier analysis provides a powerful tool for solving linear PDEs with boundary conditions.
\end{solution}

% ===== Example 3: Fourier Series of a Square Wave Signal (inquiry-based) =====
\begin{problem}[Fourier Series of a Square Wave Signal]
In many electronic circuits, an idealized digital signal is modeled as a square wave: the voltage is held at a high level for some time, then abruptly switched to a low level, and this pattern repeats periodically. Although the physical signal may be produced by complicated circuitry, the resulting waveform can be analyzed by decomposing it into a sum of sines and cosines, each oscillating at a multiple of the fundamental frequency. This example explores how to compute such a Fourier series, and what the resulting representation tells us about discontinuous signals.

Consider the $2\pi$-periodic function $f$ defined on the interval $(-\pi,\pi)$ by
\[
f(x) = \begin{cases}
1, & 0 < x < \pi,\\[4pt]
-1, & -\pi < x < 0,
\end{cases}
\]
and then extended periodically with period $2\pi$ to all real $x$.

\smallskip

(a) Begin by understanding the geometry of $f$. Sketch its graph on $(-2\pi,2\pi)$, clearly indicating the jump discontinuities at the points $k\pi$, where $k$ is an integer. What is the average value of $f$ over one period? That is, compute
\[
\frac{1}{2\pi}\int_{-\pi}^{\pi} f(x)\,dx.
\]
How does this relate to the constant (or ``DC'') term $a_0$ in a Fourier series?

\medskip

(b) Examine the symmetry of $f$. Is $f$ an even function, an odd function, or neither? Justify your answer by checking $f(-x)$ and comparing it to $f(x)$. Based on this symmetry, which Fourier coefficients must be zero in the expansion
\[
f(x) \sim \frac{a_0}{2}+\sum_{n=1}^\infty\bigl(a_n\cos(nx)+b_n\sin(nx)\bigr)?
\]
Explain why this greatly simplifies the computation of the series.

\emph{Hint:} Recall that an odd function has only sine terms in its Fourier series, and an even function has only cosine terms.

\medskip

(c) Now compute the nonzero Fourier coefficients explicitly. Using your answer from part (b), write down the formula for the relevant coefficients (either $a_n$'s or $b_n$'s, depending on the symmetry) on $(-\pi,\pi)$:
\[
a_n = \frac{1}{\pi}\int_{-\pi}^{\pi} f(x)\cos(nx)\,dx,
\qquad
b_n = \frac{1}{\pi}\int_{-\pi}^{\pi} f(x)\sin(nx)\,dx.
\]
Reduce the integral to a simpler interval by using the definition of $f$ on $(-\pi,0)$ and $(0,\pi)$, and use any symmetry that is available. Compute an explicit formula for the nonzero coefficients and determine for which values of $n$ they vanish.

\emph{Hint:} You may find it helpful to rewrite the integral over $(-\pi,\pi)$ as a sum of integrals over $(-\pi,0)$ and $(0,\pi)$, and then make a substitution in one of them to compare the two pieces.

\medskip

(d) Assemble your work into a complete Fourier series for the square wave. Write $f(x)$ as an infinite series involving only those trigonometric terms that you have found to be nonzero:
\[
f(x)\sim \sum_{n=\text{?}}^\infty \text{(coefficient depending on $n$)}\cdot\sin(nx)
\quad\text{or}\quad
f(x)\sim \sum_{n=\text{?}}^\infty \text{(coefficient depending on $n$)}\cdot\cos(nx),
\]
whichever is appropriate. Then answer the following conceptual questions:
\begin{itemize}
  \item At which points $x$ does the Fourier series converge to $f(x)$?
  \item What is the value of the limit at the jump points $x = k\pi$?
\end{itemize}

\emph{Hint:} Recall the general convergence result for Fourier series of piecewise smooth, periodic functions: at points of continuity the series converges to the function value, while at jump discontinuities it converges to the average of the left and right limits.

\medskip

(e) Explore two variations on this model.

\begin{enumerate}
  \item Suppose instead that the signal alternates between $0$ and $1$ on each half-period, so that
  \[
  g(x) = \begin{cases}
  1, & 0 < x < \pi,\\[4pt]
  0, & -\pi < x < 0,
  \end{cases}
  \]
  extended periodically. How would the Fourier series coefficients of $g$ be related to those of $f$? In particular, how does adding a constant offset to $f$ affect its Fourier series?
  
  \item Now imagine a ``duty cycle'' that is not $50\%$: define a $2\pi$-periodic function $h$ that equals $1$ on $(0,\alpha)$ and $-1$ on $(-\pi,0)$ and $(\alpha,\pi)$, for some fixed $\alpha$ with $0<\alpha<\pi$. Without doing all the integrals, describe qualitatively what changes in the Fourier series when $\alpha$ is varied. Which coefficients are likely still zero because of symmetry, and which are not? How would the spectrum of harmonics (the relative sizes of the coefficients) depend on the length $\alpha$ of the ``on'' interval?
\end{enumerate}

\emph{Hint:} For the first variation, think about how Fourier coefficients behave under adding constants. For the second, note that changing $\alpha$ breaks some of the symmetry present in the original problem, which will affect which terms appear in the series.
\end{problem}

% ===== Example 3: Fourier Series of a Square Wave Signal (full solution) =====
\begin{problem}[Fourier Series of a Square Wave Signal]
Let $f$ be the $2\pi$-periodic function defined on $(-\pi,\pi)$ by
\[
f(x) = \begin{cases}
1, & 0 < x < \pi,\\[4pt]
-1, & -\pi < x < 0.
\end{cases}
\]
\begin{enumerate}
  \item Determine whether $f$ is even, odd, or neither, and deduce which Fourier coefficients must vanish in the expansion
  \[
  f(x) \sim \frac{a_0}{2}+\sum_{n=1}^\infty\bigl(a_n\cos(nx)+b_n\sin(nx)\bigr).
  \]
  \item Compute the nonzero Fourier coefficients explicitly and obtain a Fourier series representation of $f$.
  \item State the values to which this Fourier series converges at points of continuity and at the jump points $x=k\pi$, $k\in\mathbb{Z}$, and briefly comment on how this example illustrates the use of Fourier series for discontinuous periodic signals.
\end{enumerate}
\end{problem}

\begin{solution}
We are given a square wave that takes the value $-1$ on the left half of each period and $1$ on the right half, repeated with period $2\pi$. The goal is to expand this function in a Fourier series and then interpret the resulting trigonometric sum.

\medskip

\textbf{1. Symmetry and vanishing coefficients.}
We first determine the parity of $f$. For $x\in(0,\pi)$ we have $f(x)=1$, while $-x\in(-\pi,0)$ and hence $f(-x)=-1=-f(x)$. Similarly, for $x\in(-\pi,0)$ we have $f(x)=-1$, while $-x\in(0,\pi)$ and $f(-x)=1=-f(x)$. Thus
\[
f(-x)=-f(x)\quad\text{for all }x\in(-\pi,\pi),
\]
so $f$ is an odd function.

In a Fourier series on $(-\pi,\pi)$,
\[
f(x)\sim \frac{a_0}{2}+\sum_{n=1}^\infty\bigl(a_n\cos(nx)+b_n\sin(nx)\bigr),
\]
the cosine terms represent the even part of the function and the sine terms represent the odd part. Since $f$ is odd, its even part is identically zero, which forces $a_0=0$ and $a_n=0$ for all $n\geq 1$. Only the sine coefficients $b_n$ can be nonzero. Therefore we expect a pure sine series:
\[
f(x)\sim \sum_{n=1}^\infty b_n\sin(nx).
\]

\medskip

\textbf{2. Computation of the coefficients.}
By the standard Fourier formula on $(-\pi,\pi)$, the sine coefficients are
\[
b_n = \frac{1}{\pi}\int_{-\pi}^{\pi} f(x)\sin(nx)\,dx,\qquad n\ge 1.
\]
We now use the definition of $f$ on the two halves of the interval:
\[
b_n = \frac{1}{\pi}\left(\int_{-\pi}^{0}(-1)\sin(nx)\,dx+\int_{0}^{\pi}1\cdot\sin(nx)\,dx\right).
\]
This simplifies to
\[
b_n = \frac{1}{\pi}\left(-\int_{-\pi}^{0}\sin(nx)\,dx+\int_{0}^{\pi}\sin(nx)\,dx\right).
\]
We evaluate these standard integrals. The antiderivative of $\sin(nx)$ is $-\cos(nx)/n$, so
\[
\int_{0}^{\pi}\sin(nx)\,dx
= \left[-\frac{\cos(nx)}{n}\right]_{0}^{\pi}
= -\frac{\cos(n\pi)}{n}+\frac{\cos(0)}{n}
= \frac{1-\cos(n\pi)}{n}.
\]
Similarly,
\[
\int_{-\pi}^{0}\sin(nx)\,dx
= \left[-\frac{\cos(nx)}{n}\right]_{-\pi}^{0}
= -\frac{\cos(0)}{n}+\frac{\cos(-n\pi)}{n}
= -\frac{1}{n}+\frac{\cos(n\pi)}{n}
= \frac{\cos(n\pi)-1}{n}.
\]
Therefore
\[
-\int_{-\pi}^{0}\sin(nx)\,dx
= -\frac{\cos(n\pi)-1}{n}
= \frac{1-\cos(n\pi)}{n}.
\]
Substituting back, we obtain
\[
b_n = \frac{1}{\pi}\left(\frac{1-\cos(n\pi)}{n}+\frac{1-\cos(n\pi)}{n}\right)
= \frac{1}{\pi}\cdot\frac{2\bigl(1-\cos(n\pi)\bigr)}{n}
= \frac{2}{\pi n}\bigl(1-(-1)^n\bigr).
\]

We simplify this expression by distinguishing even and odd $n$:
\[
(-1)^n =
\begin{cases}
1, & n \text{ even},\\
-1, & n \text{ odd},
\end{cases}
\]
so
\[
1-(-1)^n =
\begin{cases}
0, & n \text{ even},\\
2, & n \text{ odd}.
\end{cases}
\]
Thus
\[
b_n =
\begin{cases}
0, & n \text{ even},\\[4pt]
\dfrac{4}{\pi n}, & n \text{ odd}.
\end{cases}
\]

It is often convenient to reindex the series over odd integers by writing $n=2k-1$, where $k=1,2,3,\dots$. Then $b_{2k-1}=\dfrac{4}{\pi(2k-1)}$ and $b_{2k}=0$.

\medskip

\textbf{3. The Fourier series for the square wave.}
Since all cosine coefficients vanish, the Fourier series for $f$ is
\[
f(x)\sim \sum_{n=1}^\infty b_n\sin(nx)
=\sum_{\substack{n=1\\ n\ \text{odd}}}^\infty \frac{4}{\pi n}\sin(nx).
\]
Reindexing over odd integers as $n=2k-1$ gives the standard form
\[
f(x)\sim \frac{4}{\pi}\sum_{k=1}^\infty \frac{1}{2k-1}\sin\bigl((2k-1)x\bigr).
\]

This representation shows that the square wave is built entirely from odd sine harmonics of the fundamental frequency. The absence of even harmonics (all even $n$ terms vanish) is a consequence of the particular symmetry and half-period structure of the function.

\medskip

\textbf{4. Convergence and behavior at the jumps.}
The function $f$ is piecewise constant on $(-\pi,\pi)$, with jump discontinuities at $x=-\pi,0,\pi$. It is piecewise smooth and periodic, so the standard convergence theorem for Fourier series applies: the Fourier series of $f$ converges at every point $x\in\mathbb{R}$, and
\begin{itemize}
  \item at each point $x$ where $f$ is continuous, the series converges to $f(x)$;
  \item at each point of discontinuity $x_0$, the series converges to the average of the left and right limits,
  \[
  \frac{1}{2}\bigl(f(x_0^-)+f(x_0^+)\bigr).
  \]
\end{itemize}

In our case, on $(-\pi,0)$ the function is constantly $-1$, and on $(0,\pi)$ it is constantly $1$, so at any $x$ with $-\pi<x<0$ or $0<x<\pi$ the Fourier series converges to $f(x)$, that is, to $-1$ or $1$ respectively.

At $x=0$, we have $f(0^-)= -1$ and $f(0^+)=1$, so the series converges to
\[
\frac{1}{2}\bigl(-1+1\bigr)=0.
\]
Similarly, at $x=\pm\pi$ the left and right limits are $f(\pi^-)=1$ and $f(\pi^+)=-1$ (using periodicity), so the average is again $0$. By periodicity, the same holds at all points $x=k\pi$ for integer $k$. Thus the Fourier series converges to $0$ at each jump point, which is the midpoint between the two levels.

This behavior is typical of Fourier series approximating discontinuous functions: near the jump, the partial sums exhibit overshoot and oscillation (the Gibbs phenomenon), but the infinite series converges to the midpoint of the jump.

\medskip

\textbf{5. Conceptual significance.}
This example illustrates several central ideas from the introduction to Fourier series:
\begin{itemize}
  \item \emph{Symmetry and simplification:} Recognizing that $f$ is odd immediately reduces the general sine–cosine expansion to a much simpler sine series.
  \item \emph{Orthogonality and coefficient formulas:} The explicit computation of $b_n$ uses the orthogonality of $\sin(nx)$ over $(-\pi,\pi)$ and the standard integral formulas, revealing precisely which harmonics are present.
  \item \emph{Representation of discontinuous signals:} Even though $f$ has jump discontinuities, it can be represented by a convergent trigonometric series. The convergence to the midpoint at jumps is a key feature when modeling idealized signals such as square waves in engineering.
\end{itemize}
Thus the square wave serves as a canonical example showing how Fourier series decompose a simple but discontinuous periodic function into an infinite sum of smooth sinusoidal components.
\end{solution}

% ===== Example 4: Even and Odd Extensions: Half-Range Expansions (inquiry-based) =====
\begin{problem}[Even and Odd Extensions: Half-Range Expansions]
In many heat or wave problems, the physical domain is a rod or string occupying the interval $0 < x < L$. However, the standard Fourier series machinery is most naturally built on symmetric intervals $[-L,L]$. One way to connect these two viewpoints is to ``imagine'' a mirror image of the rod on the left, either reflecting the temperature profile (an even extension) or flipping its sign (an odd extension). In this problem, you will discover how such extensions lead to \emph{half-range cosine} and \emph{half-range sine} series for a function given only on $(0,L)$.

Let $L>0$ be fixed, and consider the function
\[
f(x) = x, \qquad 0 < x < L.
\]
We would like to represent $f$ on $(0,L)$ using series of sines or cosines only.

\smallskip

(a) We first define extensions of $f$ to the symmetric interval $[-L,L]$.

\begin{itemize}
    \item Define the \emph{even extension} $f_{\mathrm{even}}$ of $f$ to $[-L,L]$ by
    \[
    f_{\mathrm{even}}(x) = 
    \begin{cases}
    f(x), & 0 \le x \le L,\\
    f(-x), & -L \le x < 0.
    \end{cases}
    \]
    \item Define the \emph{odd extension} $f_{\mathrm{odd}}$ of $f$ to $[-L,L]$ by
    \[
    f_{\mathrm{odd}}(x) =
    \begin{cases}
    f(x), & 0 \le x \le L,\\
    -f(-x), & -L \le x < 0.
    \end{cases}
    \]
\end{itemize}

(i) Write down explicit formulas for $f_{\mathrm{even}}(x)$ and $f_{\mathrm{odd}}(x)$ for $-L \le x \le L$.  

(ii) Sketch $f$ on $(0,L)$, and then sketch $f_{\mathrm{even}}$ and $f_{\mathrm{odd}}$ on $[-L,L]$.  

(iii) Verify directly from your formulas that $f_{\mathrm{even}}$ is an even function and $f_{\mathrm{odd}}$ is an odd function.

\emph{Hint:} Recall that a function $g$ is even if $g(-x) = g(x)$ and odd if $g(-x) = -g(x)$.

\smallskip

(b) One of the main reasons to care about even and odd extensions is that they simplify Fourier series. Suppose $g$ is integrable on $[-L,L]$ and has a Fourier series
\[
g(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty \left( a_n \cos\frac{n\pi x}{L} + b_n \sin\frac{n\pi x}{L} \right).
\]

(i) Show that if $g$ is even, then all the sine coefficients $b_n$ must be zero.  

(ii) Show that if $g$ is odd, then $a_0 = 0$ and all the cosine coefficients $a_n$ must be zero.

\emph{Hint:} Use the definitions
\[
a_n = \frac{1}{L} \int_{-L}^{L} g(x)\cos\frac{n\pi x}{L}\,dx, 
\qquad
b_n = \frac{1}{L} \int_{-L}^{L} g(x)\sin\frac{n\pi x}{L}\,dx.
\]
Think about the parity (evenness or oddness) of the integrands. When is an integral of an odd function over $[-L,L]$ equal to zero?

\smallskip

(c) Now use part (b) to express the Fourier series of $f_{\mathrm{even}}$ and $f_{\mathrm{odd}}$.

(i) Argue that $f_{\mathrm{even}}$ has a Fourier series involving only cosines:
\[
f_{\mathrm{even}}(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty a_n \cos\frac{n\pi x}{L}.
\]
Derive formulas for $a_0$ and $a_n$ in terms of integrals over $(0,L)$ only, using the evenness of $f_{\mathrm{even}}$.

(ii) Argue that $f_{\mathrm{odd}}$ has a Fourier series involving only sines:
\[
f_{\mathrm{odd}}(x) \sim \sum_{n=1}^\infty b_n \sin\frac{n\pi x}{L}.
\]
Derive a formula for $b_n$ in terms of an integral over $(0,L)$ only, using the oddness of $f_{\mathrm{odd}}$.

\emph{Hint:} For an even function $h$, 
\[
\int_{-L}^{L} h(x)\,dx = 2\int_{0}^{L} h(x)\,dx.
\]
For an odd function $h$, the integral over $[-L,L]$ is zero.

\smallskip

(d) Compute the actual Fourier coefficients for our specific function $f(x) = x$.

(i) Using your formulas from (c), compute the cosine coefficients $a_0$ and $a_n$ of $f_{\mathrm{even}}(x)$, and write down the full cosine series for $f_{\mathrm{even}}$.  

(ii) Using your formulas from (c), compute the sine coefficients $b_n$ of $f_{\mathrm{odd}}(x)$, and write down the full sine series for $f_{\mathrm{odd}}$.  

(iii) Carefully state the resulting \emph{half-range cosine series} and \emph{half-range sine series} for $f(x)=x$ on $(0,L)$.

\emph{Hint:} For $f_{\mathrm{even}}$, you will need integrals of the form $\int_0^L x\cos(\frac{n\pi x}{L})\,dx$. For $f_{\mathrm{odd}}$, you will need $\int_0^L x\sin(\frac{n\pi x}{L})\,dx$. Integration by parts is helpful.

\smallskip

(e) ``What if'' and interpretation questions.

(i) In a heat equation problem on $0 < x < L$, if you model the end $x=0$ as \emph{insulated} (no heat flux across the boundary), the mathematical boundary condition is $u_x(0,t)=0$. Explain informally why an even extension of the initial temperature profile is natural for such a condition, and why this leads to a cosine series in $x$.

(ii) If instead the end $x=0$ is held at fixed temperature $0$ so that $u(0,t)=0$, explain informally why an odd extension of the initial profile is natural, and why this leads to a sine series in $x$.

(iii) Suppose now that $f$ on $(0,L)$ does \emph{not} satisfy $f(0)=0$. Can you still form an odd extension? What happens at $x=0$ and how would this affect the Fourier series? Describe the issue qualitatively and relate it to the fact that Fourier series converge in an averaged sense at jump discontinuities.

\end{problem}

% ===== Example 4: Even and Odd Extensions: Half-Range Expansions (full solution) =====
\begin{problem}[Even and Odd Extensions: Half-Range Expansions]
Let $L>0$ and $f(x)=x$ for $0<x<L$.

(a) Define the even and odd extensions $f_{\mathrm{even}}$ and $f_{\mathrm{odd}}$ of $f$ to $[-L,L]$.  

(b) Using the parity of $f_{\mathrm{even}}$ and $f_{\mathrm{odd}}$, derive formulas for the Fourier cosine coefficients $a_n$ and sine coefficients $b_n$ in terms of integrals over $(0,L)$ only.  

(c) Compute the Fourier cosine series of $f_{\mathrm{even}}$ and the Fourier sine series of $f_{\mathrm{odd}}$ on $[-L,L]$.  

(d) From these, write explicitly the half-range cosine and half-range sine series that represent $f(x)=x$ on $(0,L)$.  

(e) Briefly explain how even and odd extensions correspond to Neumann ($u_x(0,t)=0$) and Dirichlet ($u(0,t)=0$) boundary conditions, respectively, in one-dimensional heat or wave equations.
\end{problem}

\begin{solution}
We are given $f(x)=x$ for $0<x<L$ and wish to obtain half-range cosine and sine series on $(0,L)$ by using even and odd extensions to $[-L,L]$. This example illustrates the standard Fourier series idea that symmetry (evenness or oddness) eliminates either the sine or cosine part of the series and is central in constructing half-range expansions.

\medskip

\noindent\textbf{(a) Even and odd extensions.}
By definition, the even extension $f_{\mathrm{even}}$ of $f$ to $[-L,L]$ is
\[
f_{\mathrm{even}}(x) =
\begin{cases}
f(x) = x, & 0 \le x \le L,\\[4pt]
f(-x) = -x, & -L \le x < 0,
\end{cases}
\]
so $f_{\mathrm{even}}(x)=|x|$ on $[-L,L]$.

The odd extension $f_{\mathrm{odd}}$ is
\[
f_{\mathrm{odd}}(x) =
\begin{cases}
f(x) = x, & 0 \le x \le L,\\[4pt]
-\,f(-x) = -(-x) = x, & -L \le x < 0,
\end{cases}
\]
so $f_{\mathrm{odd}}(x)=x$ on $[-L,L]$.

It is straightforward to check parity. For $f_{\mathrm{even}}(x)=|x|$ we have $f_{\mathrm{even}}(-x)=|-x|=|x|=f_{\mathrm{even}}(x)$, so it is even. For $f_{\mathrm{odd}}(x)=x$ we have $f_{\mathrm{odd}}(-x)=-x=-f_{\mathrm{odd}}(x)$, so it is odd.

\medskip

\noindent\textbf{(b) Consequences of parity for Fourier coefficients.}
Let $g$ be integrable on $[-L,L]$ with Fourier series
\[
g(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty\left( a_n\cos\frac{n\pi x}{L} + b_n\sin\frac{n\pi x}{L}\right).
\]
The coefficients are
\[
a_n = \frac{1}{L}\int_{-L}^{L} g(x)\cos\frac{n\pi x}{L}\,dx, 
\quad
b_n = \frac{1}{L}\int_{-L}^{L} g(x)\sin\frac{n\pi x}{L}\,dx.
\]

If $g$ is even, then the product $g(x)\cos\frac{n\pi x}{L}$ is even (even $\times$ even), while $g(x)\sin\frac{n\pi x}{L}$ is odd (even $\times$ odd). The integral of an odd function over $[-L,L]$ is zero, so $b_n=0$ for all $n$. Therefore the Fourier series of an even function contains only cosine terms (and possibly $a_0$).

If $g$ is odd, then $g(x)\cos\frac{n\pi x}{L}$ is odd (odd $\times$ even), and $g(x)\sin\frac{n\pi x}{L}$ is even (odd $\times$ odd). Thus $a_n = 0$ for all $n\ge 0$, while the $b_n$ may be nonzero. Hence the Fourier series of an odd function contains only sine terms.

\medskip

\noindent\textbf{(c) Reduction to integrals over $(0,L)$.}

\emph{Even case.} Since $f_{\mathrm{even}}$ is even, its Fourier series takes the form
\[
f_{\mathrm{even}}(x) \sim \frac{a_0}{2} + \sum_{n=1}^\infty a_n\cos\frac{n\pi x}{L},
\]
with
\[
a_n = \frac{1}{L}\int_{-L}^{L} f_{\mathrm{even}}(x)\cos\frac{n\pi x}{L}\,dx.
\]
The integrand is even, because $f_{\mathrm{even}}$ and the cosine are both even, so
\[
a_n = \frac{2}{L}\int_{0}^{L} f_{\mathrm{even}}(x)\cos\frac{n\pi x}{L}\,dx
     = \frac{2}{L}\int_{0}^{L} x\cos\frac{n\pi x}{L}\,dx,
\]
for $n\ge1$. Similarly,
\[
a_0 = \frac{1}{L}\int_{-L}^{L} f_{\mathrm{even}}(x)\,dx
    = \frac{2}{L}\int_{0}^{L} x\,dx.
\]

\emph{Odd case.} Since $f_{\mathrm{odd}}$ is odd, its Fourier series has only sine terms:
\[
f_{\mathrm{odd}}(x) \sim \sum_{n=1}^\infty b_n\sin\frac{n\pi x}{L},
\]
with
\[
b_n = \frac{1}{L}\int_{-L}^{L} f_{\mathrm{odd}}(x)\sin\frac{n\pi x}{L}\,dx.
\]
Here the integrand is even (odd $\times$ odd), so
\[
b_n = \frac{2}{L}\int_{0}^{L} f_{\mathrm{odd}}(x)\sin\frac{n\pi x}{L}\,dx
     = \frac{2}{L}\int_{0}^{L} x\sin\frac{n\pi x}{L}\,dx.
\]

Thus the coefficients are given entirely by integrals over $(0,L)$.

\medskip

\noindent\textbf{(d) Computing the coefficients and the series.}

\emph{Cosine series for $f_{\mathrm{even}}(x)=|x|$.} 

First compute $a_0$:
\[
a_0 = \frac{2}{L}\int_0^L x\,dx = \frac{2}{L}\cdot \frac{L^2}{2} = L.
\]
Therefore the constant term is $a_0/2 = L/2$.

For $n\ge 1$,
\[
a_n = \frac{2}{L}\int_0^L x\cos\frac{n\pi x}{L}\,dx.
\]
Use integration by parts. Let
\[
u=x,\quad dv=\cos\frac{n\pi x}{L}\,dx,\quad du=dx,\quad v=\frac{L}{n\pi}\sin\frac{n\pi x}{L}.
\]
Then
\[
\int_0^L x\cos\frac{n\pi x}{L}\,dx 
= \left. x\cdot\frac{L}{n\pi}\sin\frac{n\pi x}{L}\right|_0^L 
  - \int_0^L \frac{L}{n\pi}\sin\frac{n\pi x}{L}\,dx.
\]
The boundary term is zero, since $\sin\frac{n\pi L}{L} = \sin(n\pi)=0$ and $\sin 0=0$. Thus
\[
\int_0^L x\cos\frac{n\pi x}{L}\,dx 
= -\frac{L}{n\pi}\int_0^L \sin\frac{n\pi x}{L}\,dx.
\]
The remaining integral is
\[
\int_0^L \sin\frac{n\pi x}{L}\,dx
= \left.-\frac{L}{n\pi}\cos\frac{n\pi x}{L}\right|_0^L
= -\frac{L}{n\pi}\bigl(\cos(n\pi)-1\bigr)
= -\frac{L}{n\pi}\bigl((-1)^n -1\bigr).
\]
Therefore
\[
\int_0^L x\cos\frac{n\pi x}{L}\,dx
= -\frac{L}{n\pi}\cdot\left(-\frac{L}{n\pi}\bigl((-1)^n-1\bigr)\right)
= \frac{L^2}{n^2\pi^2}\bigl((-1)^n-1\bigr).
\]
Consequently,
\[
a_n = \frac{2}{L}\cdot \frac{L^2}{n^2\pi^2}\bigl((-1)^n-1\bigr)
    = \frac{2L}{n^2\pi^2}\bigl((-1)^n-1\bigr).
\]

Note that if $n$ is even, then $(-1)^n=1$ and $a_n=0$. If $n$ is odd, say $n=2k+1$, then $(-1)^n=-1$, so $(-1)^n-1=-2$ and
\[
a_n = -\frac{4L}{n^2\pi^2}.
\]
Thus only odd cosine terms appear. We can write
\[
f_{\mathrm{even}}(x)=|x| \sim \frac{L}{2} - \frac{4L}{\pi^2}\sum_{\substack{n=1\\ n\ \mathrm{odd}}}^{\infty} \frac{1}{n^2}\cos\frac{n\pi x}{L}.
\]

\emph{Sine series for $f_{\mathrm{odd}}(x)=x$.}

For $n\ge1$,
\[
b_n = \frac{2}{L}\int_0^L x\sin\frac{n\pi x}{L}\,dx.
\]
Again use integration by parts, but now let
\[
u=x,\quad dv=\sin\frac{n\pi x}{L}\,dx,\quad du=dx,\quad v=-\frac{L}{n\pi}\cos\frac{n\pi x}{L}.
\]
Then
\[
\int_0^L x\sin\frac{n\pi x}{L}\,dx
= \left. -x\frac{L}{n\pi}\cos\frac{n\pi x}{L}\right|_0^L
  + \int_0^L \frac{L}{n\pi}\cos\frac{n\pi x}{L}\,dx.
\]
The boundary term is
\[
-\,\frac{L}{n\pi}\bigl(L\cos(n\pi) - 0\cdot\cos 0\bigr)
= -\frac{L^2}{n\pi}(-1)^n.
\]
The remaining integral is
\[
\int_0^L \frac{L}{n\pi}\cos\frac{n\pi x}{L}\,dx
= \frac{L}{n\pi}\left.\frac{L}{n\pi}\sin\frac{n\pi x}{L}\right|_0^L = 0,
\]
since the sine vanishes at both endpoints. Thus
\[
\int_0^L x\sin\frac{n\pi x}{L}\,dx = -\frac{L^2}{n\pi}(-1)^n.
\]
It follows that
\[
b_n = \frac{2}{L}\cdot\left(-\frac{L^2}{n\pi}(-1)^n\right)
    = -\frac{2L}{n\pi}(-1)^n
    = \frac{2L}{n\pi}(-1)^{n+1}.
\]

Therefore the sine series of $f_{\mathrm{odd}}(x)=x$ on $[-L,L]$ is
\[
f_{\mathrm{odd}}(x)=x \sim \sum_{n=1}^\infty \frac{2L}{n\pi}(-1)^{n+1}\sin\frac{n\pi x}{L}.
\]

\medskip

\noindent\textbf{Half-range expansions on $(0,L)$.}

On $(0,L)$, both $f_{\mathrm{even}}$ and $f_{\mathrm{odd}}$ coincide with $f(x)=x$. Hence we may simply restrict the above series to $(0,L)$ to obtain the half-range series.

The \emph{half-range cosine series} for $f(x)=x$ on $(0,L)$ is 
\[
x \sim \frac{L}{2} - \frac{4L}{\pi^2}\sum_{\substack{n=1\\ n\ \mathrm{odd}}}^{\infty} \frac{1}{n^2}\cos\frac{n\pi x}{L},
\qquad 0<x<L.
\]

The \emph{half-range sine series} for $f(x)=x$ on $(0,L)$ is
\[
x \sim \sum_{n=1}^\infty \frac{2L}{n\pi}(-1)^{n+1}\sin\frac{n\pi x}{L},
\qquad 0<x<L.
\]

These are simply the Fourier cosine series of $|x|$ and the Fourier sine series of $x$ on $[-L,L]$, viewed on the right half of the interval.

\medskip

\noindent\textbf{(e) Interpretation in terms of boundary conditions.}

In one-dimensional heat or wave equations on $0<x<L$, spatial boundary conditions at $x=0$ determine which eigenfunctions are appropriate:

\begin{itemize}
    \item If the end $x=0$ is \emph{insulated} (Neumann condition $u_x(0,t)=0$), the flux across the boundary is zero. Spatial eigenfunctions satisfying $X'(0)=0$ are cosines: $X_n(x)=\cos\frac{n\pi x}{L}$. Extending the temperature profile evenly across $x=0$ produces an even function, whose derivative is odd and therefore vanishes at the origin. This is consistent with $u_x(0,t)=0$ and leads naturally to expansions in cosine series.

    \item If the end $x=0$ is held at temperature zero (Dirichlet condition $u(0,t)=0$), the spatial eigenfunctions satisfying $X(0)=0$ are sines: $X_n(x)=\sin\frac{n\pi x}{L}$. Extending the solution oddly across $x=0$ forces $u(0,t)=0$ (since an odd function vanishes at the origin), and so an odd extension of the initial profile leads naturally to a sine series.

\end{itemize}

If $f$ on $(0,L)$ does not satisfy $f(0)=0$, one can still define an odd extension by setting
\[
\tilde f_{\mathrm{odd}}(x) =
\begin{cases}
f(x), & 0<x\le L,\\
-\,f(-x), & -L\le x<0,\\
0, & x=0.
\end{cases}
\]
This introduces a jump discontinuity at $x=0$ unless $f(0+)=0$. The resulting sine series converges to $0$ at $x=0$ (the midpoint of the jump), and to the odd extension elsewhere. This reflects the general principle that Fourier series converge at a jump to the average of the left and right limits. Thus, even when the original data do not exactly match the boundary condition, half-range sine or cosine series still approximate the function in the $L^2$ sense, but pointwise behavior at the boundary reflects the chosen symmetry.

Overall, this example shows how the introductory ideas of Fourier series—orthogonality of sines and cosines, parity, and eigenfunctions for boundary value problems—combine to produce useful half-range expansions from data given only on $(0,L)$.

\end{solution}

% ===== Example 5: From Trigonometric Series to Complex Exponential Form (inquiry-based) =====
\begin{problem}[From Trigonometric Series to Complex Exponential Form]
In applications, real-valued periodic signals are often expanded in a Fourier series using sines and cosines. On the other hand, many theoretical developments in Fourier analysis, linear algebra, and complex analysis prefer to work with complex exponentials $e^{inx}$. These two descriptions of the same function look quite different, but they are mathematically equivalent. In this problem you will gently uncover how to pass back and forth between these two viewpoints.

Consider a real-valued $2\pi$-periodic function $f$ with a (formal) trigonometric Fourier series
\[
f(x) \sim \frac{a_0}{2} \;+\; \sum_{n=1}^{\infty} \bigl( a_n \cos(nx) + b_n \sin(nx) \bigr),
\]
where $a_n, b_n \in \mathbb{R}$ for all $n$.

\smallskip

(a) Warm-up: Start with a single frequency. Let
\[
g(x) = a \cos x + b \sin x,
\]
where $a,b \in \mathbb{R}$. Use Euler's formulas
\[
\cos x = \frac{e^{ix} + e^{-ix}}{2}, \qquad \sin x = \frac{e^{ix} - e^{-ix}}{2i}
\]
to rewrite $g(x)$ as a linear combination of $e^{ix}$ and $e^{-ix}$:
\[
g(x) = c_1 e^{ix} + c_{-1} e^{-ix}
\]
for some complex numbers $c_1$ and $c_{-1}$. 

\begin{itemize}
\item[(i)] Compute $c_1$ and $c_{-1}$ in terms of $a$ and $b$.
\item[(ii)] Solve for $a$ and $b$ in terms of $c_1$ and $c_{-1}$, and note any symmetry relating $c_{-1}$ and $c_1$ when $g$ is real-valued.
\end{itemize}
Hint: Collect the coefficients of $e^{ix}$ and $e^{-ix}$ after substituting the Euler formulas.

\smallskip

(b) Now extend this idea to all integer frequencies. For each $n \ge 1$, use the Euler identities to write
\[
a_n \cos(nx) + b_n \sin(nx)
\]
as a linear combination of $e^{inx}$ and $e^{-inx}$:
\[
a_n \cos(nx) + b_n \sin(nx) = c_n e^{inx} + c_{-n} e^{-inx}.
\]
Find formulas for $c_n$ and $c_{-n}$ in terms of $a_n$ and $b_n$. 

Hint: This is exactly the same algebra as in part (a), with $x$ replaced by $nx$.

\smallskip

(c) Combine your work from part (b) with the constant term $\frac{a_0}{2}$ to show that $f$ can be written (formally) as a complex exponential series
\[
f(x) \sim \sum_{n=-\infty}^{\infty} c_n e^{inx},
\]
and express $c_0$, $c_n$ (for $n \ge 1$), and $c_{-n}$ (for $n \ge 1$) explicitly in terms of $a_n$ and $b_n$. 

Then, invert these relations: solve for $a_n$ and $b_n$ in terms of the complex coefficients $c_n$. 

Hint: For $n \ge 1$, you already know how $a_n \cos(nx) + b_n \sin(nx)$ splits into the $n$ and $-n$ modes. Think about what happens when you sum over all $n$.

\smallskip

(d) Reality condition. Suppose $f$ is real-valued for all $x$. What does this imply about the complex coefficients $c_n$? Show that
\[
c_0 \in \mathbb{R}, \qquad c_{-n} = \overline{c_n} \quad \text{for all } n \ge 1.
\]
Conversely, show that if a formal complex exponential series
\[
\sum_{n=-\infty}^{\infty} c_n e^{inx}
\]
satisfies $c_0 \in \mathbb{R}$ and $c_{-n} = \overline{c_n}$ for all $n \ge 1$, then the series (if convergent) represents a real-valued function. 

Hint: Use that $\overline{e^{inx}} = e^{-inx}$ and that $f$ being real means $f(x) = \overline{f(x)}$ for all $x$.

\smallskip

(e) Explorations and extensions.

\begin{itemize}
\item[(i)] Suppose now that $f$ is \emph{not} necessarily real-valued, but complex-valued. Which parts of your reasoning in (c) and (d) still go through? What can you say about the relation between $a_n, b_n$ and $c_n$ in this setting?
\item[(ii)] How would your formulas change if the period of $f$ were $2L$ instead of $2\pi$? Write down what the complex exponential series would look like in that case, and how the “frequency” $n$ should be rescaled.
\item[(iii)] Conceptual question: In what ways might the complex exponential form
\[
\sum_{n=-\infty}^{\infty} c_n e^{inx}
\]
be more convenient than the sine–cosine form when thinking of Fourier series as coordinates in a vector space or when using tools from complex analysis?
\end{itemize}
\end{problem}

% ===== Example 5: From Trigonometric Series to Complex Exponential Form (full solution) =====
\begin{problem}[From Trigonometric Series to Complex Exponential Form]
Let $f$ be a (formal) real-valued $2\pi$-periodic function with trigonometric Fourier series
\[
f(x) \sim \frac{a_0}{2} + \sum_{n=1}^{\infty} \bigl(a_n \cos(nx) + b_n \sin(nx)\bigr),
\qquad a_n, b_n \in \mathbb{R}.
\]
\begin{enumerate}
\item[(i)] Using Euler's formulas
\[
\cos(nx) = \frac{e^{inx} + e^{-inx}}{2}, 
\qquad 
\sin(nx) = \frac{e^{inx} - e^{-inx}}{2i},
\]
show that $f$ can also be written (formally) in the complex exponential form
\[
f(x) \sim \sum_{n=-\infty}^{\infty} c_n e^{inx}
\]
for suitable complex coefficients $c_n$.
\item[(ii)] Derive explicit formulas for $c_n$ in terms of $a_n$ and $b_n$, and for $a_n$, $b_n$ in terms of $c_n$.
\item[(iii)] Prove that $f$ is real-valued if and only if
\[
c_0 \in \mathbb{R}
\quad\text{and}\quad
c_{-n} = \overline{c_n} \quad \text{for all } n \ge 1.
\]
\end{enumerate}
\end{problem}

\begin{solution}
We begin from the trigonometric Fourier series
\[
f(x) \sim \frac{a_0}{2} + \sum_{n=1}^{\infty} \bigl(a_n \cos(nx) + b_n \sin(nx)\bigr),
\]
where $a_n$ and $b_n$ are real numbers. The main idea is to use Euler's formulas to express the trigonometric functions in terms of complex exponentials, and then to group the coefficients of $e^{inx}$ for all integers $n$.

\medskip

\noindent\textbf{Step 1: Rewrite a single harmonic in complex exponential form.}

Fix $n \ge 1$ and consider the $n$th harmonic
\[
a_n \cos(nx) + b_n \sin(nx).
\]
Using Euler’s formulas
\[
\cos(nx) = \frac{e^{inx} + e^{-inx}}{2}, 
\qquad 
\sin(nx) = \frac{e^{inx} - e^{-inx}}{2i},
\]
we substitute to obtain
\begin{align*}
a_n \cos(nx) + b_n \sin(nx)
&= a_n \cdot \frac{e^{inx} + e^{-inx}}{2} 
  + b_n \cdot \frac{e^{inx} - e^{-inx}}{2i} \\
&= \left(\frac{a_n}{2} + \frac{b_n}{2i}\right)e^{inx}
   + \left(\frac{a_n}{2} - \frac{b_n}{2i}\right) e^{-inx}.
\end{align*}
It is convenient to rewrite $\frac{1}{i} = -i$, so
\[
\frac{b_n}{2i} = -\frac{i b_n}{2}.
\]
Thus we have
\[
a_n \cos(nx) + b_n \sin(nx)
= \left(\frac{a_n}{2} - \frac{i b_n}{2}\right)e^{inx}
  + \left(\frac{a_n}{2} + \frac{i b_n}{2}\right)e^{-inx}.
\]
This suggests defining
\[
c_n := \frac{a_n - i b_n}{2},
\qquad
c_{-n} := \frac{a_n + i b_n}{2}.
\]
Then
\[
a_n \cos(nx) + b_n \sin(nx) = c_n e^{inx} + c_{-n} e^{-inx}.
\]

\medskip

\noindent\textbf{Step 2: Assemble the full complex exponential series.}

Now we apply this decomposition to each harmonic in the Fourier series for $f$. We obtain
\begin{align*}
f(x) 
&\sim \frac{a_0}{2}
   + \sum_{n=1}^{\infty} \bigl(a_n \cos(nx) + b_n \sin(nx)\bigr) \\
&= \frac{a_0}{2}
   + \sum_{n=1}^{\infty} \bigl(c_n e^{inx} + c_{-n} e^{-inx}\bigr),
\end{align*}
where for $n \ge 1$ we have defined
\[
c_n = \frac{a_n - i b_n}{2},
\qquad
c_{-n} = \frac{a_n + i b_n}{2}.
\]
If we additionally define
\[
c_0 := \frac{a_0}{2},
\]
then the series may be written more symmetrically as
\[
f(x) \sim \sum_{n=-\infty}^{\infty} c_n e^{inx}.
\]
This proves part (i) of the problem.

\medskip

\noindent\textbf{Step 3: Relations between $(a_n,b_n)$ and $c_n$.}

We already obtained formulas for $c_n$ in terms of $a_n$ and $b_n$ for $n \ge 1$:
\[
c_n = \frac{a_n - i b_n}{2}, \qquad c_{-n} = \frac{a_n + i b_n}{2},
\]
and $c_0 = a_0 / 2$.

Conversely, to express $a_n$ and $b_n$ in terms of $c_n$ and $c_{-n}$, we simply solve these linear equations. For $n \ge 1$ we add and subtract:
\[
c_n + c_{-n} 
= \frac{a_n - i b_n}{2} + \frac{a_n + i b_n}{2}
= a_n,
\]
and
\[
c_{-n} - c_n
= \frac{a_n + i b_n}{2} - \frac{a_n - i b_n}{2}
= i b_n.
\]
Therefore
\[
a_n = c_n + c_{-n},
\qquad
b_n = \frac{1}{i}\,(c_{-n} - c_n) = -i(c_{-n} - c_n).
\]
Since $1/i = -i$, this expression is purely algebraic and valid for complex $c_n$. For the constant term, we have
\[
a_0 = 2 c_0.
\]

This completes the formulas in part (ii):
\[
\boxed{
\begin{aligned}
c_0 &= \frac{a_0}{2},\\[0.3em]
c_n &= \frac{a_n - i b_n}{2}, \quad c_{-n} = \frac{a_n + i b_n}{2} \quad (n \ge 1),
\end{aligned}}
\]
and
\[
\boxed{
\begin{aligned}
a_0 &= 2c_0,\\[0.3em]
a_n &= c_n + c_{-n}, \quad
b_n = -i(c_{-n} - c_n) \quad (n \ge 1).
\end{aligned}}
\]

\medskip

\noindent\textbf{Step 4: Characterizing real-valued series via $c_{-n} = \overline{c_n}$.}

We now address part (iii), which concerns when the complex exponential series represents a real-valued function.

\smallskip

\emph{($\Rightarrow$) If $f$ is real-valued, then $c_0 \in \mathbb{R}$ and $c_{-n} = \overline{c_n}$ for all $n \ge 1$.}

Because $f$ is real-valued and the real Fourier coefficients $a_n$ and $b_n$ are real numbers, we have $a_0 \in \mathbb{R}$ and $a_n, b_n \in \mathbb{R}$ for $n \ge 1$. From the formulas obtained above,
\[
c_0 = \frac{a_0}{2},
\]
so immediately $c_0$ is real.

For $n \ge 1$ we have
\[
c_n = \frac{a_n - i b_n}{2},
\qquad
c_{-n} = \frac{a_n + i b_n}{2}.
\]
Since $a_n$ and $b_n$ are real, complex conjugation gives
\[
\overline{c_n} 
= \frac{a_n + i b_n}{2}
= c_{-n}.
\]
Thus $c_{-n} = \overline{c_n}$ for every $n \ge 1$. This establishes one direction.

\smallskip

\emph{($\Leftarrow$) If $c_0 \in \mathbb{R}$ and $c_{-n} = \overline{c_n}$ for all $n \ge 1$, then the series represents a real-valued function (where convergent).}

Assume a formal series
\[
f(x) \sim \sum_{n=-\infty}^{\infty} c_n e^{inx}
\]
has coefficients satisfying $c_0 \in \mathbb{R}$ and the symmetry $c_{-n} = \overline{c_n}$ for all $n \ge 1$. For each $x$, take the complex conjugate termwise:
\[
\overline{f(x)} 
\sim \overline{\sum_{n=-\infty}^{\infty} c_n e^{inx}}
= \sum_{n=-\infty}^{\infty} \overline{c_n} \, \overline{e^{inx}}
= \sum_{n=-\infty}^{\infty} \overline{c_n} \, e^{-inx},
\]
since $\overline{e^{inx}} = e^{-inx}$.

Now relabel the index $m = -n$:
\[
\overline{f(x)} 
= \sum_{m=-\infty}^{\infty} \overline{c_{-m}} \, e^{imx}.
\]
By hypothesis $\overline{c_{-m}} = c_m$ for all $m \neq 0$, and $\overline{c_0} = c_0$ since $c_0$ is real. Thus
\[
\overline{f(x)} = \sum_{m=-\infty}^{\infty} c_m e^{imx} \sim f(x).
\]
That is, the conjugate series coincides with the original series. Therefore wherever the series converges, its value satisfies $\overline{f(x)} = f(x)$, which means $f(x)$ is real. This proves the converse.

\medskip

\noindent\textbf{Conceptual remarks.}

This example illustrates how the sine–cosine and complex exponential forms of Fourier series are simply two coordinate systems on the same space of periodic functions. The passage between them rests on Euler's formulas, and the key structural idea is that $\{e^{inx}\}_{n \in \mathbb{Z}}$ forms an orthogonal family (in $L^{2}[-\pi,\pi]$), making it a very natural basis from the perspective of linear algebra. 

Moreover, the symmetry condition $c_{-n} = \overline{c_n}$ succinctly encodes the real-valuedness of $f$, which is somewhat more cumbersome to express in terms of $a_n$ and $b_n$. In the broader context of Fourier series, this complex exponential viewpoint is especially powerful when relating Fourier analysis to eigenfunction expansions, to the spectral theory of differential operators, and to methods of complex analysis (such as contour integration), all of which heavily favor the use of $e^{inx}$ over sines and cosines.

\end{solution}

\section{Properties of the Fourier Series}
% TODO: Add narrative / plan for this section.

% TODO: Use prompts_for_sections.py to design examples and add them here.

\section{Riemann–Lebesgue Lemma}
% TODO: Add narrative / plan for this section.

% TODO: Use prompts_for_sections.py to design examples and add them here.

\section{Gibbs Phenomenon}
% TODO: Add narrative / plan for this section.

% TODO: Use prompts_for_sections.py to design examples and add them here.

\section{Laplace Transform}
% --- Narrative plan (auto-generated) ---
% This section develops the Laplace transform as a central tool for solving linear differential equations, especially those driven by external forcing and subject to initial conditions. The main idea is to convert problems in the time domain, typically expressed as ordinary differential equations or simple partial differential equations, into algebraic problems in a new variable, often called the complex frequency. Once in this transformed setting, the equations become easier to manipulate and solve, after which the inverse transform returns the solution to the original time variable.
%
% The Laplace transform is especially valuable in applied mathematics because it handles discontinuous inputs, impulsive forces, and growing exponentials in a unified way. It provides a natural language for describing dynamical systems, electrical circuits, and control systems, and it shares many themes with Fourier analysis, such as convolution and spectral representation. Conceptually, it connects to complex analysis through analytic continuation and contour integration, and it complements Fourier transforms in the analysis of PDEs by focusing on initial-value problems and one-sided time evolution rather than periodic or infinite-time behavior.
%
% Throughout this section, we will build a toolkit that includes basic transform pairs, operational rules (such as shifting and differentiation), and systematic methods for partial fraction decomposition in the transform domain. Along the way, you will see how Laplace transforms streamline the solution of linear ODEs and simple PDEs, how they encode initial conditions directly into algebraic equations, and how they interact with convolution to describe input–output relations in linear systems.

% ===== Example 1: Exponential Growth and Decay via Laplace Transform (inquiry-based) =====
\begin{problem}[Exponential Growth and Decay via Laplace Transform]
Many physical and biological processes are modeled by first-order linear differential equations.  For example, a population that grows at a rate proportional to its size, or a radioactive substance that decays at a rate proportional to the amount present, can both be described by a simple exponential law.  In this problem you will re-discover the classical exponential growth and decay formulas, but this time \emph{purely} through the Laplace transform.  The goal is to practice writing down the Laplace transform of the unknown function, using the initial condition, and solving the resulting algebraic equation before inverting it.

Consider the initial value problem
\[
y'(t) = k\,y(t), \qquad y(0)=y_0,
\]
where $k$ and $y_0$ are real constants.  Assume $t \ge 0$ and that $y(t)$ is of exponential order so that its Laplace transform exists.

\medskip

(a) Recall or derive the Laplace transform of a derivative.  Let $Y(s) = \mathcal{L}\{y(t)\}(s) = \displaystyle\int_0^{\infty} e^{-st} y(t)\,dt$.  Starting from the definition of the Laplace transform, compute
\[
\mathcal{L}\{y'(t)\}(s) = \int_0^{\infty} e^{-st} y'(t)\,dt
\]
using integration by parts, and express your answer in terms of $Y(s)$ and $y(0)$.  
Hint: For integration by parts, take $u = e^{-st}$ and $dv = y'(t)\,dt$.

\medskip

(b) Apply the Laplace transform to both sides of the differential equation $y'(t) = k\,y(t)$.  Use your result from part (a) and the initial condition $y(0)=y_0$ to obtain an algebraic equation for $Y(s)$.  Solve this equation explicitly for $Y(s)$.
% Hint: You should get an expression of the form $Y(s) = \dfrac{y_0}{s - k}$.

\medskip

(c) In order to recover $y(t)$ from $Y(s)$, you need to recognize $Y(s)$ as the Laplace transform of a familiar function.  Recall from a Laplace transform table (or check directly from the definition) that
\[
\mathcal{L}\left\{e^{at}\right\}(s) = \frac{1}{s-a}, \qquad \text{for } s>a.
\]
Use this fact, together with your expression for $Y(s)$ in part (b), to determine $y(t)$.  Write your final answer explicitly in terms of $k$, $y_0$, and $t$.
% Hint: Compare $Y(s)$ with $\dfrac{1}{s-a}$ and identify $a$.

\medskip

(d) Interpret your formula for $y(t)$ in two important cases:
\begin{itemize}
  \item[(i)] $k>0$ (for instance, modeling population growth with proportional birth rate),
  \item[(ii)] $k<0$ (for instance, modeling radioactive decay or Newtonian cooling toward zero temperature).
\end{itemize}
Explain in one or two sentences for each case how the sign of $k$ affects the qualitative behavior of $y(t)$ as $t \to \infty$.

\medskip

(e) ``What if'' extensions.

\begin{itemize}
  \item[(i)] Suppose now that there is a constant input term, for example a population with births proportional to its size \emph{and} a constant immigration rate.  Consider
  \[
  y'(t) = k\,y(t) + b, \qquad y(0) = y_0,
  \]
  where $b$ is a constant.  Without carrying out all the algebra, outline how you would modify the Laplace transform steps from parts (a)--(c) to solve this new problem.  Which additional Laplace transform do you need to know, beyond the ones already used?
  % Hint: You will need $\mathcal{L}\{1\}(s)$.

  \item[(ii)] For the equation in part (e)(i), imagine $k < 0$ (a decay term) and $b > 0$ (a constant source).  Based on your physical intuition, do you expect $y(t)$ to blow up, decay to zero, or approach some steady-state value as $t \to \infty$?  Briefly justify your answer using the structure of the differential equation (you do not need to compute the exact solution, but you may if you wish).
\end{itemize}

\end{problem}

% ===== Example 1: Exponential Growth and Decay via Laplace Transform (full solution) =====
\begin{problem}[Exponential Growth and Decay via Laplace Transform]
Use the Laplace transform to solve the following initial value problems, and briefly interpret the behavior of the solutions:

\begin{enumerate}
  \item[(i)] $y'(t) = k\,y(t)$, \; $y(0) = y_0$, where $k$ and $y_0$ are real constants.
  \item[(ii)] $y'(t) = k\,y(t) + b$, \; $y(0) = y_0$, where $k$, $b$, and $y_0$ are real constants.
\end{enumerate}

Assume $t \ge 0$ and that in each case the solution is of exponential order so its Laplace transform exists.
\end{problem}

\begin{solution}
The central idea of using the Laplace transform to solve an initial value problem is that the transform converts the differential equation, together with its initial condition, into an algebraic equation for the Laplace transform of the unknown function.  After solving this algebraic equation, we invert the transform to recover the solution in the time domain.  This example illustrates this general procedure in the simplest possible setting of first-order linear equations modeling exponential growth and decay.

\medskip

We begin with the Laplace transform of a derivative.  Let $y(t)$ be a function with Laplace transform
\[
Y(s) = \mathcal{L}\{y(t)\}(s) = \int_0^{\infty} e^{-st} y(t)\,dt.
\]
We compute the transform of $y'(t)$ directly from the definition:
\[
\mathcal{L}\{y'(t)\}(s) = \int_0^{\infty} e^{-st} y'(t)\,dt.
\]
We apply integration by parts with $u = e^{-st}$ and $dv = y'(t)\,dt$, so that $du = -s e^{-st}\,dt$ and $v = y(t)$.  Then
\[
\int_0^{\infty} e^{-st} y'(t)\,dt
= \Bigl[e^{-st} y(t)\Bigr]_0^{\infty} - \int_0^{\infty} (-s e^{-st}) y(t)\,dt
= \Bigl[e^{-st} y(t)\Bigr]_0^{\infty} + s \int_0^{\infty} e^{-st} y(t)\,dt.
\]
Under the usual growth assumptions that guarantee the existence of the Laplace transform, we have $e^{-st} y(t) \to 0$ as $t \to \infty$ when $\Re(s)$ is large enough.  Hence
\[
\Bigl[e^{-st} y(t)\Bigr]_0^{\infty} = 0 - y(0) = -y(0).
\]
Therefore
\[
\mathcal{L}\{y'(t)\}(s) = -y(0) + s Y(s) = s Y(s) - y(0).
\]
This identity is the key formula that incorporates the initial value into the transformed equation.

\medskip

\noindent\textbf{Part (i):} Solve $y'(t) = k\,y(t)$, $y(0) = y_0$.

Applying the Laplace transform to both sides of the differential equation gives
\[
\mathcal{L}\{y'(t)\}(s) = \mathcal{L}\{k\,y(t)\}(s).
\]
Using linearity of the Laplace transform and the derivative formula just derived, we obtain
\[
s Y(s) - y(0) = k\,Y(s).
\]
Substituting the initial condition $y(0) = y_0$ yields
\[
s Y(s) - y_0 = k\,Y(s).
\]
We now solve this algebraic equation for $Y(s)$.  Rearranging terms,
\[
s Y(s) - k Y(s) = y_0
\quad\Longrightarrow\quad
(s - k) Y(s) = y_0
\quad\Longrightarrow\quad
Y(s) = \frac{y_0}{s - k}.
\]

To find $y(t)$, we recognize $Y(s)$ as a standard Laplace transform.  From a Laplace transform table, or from a direct computation, we know that
\[
\mathcal{L}\{e^{at}\}(s) = \frac{1}{s-a}, \qquad \text{for } s > a.
\]
Thus
\[
\mathcal{L}\{y_0 e^{kt}\}(s) = y_0 \,\mathcal{L}\{e^{kt}\}(s) = y_0 \cdot \frac{1}{s-k} = \frac{y_0}{s-k}.
\]
By uniqueness of the Laplace transform, it follows that
\[
y(t) = y_0 e^{kt}.
\]

We now briefly interpret this solution.  If $k>0$, then the exponential factor $e^{kt}$ grows without bound as $t\to\infty$, so the solution models exponential growth, as in a population with a birth rate proportional to its size.  If $k<0$, then $e^{kt}$ decays to zero as $t\to\infty$, so the solution models exponential decay, as in radioactive decay or cooling toward the ambient temperature (taken to be zero in this simplified model).

\medskip

\noindent\textbf{Part (ii):} Solve $y'(t) = k\,y(t) + b$, $y(0) = y_0$.

We now consider the equation with a constant source term $b$:
\[
y'(t) = k\,y(t) + b, \qquad y(0) = y_0.
\]
Again we apply the Laplace transform to both sides:
\[
\mathcal{L}\{y'(t)\}(s) = \mathcal{L}\{k\,y(t) + b\}(s).
\]
Using linearity and the derivative formula, we obtain
\[
s Y(s) - y(0) = k Y(s) + \mathcal{L}\{b\}(s).
\]
We substitute $y(0) = y_0$ and note that $\mathcal{L}\{b\}(s) = b\,\mathcal{L}\{1\}(s) = b \cdot \dfrac{1}{s} = \dfrac{b}{s}$.  This gives
\[
s Y(s) - y_0 = k Y(s) + \frac{b}{s}.
\]
We solve for $Y(s)$:
\[
s Y(s) - k Y(s) = y_0 + \frac{b}{s}
\quad\Longrightarrow\quad
(s - k) Y(s) = y_0 + \frac{b}{s}.
\]
Dividing by $(s-k)$, we obtain
\[
Y(s) = \frac{y_0}{s-k} + \frac{b}{s(s-k)}.
\]

To invert this transform, we rewrite the second term by partial fractions.  We seek constants $A$ and $B$ such that
\[
\frac{b}{s(s-k)} = \frac{A}{s} + \frac{B}{s-k}.
\]
Clearing denominators,
\[
b = A(s-k) + B s = (A+B)s - Ak \quad\text{for all } s.
\]
Equating coefficients of like powers of $s$, we obtain the system
\[
A + B = 0, \qquad -Ak = b.
\]
From $-Ak = b$ we have $A = -\dfrac{b}{k}$ (assuming $k \ne 0$; the case $k=0$ can be handled separately below).  Then $B = -A = \dfrac{b}{k}$.  Hence
\[
\frac{b}{s(s-k)} = -\frac{b}{k}\,\frac{1}{s} + \frac{b}{k}\,\frac{1}{s-k}.
\]
Therefore
\[
Y(s) = \frac{y_0}{s-k} - \frac{b}{k}\,\frac{1}{s} + \frac{b}{k}\,\frac{1}{s-k}
= \left( y_0 + \frac{b}{k} \right)\frac{1}{s-k} - \frac{b}{k}\,\frac{1}{s}.
\]

We now invert term by term using the known transforms
\[
\mathcal{L}\{e^{kt}\}(s) = \frac{1}{s-k}, \qquad \mathcal{L}\{1\}(s) = \frac{1}{s}.
\]
Thus
\[
y(t) = \mathcal{L}^{-1}\{Y(s)\}(t)
= \left( y_0 + \frac{b}{k} \right) e^{kt} - \frac{b}{k}\cdot 1
= \left( y_0 + \frac{b}{k} \right)e^{kt} - \frac{b}{k}.
\]

If desired, one can rewrite this more suggestively as
\[
y(t) = -\frac{b}{k} + \left( y_0 + \frac{b}{k} \right) e^{kt}.
\]
This form makes the long-time behavior transparent.  When $k<0$ and $b>0$, the exponential term $e^{kt}$ decays to zero, and the solution approaches the constant value $-\dfrac{b}{k} > 0$ as $t\to\infty$.  Thus the system settles to a steady state determined by the balance between decay and constant input.  When $k>0$ and $b\ne 0$, the exponential term causes $y(t)$ to grow (in magnitude) exponentially, so no finite steady state exists.

Finally, we briefly mention the case $k=0$.  The equation becomes $y'(t) = b$, with solution $y(t) = y_0 + bt$, which is linear growth or decay depending on the sign of $b$.  This case is also easily handled by the Laplace transform, but the partial fraction step above needs minor modification.

\medskip

In summary, this example illustrates the standard Laplace transform method for solving initial value problems: we transform the differential equation, use the initial condition through the derivative formula $\mathcal{L}\{y'\} = sY(s) - y(0)$, solve the resulting algebraic equation for $Y(s)$, and then use known inverse transforms (and partial fractions when needed) to recover $y(t)$.  Even in these simple exponential growth and decay models, the method highlights how initial data and forcing terms (like the constant $b$) are encoded in the algebraic structure of the transform.
\end{solution}

% ===== Example 2: Damped Harmonic Oscillator with Forcing (inquiry-based) =====
\begin{problem}[Damped Harmonic Oscillator with Forcing]
A mass attached to a spring and dashpot (damper) is a standard model for vibrations with energy loss. In many applications the mass is also driven by an external periodic force, such as a motor or an oscillating field. The resulting equation combines inertia, damping, elasticity, and forcing, and exhibits both transient and steady-state behavior. In this problem you will use the Laplace transform to solve such an initial value problem and to see how the transient and steady-state parts of the motion appear naturally.

Consider a mass whose displacement from equilibrium is $x(t)$ for $t \ge 0$. The motion is governed by
\[
m x''(t) + c x'(t) + k x(t) = F_0 \sin(\omega t), \qquad t>0,
\]
with initial conditions
\[
x(0) = 0, \qquad x'(0) = 0.
\]
Here $m>0$ is the mass, $c>0$ is the damping coefficient, $k>0$ is the spring constant, and $F_0 \sin(\omega t)$ is a sinusoidal driving force of amplitude $F_0$ and angular frequency $\omega>0$.

\smallskip

(a) Rewrite the equation in a more convenient ``standard form'' by dividing through by $m$ and introducing the parameters
\[
2\gamma = \frac{c}{m}, \qquad \omega_0^2 = \frac{k}{m}.
\]
What is the resulting differential equation for $x(t)$? State clearly the corresponding initial conditions in this notation.

\medskip

(b) Let $X(s) = \mathcal{L}\{x(t)\}(s)$ be the Laplace transform of $x(t)$. Take the Laplace transform of both sides of your standard-form equation from part (a), and use the formulas
\[
\mathcal{L}\{x'(t)\}(s) = s X(s) - x(0), \qquad
\mathcal{L}\{x''(t)\}(s) = s^2 X(s) - s x(0) - x'(0),
\]
together with $\mathcal{L}\{\sin(\omega t)\}(s) = \dfrac{\omega}{s^2+\omega^2}$.
Derive an algebraic equation for $X(s)$ and solve explicitly for $X(s)$.

\emph{Hint:} After substituting the initial conditions from part (a), every term on the left-hand side should contain a factor of $X(s)$.

\medskip

(c) Assume that the oscillator is \emph{underdamped}, that is,
\[
0 < \gamma < \omega_0.
\]
Introduce the damped natural frequency
\[
\Omega = \sqrt{\omega_0^2 - \gamma^2},
\]
and show that
\[
s^2 + 2\gamma s + \omega_0^2 = (s+\gamma)^2 + \Omega^2.
\]
Rewrite your expression for $X(s)$ in the form
\[
X(s) = \frac{F_0 \,\omega}{(s^2+\omega^2)\big((s+\gamma)^2+\Omega^2\big)}.
\]
Set up a partial fraction decomposition of the form
\[
X(s) = \frac{A s + B}{s^2+\omega^2} \;+\; \frac{C(s+\gamma) + D}{(s+\gamma)^2+\Omega^2},
\]
where $A,B,C,D$ are constants depending on $\gamma,\omega_0,\omega,F_0$.

\emph{Hint:} You do not need to determine $A,B,C,D$ yet; first write down the identity obtained by multiplying both sides by $(s^2+\omega^2)\big((s+\gamma)^2+\Omega^2\big)$ and equating coefficients of powers of $s$.

\medskip

(d) Now determine the constants $A,B,C,D$ by comparing coefficients of $1,s,s^2,s^3$ in the identity from part (c). Then use the inverse Laplace transform and the shift rule
\[
\mathcal{L}^{-1}\left\{\frac{s}{s^2+\alpha^2}\right\}(t) = \cos(\alpha t), \qquad
\mathcal{L}^{-1}\left\{\frac{1}{s^2+\alpha^2}\right\}(t) = \frac{1}{\alpha}\sin(\alpha t),
\]
\[
\mathcal{L}^{-1}\{F(s+\gamma)\}(t) = e^{-\gamma t} f(t) \quad\text{if}\quad \mathcal{L}\{f\}(s) = F(s),
\]
to obtain an explicit formula for $x(t)$ for $t\ge 0$.

Rewrite your answer in the form
\[
x(t) = x_{\text{tr}}(t) + x_{\text{ss}}(t),
\]
where $x_{\text{tr}}(t)$ is a \emph{transient} term and $x_{\text{ss}}(t)$ is a \emph{steady-state} term. Which part decays to zero as $t\to\infty$? Which part persists for large $t$?

\emph{Hint:} After taking inverse Laplace transforms, group together all terms that are multiplied by $e^{-\gamma t}$.

\medskip

(e) (Explorations.)

\begin{enumerate}
  \item Suppose instead that the force is a step input
  \[
  f(t) = F_0\,u(t),
  \]
  where $u(t)$ is the Heaviside unit step function. Without carrying out all the algebra, describe how the Laplace-transform solution would change compared to the sinusoidal forcing case. In particular, what would $F(s)$ be on the right-hand side, and what kinds of time-domain functions (sines, cosines, exponentials, constants, ramps) would you expect in $x(t)$?
  
  \item For the sinusoidal forcing considered above, focus on the steady-state term $x_{\text{ss}}(t)$ and its amplitude. Using your expression for $x_{\text{ss}}(t)$, express it in the form
  \[
  x_{\text{ss}}(t) = R(\omega)\,\sin(\omega t - \delta(\omega)),
  \]
  for some amplitude $R(\omega)$ and phase shift $\delta(\omega)$. How does $R(\omega)$ behave as the damping $\gamma$ becomes small and the driving frequency $\omega$ approaches the undamped natural frequency $\omega_0$ (the resonance phenomenon)?
  
  \emph{Hint:} You may find it helpful to recall that a linear combination of $\sin(\omega t)$ and $\cos(\omega t)$ can be written as a single sinusoid with a phase shift.
\end{enumerate}

\end{problem}

% ===== Example 2: Damped Harmonic Oscillator with Forcing (full solution) =====
\begin{problem}[Damped Harmonic Oscillator with Forcing]
Consider the forced, damped harmonic oscillator
\[
m x''(t) + c x'(t) + k x(t) = F_0 \sin(\omega t), \qquad t>0,
\]
with initial conditions $x(0)=0$, $x'(0)=0$. Introduce the parameters
\[
2\gamma = \frac{c}{m}, \qquad \omega_0^2 = \frac{k}{m},
\]
assume the underdamped case $0<\gamma<\omega_0$, and set $\Omega = \sqrt{\omega_0^2-\gamma^2}$. 

Using the Laplace transform, solve explicitly for $x(t)$ for $t\ge 0$, and decompose the solution into a transient part and a steady-state part. Identify which part governs the long-time behavior and briefly relate your findings to resonance and to the use of the Laplace transform for such problems.
\end{problem}

\begin{solution}
We first rewrite the differential equation in a convenient form. Dividing by $m$ and introducing
\[
2\gamma = \frac{c}{m}, \qquad \omega_0^2 = \frac{k}{m},
\]
we obtain
\[
x''(t) + 2\gamma x'(t) + \omega_0^2 x(t) = \frac{F_0}{m} \sin(\omega t).
\]
Since $\frac{F_0}{m}$ is just a constant amplitude, we rename it as $F_0$ for simplicity and work with
\begin{equation}\label{eq:standard}
x''(t) + 2\gamma x'(t) + \omega_0^2 x(t) = F_0 \sin(\omega t), \qquad x(0)=0,\quad x'(0)=0.
\end{equation}

The central idea of the Laplace-transform method is to convert this initial value problem into an algebraic equation for the Laplace transform $X(s) = \mathcal{L}\{x(t)\}(s)$, solve for $X(s)$, and then invert the transform. A key advantage is that the initial conditions are incorporated automatically.

Taking the Laplace transform of both sides of \eqref{eq:standard}, and using
\[
\mathcal{L}\{x'(t)\}(s) = s X(s) - x(0), \qquad
\mathcal{L}\{x''(t)\}(s) = s^2 X(s) - s x(0) - x'(0),
\]
together with the given initial conditions $x(0)=0$ and $x'(0)=0$, we obtain
\[
\mathcal{L}\{x''\}(s) = s^2 X(s), \qquad \mathcal{L}\{x'\}(s) = s X(s).
\]
Therefore
\[
s^2 X(s) + 2\gamma \, s X(s) + \omega_0^2 X(s)
  = F_0\,\mathcal{L}\{\sin(\omega t)\}(s)
  = F_0\,\frac{\omega}{s^2+\omega^2}.
\]
Factoring $X(s)$ on the left-hand side gives
\[
\bigl(s^2 + 2\gamma s + \omega_0^2\bigr) X(s)
  = F_0\,\frac{\omega}{s^2+\omega^2},
\]
so
\[
X(s) = \frac{F_0\,\omega}{(s^2+2\gamma s+\omega_0^2)(s^2+\omega^2)}.
\]

We now assume the underdamped regime $0<\gamma<\omega_0$ and introduce the damped natural frequency
\[
\Omega = \sqrt{\omega_0^2 - \gamma^2}>0.
\]
Completing the square in the quadratic $s^2+2\gamma s+\omega_0^2$ yields
\[
s^2+2\gamma s+\omega_0^2 = (s+\gamma)^2 + (\omega_0^2-\gamma^2)
  = (s+\gamma)^2 + \Omega^2,
\]
so we can rewrite $X(s)$ as
\[
X(s) = \frac{F_0\,\omega}{(s^2+\omega^2)\bigl((s+\gamma)^2+\Omega^2\bigr)}.
\]

To invert the transform, it is convenient to decompose $X(s)$ into partial fractions adapted to the standard Laplace transform formulas for sines and cosines and to the shift rule. We seek constants $A,B,C,D$ such that
\begin{equation}\label{eq:pf}
X(s) = \frac{A s + B}{s^2+\omega^2} \;+\; \frac{C (s+\gamma) + D}{(s+\gamma)^2+\Omega^2}.
\end{equation}
Multiplying both sides of \eqref{eq:pf} by $(s^2+\omega^2)\bigl((s+\gamma)^2+\Omega^2\bigr)$, we obtain the identity
\[
F_0 \omega = (A s + B)\bigl((s+\gamma)^2+\Omega^2\bigr)
             + \bigl(C (s+\gamma) + D\bigr)(s^2+\omega^2),
\]
which must hold for all $s$. Expanding and equating coefficients of powers of $s$ gives a linear system for $A,B,C,D$.

First, recall that $(s+\gamma)^2+\Omega^2 = s^2 + 2\gamma s + \omega_0^2$. Then
\begin{align*}
(A s + B)(s^2 + 2\gamma s + \omega_0^2)
 &= A s^3 + (2A\gamma + B)s^2 + (A\omega_0^2 + 2B\gamma)s + B\omega_0^2,\\
\bigl(C(s+\gamma) + D\bigr)(s^2+\omega^2)
 &= C s^3 + (C\gamma + D)s^2 + C\omega^2 s + (C\gamma + D)\omega^2.
\end{align*}
Adding these two expressions, we obtain
\[
F_0 \omega
  = (A+C)s^3
    + \bigl(2A\gamma + B + C\gamma + D\bigr)s^2
    + \bigl(A\omega_0^2 + 2B\gamma + C\omega^2\bigr)s
    + \bigl(B\omega_0^2 + (C\gamma + D)\omega^2\bigr).
\]
Since the right-hand side must equal the constant polynomial $F_0 \omega$, the coefficients of $s^3$, $s^2$, and $s$ must vanish, and the constant term must equal $F_0\omega$. Thus we have
\begin{align*}
s^3:& \quad A + C = 0,\\
s^2:& \quad 2A\gamma + B + C\gamma + D = 0,\\
s^1:& \quad A\omega_0^2 + 2B\gamma + C\omega^2 = 0,\\
s^0:& \quad B\omega_0^2 + (C\gamma + D)\omega^2 = F_0 \omega.
\end{align*}

From $A + C = 0$ we obtain $C = -A$. Substituting this into the $s^2$-equation yields
\[
2A\gamma + B - A\gamma + D = 0
 \quad\Longrightarrow\quad
\gamma A + B + D = 0
 \quad\Longrightarrow\quad
D = -\gamma A - B.
\]
Using $C=-A$ in the $s^1$-equation gives
\[
A\omega_0^2 + 2B\gamma - A\omega^2 = 0
 \quad\Longrightarrow\quad
A(\omega_0^2-\omega^2) + 2\gamma B = 0
 \quad\Longrightarrow\quad
B = -\frac{A(\omega_0^2-\omega^2)}{2\gamma}.
\]
Finally, in the constant term we substitute $C=-A$ and $D=-\gamma A - B$:
\[
B\omega_0^2 + (C\gamma + D)\omega^2 = B\omega_0^2 + (-A\gamma -\gamma A - B)\omega^2
  = B(\omega_0^2 - \omega^2) - 2\gamma A \omega^2.
\]
Setting this equal to $F_0 \omega$ gives
\[
B(\omega_0^2 - \omega^2) - 2\gamma A \omega^2 = F_0 \omega.
\]
Sub
stituting the expression for $B$ into this equation yields
\[
-\frac{A(\omega_0^2-\omega^2)^2}{2\gamma} - 2\gamma A \omega^2 = F_0 \omega.
\]
Factor out $A$ and combine terms:
\[
-A\left(\frac{(\omega_0^2-\omega^2)^2}{2\gamma} + 2\gamma \omega^2\right) = F_0 \omega,
\]
so
\[
A\left(\frac{(\omega_0^2-\omega^2)^2 + 4\gamma^2 \omega^2}{2\gamma}\right) = -F_0 \omega.
\]
Thus
\[
A = -\frac{2\gamma F_0 \omega}{(\omega_0^2-\omega^2)^2 + 4\gamma^2 \omega^2}.
\]
Recall that
\[
B = -\frac{A(\omega_0^2-\omega^2)}{2\gamma},\quad
C = -A,\quad
D = -\gamma A - B.
\]
Substituting for $A$ gives
\[
C = \frac{2\gamma F_0 \omega}{(\omega_0^2-\omega^2)^2 + 4\gamma^2 \omega^2},
\]
\[
B = \frac{F_0 \omega(\omega_0^2-\omega^2)}{(\omega_0^2-\omega^2)^2 + 4\gamma^2 \omega^2},
\]
\[
D = \frac{F_0 \omega\bigl(2\gamma^2 - \omega_0^2 + \omega^2\bigr)}
           {(\omega_0^2-\omega^2)^2 + 4\gamma^2 \omega^2}.
\]

For convenience, define the common denominator
\[
\Delta(\omega) = (\omega_0^2-\omega^2)^2 + 4\gamma^2 \omega^2 > 0.
\]

Now we invert the Laplace transform. From \eqref{eq:pf} we have
\[
X(s) = \frac{A s + B}{s^2+\omega^2} \;+\; \frac{C (s+\gamma) + D}{(s+\gamma)^2+\Omega^2},
\]
so by linearity and the standard formulas
\[
\mathcal{L}^{-1}\left\{\frac{s}{s^2+\alpha^2}\right\} = \cos(\alpha t), \quad
\mathcal{L}^{-1}\left\{\frac{1}{s^2+\alpha^2}\right\} = \frac{1}{\alpha}\sin(\alpha t),
\]
and the shift rule
\[
\mathcal{L}^{-1}\{F(s+\gamma)\}(t) = e^{-\gamma t} f(t),
\]
we get
\begin{align*}
x(t)
 &= A\cos(\omega t) + \frac{B}{\omega}\sin(\omega t)
    + C e^{-\gamma t}\cos(\Omega t)
    + \frac{D}{\Omega} e^{-\gamma t}\sin(\Omega t).
\end{align*}
Substituting $A,B,C,D$ in terms of $\Delta(\omega)$, we can separate $x(t)$ naturally into a steady-state and a transient part.

\medskip\noindent
\textbf{Steady-state term.}
The terms involving $\sin(\omega t)$ and $\cos(\omega t)$ (with no exponential factor) form the steady-state solution:
\begin{align*}
x_{\text{ss}}(t)
 &= A\cos(\omega t) + \frac{B}{\omega}\sin(\omega t)\\
 &= \frac{F_0}{\Delta(\omega)}\Bigl[-2\gamma\omega\cos(\omega t)
     + (\omega_0^2-\omega^2)\sin(\omega t)\Bigr].
\end{align*}
This is a sinusoidal motion at the driving frequency $\omega$ that persists for large $t$.

\medskip\noindent
\textbf{Transient term.}
The terms multiplied by $e^{-\gamma t}$ form the transient:
\begin{align*}
x_{\text{tr}}(t)
 &= C e^{-\gamma t}\cos(\Omega t)
    + \frac{D}{\Omega}e^{-\gamma t}\sin(\Omega t)\\
 &= e^{-\gamma t}\left[
      \frac{2\gamma F_0 \omega}{\Delta(\omega)}\cos(\Omega t)
      + \frac{F_0 \omega\bigl(2\gamma^2 - \omega_0^2 + \omega^2\bigr)}
             {\Omega\,\Delta(\omega)}\sin(\Omega t)\right].
\end{align*}
Because of the factor $e^{-\gamma t}$, we have
\[
\lim_{t\to\infty} x_{\text{tr}}(t) = 0.
\]

\medskip\noindent
\textbf{Final solution and long-time behavior.}
Combining the two parts,
\[
x(t) = x_{\text{tr}}(t) + x_{\text{ss}}(t),
\]
where
\[
x_{\text{tr}}(t) = e^{-\gamma t}\left[
      \frac{2\gamma F_0 \omega}{\Delta(\omega)}\cos(\Omega t)
      + \frac{F_0 \omega\bigl(2\gamma^2 - \omega_0^2 + \omega^2\bigr)}
             {\Omega\,\Delta(\omega)}\sin(\Omega t)\right],
\]
\[
x_{\text{ss}}(t) =
\frac{F_0}{\Delta(\omega)}\Bigl[-2\gamma\omega\cos(\omega t)
     + (\omega_0^2-\omega^2)\sin(\omega t)\Bigr].
\]

The \emph{transient} $x_{\text{tr}}(t)$ decays to zero as $t\to\infty$ because of the factor $e^{-\gamma t}$; it represents damped oscillations at the damped natural frequency $\Omega$. The \emph{steady-state} term $x_{\text{ss}}(t)$ persists for large $t$ and oscillates at the driving frequency $\omega$ with a phase shift relative to the forcing.

Writing $x_{\text{ss}}$ in the form
\[
x_{\text{ss}}(t) = R(\omega)\,\sin(\omega t - \delta(\omega)),
\]
one finds
\[
R(\omega)
  = \frac{F_0}{\sqrt{(\omega_0^2-\omega^2)^2 + 4\gamma^2\omega^2}},
\]
so as the damping $\gamma$ becomes small and the driving frequency $\omega$ approaches $\omega_0$, the denominator becomes very small and the amplitude $R(\omega)$ becomes very large: this is the resonance phenomenon.

From the Laplace-transform viewpoint, the poles of $X(s)$ at $s=-\gamma\pm i\Omega$ produce the transient (damped natural oscillations), while the poles at $s=\pm i\omega$ generate the steady-state response at the driving frequency. The method systematically incorporates initial conditions and makes the separation between transient and steady-state behavior transparent in terms of these poles.

\end{solution}

% ===== Example 3: Step and Impulse Responses of a Linear System (inquiry-based) =====
\begin{problem}[Step and Impulse Responses of a Linear System]
Many simple physical systems---for example an \emph{RC circuit} (a resistor and a capacitor in series)---can be modeled by a first-order linear differential equation.  If the input is a voltage source \(f(t)\) that can jump suddenly (a step) or act for a very short time (an impulse), then analyzing the response \(y(t)\) directly in the time domain can be uncomfortable, because of discontinuities and generalized functions (distributions).  The Laplace transform allows us to treat these inputs on essentially the same footing as smooth ones, by turning the differential equation into an algebraic equation.

In this problem we study the system
\[
y'(t) + y(t) = f(t), \qquad t>0, \qquad y(0)=0,
\]
as a model of such a circuit.  We will compute its response to a unit step input and to a Dirac delta impulse, and compare the two.

\medskip

(a) \textbf{Warm-up: the homogeneous system.}  
Consider first the homogeneous equation
\[
y'(t) + y(t) = 0, \qquad t>0, \qquad y(0)=y_0.
\]
Solve this initial value problem explicitly for \(y(t)\).  Describe qualitatively how solutions behave as \(t\to\infty\).

\smallskip
Hint: Separate variables or use the standard method for constant-coefficient linear equations.

\medskip

(b) \textbf{Laplace domain formulation for a general input.}  
Now return to the forced equation
\[
y'(t) + y(t) = f(t), \qquad t>0, \qquad y(0)=0,
\]
where \(f\) is some input that is zero for \(t<0\).

(i) Take the Laplace transform of both sides of the differential equation and use the formula for the Laplace transform of a derivative,
\[
\mathcal{L}\{y'(t)\}(s) = sY(s) - y(0),
\]
to obtain an algebraic relation between \(Y(s)=\mathcal{L}\{y\}(s)\) and \(F(s)=\mathcal{L}\{f\}(s)\).

(ii) Solve this algebraic relation for \(Y(s)\) in terms of \(F(s)\).  Write your answer in the form
\[
Y(s) = H(s)\,F(s)
\]
and identify the function \(H(s)\).  This function \(H\) is called the \emph{transfer function} of the system.

\smallskip
% Hint: Be careful to use the given initial condition \(y(0)=0\).

\medskip

(c) \textbf{Step response.}  
Let the input be a unit step function
\[
f(t) = u(t) = 
\begin{cases}
0, & t<0,\\
1, & t\ge 0.
\end{cases}
\]
This models suddenly applying a constant voltage at time \(t=0\) to a circuit that was previously off.

(i) Compute the Laplace transform \(F(s)=\mathcal{L}\{u(t)\}(s)\).

(ii) Use your expression for \(Y(s)\) from part (b) to find \(Y_{\text{step}}(s)\), the Laplace transform of the corresponding output \(y_{\text{step}}(t)\).

(iii) Invert the Laplace transform to find an explicit formula for \(y_{\text{step}}(t)\) for \(t>0\).  Sketch the graph of \(y_{\text{step}}(t)\).  How does this compare with your qualitative description from part (a)?

\smallskip
Hint: You should obtain a simple rational function in \(s\); use partial fractions and a Laplace table.

\medskip

(d) \textbf{Impulse response.}  
Now let the input be a Dirac delta impulse:
\[
f(t) = \delta(t).
\]
You can think of this as an “infinitely short, unit area” pulse of input at time \(t=0\).

(i) Recall (or accept for this problem) that \(\mathcal{L}\{\delta(t)\}(s) = 1\).  Use this together with your formula from part (b) to find \(Y_{\text{imp}}(s)\), the Laplace transform of the impulse response \(y_{\text{imp}}(t)\).

(ii) Invert the Laplace transform to find \(y_{\text{imp}}(t)\) for \(t>0\).

(iii) Compare your formulas for \(y_{\text{step}}(t)\) and \(y_{\text{imp}}(t)\).  How are they related?  In particular, compute the (distributional) derivative of the step response and relate it to the impulse response.

\smallskip
Hint: Differentiate your formula for \(y_{\text{step}}(t)\) for \(t>0\).  What happens at \(t=0\), where the step function jumps?

\medskip

(e) \textbf{Extensions and “what if” questions.}

(i) Suppose instead the system is
\[
y'(t) + a\,y(t) = f(t), \qquad y(0)=0,
\]
where \(a>0\) is a constant.  Without doing all calculations in full detail, predict how the step and impulse responses will change as \(a\) varies.  What happens in the limits \(a\to 0^+\) and \(a\to\infty\)?

(ii) One of the central ideas of linear systems theory is that, for a linear time-invariant system, the output for a general input \(f\) can be written as a convolution of \(f\) with the impulse response.  Using the relation
\[
Y(s) = H(s)\,F(s)
\]
from part (b), and the Laplace transform identity \(\mathcal{L}\{(f*g)(t)\} = F(s)G(s)\), explain informally why the output \(y\) must be of the form
\[
y(t) = \int_0^t h(t-\tau)\,f(\tau)\,d\tau,
\]
where \(h(t)\) is the impulse response you found in part (d).  (You do not need to give a fully rigorous proof, but outline the main idea.)

\end{problem}

% ===== Example 3: Step and Impulse Responses of a Linear System (full solution) =====
\begin{problem}[Step and Impulse Responses of a Linear System]
Consider the linear system
\[
y'(t) + y(t) = f(t), \qquad t>0, \qquad y(0)=0,
\]
where \(f(t)\) is an input that vanishes for \(t<0\).

(a) Use the Laplace transform to show that for any such input, the Laplace transform \(Y(s)\) of the output satisfies
\[
Y(s) = \frac{1}{s+1}\,F(s),
\]
where \(F(s)\) is the Laplace transform of \(f(t)\).

(b) Let \(f(t) = u(t)\) be the unit step function.  Find the corresponding output \(y_{\mathrm{step}}(t)\) (the step response) for \(t>0\).

(c) Let \(f(t) = \delta(t)\) be the Dirac delta impulse at \(t=0\).  Using \(\mathcal{L}\{\delta(t)\}(s) = 1\), find the corresponding output \(y_{\mathrm{imp}}(t)\) (the impulse response) for \(t>0\).

(d) Show that, for this system, the impulse response is the (distributional) derivative of the step response.  Briefly explain how this relation reflects the way the Laplace transform converts differential equations with discontinuous or impulsive inputs into algebraic equations.

\end{problem}

\begin{solution}
We study a first-order linear time-invariant system driven by inputs that can be discontinuous (a unit step) or even impulsive (a Dirac delta).  The Laplace transform will allow us to treat these inputs in an algebraic way, bypassing some of the technicalities of distributions in the time domain.

\medskip

\noindent\textbf{(a) General Laplace-domain relation.}
We are given
\[
y'(t) + y(t) = f(t), \qquad t>0, \qquad y(0)=0,
\]
with \(f(t)=0\) for \(t<0\).  Let \(Y(s) = \mathcal{L}\{y(t)\}(s)\) and \(F(s) = \mathcal{L}\{f(t)\}(s)\).

Taking the Laplace transform of both sides and using linearity gives
\[
\mathcal{L}\{y'(t)\}(s) + \mathcal{L}\{y(t)\}(s) = \mathcal{L}\{f(t)\}(s).
\]
By the standard formula for the Laplace transform of a derivative,
\[
\mathcal{L}\{y'(t)\}(s) = sY(s) - y(0).
\]
Since \(y(0)=0\), we obtain
\[
sY(s) + Y(s) = F(s).
\]
Factoring \(Y(s)\) yields
\[
(s+1)Y(s) = F(s),
\]
so
\[
Y(s) = \frac{1}{s+1}\,F(s).
\]
The function
\[
H(s) = \frac{1}{s+1}
\]
is the \emph{transfer function} of the system: it describes how each Laplace-frequency component of the input is “scaled” to produce the output.  This exhibits the central Laplace-transform idea for linear time-invariant systems: the differential equation has been converted into a simple algebraic relation between \(Y\) and \(F\).

\medskip

\noindent\textbf{(b) Step response.}
Now take the input to be the unit step function
\[
f(t) = u(t) = 
\begin{cases}
0, & t<0,\\
1, & t\ge 0.
\end{cases}
\]
Its Laplace transform is well known:
\[
F(s) = \mathcal{L}\{u(t)\}(s) = \int_0^\infty e^{-st}\cdot 1\,dt = \frac{1}{s}, \qquad \Re(s)>0.
\]
Using the relation from part (a),
\[
Y_{\text{step}}(s) = \frac{1}{s+1} \cdot \frac{1}{s} = \frac{1}{s(s+1)}.
\]

To find \(y_{\text{step}}(t)\), we invert this Laplace transform.  We decompose by partial fractions:
\[
\frac{1}{s(s+1)} = \frac{A}{s} + \frac{B}{s+1}.
\]
Solving \(1 = A(s+1)+Bs\) gives \(A=1\), \(B=-1\).  Thus
\[
\frac{1}{s(s+1)} = \frac{1}{s} - \frac{1}{s+1}.
\]
Using the standard table,
\[
\mathcal{L}^{-1}\left\{\frac{1}{s}\right\}(t) = 1 \quad\text{and}\quad
\mathcal{L}^{-1}\left\{\frac{1}{s+1}\right\}(t) = e^{-t},
\]
for \(t>0\).  Therefore,
\[
y_{\text{step}}(t) = 1 - e^{-t}, \qquad t>0.
\]

This is the familiar exponential “charging” curve: the response starts at \(y_{\text{step}}(0+)=0\) and increases monotonically toward the steady state value \(1\) as \(t\to\infty\).  This behavior matches the homogeneous solution \(y(t)=Ce^{-t}\) found by solving \(y'+y=0\): the homogeneous part decays exponentially, while the effect of the constant forcing produces a constant steady state.

\medskip

\noindent\textbf{(c) Impulse response.}
Next let the input be a Dirac delta impulse:
\[
f(t) = \delta(t).
\]
The Dirac delta is not an ordinary function but a distribution, characterized by
\[
\int_{-\infty}^{\infty} \delta(t)\,\varphi(t)\,dt = \varphi(0)
\]
for every smooth test function \(\varphi\).  Its Laplace transform is
\[
\mathcal{L}\{\delta(t)\}(s) = \int_0^\infty e^{-st}\,\delta(t)\,dt = 1.
\]

Thus \(F(s) = 1\), and from the general relation \(Y(s)=\frac{1}{s+1}F(s)\) we obtain
\[
Y_{\text{imp}}(s) = \frac{1}{s+1}.
\]
The inverse Laplace transform is again standard:
\[
y_{\text{imp}}(t) = \mathcal{L}^{-1}\left\{\frac{1}{s+1}\right\}(t)
= e^{-t}, \qquad t>0.
\]

This function \(y_{\text{imp}}(t) = e^{-t}\) (for \(t>0\)) is called the \emph{impulse response} of the system.  Physically, it represents the output when the system, initially at rest, is excited by an “infinitely short” unit-area input at \(t=0\).  The system responds with an immediate jump in its state and then relaxes exponentially back toward zero.

\medskip

\noindent\textbf{(d) Relation between step and impulse responses.}
We have found
\[
y_{\text{step}}(t) = 1 - e^{-t}, \qquad t>0,
\]
and
\[
y_{\text{imp}}(t) = e^{-t}, \qquad t>0.
\]

If we differentiate \(y_{\text{step}}(t)\) for \(t>0\), we obtain
\[
\frac{d}{dt} y_{\text{step}}(t) = \frac{d}{dt}(1 - e^{-t}) = e^{-t} = y_{\text{imp}}(t), \qquad t>0.
\]
So for strictly positive times, the impulse response is the ordinary derivative of the step response.

To fully account for the behavior at \(t=0\), we recall that the true step response should be written as
\[
y_{\text{step}}(t) = \bigl(1 - e^{-t}\bigr)u(t),
\]
where \(u(t)\) is the unit step function, so that \(y_{\text{step}}(t)=0\) for \(t<0\).  In the sense of distributions, the derivative of \(u(t)\) is the Dirac delta:
\[
\frac{d}{dt}u(t) = \delta(t).
\]
Using the product rule in the distributional sense,
\[
\frac{d}{dt}\bigl(1 - e^{-t}\bigr)u(t)
= \bigl(1 - e^{-t}\bigr)\delta(t) + e^{-t}u(t).
\]
The term involving \(\delta(t)\) accounts for the immediate jump at \(t=0\), while for \(t>0\) the derivative reduces to \(e^{-t}\).  Thus, up to the singular term at \(t=0\), the impulse response and the derivative of the step response coincide.  More succinctly, one often writes informally
\[
\frac{d}{dt} y_{\text{step}}(t) = y_{\text{imp}}(t),
\]
understanding this as an identity of distributions.

This relation reflects a deeper structural fact: in a linear time-invariant system, the step input \(u(t)\) is the time integral of the impulse input \(\delta(t)\), and the output operator is linear.  Consequently, the output for a step is an integral (in time) of the output for an impulse, and differentiating in time reverses that integration.

In the Laplace domain, this connection appears very naturally.  The step input has Laplace transform \(1/s\), and the impulse input has Laplace transform \(1\).  Thus
\[
Y_{\text{step}}(s) = H(s)\cdot \frac{1}{s} 
\quad\text{and}\quad
Y_{\text{imp}}(s) = H(s)\cdot 1,
\]
with \(H(s)=1/(s+1)\).  Since multiplication by \(s\) in the Laplace domain corresponds to time differentiation in the original domain (again in the distributional sense),
\[
\mathcal{L}\left\{\frac{d}{dt}y_{\text{step}}(t)\right\}(s) 
= sY_{\text{step}}(s) - y_{\text{step}}(0)
= s\left(H(s)\cdot\frac{1}{s}\right) - y_{\text{step}}(0)
= H(s) - y_{\text{step}}(0).
\]
Because the system is initially at rest, \(y_{\text{step}}(0)=0\), and so
\[
\mathcal{L}\left\{\frac{d}{dt}y_{\text{step}}(t)\right\}(s) = H(s) = Y_{\text{imp}}(s).
\]
Thus, at the level of Laplace transforms, the identity “impulse response = derivative of step response” becomes the simple algebraic equality \(H(s) = H(s)\).

\medskip

\noindent\textbf{Conceptual summary.}
This example illustrates a central theme of the Laplace transform in the analysis of linear systems.  The time-domain differential equation
\[
y'(t) + y(t) = f(t)
\]
is turned into the algebraic relation
\[
Y(s) = H(s)F(s)
\]
in the Laplace domain.  Discontinuous inputs (like the step) and distributional inputs (like the impulse) are naturally accommodated through their Laplace transforms, \(\mathcal{L}\{u\}(s)=1/s\) and \(\mathcal{L}\{\delta\}(s)=1\).  The step and impulse responses emerge as particularly important cases, and their close relationship (the impulse response is the derivative of the step response) becomes transparent both in time and in the Laplace domain.  More generally, for any linear time-invariant system, the Laplace transform provides a powerful framework in which solving differential equations with complicated inputs reduces to algebraic manipulation of transforms.

\end{solution}

% ===== Example 4: Convolution and the Laplace Transform (inquiry-based) =====
\begin{problem}[Convolution and the Laplace Transform]
In many physical systems (such as electrical circuits, mechanical vibration systems, and thermal diffusion problems), the system is modeled as \emph{linear} and \emph{time-invariant} (LTI). For such systems, a short “impulse” input at time $t=0$ produces a characteristic response called the \emph{impulse response}. If $h(t)$ denotes this impulse response, then the output $y(t)$ corresponding to a general input $f(t)$ is given by the \emph{convolution} of $h$ and $f$. In this problem we explore how to compute this convolution directly in the time domain and then see how the Laplace transform simplifies the computation by turning convolution into multiplication.

Consider a causal LTI system with impulse response
\[
h(t) = e^{-t}u(t),
\]
where $u(t)$ is the unit step function. For any input $f(t)$, the output is
\[
y(t) = (h * f)(t) = \int_0^t h(t-\tau)\, f(\tau)\, d\tau
      = \int_0^t e^{-(t-\tau)} f(\tau)\, d\tau,
\]
for $t \ge 0$.

\smallskip

(a) Suppose the input is a unit step, $f(t) = u(t)$. Write the convolution integral for $y(t)$ explicitly and compute $y(t)$ as a function of $t$. Then, sketch both $f(t)$ and $y(t)$ on the same axes for $t \ge 0$.  
\emph{Hint:} Remember that $u(t) = 1$ for $t>0$, so you may drop the $u(t)$ factor inside the integral once the limits are fixed.

\smallskip

(b) Now analyze the same step input using the Laplace transform. Compute $H(s) = \mathcal{L}\{h(t)\}(s)$ and $F(s) = \mathcal{L}\{f(t)\}(s)$, and form the product
\[
Y(s) = H(s)F(s).
\]
Then use partial fractions to find an explicit formula for $Y(s)$ and invert the Laplace transform to recover $y(t)$. Compare your result with part (a).  
\emph{Hint:} Recall that $\mathcal{L}\{e^{-t}u(t)\}(s) = \dfrac{1}{s+1}$ and $\mathcal{L}\{u(t)\}(s) = \dfrac{1}{s}$ for $\operatorname{Re}(s)$ sufficiently large.

\smallskip

(c) Next, consider a \emph{ramp} input
\[
f(t) = t\,u(t).
\]
First, write the convolution integral
\[
y(t) = \int_0^t e^{-(t-\tau)} \,\tau\, d\tau
\]
for this new input. Simplify the integrand and set up a plan to evaluate the integral. You may attempt to compute the integral directly.  
\emph{Hint:} Factor out $e^{-t}$ to obtain $y(t) = e^{-t}\displaystyle\int_0^t e^{\tau}\tau\, d\tau$. For the remaining integral, consider integration by parts.

\smallskip

(d) Now use the Laplace transform to find the response to the ramp input. Compute $F(s) = \mathcal{L}\{t\,u(t)\}(s)$, then form
\[
Y(s) = H(s)F(s),
\]
and use partial fractions to find $Y(s)$. Invert the Laplace transform to obtain an explicit expression for $y(t)$ in the time domain. If you computed the integral in part (c), check that your answer here matches the time-domain computation.  
\emph{Hint:} You may recall $\mathcal{L}\{t\,u(t)\}(s) = \dfrac{1}{s^2}$. For partial fractions, try the ansatz
\[
\frac{1}{s^2(s+1)} = \frac{A}{s} + \frac{B}{s^2} + \frac{C}{s+1}.
\]

\smallskip

(e) \textbf{Extensions and “what if” questions.}
\begin{enumerate}
  \item[(i)] Suppose instead that the system has impulse response
    \[
    h_\alpha(t) = e^{-\alpha t}u(t)
    \]
    for some constant $\alpha > 0$. Compute $H_\alpha(s) = \mathcal{L}\{h_\alpha(t)\}(s)$. For a step input $f(t) = u(t)$, use the Laplace transform to find the corresponding output $y_\alpha(t)$. How does the parameter $\alpha$ affect the \emph{speed} of the system's response?
    \item[(ii)] The example above suggests that the Laplace transform turns convolution into multiplication. Starting from the definition
    \[
    (h*f)(t) = \int_0^t h(t-\tau)f(\tau)\, d\tau,
    \]
    and the definition of the Laplace transform, sketch how one might prove the \emph{convolution theorem}
    \[
    \mathcal{L}\{h*f\}(s) = \mathcal{L}\{h\}(s)\,\mathcal{L}\{f\}(s)
    \]
    for reasonably nice functions supported on $[0,\infty)$.  
    \emph{Hint:} Write
    \[
    \mathcal{L}\{h*f\}(s) = \int_0^\infty e^{-st} \left( \int_0^t h(t-\tau)f(\tau)\, d\tau \right) dt
    \]
    and consider interchanging the order of integration in the $(t,\tau)$-plane.
\end{enumerate}

\end{problem}

% ===== Example 4: Convolution and the Laplace Transform (full solution) =====
\begin{problem}[Convolution and the Laplace Transform]
Consider a causal LTI system with impulse response $h(t) = e^{-t}u(t)$, where $u(t)$ is the unit step function, and output
\[
y(t) = (h*f)(t) = \int_0^t e^{-(t-\tau)} f(\tau)\, d\tau
\]
for an input $f(t)$.

\begin{enumerate}
  \item[(a)] For the step input $f(t) = u(t)$, compute $y(t)$ directly from the convolution integral.
  \item[(b)] For the same step input, compute $y(t)$ using Laplace transforms: find $H(s)$, $F(s)$, and $Y(s) = H(s)F(s)$, then invert the Laplace transform. Verify that this agrees with part (a).
  \item[(c)] For the ramp input $f(t) = t\,u(t)$, compute $y(t)$ using Laplace transforms. (You may optionally verify your answer by direct evaluation of the convolution integral.)
\end{enumerate}
Explain briefly how this example illustrates the use of the Laplace transform to handle convolution in LTI systems.

\end{problem}

\begin{solution}
We are given a causal LTI system with impulse response
\[
h(t) = e^{-t}u(t),
\]
so for $t \ge 0$ the output corresponding to an input $f$ is
\[
y(t) = (h*f)(t) = \int_0^t e^{-(t-\tau)} f(\tau)\, d\tau.
\]
This is the standard convolution representation for a causal LTI system.

\medskip

\noindent\textbf{(a) Step input: direct convolution.}

Let $f(t) = u(t)$, the unit step function. For $t > 0$, we have $f(\tau) = 1$ for $0 \le \tau \le t$, so the convolution integral becomes
\[
y(t) = \int_0^t e^{-(t-\tau)}\cdot 1\, d\tau
     = \int_0^t e^{-(t-\tau)}\, d\tau.
\]
It is convenient to pull out the factor depending only on $t$:
\[
e^{-(t-\tau)} = e^{-t} e^{\tau},
\]
so
\[
y(t) = e^{-t} \int_0^t e^{\tau}\, d\tau
     = e^{-t}\bigl[e^{\tau}\bigr]_{\tau=0}^{\tau=t}
     = e^{-t}(e^t - 1)
     = 1 - e^{-t}.
\]
Since the system is causal and the input is zero for $t<0$, we write
\[
y(t) = (1 - e^{-t})u(t).
\]

\medskip

\noindent\textbf{(b) Step input: Laplace transform method.}

We now compute the same response using Laplace transforms. First compute the Laplace transforms of $h$ and $f$.

For $h(t) = e^{-t}u(t)$, we recall the standard Laplace transform
\[
\mathcal{L}\{e^{at}u(t)\}(s) = \frac{1}{s-a}, \quad \operatorname{Re}(s) > \operatorname{Re}(a).
\]
Here $a = -1$, so
\[
H(s) = \mathcal{L}\{h(t)\}(s) = \mathcal{L}\{e^{-t}u(t)\}(s) = \frac{1}{s+1}.
\]
For the step input $f(t) = u(t)$ we have
\[
F(s) = \mathcal{L}\{u(t)\}(s) = \frac{1}{s}, \quad \operatorname{Re}(s) > 0.
\]
The Laplace transform of the output, by the convolution theorem, is the product
\[
Y(s) = H(s)F(s) = \frac{1}{s+1}\cdot \frac{1}{s} = \frac{1}{s(s+1)}.
\]

To invert this, we perform a partial fraction decomposition:
\[
\frac{1}{s(s+1)} = \frac{A}{s} + \frac{B}{s+1}.
\]
Multiplying both sides by $s(s+1)$ gives
\[
1 = A(s+1) + Bs = (A+B)s + A.
\]
Matching coefficients, we obtain
\[
A = 1, \qquad A + B = 0 \quad \Rightarrow \quad B = -1.
\]
Hence
\[
Y(s) = \frac{1}{s} - \frac{1}{s+1}.
\]
Taking inverse Laplace transforms term by term, we use
\[
\mathcal{L}^{-1}\left\{\frac{1}{s}\right\}(t) = u(t),
\qquad
\mathcal{L}^{-1}\left\{\frac{1}{s+1}\right\}(t) = e^{-t}u(t).
\]
Therefore,
\[
y(t) = \mathcal{L}^{-1}\{Y(s)\}(t)
     = \left(1 - e^{-t}\right)u(t).
\]
This agrees exactly with the result obtained in part (a). This illustrates how the convolution integral can be replaced by an algebraic product in the Laplace domain, often simplifying the computation.

\medskip

\noindent\textbf{(c) Ramp input: Laplace transform method.}

Now let $f(t) = t\,u(t)$, the ramp function. We again use the Laplace transform to find the output.

First, recall the Laplace transform of $t\,u(t)$:
\[
F(s) = \mathcal{L}\{t\,u(t)\}(s) = \frac{1}{s^2}, \quad \operatorname{Re}(s) > 0.
\]
We already have
\[
H(s) = \frac{1}{s+1}.
\]
Hence the Laplace transform of the output is
\[
Y(s) = H(s)F(s) = \frac{1}{s+1} \cdot \frac{1}{s^2} = \frac{1}{s^2(s+1)}.
\]

We decompose this into partial fractions. We seek constants $A, B, C$ such that
\[
\frac{1}{s^2(s+1)} = \frac{A}{s} + \frac{B}{s^2} + \frac{C}{s+1}.
\]
Multiplying by $s^2(s+1)$ yields
\[
1 = A s(s+1) + B(s+1) + C s^2.
\]
Expanding the right-hand side:
\[
A s(s+1) = A(s^2 + s), \quad B(s+1) = Bs + B,
\]
so
\[
1 = (A + C)s^2 + (A + B)s + B.
\]
Matching coefficients of powers of $s$ gives the system
\[
\begin{cases}
A + C = 0, \\
A + B = 0, \\
B = 1.
\end{cases}
\]
From $B = 1$ we get $A = -1$, and then $C = -A = 1$. Thus
\[
Y(s) = -\,\frac{1}{s} + \frac{1}{s^2} + \frac{1}{s+1}.
\]

We now invert term by term:
\[
\mathcal{L}^{-1}\left\{-\frac{1}{s}\right\}(t) = -u(t), \quad
\mathcal{L}^{-1}\left\{\frac{1}{s^2}\right\}(t) = t\,u(t), \quad
\mathcal{L}^{-1}\left\{\frac{1}{s+1}\right\}(t) = e^{-t}u(t).
\]
Therefore
\[
y(t) = \bigl(-1 + t + e^{-t}\bigr)u(t)
     = (t - 1 + e^{-t})u(t).
\]

If we wish, we can confirm this by direct evaluation of the convolution integral. Starting from
\[
y(t) = \int_0^t e^{-(t-\tau)} \,\tau\, d\tau,
\]
we write
\[
e^{-(t-\tau)} = e^{-t}e^{\tau}
\]
to obtain
\[
y(t) = e^{-t}\int_0^t \tau e^{\tau}\, d\tau.
\]
An antiderivative of $\tau e^{\tau}$ is $e^{\tau}(\tau - 1)$, so
\[
\int_0^t \tau e^{\tau}\, d\tau
= \bigl[e^{\tau}(\tau - 1)\bigr]_{\tau=0}^{\tau=t}
= e^{t}(t-1) - e^{0}(-1)
= e^{t}(t-1) + 1.
\]
Multiplying by $e^{-t}$ gives
\[
y(t) = e^{-t}\bigl(e^{t}(t-1) + 1\bigr) = (t-1) + e^{-t}.
\]
Including causality via $u(t)$, this becomes
\[
y(t) = (t - 1 + e^{-t})u(t),
\]
which agrees with the Laplace-transform computation.

\medskip

\noindent\textbf{Discussion and connection to Laplace transforms.}

This example captures several central ideas of the Laplace transform in the study of LTI systems:

\begin{itemize}
  \item In the time domain, the output of a causal LTI system is a convolution $y = h * f$. Direct evaluation of the convolution integral can be straightforward for simple inputs (such as the step) but becomes more cumbersome for more complicated inputs (such as the ramp).
  \item The Laplace transform converts the convolution operation into multiplication: $\mathcal{L}\{h*f\} = \mathcal{L}\{h\}\,\mathcal{L}\{f\} = H(s)F(s)$. This allows us to replace an integral computation by algebraic manipulations in the $s$-domain.
  \item Once $Y(s)$ is found, we recover $y(t)$ via the inverse Laplace transform, often using partial fraction decompositions and standard transform pairs.
\end{itemize}

Thus, the Laplace transform provides a powerful method for analyzing the response of LTI systems to various inputs, especially when direct convolution in the time domain would be difficult.

\end{solution}

% ===== Example 5: Using Laplace Transforms for a Simple PDE: The Heat Equation on a Half-Line (inquiry-based) =====
\begin{problem}[Using Laplace Transforms for a Simple PDE: The Heat Equation on a Half-Line]
Consider a very long, thin rod occupying the half-line $x>0$. At time $t=0$ the rod is at zero temperature everywhere. For $t>0$ we suddenly clamp the end at $x=0$ to a fixed temperature of $1$ (say, in suitable nondimensional units), while the rest of the rod is initially cold. Heat then begins to flow from the heated end into the rod. We model this using the one-dimensional heat equation with thermal diffusivity $\kappa>0$ on the half-line.

We will use the Laplace transform in time to reduce this initial–boundary value problem to an ordinary differential equation in the spatial variable $x$. Solving this ordinary differential equation and then inverting the Laplace transform will yield an explicit formula for the temperature $u(x,t)$ as a function of position and time.

The problem is:
\[
\begin{cases}
u_t(x,t) = \kappa\,u_{xx}(x,t), & x>0,\ t>0,\\[4pt]
u(0,t) = 1, & t>0,\\[4pt]
u(x,0) = 0, & x>0,\\[4pt]
u(x,t)\ \text{remains bounded as } x\to +\infty, & t>0.
\end{cases}
\]

\medskip

(a) Before doing any calculations, think qualitatively about the solution.  
Describe in a few sentences what you expect the temperature profile $u(x,t)$ to look like for:
\begin{itemize}
    \item very small times $t\downarrow 0$, and
    \item very large times $t\to\infty$,
\end{itemize}
at a fixed position $x>0$. In particular, do you expect $u(x,t)$ to increase or decrease in time at a fixed $x$, and is there any limiting temperature as $t\to\infty$ for fixed $x$?

\medskip

(b) We now introduce the Laplace transform in time. For each fixed $x>0$, define
\[
U(x,s) = \mathcal{L}_t\{u(x,t)\}(s) = \int_0^\infty e^{-st}u(x,t)\,dt,\qquad s>0.
\]
Apply the Laplace transform with respect to $t$ to the heat equation and to the initial condition.  

Carefully justify each step and show that $U$ satisfies a second-order ordinary differential equation in $x$ of the form
\[
\kappa\,U_{xx}(x,s) - s\,U(x,s) = 0,\qquad x>0,\ s>0.
\]
What boundary conditions in $x$ does $U(x,s)$ satisfy?

Hint: Use the property $\mathcal{L}\{u_t\}(s) = s U(x,s) - u(x,0)$ and recall that the Laplace transform in $t$ does not affect the variable $x$.

\medskip

(c) Now solve the ordinary differential equation in $x$ found in part (b).  

(i) Solve the homogeneous equation
\[
\kappa\,U_{xx} - s\,U = 0
\]
for fixed $s>0$ and find the general solution $U(x,s)$.

(ii) Use the boundary condition at $x=0$ and the requirement that $U(x,s)$ remain bounded as $x\to\infty$ to determine the constants in your general solution and obtain an explicit formula for $U(x,s)$.

Hint: The characteristic equation for the ODE is quadratic in the spatial growth rate, and you should find exponentials of the form $e^{\pm \sqrt{s/\kappa}\,x}$. Which exponential is compatible with boundedness as $x\to\infty$?

\medskip

(d) We now invert the Laplace transform in order to recover $u(x,t)$.

(i) Show that your expression for $U(x,s)$ from part (c) can be written in the form
\[
U(x,s) = \frac{1}{s}\,\exp\!\bigl(-x\sqrt{s/\kappa}\,\bigr).
\]

(ii) By consulting a Laplace transform table (or otherwise), find the inverse Laplace transform of
\[
\frac{1}{s}\,\exp\!\bigl(-a\sqrt{s}\,\bigr),\qquad a>0.
\]
Express your answer in terms of the complementary error function
\[
\operatorname{erfc}(z) = \frac{2}{\sqrt{\pi}}\int_z^\infty e^{-y^2}\,dy.
\]

(iii) Use the result from (ii) with a suitable choice of $a$ to write an explicit formula for $u(x,t)$. Briefly check that your formula is consistent with your qualitative expectations from part (a) (for example, check what happens when $t\downarrow 0$ and when $x\to 0$).

% Hint: A standard transform pair is
% \[
% \mathcal{L}^{-1}\!\left\{\frac{1}{s}\,e^{-a\sqrt{s}}\right\}(t)
% = \operatorname{erfc}\!\left(\frac{a}{2\sqrt{t}}\right),\quad a>0.
% \]

\medskip

(e) Exploratory questions.

(i) Suppose instead that the boundary temperature is not a step, but decays exponentially in time:
\[
u(0,t) = e^{-bt},\qquad b>0,\ t>0,
\]
while $u(x,0)=0$ as before. How would the transformed problem for $U(x,s)$ change? Write down the new boundary condition for $U(0,s)$ and the corresponding formula for $U(x,s)$ (do not invert the transform).

(ii) Briefly discuss how you might modify the Laplace transform approach if, in addition to the boundary condition $u(0,t)=1$, you also had a nonzero initial temperature distribution $u(x,0)=f(x)$ for a given function $f$. What changes in the transformed ordinary differential equation and its right-hand side?

Hint: Look back at the formula $\mathcal{L}\{u_t\}(s) = s U(x,s) - u(x,0)$ and think about how a nonzero $f(x)$ appears in the transformed equation.
\end{problem}

% ===== Example 5: Using Laplace Transforms for a Simple PDE: The Heat Equation on a Half-Line (full solution) =====
\begin{problem}[Using Laplace Transforms for a Simple PDE: The Heat Equation on a Half-Line]
Consider the heat equation on the half-line $x>0$ with thermal diffusivity $\kappa>0$:
\[
u_t(x,t) = \kappa\,u_{xx}(x,t),\qquad x>0,\ t>0,
\]
subject to the boundary and initial conditions
\[
u(0,t) = 1,\quad t>0;\qquad u(x,0)=0,\quad x>0;
\]
and the requirement that $u(x,t)$ remain bounded as $x\to\infty$ for each fixed $t>0$.
Using the Laplace transform in time, solve this initial–boundary value problem and express the solution $u(x,t)$ explicitly in terms of the complementary error function $\operatorname{erfc}$.
\end{problem}

\begin{solution}
We are solving the heat equation on the semi-infinite rod with a step change in boundary temperature at $x=0$. The key idea is to apply the Laplace transform in the time variable, which turns the time derivative into multiplication by $s$ and incorporates the initial condition algebraically. This reduces the partial differential equation to an ordinary differential equation in $x$ for each transform parameter $s>0$. We then solve this ordinary differential equation using standard methods and finally invert the Laplace transform using a known transform pair involving the complementary error function.

\medskip

\noindent\textbf{1. Laplace transform in time.}
For each fixed $x>0$, define the Laplace transform of $u(x,t)$ with respect to $t$ by
\[
U(x,s) = \mathcal{L}_t\{u(x,t)\}(s) = \int_0^\infty e^{-st}u(x,t)\,dt,\qquad s>0.
\]
We now transform the partial differential equation and the boundary and initial conditions.

First, we use the standard property of the Laplace transform for time derivatives:
\[
\mathcal{L}_t\{u_t(x,t)\}(s) = s\,U(x,s) - u(x,0).
\]
Since $u(x,0)=0$ for all $x>0$, this reduces to
\[
\mathcal{L}_t\{u_t(x,t)\}(s) = s\,U(x,s).
\]
Next, observe that the Laplace transform in $t$ does not affect derivatives with respect to $x$, so
\[
\mathcal{L}_t\{u_{xx}(x,t)\}(s) = U_{xx}(x,s).
\]
Applying $\mathcal{L}_t$ to the heat equation $u_t = \kappa u_{xx}$ yields
\[
s\,U(x,s) = \kappa\,U_{xx}(x,s),\qquad x>0,\ s>0.
\]
We may rewrite this as the second-order ordinary differential equation
\[
\kappa\,U_{xx}(x,s) - s\,U(x,s) = 0,\qquad x>0,\ s>0.
\]

Now we transform the boundary condition at $x=0$. We have $u(0,t) = 1$ for $t>0$, so
\[
U(0,s) = \mathcal{L}_t\{u(0,t)\}(s) = \mathcal{L}_t\{1\}(s) = \frac{1}{s}.
\]
Finally, the requirement that $u(x,t)$ remain bounded as $x\to\infty$ for each $t$ implies that, for each $s>0$, $U(x,s)$ should remain bounded as $x\to\infty$.

Thus, for each fixed $s>0$, we must solve the boundary value problem
\[
\begin{cases}
\kappa\,U_{xx}(x,s) - s\,U(x,s) = 0, & x>0,\\[4pt]
U(0,s) = \dfrac{1}{s},\\[4pt]
U(x,s)\ \text{bounded as } x\to\infty.
\end{cases}
\]

\medskip

\noindent\textbf{2. Solving the transformed ODE in $x$.}
Fix $s>0$ and consider the ordinary differential equation
\[
\kappa\,U_{xx} - s\,U = 0.
\]
This is a constant-coefficient linear ODE. Its characteristic equation is
\[
\kappa r^2 - s = 0 \quad\Longrightarrow\quad r^2 = \frac{s}{\kappa}.
\]
Hence
\[
r = \pm \sqrt{\frac{s}{\kappa}}.
\]
The general solution is therefore
\[
U(x,s) = A(s)\,e^{\sqrt{s/\kappa}\,x} + B(s)\,e^{-\sqrt{s/\kappa}\,x},
\]
where $A(s)$ and $B(s)$ are functions of the transform variable $s$ to be determined from the boundary conditions.

The boundedness condition as $x\to\infty$ forces $A(s)=0$, because $e^{\sqrt{s/\kappa} x}$ grows exponentially as $x\to\infty$ for each $s>0$. Thus
\[
U(x,s) = B(s)\,e^{-\sqrt{s/\kappa}\,x}.
\]
Imposing the boundary condition at $x=0$,
\[
U(0,s) = B(s) = \frac{1}{s},
\]
we obtain
\[
U(x,s) = \frac{1}{s}\,e^{-\sqrt{s/\kappa}\,x}.
\]
This is the Laplace transform (in $t$) of the solution we seek:
\[
U(x,s) = \mathcal{L}_t\{u(x,t)\}(s) = \frac{1}{s}\,\exp\!\bigl(-x\sqrt{s/\kappa}\,\bigr).
\]

\medskip

\noindent\textbf{3. Inverse Laplace transform and appearance of $\operatorname{erfc}$.}
To find $u(x,t)$, we must invert the Laplace transform. For each fixed $x>0$, we can view $U(x,s)$ as a function of $s$:
\[
U(x,s) = \frac{1}{s}\,\exp\!\left(-x\sqrt{\frac{s}{\kappa}}\right).
\]
Introduce the parameter
\[
a = \frac{x}{\sqrt{\kappa}},
\]
so that
\[
U(x,s) = \frac{1}{s}\,e^{-a\sqrt{s}}.
\]

A standard Laplace transform pair (which can be found in most tables or derived by an explicit computation using the Gaussian integral) is
\[
\mathcal{L}_t^{-1}\!\left\{\frac{1}{s}\,e^{-a\sqrt{s}}\right\}(t)
= \operatorname{erfc}\!\left(\frac{a}{2\sqrt{t}}\right),\qquad a>0,\ t>0,
\]
where the complementary error function is defined by
\[
\operatorname{erfc}(z)
= \frac{2}{\sqrt{\pi}}\int_z^\infty e^{-y^2}\,dy.
\]

Using this transform pair with $a=x/\sqrt{\kappa}$, we obtain
\[
u(x,t)
= \mathcal{L}_t^{-1}\{U(x,s)\}(t)
= \mathcal{L}_t^{-1}\!\left\{\frac{1}{s}\,e^{-(x/\sqrt{\kappa})\sqrt{s}}\right\}(t)
= \operatorname{erfc}\!\left(\frac{x/\sqrt{\kappa}}{2\sqrt{t}}\right).
\]
Simplifying the argument of $\operatorname{erfc}$ gives
\[
u(x,t) = \operatorname{erfc}\!\left(\frac{x}{2\sqrt{\kappa t}}\right),\qquad x>0,\ t>0.
\]

Thus the temperature in the semi-infinite rod is given explicitly by
\[
\boxed{\,u(x,t) = \operatorname{erfc}\!\left(\frac{x}{2\sqrt{\kappa t}}\right)\,}.
\]

\medskip

\noindent\textbf{4. Checking consistency with conditions and qualitative behavior.}
We briefly check that this formula is consistent with the initial and boundary conditions and with physical intuition.

\emph{Boundary condition at $x=0$.}  
At $x=0$ we have
\[
u(0,t) = \operatorname{erfc}\!\left(0\right) = 1,
\]
because
\[
\operatorname{erfc}(0) = \frac{2}{\sqrt{\pi}}\int_0^\infty e^{-y^2}\,dy = 1.
\]
This matches the prescribed boundary temperature $u(0,t)=1$.

\emph{Initial condition as $t\downarrow 0$.}  
Fix $x>0$ and let $t\downarrow 0$. Then $x/(2\sqrt{\kappa t})\to +\infty$, and since $\operatorname{erfc}(z)\to 0$ as $z\to+\infty$, we obtain
\[
\lim_{t\downarrow 0}u(x,t)
= \lim_{t\downarrow 0}\operatorname{erfc}\!\left(\frac{x}{2\sqrt{\kappa t}}\right) = 0,
\]
in agreement with the initial condition $u(x,0)=0$.

\emph{Behavior for large $t$ at fixed $x$.}  
For each fixed $x>0$, as $t\to\infty$ we have $x/(2\sqrt{\kappa t})\to 0$, and thus
\[
\lim_{t\to\infty}u(x,t)
= \lim_{t\to\infty}\operatorname{erfc}\!\left(\frac{x}{2\sqrt{\kappa t}}\right) = 1.
\]
Physically, this means that as time becomes very large, the entire rod (at any fixed finite distance from $x=0$) tends to the boundary temperature $1$. This is exactly what we expect: given enough time, the heat has diffused into the rod and equilibrated locally to the boundary temperature.

\emph{Boundedness as $x\to\infty$.}  
For any fixed $t>0$, as $x\to\infty$ the argument $x/(2\sqrt{\kappa t})\to\infty$ and hence $u(x,t)\to 0$. Thus the temperature far away from the heated end remains small (and in fact tends to zero at each fixed time), which is compatible with the half-infinite geometry and the finite speed at which thermal influence propagates in this diffusive model.

\medskip

\noindent\textbf{5. Conceptual summary.}
This example illustrates a central idea in the use of the Laplace transform for partial differential equations: transforming in time turns the time derivative into an algebraic factor $s$, while initial conditions appear as simple additional terms. The heat equation $u_t = \kappa u_{xx}$ becomes, for each $s>0$, a boundary value ordinary differential equation in the spatial variable $x$,
\[
\kappa U_{xx} - s U = 0,
\]
with transformed boundary data obtained by taking the Laplace transform of the original time-dependent boundary condition. Solving this ordinary differential equation is straightforward, and the remaining step is to invert the Laplace transform, which can often be done using tables or known transform pairs. In this case, the inversion naturally introduces the complementary error function, reflecting the Gaussian nature of heat diffusion. Thus the Laplace transform serves as a powerful tool to convert an initial–boundary value problem for a parabolic PDE into a more tractable sequence of ordinary differential problems.
\end{solution}

\section{From Differential to Algebraic Equations with FT, FS and LT}
% --- Narrative plan (auto-generated) ---
% This section develops the central idea that Fourier transforms, Fourier series, and Laplace transforms convert differential equations into algebraic equations in transform space. By learning to move back and forth between the physical domain (time, space) and the transform domain (frequency, complex parameter), we turn derivatives into simple multipliers. This creates a powerful and systematic route for solving initial value problems, boundary value problems, and forced systems.
%
% The methods here are essential in applied mathematics because many partial differential equations, such as the heat and wave equations, become tractable only after a suitable transform has diagonalized the differential operator. Dynamical systems and control problems are often most clearly understood through their frequency-domain or Laplace-domain representations, where stability and resonance can be read off from poles and spectra. The same ideas connect naturally to complex analysis through contour integration and the inversion formulas, and to linear algebra through the interpretation of transforms as changes of basis in infinite-dimensional function spaces.
%
% Throughout this section, we will proceed from simple ordinary differential equations to more elaborate partial differential equations, beginning with concrete, low-dimensional examples and gradually introducing the standard techniques: applying a transform, solving the resulting algebraic equation, and inverting the transform. Along the way we highlight the parallels between Fourier series, Fourier transforms, and Laplace transforms, and show how they provide complementary perspectives on the same underlying principle: derivatives become multiplication, and complicated operators become simple in the right representation.

% ===== Example 1: First-Order Linear ODE Solved by Laplace Transform (inquiry-based) =====
\begin{problem}[First-Order Linear ODE Solved by Laplace Transform]
In many simple electrical circuits, such as an $RC$ circuit, the voltage $v(t)$ across a capacitor satisfies a first-order linear ordinary differential equation. If a constant input voltage $V_0$ is suddenly applied at time $t=0$, then (after choosing appropriate units so that $RC=1$) the voltage across the capacitor satisfies
\[
v'(t) + v(t) = V_0,\qquad t>0,
\]
with some given initial voltage $v(0)=v_0$ on the capacitor. In this problem you will use the Laplace transform to convert this differential equation into an algebraic equation in the Laplace variable $s$, solve it there, and then invert the transform to find $v(t)$.

(a) Recall that the Laplace transform of a function $f(t)$ defined for $t\geq 0$ is
\[
\mathcal{L}\{f\}(s) = F(s) = \int_{0}^{\infty} e^{-st} f(t)\,dt,
\]
for those values of $s$ where the integral converges. Starting from this definition, compute the Laplace transform of the derivative $v'(t)$ in terms of $V(s) = \mathcal{L}\{v\}(s)$ and the initial value $v(0)$.

\emph{Hint:} Write $\mathcal{L}\{v'\}(s) = \int_0^\infty e^{-st} v'(t)\,dt$ and integrate by parts, taking $u=e^{-st}$ and $dv=v'(t)\,dt$.

(b) Apply the Laplace transform to both sides of the differential equation
\[
v'(t) + v(t) = V_0,\qquad v(0)=v_0.
\]
Use your result from part (a) and the fact that $\mathcal{L}\{V_0\}(s) = \dfrac{V_0}{s}$ to obtain an algebraic equation for $V(s) = \mathcal{L}\{v\}(s)$ that explicitly involves $v_0$.

\emph{Hint:} Transform each term separately and remember that the Laplace transform is linear.

(c) Solve the algebraic equation from part (b) for $V(s)$. Then rewrite your answer in a form that is convenient for taking the inverse Laplace transform. In particular, show that you can write $V(s)$ as a sum of simple rational functions whose inverse transforms are standard.

\emph{Hint:} You should find an expression of the form
\[
V(s) = \frac{v_0}{s+1} + \frac{V_0}{s(s+1)}.
\]
Then perform a partial fraction decomposition of $\dfrac{V_0}{s(s+1)}$:
\[
\frac{V_0}{s(s+1)} = \frac{A}{s} + \frac{B}{s+1}
\]
for appropriate constants $A$ and $B$.

(d) Use a Laplace transform table (or known basic transforms) to take the inverse Laplace transform of your expression for $V(s)$ and thereby find an explicit formula for $v(t)$ for $t\ge 0$. Simplify your answer as much as possible.

\emph{Hint:} You should encounter transforms of the form
\[
\mathcal{L}^{-1}\left\{\frac{1}{s}\right\}(t) = 1,\qquad
\mathcal{L}^{-1}\left\{\frac{1}{s+1}\right\}(t) = e^{-t}.
\]

(e) Interpretation and extensions.

\begin{enumerate}
  \item[(i)] Compute $\displaystyle \lim_{t\to\infty} v(t)$ and explain what this limit means physically for the charging of the capacitor.
  
  \item[(ii)] Suppose now that the input voltage is not constant, but instead decays exponentially: $V_{\text{in}}(t) = V_0 e^{-\alpha t}$ with $\alpha>0$. The equation becomes
  \[
  v'(t) + v(t) = V_0 e^{-\alpha t},\qquad v(0)=v_0.
  \]
  Without carrying out all the details, outline how you would modify your Laplace transform solution to handle this new right-hand side. Which Laplace transform identities would you need, and where would $\alpha$ appear in the algebraic equation?
  
  \item[(iii)] How would your work change if the coefficient in front of $v(t)$ were some positive constant $a>0$, giving the more general equation
  \[
  v'(t) + a\,v(t) = V_0,\qquad v(0)=v_0?
  \]
  Describe briefly (in words or formulas) how the form of $V(s)$ and then $v(t)$ would change.
\end{enumerate}
\end{problem}

% ===== Example 1: First-Order Linear ODE Solved by Laplace Transform (full solution) =====
\begin{problem}[First-Order Linear ODE Solved by Laplace Transform]
Solve the initial value problem
\[
v'(t) + v(t) = V_0,\qquad t>0,\qquad v(0)=v_0,
\]
where $V_0$ and $v_0$ are constants, using the Laplace transform. Show explicitly how the initial condition appears in the transformed (algebraic) equation, and compute $v(t)$ for $t\ge 0$ by inverting the transform.
\end{problem}

\begin{solution}
We are given the first-order linear ordinary differential equation
\[
v'(t) + v(t) = V_0,\qquad t>0,\qquad v(0)=v_0.
\]
This equation models, for instance, the voltage $v(t)$ across a capacitor in a simple $RC$ circuit when a constant input voltage $V_0$ is applied at $t=0$ and the time scale has been chosen so that $RC=1$.

The central idea of the Laplace transform method is to convert the differential equation in the time variable $t$ into an algebraic equation in the Laplace variable $s$. The derivative $v'(t)$ will transform into a linear expression involving $s$ and the transform $V(s)$, and the initial condition will enter as a simple constant term.

Let $V(s) = \mathcal{L}\{v\}(s)$ denote the Laplace transform of $v(t)$. By definition,
\[
V(s) = \mathcal{L}\{v\}(s) = \int_0^\infty e^{-st} v(t)\,dt
\]
for those $s$ for which the integral converges.

We first recall the Laplace transform of a derivative. Assuming $v$ is sufficiently smooth and of exponential order, we compute
\[
\mathcal{L}\{v'\}(s)
= \int_0^\infty e^{-st} v'(t)\,dt.
\]
Integrating by parts, with $u = e^{-st}$ and $dv = v'(t)\,dt$, so that $du = -s e^{-st}\,dt$ and $v$ is an antiderivative of $v'$, we obtain
\[
\int_0^\infty e^{-st} v'(t)\,dt
= \bigl[e^{-st} v(t)\bigr]_{t=0}^{t=\infty} + s \int_0^\infty e^{-st} v(t)\,dt.
\]
Under the usual assumption that $e^{-st} v(t) \to 0$ as $t\to\infty$ for $\operatorname{Re}(s)$ sufficiently large, the boundary term at infinity vanishes. Evaluating at $t=0$ gives $e^0 v(0) = v_0$. Therefore
\[
\mathcal{L}\{v'\}(s) = -v_0 + s V(s) = sV(s) - v_0.
\]

Now we apply the Laplace transform term-by-term to the differential equation. Using linearity of the transform, we get
\[
\mathcal{L}\{v'\}(s) + \mathcal{L}\{v\}(s) = \mathcal{L}\{V_0\}(s).
\]
Substituting $\mathcal{L}\{v'\}(s) = sV(s) - v_0$ and $\mathcal{L}\{v\}(s) = V(s)$, and noting that the Laplace transform of a constant $V_0$ is
\[
\mathcal{L}\{V_0\}(s) = \int_0^\infty e^{-st} V_0\,dt = \frac{V_0}{s}
\]
for $\operatorname{Re}(s) > 0$, we obtain the algebraic equation
\[
\bigl(sV(s) - v_0\bigr) + V(s) = \frac{V_0}{s}.
\]
That is,
\[
(s+1) V(s) - v_0 = \frac{V_0}{s}.
\]
Rearranging, we explicitly see the contribution of the initial condition:
\[
(s+1) V(s) = v_0 + \frac{V_0}{s}.
\]

We now solve this algebraic equation for $V(s)$:
\[
V(s) = \frac{v_0}{s+1} + \frac{V_0}{s(s+1)}.
\]
The first term already has a simple form, but the second term is a rational function that we wish to decompose into simpler pieces suited for inversion. We perform a partial fraction decomposition on
\[
\frac{V_0}{s(s+1)}.
\]
We seek constants $A$ and $B$ such that
\[
\frac{V_0}{s(s+1)} = \frac{A}{s} + \frac{B}{s+1}.
\]
Multiplying both sides by $s(s+1)$ gives
\[
V_0 = A(s+1) + Bs = (A+B)s + A.
\]
Identifying coefficients of like powers of $s$, we obtain the system
\[
A + B = 0,\qquad A = V_0.
\]
Hence $A = V_0$ and $B = -V_0$. Therefore
\[
\frac{V_0}{s(s+1)} = \frac{V_0}{s} - \frac{V_0}{s+1}.
\]

Substituting this back into the expression for $V(s)$ yields
\[
V(s) = \frac{v_0}{s+1} + \left(\frac{V_0}{s} - \frac{V_0}{s+1}\right)
= \frac{V_0}{s} + \frac{v_0 - V_0}{s+1}.
\]

We are now ready to invert the Laplace transform term-by-term. We use the standard formulas
\[
\mathcal{L}^{-1}\left\{\frac{1}{s}\right\}(t) = 1,\qquad
\mathcal{L}^{-1}\left\{\frac{1}{s+1}\right\}(t) = e^{-t}.
\]
By linearity of the inverse transform,
\[
v(t) = \mathcal{L}^{-1}\{V(s)\}(t)
= V_0\,\mathcal{L}^{-1}\left\{\frac{1}{s}\right\}(t)
+ (v_0 - V_0)\,\mathcal{L}^{-1}\left\{\frac{1}{s+1}\right\}(t).
\]
Thus
\[
v(t) = V_0 \cdot 1 + (v_0 - V_0) e^{-t} = V_0 + (v_0 - V_0) e^{-t},\qquad t\ge 0.
\]

This is the explicit solution of the initial value problem. One can easily verify that it satisfies both the differential equation and the initial condition: at $t=0$,
\[
v(0) = V_0 + (v_0 - V_0) e^{0} = v_0,
\]
and a direct calculation of $v'(t)$ shows that $v'(t) + v(t) = V_0$ for all $t$.

It is also instructive to examine the long-time behavior. Since $e^{-t} \to 0$ as $t\to\infty$, we have
\[
\lim_{t\to\infty} v(t) = V_0.
\]
Physically, this means that the voltage across the capacitor approaches the input voltage $V_0$ as time goes on. The difference $v(t) - V_0 = (v_0 - V_0) e^{-t}$ decays exponentially with time constant $1$, which in a more general model would be the product $RC$.

From the perspective of the chapter, this example illustrates clearly how the Laplace transform converts a differential equation into an algebraic equation in $s$. The derivative $v'(t)$ becomes $sV(s) - v_0$, so the initial condition enters as a constant term. Solving the algebraic equation for $V(s)$ and then using partial fractions allows us to express $V(s)$ as a sum of simple rational functions whose inverse transforms are known. In this way, the original time-domain problem is solved by an algebraic manipulation in the Laplace domain followed by a table lookup. This is a prototypical instance of the theme “from differential to algebraic equations” using integral transforms.
\end{solution}

% ===== Example 2: Heat Equation on a Finite Rod via Fourier Sine Series (inquiry-based) =====
\begin{problem}[Heat Equation on a Finite Rod via Fourier Sine Series]
Consider a thin, homogeneous rod of length $L$ lying along the $x$-axis from $x=0$ to $x=L$. We assume that heat can flow only along the rod, and that the ends of the rod are kept in contact with large ice baths so that their temperature is held at zero for all time. The temperature distribution $u(x,t)$ in the rod evolves according to the one-dimensional heat equation, which couples time derivatives and second spatial derivatives. Our goal is to see how expanding $u(x,t)$ in a Fourier sine series in $x$ turns this partial differential equation into a decoupled family of ordinary differential equations in $t$.

Throughout, let $\kappa>0$ denote the thermal diffusivity, and let $f(x)$ denote the initial temperature distribution along the rod at time $t=0$.

\smallskip

(a) \textbf{Setting up the model.}
Write down the initial-boundary value problem (IBVP) for $u(x,t)$ describing this situation. That is, write the governing partial differential equation, the boundary conditions at $x=0$ and $x=L$, and the initial condition at $t=0$.

\emph{Hint:} The PDE is the standard heat equation in one space dimension; the ends of the rod are held at zero temperature.

\smallskip

(b) \textbf{Separation of variables and the spatial eigenvalue problem.}
We look for separated solutions of the form $u(x,t)=X(x)T(t)$.

\quad (i) Substitute $u(x,t)=X(x)T(t)$ into your PDE from part (a) and separate variables to obtain an equation of the form
\[
\frac{T'(t)}{\kappa\,T(t)} = \frac{X''(x)}{X(x)} = \text{(constant)}.
\]

Call this constant $-\lambda$, and derive the resulting pair of ordinary differential equations for $X$ and $T$.

\quad (ii) Use the boundary conditions at $x=0$ and $x=L$ to obtain a boundary value problem for $X(x)$ alone. Show that nontrivial solutions $X(x)$ can exist only for certain special values of $\lambda$, and identify these $\lambda$ as eigenvalues of the spatial problem.

\emph{Hint:} You should obtain a second-order ODE $X''+\lambda X=0$ with homogeneous Dirichlet boundary conditions $X(0)=X(L)=0$. Recall from ordinary differential equations how to solve such problems and how boundary conditions quantize the allowed values of $\lambda$.

\smallskip

(c) \textbf{Finding eigenfunctions and time dependence.}
Continue the analysis from part (b).

\quad (i) Solve the eigenvalue problem for $X$ explicitly, and show that the admissible eigenvalues are
\[
\lambda_n = \left(\frac{n\pi}{L}\right)^2, \qquad n=1,2,3,\dots,
\]
with corresponding eigenfunctions
\[
X_n(x) = \sin\left(\frac{n\pi x}{L}\right).
\]

\quad (ii) For each $n$, solve the time ODE for $T(t)$ associated to $\lambda=\lambda_n$. Show that the separated solutions have the form
\[
u_n(x,t) = \sin\!\left(\frac{n\pi x}{L}\right) e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t},
\qquad n=1,2,3,\dots.
\]

\emph{Hint:} After choosing $\lambda=\lambda_n$, the time equation is a first-order linear ODE $T'(t) = -\kappa\lambda_n T(t)$.

\smallskip

(d) \textbf{Superposition, Fourier sine series, and the role of orthogonality.}
The heat equation is linear, so sums of solutions are again solutions.

\quad (i) Argue that for any sequence of constants $(b_n)_{n\ge 1}$, the series
\[
u(x,t) = \sum_{n=1}^{\infty} b_n\,e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t}\,\sin\!\left(\frac{n\pi x}{L}\right)
\]
formally satisfies the PDE and the boundary conditions.

\quad (ii) Impose the initial condition $u(x,0)=f(x)$ and explain why this leads to the requirement that
\[
f(x) = \sum_{n=1}^{\infty} b_n \sin\!\left(\frac{n\pi x}{L}\right)
\]
be the Fourier sine series expansion of $f$ on $[0,L]$.

\quad (iii) Using the orthogonality of the sine functions on $[0,L]$, derive a formula for the coefficients $b_n$ in terms of $f(x)$.

\emph{Hint:} Multiply the identity in (ii) by $\sin\!\left(\frac{m\pi x}{L}\right)$ and integrate from $0$ to $L$. Recall that
\[
\int_0^L \sin\!\left(\frac{n\pi x}{L}\right)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= \begin{cases}
0, & m\neq n,\\[4pt]
\dfrac{L}{2}, & m=n.
\end{cases}
\]

\smallskip

(e) \textbf{Putting it all together and extensions.}

\quad (i) Combine your work to write the final formula for the solution $u(x,t)$ in terms of $f(x)$. Briefly describe in words how each sine mode behaves as time increases, and what happens to the temperature distribution as $t\to\infty$.

\quad (ii) (Extension: alternative boundary condition.) Suppose instead that the left end of the rod is insulated, so that no heat flows through $x=0$, while the right end is still held at zero temperature:
\[
u_x(0,t)=0,\qquad u(L,t)=0.
\]
How would you expect the spatial eigenfunctions and eigenvalues to change? Which trigonometric functions (sines, cosines, or a mixture) would naturally appear now, and why?

\quad (iii) (Extension: nonzero boundary temperature.) If both ends of the rod are held at a nonzero constant temperature $u(0,t)=u(L,t)=U_0$, how could you modify the method so that you can still use a Fourier sine series? Outline the main idea without doing all the computations.

\emph{Hint:} For (iii), think about first subtracting a simple steady-state solution that already satisfies the boundary conditions, and then solving for the remaining part with homogeneous boundary conditions.
\end{problem}

% ===== Example 2: Heat Equation on a Finite Rod via Fourier Sine Series (full solution) =====
\begin{problem}[Heat Equation on a Finite Rod via Fourier Sine Series]
Let $u(x,t)$ denote the temperature in a thin homogeneous rod of length $L$, for $0<x<L$, $t>0$. Consider the initial-boundary value problem
\[
u_t = \kappa u_{xx}, \quad 0<x<L,\ t>0,
\]
\[
u(0,t)=0,\quad u(L,t)=0,\quad t>0,
\]
\[
u(x,0)=f(x),\quad 0<x<L,
\]
where $\kappa>0$ and $f$ is a given function. Using separation of variables and a Fourier sine series in $x$, find an explicit series formula for $u(x,t)$ in terms of $f$.
\end{problem}

\begin{solution}
We solve the homogeneous heat equation with homogeneous Dirichlet boundary conditions by separation of variables and the eigenfunction expansion method. This will exhibit how the second derivative in space becomes multiplication by a scalar in an appropriate basis, turning the partial differential equation into algebraic relations for Fourier coefficients.

\medskip

\textbf{1. Separation of variables and the eigenvalue problem.}
We seek separated solutions of the form
\[
u(x,t) = X(x)T(t).
\]
Substituting into the PDE $u_t=\kappa u_{xx}$ gives
\[
X(x)T'(t) = \kappa X''(x)T(t).
\]
Assuming $X$ and $T$ are not identically zero, we can divide both sides by $\kappa X(x)T(t)$ and obtain
\[
\frac{T'(t)}{\kappa T(t)} = \frac{X''(x)}{X(x)} = -\lambda,
\]
for some constant $-\lambda$ independent of $x$ and $t$. This leads to the pair of ordinary differential equations
\[
T'(t) + \kappa\lambda T(t) = 0,\qquad X''(x) + \lambda X(x) = 0.
\]
The boundary conditions $u(0,t)=u(L,t)=0$ become
\[
X(0)T(t) = 0,\qquad X(L)T(t)=0 \quad \text{for all } t>0.
\]
Since we are interested in nontrivial solutions $u$, we cannot have $T\equiv 0$, so we require
\[
X(0)=0,\qquad X(L)=0.
\]
Thus $X$ must solve the boundary value problem
\[
X''(x) + \lambda X(x) = 0,\quad 0<x<L,\qquad X(0)=X(L)=0.
\]
This is a standard Sturm--Liouville eigenvalue problem.

\medskip

\textbf{2. Solving the spatial eigenvalue problem.}
We determine the admissible values of $\lambda$ and the corresponding eigenfunctions $X$.

Consider three cases:

\emph{Case 1: $\lambda<0$.} Write $\lambda=-\mu^2$ with $\mu>0$. Then the equation becomes
\[
X''(x) - \mu^2 X(x) = 0,
\]
with general solution $X(x) = A e^{\mu x} + B e^{-\mu x}$. The boundary condition $X(0)=0$ gives $A+B=0$, so $B=-A$ and $X(x)=A(e^{\mu x}-e^{-\mu x}) = 2A\sinh(\mu x)$. The condition $X(L)=0$ then forces $A\sinh(\mu L)=0$, hence $A=0$ since $\sinh(\mu L)\neq 0$. Thus only the trivial solution exists, so $\lambda<0$ yields no eigenvalues.

\emph{Case 2: $\lambda=0$.} The equation becomes $X''(x)=0$ with general solution $X(x)=Ax+B$. The condition $X(0)=0$ gives $B=0$, and $X(L)=0$ gives $AL=0$, so $A=0$. Again we obtain only the trivial solution; thus $\lambda=0$ is not an eigenvalue.

\emph{Case 3: $\lambda>0$.} Write $\lambda=\mu^2$ with $\mu>0$. The equation is
\[
X''(x)+\mu^2 X(x) = 0,
\]
whose general solution is $X(x)=A\cos(\mu x)+B\sin(\mu x)$. The boundary condition $X(0)=0$ forces $A=0$, so $X(x)=B\sin(\mu x)$. The condition $X(L)=0$ then implies $B\sin(\mu L)=0$. For a nontrivial solution we require $B\neq 0$, so $\sin(\mu L)=0$, which holds if and only if
\[
\mu L = n\pi,\quad n=1,2,3,\dots.
\]
Therefore
\[
\mu_n = \frac{n\pi}{L},\qquad \lambda_n = \mu_n^2 = \left(\frac{n\pi}{L}\right)^2,
\]
and corresponding eigenfunctions are
\[
X_n(x) = \sin\left(\frac{n\pi x}{L}\right),\qquad n=1,2,3,\dots.
\]

Thus the spatial eigenfunctions form a sine basis adapted to the homogeneous Dirichlet boundary conditions.

\medskip

\textbf{3. Time dependence and separated solutions.}
For each eigenvalue $\lambda_n$, the time equation
\[
T'(t) + \kappa \lambda_n T(t) = 0
\]
is a first-order linear ODE with solution
\[
T_n(t) = C_n e^{-\kappa \lambda_n t}
= C_n e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t},
\]
where $C_n$ is a constant.

Hence, for each $n\ge 1$, we obtain a separated solution
\[
u_n(x,t) = X_n(x)T_n(t)
= C_n \sin\left(\frac{n\pi x}{L}\right)
   e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t}.
\]
Because the heat equation is linear and homogeneous, any finite linear combination of the $u_n$ is also a solution. Motivated by completeness of the sine functions, we consider the infinite series
\[
u(x,t) = \sum_{n=1}^{\infty} b_n e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t}
\sin\left(\frac{n\pi x}{L}\right),
\]
where the constants $b_n$ are to be determined from the initial condition.

Each term in the series satisfies the PDE and the boundary conditions individually, and under reasonable assumptions on $f$ this series converges sufficiently well to inherit these properties termwise.

\medskip

\textbf{4. Imposing the initial condition and using Fourier sine series.}
At time $t=0$ the series reduces to
\[
u(x,0) = \sum_{n=1}^{\infty} b_n \sin\left(\frac{n\pi x}{L}\right).
\]
The initial condition $u(x,0)=f(x)$ therefore demands that $f$ admit a Fourier sine series expansion on $[0,L]$:
\[
f(x) \sim \sum_{n=1}^{\infty} b_n \sin\left(\frac{n\pi x}{L}\right).
\]

The sine functions are orthogonal on $[0,L]$:
\[
\int_0^L \sin\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,dx
= \begin{cases}
0, & n\neq m,\\[4pt]
\dfrac{L}{2}, & n=m.
\end{cases}
\]
To extract $b_m$, multiply the series for $f(x)$ by $\sin\left(\frac{m\pi x}{L}\right)$ and integrate from $0$ to $L$:
\[
\int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx
= \int_0^L \left(\sum_{n=1}^{\infty} b_n \sin\left(\frac{n\pi x}{L}\right)\right)
               \sin\left(\frac{m\pi x}{L}\right)\,dx.
\]
Assuming we can interchange sum and integral, orthogonality gives
\[
\int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx
= \sum_{n=1}^{\infty} b_n \int_0^L \sin\left(\frac{n\pi x}{L}\right)
                             \sin\left(\frac{m\pi x}{L}\right)\,dx
= b_m \frac{L}{2}.
\]
Thus
\[
b_m = \frac{2}{L}\int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx,
\qquad m=1,2,3,\dots.
\]

\medskip

\textbf{5. Final solution formula and qualitative behavior.}
We have now determined both the spatial dependence and the time dependence. The solution to the initial-boundary value problem is
\[
u(x,t) = \sum_{n=1}^{\infty}
\left(\frac{2}{L}\int_0^L f(s)\sin\left(\frac{n\pi s}{L}\right)\,ds\right)
e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t}
\sin\left(\frac{n\pi x}{L}\right),
\]
provided $f$ is sufficiently regular for the Fourier series and the manipulations above to be justified.

Each mode
\[
\sin\left(\frac{n\pi x}{L}\right)
\]
is an eigenfunction of the spatial second-derivative operator $u\mapsto u_{xx}$ with eigenvalue $-\left(\frac{n\pi}{L}\right)^2$. In the sine basis, the spatial operator is diagonal: applying $u_{xx}$ multiplies the $n$-th Fourier sine coefficient by $-\left(\frac{n\pi}{L}\right)^2$. The heat equation $u_t=\kappa u_{xx}$ therefore translates into a family of decoupled scalar ODEs
\[
\frac{d}{dt}b_n(t) = -\kappa\left(\frac{n\pi}{L}\right)^2 b_n(t),
\]
whose solutions are the exponential factors $e^{-\kappa\left(\frac{n\pi}{L}\right)^2 t}$. This is the essence of the theme of this chapter: a differential equation in $(x,t)$ becomes a simple algebraic relation in the transformed (Fourier) domain.

In terms of qualitative behavior, higher-frequency modes (large $n$) decay more rapidly because their decay rate $\kappa (n\pi/L)^2$ is larger. As $t\to\infty$, every exponential factor tends to zero, so $u(x,t)\to 0$ for all $x$: the rod cools down to the boundary temperature, which is zero everywhere.

This example illustrates how a suitable Fourier sine series converts a PDE with spatial derivatives and boundary conditions into an infinite diagonal system of ordinary differential equations in time, with the Laplacian acting as a scalar multiplier on each Fourier mode.
\end{solution}

% ===== Example 3: Heat Equation on the Whole Line via Fourier Transform (inquiry-based) =====
\begin{problem}[Heat Equation on the Whole Line via Fourier Transform]
Consider a very long, effectively infinite, thin rod lying along the real line $\mathbb{R}$.  
We assume that heat diffuses along the rod according to the heat equation, and that initially the temperature distribution is localized: it is significant near the origin and decays rapidly as $|x| \to \infty$.  
On a finite interval, we diagonalized the Laplacian using Fourier series; on the whole line, the analogous tool is the Fourier transform, which decomposes functions into a continuous superposition of plane waves.  
In this problem you will discover how the Fourier transform converts the heat equation into a family of algebraic (actually ODE) equations and leads to the classical Gaussian heat kernel.

Throughout, fix a diffusion constant $\kappa>0$, and consider the Cauchy problem
\[
\begin{cases}
u_t(x,t) = \kappa\,u_{xx}(x,t), & x \in \mathbb{R},\ t>0,\\[0.2em]
u(x,0) = f(x), & x \in \mathbb{R},
\end{cases}
\]
where $f$ is a sufficiently nice, rapidly decaying function.

We use the following Fourier transform convention in the spatial variable $x$:
\[
\widehat{g}(\xi) := \displaystyle\int_{-\infty}^{\infty} g(x)\,e^{-i\xi x}\,dx,
\qquad
g(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{g}(\xi)\,e^{i\xi x}\,d\xi.
\]

\medskip

(a) \emph{Fourier transform and differentiation.}  
Let $g$ be a sufficiently smooth and rapidly decaying function.  

\quad(i) Using integration by parts, show that
\[
\widehat{g'}(\xi) = i\xi\,\widehat{g}(\xi).
\]
(Hint: Integrate $g'(x)e^{-i\xi x}$ by parts and use that $g(x)\to 0$ as $|x|\to\infty$.)

\quad(ii) Use part (i) to show that
\[
\widehat{g''}(\xi) = -\xi^2\,\widehat{g}(\xi).
\]

\medskip

(b) \emph{Transforming the heat equation.}  
Assume that for each fixed $t>0$, the function $x\mapsto u(x,t)$ decays sufficiently fast at $\pm\infty$ so that you can apply the Fourier transform in $x$ and differentiate under the integral sign.

\quad(i) Define
\[
\widehat{u}(\xi,t) := \int_{-\infty}^{\infty} u(x,t)\,e^{-i\xi x}\,dx.
\]
Show that
\[
\partial_t \widehat{u}(\xi,t)
= \kappa\,\widehat{u_{xx}}(\xi,t).
\]
(Hint: Differentiate inside the integral with respect to $t$.)

\quad(ii) Use part (a) to express $\widehat{u_{xx}}(\xi,t)$ in terms of $\widehat{u}(\xi,t)$, and deduce that for each fixed frequency $\xi \in \mathbb{R}$, the function $t \mapsto \widehat{u}(\xi,t)$ satisfies a first-order linear ODE.  
Write down this ODE explicitly, together with the initial condition at $t=0$, expressed in terms of $\widehat{f}(\xi)$.

\medskip

(c) \emph{Solving the transformed problem.}  

\quad(i) Solve the ODE found in part (b) for $\widehat{u}(\xi,t)$ in terms of $\widehat{f}(\xi)$, $\xi$, $t$, and $\kappa$.  

\quad(ii) Interpret the solution as ``each Fourier mode decays exponentially in time''.  
What is the decay rate of the mode corresponding to frequency $\xi$?

\medskip

(d) \emph{Inverting the transform and the heat kernel.}  

\quad(i) Use the inverse Fourier transform to write $u(x,t)$ as
\[
u(x,t) = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa \xi^2 t}\,\widehat{f}(\xi)\,e^{i\xi x}\,d\xi.
\]
Then use the Fourier inversion formula for $f$ to express $u(x,t)$ as a convolution
\[
u(x,t) = (K_t * f)(x) := \int_{-\infty}^{\infty} K_t(x-y)\,f(y)\,dy,
\]
for some kernel $K_t$.  
Write $K_t$ as an explicit inverse Fourier transform:
\[
K_t(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa \xi^2 t}\,e^{i\xi x}\,d\xi.
\]

\quad(ii) Compute $K_t(x)$ explicitly.  
To do this, recall the Gaussian integral
\[
\int_{-\infty}^{\infty} e^{-a z^2}\,dz = \sqrt{\frac{\pi}{a}}, \qquad a>0,
\]
and complete the square in the exponent of $e^{-\kappa \xi^2 t + i\xi x}$.  
Show that
\[
K_t(x) = \frac{1}{\sqrt{4\pi \kappa t}}\,\exp\!\left(-\frac{x^2}{4\kappa t}\right).
\]
(Hint: First write $-\kappa t\,\xi^2 + i\xi x$ as $-\kappa t\bigl(\xi - \tfrac{i x}{2\kappa t}\bigr)^2 - \tfrac{x^2}{4\kappa t}$.)

\quad(iii) Conclude that the solution of the Cauchy problem is
\[
u(x,t) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{4\pi \kappa t}}\,\exp\!\left(-\frac{(x-y)^2}{4\kappa t}\right) f(y)\,dy.
\]
Briefly interpret this formula: how does a localized initial temperature distribution spread out as time increases?

\medskip

(e) \emph{Extensions and further questions.}

\quad(i) Suppose the initial condition is a point source at the origin, modeled by the Dirac delta $\delta_0$ (so $f = \delta_0$).  
Using your kernel $K_t$, what is the corresponding solution $u(x,t)$?  
How does this relate to your answer in part (d)(ii)?

\quad(ii) What is the total heat in the rod at time $t$, that is,
\[
\int_{-\infty}^{\infty} u(x,t)\,dx?
\]
Assuming $\int_{-\infty}^{\infty} f(x)\,dx$ is finite, use the convolution formula to show that the total heat is conserved in time.

\quad(iii) (Conceptual) In what sense did the Fourier transform turn a \emph{partial} differential equation into a family of \emph{ordinary} differential equations?  
How is this analogous to what Fourier series did for the heat equation on a finite interval?
\end{problem}

% ===== Example 3: Heat Equation on the Whole Line via Fourier Transform (full solution) =====
\begin{problem}[Heat Equation on the Whole Line via Fourier Transform]
Let $\kappa>0$ and consider the Cauchy problem on the real line
\[
\begin{cases}
u_t(x,t) = \kappa\,u_{xx}(x,t), & x \in \mathbb{R},\ t>0,\\[0.2em]
u(x,0) = f(x), & x \in \mathbb{R},
\end{cases}
\]
where $f$ is sufficiently smooth and decays rapidly at infinity.

Using the Fourier transform
\[
\widehat{g}(\xi) := \displaystyle\int_{-\infty}^{\infty} g(x)\,e^{-i\xi x}\,dx,
\qquad
g(x) = \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{g}(\xi)\,e^{i\xi x}\,d\xi,
\]
solve the Cauchy problem explicitly and show that
\[
u(x,t) = \int_{-\infty}^{\infty} \frac{1}{\sqrt{4\pi \kappa t}}\,
\exp\!\left(-\frac{(x-y)^2}{4\kappa t}\right) f(y)\,dy.
\]
Identify the corresponding heat kernel $K_t$ and briefly explain how this example illustrates the passage from a differential equation in $x$ to an algebraic equation in the Fourier variable $\xi$.
\end{problem}

\begin{solution}
We begin by recalling how the Fourier transform interacts with differentiation.  
Let $g$ be smooth and rapidly decaying at $\pm\infty$.  
Then
\[
\widehat{g'}(\xi) = \int_{-\infty}^{\infty} g'(x)\,e^{-i\xi x}\,dx.
\]
Integrating by parts, with $u = e^{-i\xi x}$ and $dv = g'(x)\,dx$, we obtain
\[
\widehat{g'}(\xi)
= g(x)e^{-i\xi x}\Big|_{x=-\infty}^{x=\infty}
- \int_{-\infty}^{\infty} g(x)\,(-i\xi)\,e^{-i\xi x}\,dx.
\]
The boundary term vanishes because $g(x)\to 0$ as $|x|\to\infty$, so
\[
\widehat{g'}(\xi) = i\xi\,\widehat{g}(\xi).
\]
Applying this once more to $g'$, we find
\[
\widehat{g''}(\xi)
= i\xi\,\widehat{g'}(\xi)
= i\xi\,(i\xi\,\widehat{g}(\xi))
= -\xi^2\,\widehat{g}(\xi).
\]
Thus the second derivative in $x$ becomes multiplication by $-\xi^2$ in the Fourier domain.

We now apply the spatial Fourier transform to the heat equation.  
For each $t>0$ we define
\[
\widehat{u}(\xi,t) := \int_{-\infty}^{\infty} u(x,t)\,e^{-i\xi x}\,dx.
\]
Assuming we can differentiate under the integral sign with respect to $t$, we have
\[
\partial_t \widehat{u}(\xi,t)
= \int_{-\infty}^{\infty} u_t(x,t)\,e^{-i\xi x}\,dx.
\]
The PDE gives $u_t = \kappa u_{xx}$, hence
\[
\partial_t \widehat{u}(\xi,t)
= \kappa \int_{-\infty}^{\infty} u_{xx}(x,t)\,e^{-i\xi x}\,dx
= \kappa\,\widehat{u_{xx}}(\xi,t).
\]
Using the differentiation property just established,
\[
\widehat{u_{xx}}(\xi,t)
= -\xi^2\,\widehat{u}(\xi,t),
\]
so the transformed equation becomes
\[
\partial_t \widehat{u}(\xi,t)
= -\kappa\,\xi^2\,\widehat{u}(\xi,t).
\]
For each fixed spatial frequency $\xi \in \mathbb{R}$, this is an ordinary differential equation in $t$:
\[
\frac{d}{dt}\widehat{u}(\xi,t) = -\kappa \xi^2\,\widehat{u}(\xi,t).
\]

The initial condition $u(x,0)=f(x)$ transforms to
\[
\widehat{u}(\xi,0)
= \int_{-\infty}^{\infty} u(x,0)\,e^{-i\xi x}\,dx
= \int_{-\infty}^{\infty} f(x)\,e^{-i\xi x}\,dx
= \widehat{f}(\xi).
\]
Thus, for each $\xi$, we have the initial value problem
\[
\frac{d}{dt}\widehat{u}(\xi,t) = -\kappa \xi^2\,\widehat{u}(\xi,t),
\qquad
\widehat{u}(\xi,0) = \widehat{f}(\xi).
\]

This is a first-order linear ODE with solution
\[
\widehat{u}(\xi,t)
= e^{-\kappa \xi^2 t}\,\widehat{f}(\xi).
\]
Hence each Fourier mode of frequency $\xi$ decays exponentially in time at rate $\kappa \xi^2$: higher frequencies (larger $|\xi|$) decay faster, reflecting the smoothing effect of diffusion.

To recover $u(x,t)$ in physical space, we apply the inverse Fourier transform:
\[
u(x,t)
= \frac{1}{2\pi}\int_{-\infty}^{\infty} \widehat{u}(\xi,t)\,e^{i\xi x}\,d\xi
= \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa \xi^2 t}\,\widehat{f}(\xi)\,e^{i\xi x}\,d\xi.
\]
Substituting the Fourier representation of $\widehat{f}$,
\[
\widehat{f}(\xi)
= \int_{-\infty}^{\infty} f(y)\,e^{-i\xi y}\,dy,
\]
we obtain
\[
u(x,t)
= \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa \xi^2 t} e^{i\xi x}
\left(\int_{-\infty}^{\infty} f(y)\,e^{-i\xi y}\,dy\right)d\xi.
\]
Assuming we may interchange the order of integration, this becomes
\[
u(x,t)
= \int_{-\infty}^{\infty} f(y)
\left[
\frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa \xi^2 t}\,e^{i\xi (x-y)}\,d\xi
\right]dy.
\]
We recognize the inner integral as an inverse Fourier transform depending only on $x-y$ and $t$.  
Define the kernel
\[
K_t(z) := \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa \xi^2 t}\,e^{i\xi z}\,d\xi,
\qquad z\in\mathbb{R}.
\]
Then the solution can be written as a convolution in space:
\[
u(x,t) = (K_t * f)(x) = \int_{-\infty}^{\infty} K_t(x-y)\,f(y)\,dy.
\]

It remains to compute $K_t$ explicitly.  
We consider
\[
K_t(z) = \frac{1}{2\pi}\int_{-\infty}^{\infty} e^{-\kappa t\,\xi^2 + i\xi z}\,d\xi.
\]
We complete the square in the exponent:
\[
-\kappa t\,\xi^2 + i\xi z
= -\kappa t\left(\xi^2 - \frac{i z}{\kappa t}\xi\right)
= -\kappa t\left(\xi - \frac{i z}{2\kappa t}\right)^2 - \frac{z^2}{4\kappa t}.
\]
Thus
\[
e^{-\kappa t\,\xi^2 + i\xi z}
= \exp\!\left(-\kappa t\left(\xi - \frac{i z}{2\kappa t}\right)^2\right)
\exp\!\left(-\frac{z^2}{4\kappa t}\right).
\]
The factor $\exp\bigl(-\frac{z^2}{4\kappa t}\bigr)$ is independent of $\xi$ and can be taken outside the integral:
\[
K_t(z)
= \frac{1}{2\pi}\exp\!\left(-\frac{z^2}{4\kappa t}\right)
\int_{-\infty}^{\infty}
\exp\!\left(-\kappa t\left(\xi - \frac{i z}{2\kappa t}\right)^2\right)d\xi.
\]
We now change variables to $\eta = \xi - \frac{i z}{2\kappa t}$, so $d\xi = d\eta$.  
The path of integration can be shifted vertically in the complex plane because the integrand is an entire function and decays rapidly; the value of the Gaussian integral is unchanged.  
Hence
\[
\int_{-\infty}^{\infty}
\exp\!\left(-\kappa t\left(\xi - \frac{i z}{2\kappa t}\right)^2\right)d\xi
= \int_{-\infty}^{\infty} e^{-\kappa t\,\eta^2}\,d\eta
= \sqrt{\frac{\pi}{\kappa t}},
\]
using the standard Gaussian integral formula with $a = \kappa t>0$.  
Substituting back, we obtain
\[
K_t(z)
= \frac{1}{2\pi}\exp\!\left(-\frac{z^2}{4\kappa t}\right)
\sqrt{\frac{\pi}{\kappa t}}
= \frac{1}{\sqrt{4\pi \kappa t}}\,
\exp\!\left(-\frac{z^2}{4\kappa t}\right).
\]

Therefore,
\[
u(x,t)
= \int_{-\infty}^{\infty} \frac{1}{\sqrt{4\pi \kappa t}}\,
\exp\!\left(-\frac{(x-y)^2}{4\kappa t}\right) f(y)\,dy.
\]
This $K_t(z) = \dfrac{1}{\sqrt{4\pi \kappa t}} \exp\!\bigl(-\dfrac{z^2}{4\kappa t}\bigr)$ is called the heat kernel on the real line.  
The formula shows that an initially localized temperature profile $f$ is smoothed out by convolution with a Gaussian whose variance grows linearly in time: the heat spreads out, and the temperature at a point is a weighted average of the initial temperatures nearby, with weights given by the Gaussian.

Finally, this example illustrates the main theme of the section ``From Differential to Algebraic Equations with FT, FS and LT.''  
The spatial Fourier transform converts the spatial differential operator $u_{xx}$ into multiplication by $-\xi^2$ in frequency space, so the original partial differential equation in $(x,t)$ becomes, for each $\xi$, an ordinary differential equation in $t$ with a simple exponential solution.  
The inversion of the transform then packages these exponentially decaying modes back into physical space as a convolution with the heat kernel.  
Thus the Fourier transform plays the same diagonalizing role on the whole line that Fourier series play on a finite interval, turning a differential operator into a simple algebraic multiplier in the transformed domain.
\end{solution}

% ===== Example 4: Damped Harmonic Oscillator and Resonance in the Laplace Domain (inquiry-based) =====
\begin{problem}[Damped Harmonic Oscillator and Resonance in the Laplace Domain]
A mass--spring--damper system is a standard model for many mechanical and electrical devices. A mass $m$ is attached to a spring with stiffness $k$ and a dashpot with damping coefficient $c$, and is driven by a periodic external force. In the time domain, the resulting ordinary differential equation couples inertia, damping, and restoring forces through derivatives of the displacement. In this problem you will see how the Laplace transform converts this differential equation into an algebraic equation in the Laplace variable $s$, and how the poles of the transformed solution encode transient and steady-state behavior, including resonance.

Consider a mass $m>0$ attached to a spring (stiffness $k>0$) and a damper (damping coefficient $c>0$), subject to an external forcing
\[
F_{\text{ext}}(t) = F_0 \cos(\omega t) \quad \text{for } t>0,
\]
with constants $F_0>0$ and forcing frequency $\omega>0$. Let $x(t)$ denote the displacement of the mass from equilibrium.

\smallskip

(a) Write down the second-order linear ODE that models the motion $x(t)$ of the mass, assuming that:
\begin{itemize}
  \item the spring force is $-kx(t)$,
  \item the damping force is $-c x'(t)$,
  \item the external force is $F_0 \cos(\omega t)$.
\end{itemize}
State clearly the initial conditions you will use to model a system that is \emph{initially at rest}. Briefly explain the physical meaning of each term in your ODE.

\smallskip

(b) Recall that the Laplace transform of a function $x(t)$ is
\[
X(s) = \mathcal{L}\{x\}(s) = \int_0^\infty e^{-st} x(t)\,dt.
\]
Using the standard formulas
\[
\mathcal{L}\{x'(t)\} = s X(s) - x(0), 
\qquad
\mathcal{L}\{x''(t)\} = s^2 X(s) - s x(0) - x'(0),
\]
and
\[
\mathcal{L}\{\cos(\omega t)\} = \frac{s}{s^2 + \omega^2},
\]
take the Laplace transform of your ODE from part (a). Solve the resulting algebraic equation for $X(s)$ in terms of $m,c,k,F_0,\omega$, and $s$.

\emph{Hint:} Keep the expression factored as much as possible; look for a factor $m s^2 + c s + k$ coming from the homogeneous part.

\smallskip

(c) The denominator of $X(s)$ encodes the system’s characteristic behavior. 

\begin{enumerate}
\item[(i)] Factor the quadratic polynomial
\[
m s^2 + c s + k
\]
and denote its roots by $r_1$ and $r_2$. Under the \emph{underdamped} assumption $c^2 < 4mk$, what can you say about the real and imaginary parts of $r_1$ and $r_2$?

\item[(ii)] List all poles of $X(s)$ (that is, all values of $s$ where $X(s)$ has a singularity). Distinguish between those coming from the homogeneous dynamics and those coming from the forcing term $\cos(\omega t)$.

\item[(iii)] Explain, at a qualitative level, why poles with negative real parts correspond to \emph{transient} terms in $x(t)$, while poles on the imaginary axis correspond to \emph{persistent oscillations}.

\emph{Hint:} Think about the inverse Laplace transforms of terms like $1/(s-a)$ and $1/(s^2+\omega^2)$.
\end{enumerate}

\smallskip

(d) To make the transient and steady-state contributions explicit, consider a partial fraction decomposition of $X(s)$ of the form
\[
X(s) = \frac{A s + B}{s^2 + \omega^2} + \frac{C s + D}{m s^2 + c s + k},
\]
for suitable constants $A,B,C,D$ depending on $m,c,k,F_0,\omega$.

\begin{enumerate}
\item[(i)] Multiply both sides by $(s^2 + \omega^2)(m s^2 + c s + k)$ and equate coefficients of powers of $s$ to obtain a linear system for $A,B,C,D$. (You do not need to solve completely for $C$ and $D$, but try to express $A$ and $B$ in terms of the parameters.)

\emph{Hint:} You should get four equations by matching coefficients of $s^3,s^2,s^1,s^0$ on both sides.

\item[(ii)] Using the Laplace transform table, write down the inverse Laplace transform of each term in your decomposition. Show that
\[
\mathcal{L}^{-1}\!\left\{\frac{A s + B}{s^2 + \omega^2}\right\}(t)
= A \cos(\omega t) + \frac{B}{\omega} \sin(\omega t),
\]
while 
\[
\mathcal{L}^{-1}\!\left\{\frac{C s + D}{m s^2 + c s + k}\right\}(t)
\]
is a linear combination of decaying exponentials (possibly multiplied by sines and cosines).

\item[(iii)] Conclude that the solution $x(t)$ can be written in the form
\[
x(t) = x_{\text{tr}}(t) + x_{\text{ss}}(t),
\]
where $x_{\text{tr}}(t)$ is a transient term that decays to zero as $t\to\infty$, and
\[
x_{\text{ss}}(t) = A \cos(\omega t) + \frac{B}{\omega} \sin(\omega t)
\]
is the steady-state oscillation at the forcing frequency $\omega$.

Finally, show that $x_{\text{ss}}(t)$ can be written as a single sinusoid
\[
x_{\text{ss}}(t) = R(\omega)\cos(\omega t - \delta(\omega)),
\]
and express the amplitude $R(\omega)$ in terms of $A,B$, and $\omega$.

\emph{Hint:} Use the identity $a\cos\theta + b\sin\theta = \sqrt{a^2 + b^2}\,\cos(\theta - \delta)$ for a suitable phase shift $\delta$.
\end{enumerate}

\smallskip

(e) In this final part, you will connect the Laplace-domain picture to the phenomenon of resonance.

\begin{enumerate}
\item[(i)] Show (either by completing part (d) fully or by a separate complex-exponential calculation) that the amplitude of the steady-state response is
\[
R(\omega) = \frac{F_0}{\sqrt{(k - m\omega^2)^2 + c^2 \omega^2}}.
\]
How does $R(\omega)$ behave as a function of $\omega$ when $c>0$ is small but fixed? Where (approximately) does it attain its maximum?

\emph{Hint:} Compare $\omega$ with the undamped natural frequency $\omega_0 = \sqrt{k/m}$.

\item[(ii)] \emph{What if} the damping were removed, so $c=0$? Examine the poles of $X(s)$ and the formula for $R(\omega)$ in this case. What happens when the forcing frequency $\omega$ approaches the natural frequency $\omega_0$? Relate this to the idea of resonance and to the movement of poles in the complex $s$-plane.

\end{enumerate}

\end{problem}

% ===== Example 4: Damped Harmonic Oscillator and Resonance in the Laplace Domain (full solution) =====
\begin{problem}[Damped Harmonic Oscillator and Resonance in the Laplace Domain]
Consider the mass--spring--damper system
\[
m x''(t) + c x'(t) + k x(t) = F_0 \cos(\omega t), \qquad t>0,
\]
with constants $m>0$, $c>0$, $k>0$, forcing amplitude $F_0>0$, and forcing frequency $\omega>0$. Assume the mass is initially at rest: $x(0)=0$ and $x'(0)=0$.

\begin{enumerate}
\item[(a)] Use the Laplace transform to solve for $x(t)$, expressing the solution as a sum of a transient term and a steady-state term. Clearly identify the poles of $X(s)=\mathcal{L}\{x\}(s)$ and explain which poles correspond to transient behavior and which to persistent oscillations.
\item[(b)] Show that the steady-state solution has the form
\[
x_{\mathrm{ss}}(t) = R(\omega)\cos\bigl(\omega t - \delta(\omega)\bigr),
\]
and derive the amplitude
\[
R(\omega) = \frac{F_0}{\sqrt{(k - m\omega^2)^2 + c^2 \omega^2}}.
\]
\item[(c)] Discuss how $R(\omega)$ behaves as a function of $\omega$, and interpret the phenomenon of resonance in terms of the location of the poles of $X(s)$ in the complex $s$-plane.
\end{enumerate}
\end{problem}

\begin{solution}
We analyze the forced mass--spring--damper system using the Laplace transform to convert the differential equation into an algebraic equation in the complex variable $s$. This illustrates the general principle of this chapter: transforms turn differentiation into multiplication, making spectral properties (such as natural frequencies and damping) visible as poles of a rational function.

\medskip

\noindent\textbf{Step 1: Laplace transform of the ODE.}

We start from
\[
m x''(t) + c x'(t) + k x(t) = F_0 \cos(\omega t), \qquad t>0,
\]
with initial conditions
\[
x(0) = 0, \qquad x'(0) = 0.
\]

Let $X(s) = \mathcal{L}\{x\}(s)$. We use the standard formulas
\[
\mathcal{L}\{x'(t)\} = s X(s) - x(0), \qquad
\mathcal{L}\{x''(t)\} = s^2 X(s) - s x(0) - x'(0),
\]
and
\[
\mathcal{L}\{\cos(\omega t)\} = \frac{s}{s^2 + \omega^2}.
\]
Applying the Laplace transform to both sides of the ODE gives
\[
m\bigl(s^2 X(s) - s x(0) - x'(0)\bigr)
+ c\bigl(s X(s) - x(0)\bigr)
+ k X(s)
= F_0 \frac{s}{s^2 + \omega^2}.
\]
Using $x(0)=x'(0)=0$, this simplifies to
\[
(m s^2 + c s + k)\, X(s) = F_0 \frac{s}{s^2 + \omega^2}.
\]
Thus
\begin{equation}\label{eq:Xs-basic}
X(s) = \frac{F_0\, s}{(s^2 + \omega^2)(m s^2 + c s + k)}.
\end{equation}
We have converted the second-order ODE into an algebraic equation for $X(s)$.

\medskip

\noindent\textbf{Step 2: Poles and their interpretation.}

The denominator of $X(s)$ is
\[
(s^2 + \omega^2)(m s^2 + c s + k).
\]
The factor $s^2 + \omega^2$ has roots
\[
s = \pm i \omega,
\]
giving two purely imaginary poles. These come directly from the forcing term $\cos(\omega t)$.

The other factor is the characteristic polynomial
\[
m s^2 + c s + k = 0.
\]
Its roots are
\[
s = r_{1,2} = \frac{-c \pm \sqrt{c^2 - 4mk}}{2m}.
\]
Under the underdamped assumption $c^2 < 4mk$, the discriminant is negative, so
\[
r_{1,2} = -\alpha \pm i \beta
\]
for some $\alpha>0$ and $\beta>0$. Thus these two poles lie strictly in the left half of the complex $s$-plane.

Altogether, $X(s)$ has four poles:
\[
s = r_1,\quad s = r_2 \quad (\text{with } \Re r_{1,2}<0), 
\qquad s = i\omega,\quad s = -i\omega.
\]

To interpret them, recall that the inverse Laplace transform of $1/(s-a)$ is $e^{at}$, and the inverse transform of $1/(s^2+\omega^2)$ is a combination of $\sin(\omega t)$ and $\cos(\omega t)$. Thus:
\begin{itemize}
  \item Poles with negative real part, such as $r_{1,2}$, produce terms proportional to $e^{r_1 t}$ and $e^{r_2 t}$, which decay to zero as $t\to\infty$. These terms are the \emph{transient} response, determined by the homogeneous dynamics and initial conditions.
  \item Purely imaginary poles, such as $\pm i\omega$, produce undamped oscillations at frequency $\omega$ that do not decay. These correspond to \emph{persistent} oscillations, here driven by the periodic forcing.
\end{itemize}
We therefore expect $x(t)$ to consist of a transient part that dies out and a steady-state part oscillating at the forcing frequency $\omega$.

\medskip

\noindent\textbf{Step 3: Partial fractions and decomposition into transient and steady state.}

We now decompose $X(s)$ in a way that separates the contributions of the two quadratic factors. Because both factors are quadratic, it is natural to write
\begin{equation}\label{eq:PF}
X(s) = \frac{A s + B}{s^2 + \omega^2}
+ \frac{C s + D}{m s^2 + c s + k},
\end{equation}
for some constants $A,B,C,D$ depending on $m,c,k,F_0,\omega$.

Multiplying both sides by the common denominator $(s^2 + \omega^2)(m s^2 + c s + k)$, we obtain the identity
\[
F_0\, s
= (A s + B)(m s^2 + c s + k)
+ (C s + D)(s^2 + \omega^2),
\]
valid for all $s$. Expanding both products gives
\[
(A s + B)(m s^2 + c s + k)
= A m s^3 + (A c + B m) s^2 + (A k + B c) s + B k,
\]
and
\[
(C s + D)(s^2 + \omega^2)
= C s^3 + D s^2 + C \omega^2 s + D \omega^2.
\]
Adding and matching coefficients with $F_0 s$ (which has no $s^3$, $s^2$, or constant term) yields the system
\begin{align*}
s^3: &\quad A m + C = 0,\\
s^2: &\quad A c + B m + D = 0,\\
s^1: &\quad A k + B c + C \omega^2 = F_0,\\
s^0: &\quad B k + D \omega^2 = 0.
\end{align*}

For our purposes, we do not actually need explicit formulas for all four constants, because we know the qualitative form of the inverse transform. From \eqref{eq:PF},
\[
\mathcal{L}^{-1}\!\left\{\frac{A s + B}{s^2 + \omega^2}\right\}(t)
= A \cos(\omega t) + \frac{B}{\omega} \sin(\omega t),
\]
and the second term is the response associated with the homogeneous polynomial $m s^2 + c s + k$. In the underdamped case, its inverse transform is a linear combination of $e^{-\alpha t}\cos(\beta t)$ and $e^{-\alpha t}\sin(\beta t)$, hence decays exponentially to zero as $t\to\infty$.

Thus we can write
\[
x(t) = x_{\mathrm{tr}}(t) + x_{\mathrm{ss}}(t),
\]
where the \emph{transient} term is
\[
x_{\mathrm{tr}}(t) = \mathcal{L}^{-1}\!\left\{\frac{C s + D}{m s^2 + c s + k}\right\}(t),
\]
which decays to $0$, and the \emph{steady-state} term is
\[
x_{\mathrm{ss}}(t) 
= \mathcal{L}^{-1}\!\left\{\frac{A s + B}{s^2 + \omega^2}\right\}(t)
= A \cos(\omega t) + \frac{B}{\omega} \sin(\omega t).
\]

Any linear combination of $\cos(\omega t)$ and $\sin(\omega t)$ can be rewritten as a single sinusoid with phase shift. Using the identity
\[
a \cos(\theta) + b \sin(\theta)
= \sqrt{a^2 + b^2}\,\cos\bigl(\theta - \delta\bigr), \quad
\delta = \arctan\!\left(\frac{b}{a}\right),
\]
we obtain
\[
x_{\mathrm{ss}}(t)
= R(\omega)\cos\bigl(\omega t - \delta(\omega)\bigr),
\]
where
\[
R(\omega) = \sqrt{A^2 + \left(\frac{B}{\omega}\right)^2}
\]
is the steady-state amplitude and $\delta(\omega)$ is the phase shift.

We still need to compute $R(\omega)$ in terms of the physical parameters.

\medskip

\noindent\textbf{Step 4: Computing the steady-state amplitude $R(\omega)$.}

There are two efficient ways to find $R(\omega)$:

\begin{itemize}
  \item One can solve explicitly for $A$ and $B$ from the linear system above and substitute into $R(\omega
(\omega)}$, but this is somewhat tedious.
  \item A cleaner route is to use the transfer-function viewpoint (or an equivalent complex-exponential computation), which we outline next.
\end{itemize}

Rewrite \eqref{eq:Xs-basic} as
\[
X(s)
= \frac{1}{m s^2 + c s + k}\;\cdot\;\frac{F_0 s}{s^2 + \omega^2}
= H(s)\,\mathcal{L}\{F_0 \cos(\omega t)\}(s),
\]
where
\[
H(s) = \frac{1}{m s^2 + c s + k}
\]
is the system’s transfer function from input force $F_{\text{ext}}$ to output displacement $x$.

For a sinusoidal input $F_{\text{ext}}(t) = F_0\cos(\omega t)$, the long-time (steady-state) response of a stable linear system is sinusoidal at the same frequency, with complex \emph{gain} $H(i\omega)$:
\[
x_{\mathrm{ss}}(t)
= \Re\Bigl\{H(i\omega)\,F_0 e^{i\omega t}\Bigr\}.
\]
Thus the steady-state amplitude is
\[
R(\omega) = F_0\,|H(i\omega)|.
\]

Now
\[
H(i\omega) = \frac{1}{m (i\omega)^2 + c (i\omega) + k}
= \frac{1}{k - m\omega^2 + i c \omega}.
\]
Hence
\[
|H(i\omega)|
= \frac{1}{\sqrt{(k - m\omega^2)^2 + (c\omega)^2}},
\]
and
\[
R(\omega) = \frac{F_0}{\sqrt{(k - m\omega^2)^2 + c^2 \omega^2}}.
\]

This agrees with the amplitude obtained by writing
\[
x_{\mathrm{ss}}(t) = A \cos(\omega t) + \frac{B}{\omega} \sin(\omega t)
= R(\omega)\cos\bigl(\omega t - \delta(\omega)\bigr),
\]
with
\[
R(\omega) = \sqrt{A^2 + \left(\frac{B}{\omega}\right)^2}.
\]

This completes part (b): the steady-state solution has the form
\[
x_{\mathrm{ss}}(t) = R(\omega)\cos\bigl(\omega t - \delta(\omega)\bigr),
\quad
R(\omega) = \frac{F_0}{\sqrt{(k - m\omega^2)^2 + c^2 \omega^2}}.
\]

\medskip

\noindent\textbf{Step 5: Behavior of $R(\omega)$ and resonance.}

We now discuss how $R(\omega)$ behaves as a function of $\omega$ and interpret resonance via the poles of $X(s)$ (part (c)).

\medskip

\noindent\emph{(i) Shape of $R(\omega)$ for $c>0$.}

Recall
\[
R(\omega) = \frac{F_0}{\sqrt{(k - m\omega^2)^2 + c^2 \omega^2}},
\qquad \omega\ge 0.
\]

Basic features:
\begin{itemize}
  \item At zero frequency,
  \[
  R(0) = \frac{F_0}{k},
  \]
  which is just the static deflection of a spring under a constant force.
  \item As $\omega\to\infty$,
  \[
  (k - m\omega^2)^2 \sim m^2\omega^4,
  \]
  so $R(\omega)\sim F_0/(m\omega^2)\to 0$. Very rapid forcing produces little displacement.
  \item For $0<c\ll 1$, $R(\omega)$ has a pronounced peak near the undamped natural frequency
  \[
  \omega_0 = \sqrt{\frac{k}{m}}.
  \]
\end{itemize}

To locate the maximum more precisely, maximize $R(\omega)$ over $\omega>0$. Since $F_0$ is constant, this is equivalent to minimizing
\[
D(\omega) := (k - m\omega^2)^2 + c^2\omega^2.
\]
Differentiating and setting $D'(\omega)=0$ gives
\[
D'(\omega)
= 2(k - m\omega^2)(-2m\omega) + 2c^2\omega
= 2\omega\bigl(-2m(k - m\omega^2) + c^2\bigr)
= 0.
\]
Aside from the trivial solution $\omega=0$, we obtain
\[
-2m(k - m\omega^2) + c^2 = 0
\quad\Rightarrow\quad
2m^2\omega^2 = 2mk - c^2
\quad\Rightarrow\quad
\omega^2 = \frac{k}{m} - \frac{c^2}{2m^2}.
\]
Thus the \emph{resonant frequency} is
\[
\omega_{\mathrm{res}}
= \sqrt{\omega_0^2 - \frac{c^2}{2m^2}},
\]
which satisfies $\omega_{\mathrm{res}}<\omega_0$ and approaches $\omega_0$ as $c\to 0^+$. So for small but nonzero damping, $R(\omega)$ has a finite peak slightly below the undamped natural frequency.

\medskip

\noindent\emph{(ii) The undamped case $c=0$ and resonance.}

If we set $c=0$ in the ODE
\[
m x''(t) + k x(t) = F_0 \cos(\omega t),
\]
then
\[
X(s) = \frac{F_0\, s}{(s^2 + \omega^2)(m s^2 + k)}
= \frac{F_0\, s}{(s^2 + \omega^2)\bigl(m s^2 + k\bigr)}.
\]
The system’s transfer function becomes
\[
H(s) = \frac{1}{m s^2 + k},
\]
with poles at
\[
s = \pm i\omega_0, \qquad \omega_0 = \sqrt{\frac{k}{m}}.
\]
These poles now lie exactly on the imaginary axis. The forcing term still contributes poles at $s=\pm i\omega$ from $s^2+\omega^2$.

The steady-state amplitude formula reduces to
\[
R(\omega)
= \frac{F_0}{|k - m\omega^2|}
= \frac{F_0}{m|\omega_0^2 - \omega^2|}.
\]
As $\omega\to\omega_0$, the denominator tends to $0$, so $R(\omega)\to\infty$: the steady-state amplitude is unbounded. In the time domain, at exact resonance $\omega=\omega_0$, the particular solution contains a term that grows linearly in $t$, such as
\[
x(t) \sim \frac{F_0}{2m\omega_0}\,t\sin(\omega_0 t),
\]
illustrating classical resonance.

\medskip

\noindent\emph{(iii) Pole motion and resonance in the $s$-plane.}

From the transfer function perspective, the frequency response is
\[
H(i\omega) = \frac{1}{m(i\omega)^2 + c(i\omega) + k}
= \frac{1}{k - m\omega^2 + i c\omega}.
\]
Its magnitude $|H(i\omega)|$ is largest when $i\omega$ lies closest (in the complex plane) to a pole of $H(s)$.

\begin{itemize}
  \item For $c>0$, the poles
  \[
  r_{1,2} = \frac{-c \pm \sqrt{c^2 - 4mk}}{2m}
  \]
  lie in the left half-plane, at $-\alpha\pm i\beta$ ($\alpha>0$). The imaginary parts $\pm\beta$ are close to $\pm\omega_0$ when $c$ is small. The point $i\omega$ on the imaginary axis is closest to these poles when $\omega\approx\beta\approx\omega_0$, so $|H(i\omega)|$ and thus $R(\omega)$ attain a finite but large maximum near $\omega_0$.
  \item As $c\to 0^+$, the poles $r_{1,2}$ move horizontally toward the imaginary axis and converge to $\pm i\omega_0$. In the limit $c=0$, they sit exactly on the imaginary axis. When the forcing frequency matches $\omega_0$, the point $s=i\omega$ coincides with a pole of $H(s)$, and the gain $|H(i\omega)|$ becomes unbounded. This pole collision on the imaginary axis is the Laplace-domain picture of resonance.
\end{itemize}

Thus, resonance corresponds to the forcing frequency aligning with the imaginary part of the system’s poles. Damping pushes the poles into the left half-plane, limiting the peak amplitude; removing damping allows the poles to lie on the imaginary axis, leading to unbounded growth at resonance.

\end{solution}

% ===== Example 5: Wave Equation on a String via Fourier Series (inquiry-based) =====
\begin{problem}[Wave Equation on a String via Fourier Series]
A taut string of length $L$ is fixed at both ends and is free to vibrate transversely. Its vertical displacement from equilibrium is modeled by a function $u(x,t)$, where $x\in[0,L]$ measures position along the string and $t\ge 0$ is time. The motion is governed (under standard assumptions) by the one-dimensional wave equation
\[
u_{tt} = c^2 u_{xx},
\]
where $c>0$ is the wave speed. The endpoints are held fixed, and the initial shape and initial velocity are prescribed.

In this problem you will discover how separation of variables and Fourier \emph{sine} series naturally arise, and how they convert the partial differential equation into a family of independent ordinary differential equations for the vibration modes.

\smallskip

We consider the initial–boundary value problem
\[
\begin{cases}
u_{tt}(x,t) = c^2 u_{xx}(x,t), & 0<x<L,\ t>0,\\[0.3em]
u(0,t) = 0,\quad u(L,t) = 0, & t\ge 0,\\[0.3em]
u(x,0) = f(x),\quad u_t(x,0)=g(x), & 0\le x\le L,
\end{cases}
\]
where $f$ and $g$ are given functions.

\begin{enumerate}[(a)]
  \item \textbf{Separation of variables and the eigenvalue problem in $x$.}  

  Assume a separated solution of the form $u(x,t)=X(x)T(t)$, with $X$ not identically zero and $T$ not identically zero.
  \begin{enumerate}[(i)]
    \item Substitute $u(x,t)=X(x)T(t)$ into the wave equation and show that you obtain an equation of the form
    \[
    \frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)} = -\lambda,
    \]
    for some constant $\lambda\in\mathbb{R}$ (the separation constant).
    \item Write down the resulting ordinary differential equation for $X$ together with the boundary conditions inherited from $u(0,t)=u(L,t)=0$.
  \end{enumerate}
  Hint: Carefully divide by $c^2 X(x)T(t)$ and argue why both sides must be equal to the same constant.

  \item \textbf{Finding the allowed spatial modes.}

  You have obtained the boundary value problem
  \[
  X''(x) + \lambda X(x) = 0,\quad X(0)=0,\ X(L)=0.
  \]
  \begin{enumerate}[(i)]
    \item Consider separately the three cases $\lambda<0$, $\lambda=0$, and $\lambda>0$. For each case, solve the ODE for $X(x)$ and apply the boundary conditions. Show that nontrivial solutions $X$ exist \emph{only} for a discrete set of values $\lambda=\lambda_n$.
    \item Determine these eigenvalues $\lambda_n$ explicitly and show that the corresponding eigenfunctions can be chosen as
    \[
    X_n(x) = \sin\!\left(\frac{n\pi x}{L}\right),\quad n=1,2,3,\dots
    \]
  \end{enumerate}
  Hint: For $\lambda>0$, it is convenient to write $\lambda=\mu^2$ with $\mu>0$. Recall standard solutions of $y''+\mu^2 y=0$.

  \item \textbf{Time evolution of each mode and quantized frequencies.}

  For each eigenvalue $\lambda_n$, the corresponding time factor $T_n(t)$ satisfies
  \[
  T_n''(t) + c^2 \lambda_n\, T_n(t) = 0.
  \]
  \begin{enumerate}[(i)]
    \item Using your expression for $\lambda_n$, solve this ODE and show that
    \[
    T_n(t) = A_n\cos(\omega_n t) + B_n\sin(\omega_n t),
    \]
    for constants $A_n,B_n$, where you should determine $\omega_n$ in terms of $c$, $L$, and $n$.
    \item Interpret $\omega_n$ as a natural frequency of the string, and briefly explain in words how the boundary conditions lead to “quantized” frequencies (that is, only certain discrete frequencies are allowed).
  \end{enumerate}
  Hint: Recognize the standard harmonic oscillator equation $y'' + \omega^2 y = 0$.

  \item \textbf{Superposition and Fourier sine series for the full solution.}

  The separated solutions corresponding to each $n$ can be written as
  \[
  u_n(x,t) = \bigl(A_n\cos(\omega_n t) + B_n\sin(\omega_n t)\bigr) \sin\!\left(\frac{n\pi x}{L}\right).
  \]
  Since the wave equation is linear, we seek a general solution as a superposition
  \[
  u(x,t) = \sum_{n=1}^\infty \bigl(A_n\cos(\omega_n t) + B_n\sin(\omega_n t)\bigr) \sin\!\left(\frac{n\pi x}{L}\right).
  \]
  \begin{enumerate}[(i)]
    \item Use the initial displacement condition $u(x,0)=f(x)$ to derive an expression for $f(x)$ as a Fourier sine series. Express the coefficients $A_n$ as integrals involving $f$ and $\sin(n\pi x/L)$.
    \item Similarly, use the initial velocity condition $u_t(x,0)=g(x)$ to obtain a Fourier sine series expansion for $g(x)$ and express the coefficients $B_n$ as integrals involving $g$.
  \end{enumerate}
  Hint: Recall the orthogonality relation
  \[
  \int_0^L \sin\!\left(\frac{n\pi x}{L}\right)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
  =\begin{cases}
  0,& n\ne m,\\[0.3em]
  \dfrac{L}{2},& n=m.
  \end{cases}
  \]

  \item \textbf{Extensions and variations.}
  \begin{enumerate}[(i)]
    \item Suppose the string is initially at rest, so $g(x)\equiv 0$, but has initial shape $f(x)=x(L-x)$ on $[0,L]$. Write down (without fully evaluating the integrals) the explicit Fourier sine series coefficients that would determine $u(x,t)$ for this case.
    \item How would the analysis change if the right endpoint of the string were \emph{free} instead of fixed? That is, if $u(0,t)=0$ and $u_x(L,t)=0$ for all $t\ge 0$? Describe qualitatively (without solving in full detail) what kinds of eigenfunctions in $x$ you would expect and how the allowed frequencies $\omega_n$ might differ from the fixed–fixed case.
  \end{enumerate}
  Hint: For a free end at $x=L$, the boundary condition expresses that the spatial derivative $u_x$ vanishes there, so the eigenfunctions must satisfy $X'(L)=0$ rather than $X(L)=0$.
\end{enumerate}
\end{problem}

% ===== Example 5: Wave Equation on a String via Fourier Series (full solution) =====
\begin{problem}[Wave Equation on a String via Fourier Series]
Consider the wave equation for a taut string of length $L$ with fixed endpoints:
\[
\begin{cases}
u_{tt}(x,t) = c^2 u_{xx}(x,t), & 0<x<L,\ t>0,\\[0.3em]
u(0,t)=0,\quad u(L,t)=0, & t\ge 0,\\[0.3em]
u(x,0)=f(x),\quad u_t(x,0)=g(x), & 0\le x\le L,
\end{cases}
\]
where $c>0$ is constant and $f,g$ are given functions (assume they are piecewise smooth so that Fourier series converge in the usual sense).

(a) Use separation of variables to find the eigenfunctions and eigenvalues of the associated spatial problem and the corresponding time-dependent factors.

(b) Show that the solution can be written as a Fourier sine series
\[
u(x,t) = \sum_{n=1}^\infty \bigl(a_n\cos(\omega_n t) + b_n\sin(\omega_n t)\bigr)\sin\!\left(\frac{n\pi x}{L}\right),
\]
and derive formulas for the coefficients $a_n$ and $b_n$ in terms of $f$ and $g$. Identify the natural frequencies $\omega_n$.

\end{problem}

\begin{solution}
We solve the initial–boundary value problem
\[
u_{tt} = c^2 u_{xx},\quad 0<x<L,\ t>0,
\]
with fixed ends $u(0,t)=u(L,t)=0$ and initial conditions $u(x,0)=f(x)$, $u_t(x,0)=g(x)$, by separation of variables and Fourier sine series. The key idea is to convert the partial differential equation into a family of ordinary differential equations for the amplitudes of spatial modes.

\medskip

\textbf{1. Separation of variables.}
We seek separated solutions of the form
\[
u(x,t) = X(x)T(t),
\]
with $X$ and $T$ not identically zero. Substituting into the wave equation gives
\[
X(x)T''(t) = c^2 X''(x)T(t).
\]
Assuming $X(x)T(t)\neq 0$, we divide by $c^2 X(x)T(t)$:
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)}.
\]
The left-hand side depends only on $t$, and the right-hand side only on $x$, so both must be equal to a constant, which we denote by $-\lambda$:
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)} = -\lambda.
\]
This gives two ordinary differential equations,
\[
X''(x) + \lambda X(x) = 0,\qquad
T''(t) + c^2\lambda T(t) = 0.
\]
The boundary conditions $u(0,t)=u(L,t)=0$ become
\[
X(0)T(t)=0,\quad X(L)T(t)=0\quad\text{for all }t\ge 0.
\]
Since we seek nontrivial time dependence $T(t)\not\equiv 0$, these imply
\[
X(0)=0,\quad X(L)=0.
\]
Thus we have a boundary value problem for $X$:
\[
X''(x) + \lambda X(x) = 0,\quad X(0)=0,\ X(L)=0.
\]

\medskip

\textbf{2. Spatial eigenvalue problem.}
We analyze the possible values of $\lambda$.

\emph{Case 1: $\lambda<0$.} Write $\lambda=-\mu^2$ with $\mu>0$. Then
\[
X''(x) - \mu^2 X(x) = 0,
\]
whose general solution is $X(x)=C_1 e^{\mu x} + C_2 e^{-\mu x}$. The boundary condition $X(0)=0$ gives
\[
C_1 + C_2 = 0 \quad\Rightarrow\quad C_2 = -C_1,
\]
so $X(x)=C_1(e^{\mu x}-e^{-\mu x})=2C_1\sinh(\mu x)$. The condition $X(L)=0$ then implies $\sinh(\mu L)=0$. Since $\mu>0$, we have $\sinh(\mu L)\neq 0$, so $C_1=0$, and $X$ is the trivial solution. Thus there are no nontrivial solutions for $\lambda<0$.

\emph{Case 2: $\lambda=0$.} The ODE is $X''(x)=0$, with general solution $X(x)=C_1x + C_2$. The conditions $X(0)=0$ and $X(L)=0$ give $C_2=0$ and $C_1 L=0$, so $C_1=0$. Again, only the trivial solution exists.

\emph{Case 3: $\lambda>0$.} Write $\lambda=\mu^2$ with $\mu>0$. The ODE becomes
\[
X''(x) + \mu^2 X(x)=0,
\]
whose general solution is
\[
X(x) = A\cos(\mu x) + B\sin(\mu x).
\]
The boundary condition $X(0)=0$ implies $A=0$, so $X(x)=B\sin(\mu x)$. The condition $X(L)=0$ then gives
\[
B\sin(\mu L)=0.
\]
To have a nontrivial solution $X\not\equiv 0$, we require $B\neq 0$, hence
\[
\sin(\mu L)=0 \quad\Rightarrow\quad \mu L = n\pi,\quad n=1,2,3,\dots
\]
Thus
\[
\mu_n = \frac{n\pi}{L},\qquad \lambda_n = \mu_n^2 = \frac{n^2\pi^2}{L^2}.
\]
The corresponding eigenfunctions can be chosen (up to constant multiples) as
\[
X_n(x) = \sin\!\left(\frac{n\pi x}{L}\right),\quad n=1,2,3,\dots
\]

We have therefore discretized the spatial part: only discrete ``modes'' indexed by $n$ are compatible with the fixed–end boundary conditions.

\medskip

\textbf{3. Time-dependent factors and natural frequencies.}
For each eigenvalue $\lambda_n$, the time equation is
\[
T_n''(t) + c^2 \lambda_n T_n(t) = 0.
\]
Substituting $\lambda_n = n^2\pi^2/L^2$ gives
\[
T_n''(t) + \left(\frac{n\pi c}{L}\right)^2 T_n(t) = 0.
\]
This is the standard harmonic oscillator equation $y'' + \omega_n^2 y=0$ with
\[
\omega_n = \frac{n\pi c}{L}.
\]
Hence the general solution is
\[
T_n(t) = A_n \cos(\omega_n t) + B_n \sin(\omega_n t),
\]
for constants $A_n,B_n$.

Each separated solution corresponding to the $n$-th mode is therefore
\[
u_n(x,t) = T_n(t)X_n(x)
= \bigl(A_n\cos(\omega_n t) + B_n\sin(\omega_n t)\bigr)\sin\!\left(\frac{n\pi x}{L}\right).
\]
The quantities $\omega_n$ are the natural frequencies of the string. Because the spatial boundary conditions restrict $\mu$ to values $\mu_n = n\pi/L$, only the discrete frequencies $\omega_n = n\pi c/L$ are allowed: the boundary conditions \emph{quantize} the possible vibration frequencies.

\medskip

\textbf{4. Superposition and Fourier sine series.}
The wave equation is linear and homogeneous, so any linear combination of solutions is again a solution. We therefore form the general solution as a superposition of modes:
\[
u(x,t) = \sum_{n=1}^\infty \bigl(A_n\cos(\omega_n t) + B_n\sin(\omega_n t)\bigr)\sin\!\left(\frac{n\pi x}{L}\right),
\]
with $\omega_n = n\pi c/L$. The task is now to choose the coefficients $A_n$ and $B_n$ to satisfy the initial conditions.

\emph{Initial displacement.} At $t=0$ we have
\[
u(x,0) = \sum_{n=1}^\infty \bigl(A_n\cos(0) + B_n\sin(0)\bigr)\sin\!\left(\frac{n\pi x}{L}\right)
= \sum_{n=1}^\infty A_n \sin\!\left(\frac{n\pi x}{L}\right).
\]
The initial condition $u(x,0)=f(x)$ becomes
\[
f(x) = \sum_{n=1}^\infty A_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
which is precisely a Fourier sine series expansion of $f$ on $(0,L)$.

The sine functions are orthogonal on $[0,L]$:
\[
\int_0^L \sin\!\left(\frac{n\pi x}{L}\right)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
=
\begin{cases}
0,& n\neq m,\\[0.3em]
\dfrac{L}{2},& n=m.
\end{cases}
\]
Multiply the series for $f(x)$ by $\sin(m\pi x/L)$ and integrate from $0$ to $L$:
\[
\int_0^L f(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= \sum_{n=1}^\infty A_n \int_0^L \sin\!\left(\frac{n\pi x}{L}\right)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= A_m \frac{L}{2}.
\]
Thus
\[
A_m = \frac{2}{L} \int_0^L f(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx,\quad m=1,2,3,\dots
\]
So the coefficients $A_n$ are exactly the Fourier sine coefficients of $f$.

\emph{Initial velocity.} Differentiate $u(x,t)$ with respect to $t$:
\[
u_t(x,t) = \sum_{n=1}^\infty \bigl(-A_n\omega_n\sin(\omega_n t) + B_n\omega_n\cos(\omega_n t)\bigr)\sin\!\left(\frac{n\pi x}{L}\right).
\]
At $t=0$ this becomes
\[
u_t(x,0) = \sum_{n=1}^\infty B_n\omega_n \sin\!\left(\frac{n\pi x}{L}\right),
\]
since $\sin(0)=0$ and $\cos(0)=1$. The initial condition $u_t(x,0)=g(x)$ thus yields
\[
g(x) = \sum_{n=1}^\infty B_n\omega_n \sin\!\left(\frac{n\pi x}{L}\right).
\]
This is again a Fourier sine series, now for $g(x)$. Using orthogonality as before,
\[
\int_0^L g(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= \sum_{n=1}^\infty B_n\omega_n \int_0^L \sin\!\left(\frac{n\pi x}{L}\right)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= B_m\omega_m\frac{L}{2}.
\]
Hence
\[
B_m\omega_m = \frac{2}{L} \int_0^L g(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx,
\]
so that
\[
B_m = \frac{2}{L\omega_m} \int_0^L g(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx.
\]
Recalling $\omega_m = m\pi c/L$, we can write this as
\[
B_m = \frac{2}{L}\cdot \frac{L}{m\pi c} \int_0^L g(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx
= \frac{2}{m\pi c} \int_0^L g(x)\sin\!\left(\frac{m\pi x}{L}\right)\,dx.
\]

\medskip

\textbf{5. Final form of the solution.}
Putting everything together, the solution of the initial–boundary value problem is
\[
u(x,t) = \sum_{n=1}^\infty \left[
\left(\frac{2}{L} \int_0^L f(s)\sin\!\left(\frac{n\pi s}{L}\right)\,ds \right)\cos\!\left(\frac{n\pi c}{L} t\right)
+
\left(\frac{2}{n\pi c} \int_0^L g(s)\sin\!\left(\frac{n\pi s}{L}\right)\,ds \right)\sin\!\left(\frac{n\pi c}{L} t\right)
\right]
\sin\!\left(\frac{n\pi x}{L}\right).
\]
Equivalently, in the more compact notation requested in the problem,
\[
u(x,t) = \sum_{n=1}^\infty \bigl(a_n\cos(\omega_n t) + b_n\sin(\omega_n t)\bigr)\sin\!\left(\frac{n\pi x}{L}\right),
\]
where
\[
\omega_n = \frac{n\pi c}{L},\qquad
a_n = \frac{2}{L} \int_0^L f(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx,\qquad
b_n = \frac{2}{n\pi c} \int_0^L g(x)\sin\!\left(\frac{n\pi x}{L}\right)\,dx.
\]

\medskip

\textbf{Conceptual remark.}
This example illustrates the central theme of the section “From Differential to Algebraic Equations with FT, FS and LT.” The spatial Fourier sine series expansion of $u(x,t)$ effectively transforms the partial differential equation into an infinite collection of decoupled second-order ordinary differential equations for the time-dependent coefficients $a_n$ and $b_n$. In the “coefficient space” indexed by $n$, the wave equation reduces to algebraic relations between $\lambda_n$, $\omega_n$, and the Fourier coefficients of the initial data. This is the essence of using Fourier series (and more generally Fourier transforms and Laplace transforms) to turn differential operators into simpler multiplication operators on mode amplitudes.
\end{solution}

