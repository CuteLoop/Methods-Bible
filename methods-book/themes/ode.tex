\chapter{Ordinary Differential Equations}

\section{ODEs: Simple Cases}
% --- Narrative plan (auto-generated) ---
% In this section we study the simplest ordinary differential equations and the basic techniques for solving them. These include separable equations and linear equations with constant coefficients, both in first and second order. Although these examples may look elementary, they already capture important phenomena such as decay to equilibrium, approach to a carrying capacity, and oscillations with and without damping. The goal is to learn how to translate a verbal description of a process into an equation, and then how to analyze and solve that equation systematically.
%
% Simple ODEs lie at the foundation of applied mathematics. Many partial differential equations, when restricted to special classes of solutions such as steady states or traveling waves, reduce to ODEs of exactly the types considered here. Linear ODEs with constant coefficients are also the natural setting in which exponential and trigonometric functions emerge as building blocks, which later reappear in Fourier series, Laplace transforms, and the complex-analytic study of differential equations. The qualitative ideas we develop—for example, stability of equilibria and long-time behavior—are the first steps toward the modern theory of dynamical systems, which underpins much of mathematical modeling in physics, biology, and engineering.
%
% As you work through the examples, focus on recognizing the structure of an equation (separable, linear, autonomous) and choosing an appropriate method. These pattern-recognition skills are what allow you to tackle more complicated models later on, whether they arise as reduced forms of PDEs, as linearizations of nonlinear systems, or as test problems for numerical methods.

% ===== Example 1: Exponential Decay and Half-Life (inquiry-based) =====
\begin{problem}[Exponential Decay and Half-Life]
Many physical and biological processes can be modeled by assuming that a quantity decreases at a rate proportional to its current amount. Examples include the mass of a radioactive isotope, the concentration of a drug in the bloodstream, or the charge on a discharging capacitor. In this problem, you will build the mathematical model for such a process, solve the corresponding differential equation, and connect the model to the experimentally measurable notion of \emph{half-life}. You will then use this model to answer a practical question: how long does it take for the quantity to fall below a desired threshold?

Suppose $Q(t)$ denotes the amount of a substance (for example, mass in grams, or concentration in mg/L) present at time $t$, measured in hours.

\smallskip

(a) A common modeling assumption is that the instantaneous rate of change of $Q(t)$ is proportional to the current amount $Q(t)$, and that the substance is \emph{decaying}. Translate this verbal statement into a differential equation for $Q(t)$.

\emph{Questions to guide you:}  
\quad(i) If the rate of change is ``proportional to'' $Q(t)$, what algebraic form should $Q'(t)$ have?  
\quad(ii) What sign should the proportionality constant have for a decaying process, and what are its units?  
% Hint: Start with $Q'(t) = k\,Q(t)$ for some constant $k$. Then think about what sign $k$ must have if $Q$ is to decrease in time.

\smallskip

(b) Now solve your differential equation from part (a) in general. That is, find the general formula for $Q(t)$ in terms of the initial amount $Q(0)$ and any parameters you introduced.

Hint: Use separation of variables: rewrite the equation so that all factors involving $Q$ are on one side and all factors involving $t$ are on the other, then integrate both sides.

\smallskip

(c) Experimentally, it is common to measure the \emph{half-life} of a decaying substance. The half-life, denoted $T_{1/2}$, is defined as the time it takes for the amount of substance to decrease to one-half of its initial value. Using your solution $Q(t)$ from part (b), express the half-life $T_{1/2}$ in terms of the decay constant $k$ (the proportionality constant from part (a)).

More concretely, suppose $Q(0) = Q_0$. By the definition of half-life,
\[
Q(T_{1/2}) = \frac{1}{2} Q_0.
\]
Use this condition and your formula for $Q(t)$ to solve for $T_{1/2}$ in terms of $k$. Then, solve the resulting equation for $k$ in terms of $T_{1/2}$.

% Hint: You should get an equation involving an exponential function and a factor $\frac12$. Take natural logarithms to solve for $T_{1/2}$ or $k$.

\smallskip

(d) Suppose you have an initial amount $Q(0) = Q_0$ of a radioactive isotope with known half-life $T_{1/2}$. You would like to know how long it takes before the amount falls below a safety threshold $Q_{\text{safe}}$ (where $0 < Q_{\text{safe}} < Q_0$).

(i) Using your explicit solution $Q(t)$ and the relationship between $k$ and $T_{1/2}$ from part (c), derive a formula for the time $t_{\text{safe}}$ at which $Q(t_{\text{safe}}) = Q_{\text{safe}}$. Your answer should be expressed in terms of $Q_0$, $Q_{\text{safe}}$, and $T_{1/2}$.

(ii) Explain briefly (in one or two sentences) how you would use this formula in practice if $Q_0$, $Q_{\text{safe}}$, and $T_{1/2}$ are known from measurements.

% Hint: First, solve the equation $Q(t) = Q_{\text{safe}}$ for $t$ in terms of $k$, $Q_0$, and $Q_{\text{safe}}$. Then substitute your expression for $k$ from part (c).

\smallskip

(e) \textbf{Extensions and ``what if'' questions.}

(i) What changes in the differential equation and its solution if, instead of decaying, the quantity is \emph{growing} at a rate proportional to its current amount? State the modified differential equation and its general solution, and interpret the sign of the constant.

(ii) In some practical situations, the decaying substance is also being supplied at a constant rate $r>0$ (for instance, a drug is infused into the bloodstream at a constant rate while the body simultaneously eliminates it). Modify the differential equation from part (a) to include a constant input rate $r$, and write down the new equation. Without solving it completely, briefly predict (in words or with a sketch) how the long-term behavior of $Q(t)$ should differ from the pure decay case.

% Hint: For (ii), your new equation should have the form $Q'(t) = -k Q(t) + r$. Think about what happens as $t \to \infty$: does $Q(t)$ go to $0$, to $\infty$, or to some intermediate value?
\end{problem}

% ===== Example 1: Exponential Decay and Half-Life (full solution) =====
\begin{problem}[Exponential Decay and Half-Life]
A quantity $Q(t)$ decays at a rate proportional to its current amount, so that $Q$ satisfies
\[
Q'(t) = k\,Q(t),
\]
for some constant $k<0$.  

(a) Solve this differential equation and express $Q(t)$ in terms of $Q(0)$ and $k$.  

(b) The half-life $T_{1/2}$ is the time required for $Q$ to decrease to one-half of its initial value. Use your solution to show that
\[
T_{1/2} = \frac{\ln 2}{-k}
\quad\text{and equivalently}\quad
k = -\frac{\ln 2}{T_{1/2}}.
\]

(c) Suppose $Q(0)=Q_0$ and you want to know the time $t_\ast$ at which $Q(t_\ast) = Q_{\text{thr}}$, where $0 < Q_{\text{thr}} < Q_0$ is a given threshold. Express $t_\ast$ in terms of $Q_0$, $Q_{\text{thr}}$, and the half-life $T_{1/2}$.
\end{problem}

\begin{solution}
We are given the first-order differential equation
\[
Q'(t) = k\,Q(t), \quad k<0.
\]
This is a linear ordinary differential equation with constant coefficients and, in fact, one of the simplest examples in the class of ``simple ODEs'' considered in this section. It models exponential decay when $k$ is negative.

\medskip

\textbf{(a) Solving the differential equation.}

We use separation of variables. For $Q(t) \neq 0$, we can write
\[
\frac{dQ}{dt} = k\,Q
\quad\Longrightarrow\quad
\frac{1}{Q}\,dQ = k\,dt.
\]
We now integrate both sides with respect to $t$:
\[
\int \frac{1}{Q}\,dQ = \int k\,dt.
\]
The integrals are straightforward:
\[
\ln |Q| = kt + C,
\]
where $C$ is a constant of integration. Exponentiating both sides gives
\[
|Q| = e^{kt+C} = e^{C} e^{kt}.
\]
We can absorb $e^{C}$ into a single constant $C_1$, which may be positive or negative, so we write
\[
Q(t) = C_1 e^{kt}.
\]
To express $C_1$ in terms of the initial amount $Q(0)$, we evaluate at $t=0$:
\[
Q(0) = C_1 e^{k\cdot 0} = C_1,
\]
so $C_1 = Q(0)$. Thus the general solution can be written in the convenient form
\[
Q(t) = Q(0)\,e^{kt}.
\]
If we denote $Q(0)$ by $Q_0$ for brevity, then
\[
Q(t) = Q_0 e^{kt}.
\]
Because $k<0$, the exponential factor $e^{kt}$ decreases to zero as $t$ increases, which matches the physical idea of decay.

\medskip

\textbf{(b) Relating the decay constant to the half-life.}

By definition, the half-life $T_{1/2}$ is the time at which the amount has decreased to one-half of its initial value:
\[
Q(T_{1/2}) = \frac{1}{2} Q_0.
\]
Using our solution $Q(t) = Q_0 e^{kt}$, we substitute $t = T_{1/2}$:
\[
Q(T_{1/2}) = Q_0 e^{k T_{1/2}}.
\]
Equating this with $\frac{1}{2} Q_0$ gives
\[
Q_0 e^{k T_{1/2}} = \frac{1}{2} Q_0.
\]
Provided $Q_0 \neq 0$, we can divide both sides by $Q_0$:
\[
e^{k T_{1/2}} = \frac{1}{2}.
\]
To solve for $T_{1/2}$, we take natural logarithms:
\[
k T_{1/2} = \ln\!\left(\frac{1}{2}\right) = -\ln 2.
\]
Thus
\[
T_{1/2} = \frac{-\ln 2}{k}.
\]
Since $k<0$, we can also write this as
\[
T_{1/2} = \frac{\ln 2}{-k}.
\]
Solving the same relation instead for $k$ gives
\[
k = -\frac{\ln 2}{T_{1/2}}.
\]
These formulas show how the decay constant $k$ and the half-life $T_{1/2}$ encode the same information: knowing either one determines the other.

\medskip

\textbf{(c) Time to reach a given threshold.}

We now suppose that $Q(0) = Q_0$, and we want to find the time $t_\ast$ at which the amount has decayed to a specified threshold $Q_{\text{thr}}$ satisfying $0 < Q_{\text{thr}} < Q_0$. By definition of $t_\ast$,
\[
Q(t_\ast) = Q_{\text{thr}}.
\]
Using the explicit solution $Q(t) = Q_0 e^{kt}$, this becomes
\[
Q_0 e^{k t_\ast} = Q_{\text{thr}}.
\]
Dividing both sides by $Q_0$ yields
\[
e^{k t_\ast} = \frac{Q_{\text{thr}}}{Q_0}.
\]
We again take natural logarithms:
\[
k t_\ast = \ln\!\left(\frac{Q_{\text{thr}}}{Q_0}\right).
\]
Solving for $t_\ast$ gives
\[
t_\ast = \frac{1}{k}\,\ln\!\left(\frac{Q_{\text{thr}}}{Q_0}\right).
\]
Because $k<0$ and $0 < Q_{\text{thr}} < Q_0$, the ratio $Q_{\text{thr}}/Q_0$ lies in $(0,1)$, so its logarithm is negative. Thus $t_\ast$ is positive, as expected.

To express this purely in terms of the half-life $T_{1/2}$, we substitute the formula $k = -\dfrac{\ln 2}{T_{1/2}}$:
\[
t_\ast
= \frac{1}{-\dfrac{\ln 2}{T_{1/2}}} \,\ln\!\left(\frac{Q_{\text{thr}}}{Q_0}\right)
= -\frac{T_{1/2}}{\ln 2}\,\ln\!\left(\frac{Q_{\text{thr}}}{Q_0}\right).
\]
It is sometimes clearer to rewrite this in terms of the ratio $Q_0/Q_{\text{thr}}$:
\[
t_\ast
= \frac{T_{1/2}}{\ln 2}\,\ln\!\left(\frac{Q_0}{Q_{\text{thr}}}\right),
\]
since $\ln(Q_0/Q_{\text{thr}})$ is then positive.

This formula shows how to compute the time needed to reach any specified fraction of the initial quantity using the half-life. In practice, if $Q_0$, $Q_{\text{thr}}$, and $T_{1/2}$ are known from measurements, one simply evaluates the logarithm and multiplies by $T_{1/2}/\ln 2$.

\medskip

\textbf{Connection to the chapter theme.}

This example illustrates the central idea of this section on ``ODEs: Simple Cases'': many basic models in applied mathematics reduce to first-order linear ordinary differential equations with constant coefficients. The structure $Q'(t) = k Q(t)$ leads directly, via separation of variables, to exponential solutions. The parameters in the exponent (here, $k$ or equivalently the half-life $T_{1/2}$) have clear physical interpretations and can be related to experimentally measurable quantities. Thus, even this very simple ODE provides a powerful and widely used model in physics, chemistry, biology, and engineering.
\end{solution}

% ===== Example 2: Falling Body with Linear Air Resistance (inquiry-based) =====
\begin{problem}[Falling Body with Linear Air Resistance]
Imagine dropping a small object (like a raindrop or a bead) from rest, high above the ground. Gravity pulls it downward, but the surrounding air pushes back with a drag force that is proportional to the object's velocity. At first the object speeds up, but eventually the drag becomes strong enough to balance gravity, and the object settles into a constant ``terminal'' speed. In this problem you will build and solve the differential equation that describes this motion and see explicitly how the terminal speed emerges from the mathematics.

Assume motion along a vertical line, with the downward direction taken as positive. Let $y(t)$ denote the vertical position of the object and $v(t) = \dfrac{dy}{dt}$ its velocity. Let $m>0$ be the mass, $g>0$ the constant acceleration due to gravity, and $k>0$ the drag coefficient for the linear air resistance.

\medskip

(a) Using Newton's second law, write down the equation of motion for $v(t)$ by balancing forces. Your model should include the downward gravitational force and an upward drag force proportional to the velocity. Explain the sign of each term. (Assume the object is released from rest at $t=0$ so that $v(0)=0$.)

\medskip

(b) Rewrite your equation from part (a) in the standard first-order linear form
\[
v'(t) + a\,v(t) = b,
\]
for suitable constants $a$ and $b$. Is this equation also separable? Briefly justify your answer, and decide which method (separation of variables or integrating factor) you wish to use to solve it.

\medskip

(c) Solve the differential equation for $v(t)$ with the initial condition $v(0)=0$. Express your answer in a form that makes it easy to identify the long-time behavior of the solution.

\emph{Hint:} If you solve by separation of variables, you will encounter an integral of the form
\[
\int \frac{dv}{\alpha - \beta v} \, .
\]
A simple substitution will help. If you solve by integrating factor, look for an integrating factor of the form $e^{ct}$ for some constant $c$.

\medskip

(d) The \emph{terminal velocity} $v_{\mathrm{term}}$ is defined as the constant velocity the object approaches as $t \to \infty$. 

\begin{itemize}
  \item[(i)] Find $v_{\mathrm{term}}$ directly from the differential equation by asking for a constant solution $v(t) \equiv v_{\mathrm{term}}$.
  \item[(ii)] Verify that your explicit solution $v(t)$ from part (c) satisfies $\displaystyle \lim_{t \to \infty} v(t) = v_{\mathrm{term}}$.
\end{itemize}

Now determine the position $y(t)$ of the object, assuming $y(0)=0$ at the moment of release. Write $y(t)$ in terms of $v_{\mathrm{term}}$ and a characteristic time scale of the system. 

\emph{Hint:} First integrate $v(t)$ to obtain $y(t)$. Then look for a way to rewrite your answer using a quantity like $\tau = m/k$ (or an equivalent expression) to simplify the appearance of the formula.

\medskip

(e) Explorations and extensions.

\begin{itemize}
  \item[(i)] Suppose instead that the object is \emph{thrown upward} from the same point with initial velocity $v(0) = v_0 < 0$ (remember, downward is positive). Without resolving all integrals, sketch qualitatively what you expect the velocity $v(t)$ to look like over time. Will the object still approach the same terminal velocity? Why or why not?
  \item[(ii)] Think about the role of the drag coefficient $k$. If $k$ is very small, what does your formula for $v(t)$ suggest about the motion? If $k$ is very large, how does the behavior change? In each case, relate your conclusions to the physical picture.
\end{itemize}

\end{problem}

% ===== Example 2: Falling Body with Linear Air Resistance (full solution) =====
\begin{problem}[Falling Body with Linear Air Resistance]
An object of mass $m>0$ is dropped from rest at time $t=0$ and falls vertically under gravity in a medium that exerts a drag force proportional to its velocity. Take the downward direction as positive, let $y(t)$ be the position (downward from the release point), and let $v(t) = y'(t)$ be the velocity. Assume gravitational acceleration $g>0$ and a drag force of magnitude $k v(t)$, with $k>0$.

\begin{enumerate}
  \item[(a)] Using Newton's second law, derive the differential equation governing $v(t)$, and write it in the form
  \[
  v'(t) + a\,v(t) = b
  \]
  for appropriate constants $a$ and $b$.
  \item[(b)] Solve this equation for $v(t)$ subject to $v(0) = 0$.
  \item[(c)] Determine the terminal velocity $v_{\mathrm{term}}$ as $t \to \infty$, both by (i) finding constant solutions of the differential equation and (ii) taking the limit of your explicit solution.
  \item[(d)] Find the position $y(t)$ for $t \ge 0$, assuming $y(0)=0$, and express your answer in terms of $v_{\mathrm{term}}$ and the time scale $\tau = m/k$. Briefly explain how this example illustrates typical features of first-order linear ODEs with constant coefficients.
\end{enumerate}
\end{problem}

\begin{solution}
We take the downward vertical direction as positive. The two forces acting on the object are gravity and air resistance. Gravity exerts a constant downward force of magnitude $mg$, so in our sign convention this force is $+mg$. The drag force is proportional to the velocity and acts opposite to the direction of motion. Since we are taking downward as positive, if $v(t) > 0$ the object is moving downward and the drag force is upward, hence negative in our coordinates. Thus the drag force is $-k v(t)$, where $k > 0$ is the drag coefficient.

\medskip

\textbf{(a) Derivation of the differential equation.}
By Newton's second law, the mass times the acceleration equals the net force:
\[
m \, v'(t) = \text{(gravity)} + \text{(drag)} = mg - k v(t).
\]
Dividing by $m$ yields
\[
v'(t) = g - \frac{k}{m} v(t).
\]
Rewriting, we obtain the standard linear form
\[
v'(t) + \frac{k}{m} v(t) = g,
\]
so the constants are $a = \dfrac{k}{m}$ and $b = g$.

This is a first-order linear ordinary differential equation with constant coefficients. It is also separable, since we can write
\[
\frac{dv}{dt} = g - \frac{k}{m} v
\quad \Longrightarrow \quad
\frac{dv}{g - \frac{k}{m} v} = dt,
\]
but we will solve it using the integrating factor method, which generalizes well to more complicated linear equations.

\medskip

\textbf{(b) Solution for the velocity.}
We consider
\[
v'(t) + \frac{k}{m} v(t) = g, \qquad v(0) = 0.
\]
For a linear first-order equation of the form $v' + a v = b$ with constant $a$, an integrating factor is $e^{a t}$. In our case $a = k/m$, so we take
\[
\mu(t) = e^{(k/m)t}.
\]
Multiplying the differential equation by $\mu(t)$, we obtain
\[
e^{(k/m)t} v'(t) + \frac{k}{m} e^{(k/m)t} v(t) = g e^{(k/m)t}.
\]
The left-hand side is the derivative of the product $e^{(k/m)t} v(t)$:
\[
\frac{d}{dt} \left( e^{(k/m)t} v(t) \right)
= g e^{(k/m)t}.
\]
We integrate both sides from $0$ to $t$:
\[
e^{(k/m)t} v(t) - e^{(k/m)\cdot 0} v(0)
= g \int_0^t e^{(k/m)s}\, ds.
\]
Using $v(0) = 0$ and $e^{(k/m) \cdot 0} = 1$, we obtain
\[
e^{(k/m)t} v(t)
= g \int_0^t e^{(k/m)s}\, ds.
\]
The integral on the right is
\[
\int_0^t e^{(k/m)s}\, ds
= \left. \frac{m}{k} e^{(k/m)s} \right|_{s=0}^{s=t}
= \frac{m}{k} \left( e^{(k/m)t} - 1 \right).
\]
Hence,
\[
e^{(k/m)t} v(t)
= g \cdot \frac{m}{k} \bigl( e^{(k/m)t} - 1 \bigr).
\]
Solving for $v(t)$ gives
\[
v(t) = \frac{mg}{k} \left(1 - e^{-(k/m)t}\right).
\]

This explicit formula shows that the velocity increases from $0$ at $t=0$ and approaches a limiting value as $t$ grows.

\medskip

\textbf{(c) Terminal velocity from the differential equation and from the solution.}

\emph{(i) From constant solutions.}
A terminal velocity corresponds to motion with constant velocity, so $v(t) \equiv v_{\mathrm{term}}$ is constant in time. Substituting this into the differential equation
\[
v' + \frac{k}{m} v = g
\]
yields
\[
0 + \frac{k}{m} v_{\mathrm{term}} = g,
\]
so
\[
v_{\mathrm{term}} = \frac{mg}{k}.
\]

\emph{(ii) From the explicit solution.}
From part (b) we have
\[
v(t) = \frac{mg}{k} \left( 1 - e^{-(k/m)t} \right).
\]
As $t \to \infty$, the exponential term $e^{-(k/m)t}$ tends to $0$, so
\[
\lim_{t \to \infty} v(t) = \frac{mg}{k} \cdot (1 - 0) = \frac{mg}{k}.
\]
Thus the explicit time-dependent solution has the expected terminal velocity, which is determined entirely by the balance of the constant gravitational force and the linear drag force.

\medskip

\textbf{(d) Position as a function of time and interpretation.}

We know that $v(t) = y'(t)$ is given by
\[
v(t) = \frac{mg}{k} \left( 1 - e^{-(k/m)t} \right).
\]
We want $y(t)$ for $t \ge 0$ with $y(0) = 0$. Integrating with respect to $t$,
\[
y(t) = \int_0^t v(s)\, ds
= \int_0^t \frac{mg}{k} \left( 1 - e^{-(k/m)s} \right) ds.
\]
We can factor out the constant $\dfrac{mg}{k}$:
\[
y(t) = \frac{mg}{k} \int_0^t \left( 1 - e^{-(k/m)s} \right) ds
= \frac{mg}{k} \left[ \int_0^t 1\, ds - \int_0^t e^{-(k/m)s} ds \right].
\]
The first integral is simply $t$. For the second, we compute
\[
\int_0^t e^{-(k/m)s} ds
= \left. -\frac{m}{k} e^{-(k/m)s} \right|_{s=0}^{s=t}
= -\frac{m}{k} e^{-(k/m)t} + \frac{m}{k}.
\]
Therefore
\[
y(t) = \frac{mg}{k} \left[ t - \left( -\frac{m}{k} e^{-(k/m)t} + \frac{m}{k} \right) \right]
= \frac{mg}{k} \left[ t + \frac{m}{k} e^{-(k/m)t} - \frac{m}{k} \right].
\]
It is convenient to express the result in terms of two natural parameters: the terminal velocity
\[
v_{\mathrm{term}} = \frac{mg}{k}
\]
and the characteristic time scale
\[
\tau = \frac{m}{k}.
\]
Then the expressions simplify to
\[
v(t) = v_{\mathrm{term}} \left( 1 - e^{-t/\tau} \right),
\]
and
\[
y(t) = v_{\mathrm{term}} \left[ t + \tau e^{-t/\tau} - \tau \right]
= v_{\mathrm{term}} \left( t - \tau \left[ 1 - e^{-t/\tau} \right] \right).
\]
We can also rearrange $y(t)$ as
\[
y(t) = v_{\mathrm{term}} t - v_{\mathrm{term}} \tau \left( 1 - e^{-t/\tau} \right).
\]

This formula has a clear physical interpretation. The term $v_{\mathrm{term}} t$ is the distance the object would fall if it were moving at the terminal velocity for the entire time. The correction term
\[
v_{\mathrm{term}} \tau \left( 1 - e^{-t/\tau} \right)
\]
accounts for the fact that, at the beginning, the object is moving more slowly and only gradually approaches $v_{\mathrm{term}}$. The time scale $\tau = m/k$ measures how quickly the transient exponential term $e^{-t/\tau}$ decays: after a time on the order of a few multiples of $\tau$, the velocity is very close to $v_{\mathrm{term}}$.

\medskip

\textbf{Conceptual remarks and relation to ``simple cases'' of ODEs.}

This example illustrates several central ideas of the section on simple ordinary differential equations:

\begin{itemize}
  \item The motion is governed by a \emph{first-order linear ODE with constant coefficients}, which can be solved systematically by the integrating factor method or, in this particular case, by separation of variables.
  \item The long-term behavior is dominated by an \emph{equilibrium solution} (the terminal velocity), and the general solution is an equilibrium plus an exponentially decaying transient. This is typical for linear equations of the form $u' + a u = b$ with $a>0$.
  \item The parameters of the model, $m$, $k$, and $g$, appear naturally in the solution through the equilibrium value $v_{\mathrm{term}} = mg/k$ and the time constant $\tau = m/k$, which describe respectively the steady-state motion and the rate at which this steady state is approached.
\end{itemize}

Thus, the falling body with linear air resistance is a concrete physical setting in which the standard techniques for solving and interpreting first-order linear ODEs can be seen in action.
\end{solution}

% ===== Example 3: Logistic Population Growth (inquiry-based) =====
\begin{problem}[Logistic Population Growth]
In many introductory models of population growth, one assumes that the rate of change of the population is proportional to the current population, leading to exponential growth. However, real populations do not grow without bound: resources such as food, space, or oxygen are limited. One way to capture this effect is to assume that the growth rate decreases as the population approaches some \emph{carrying capacity} imposed by the environment. This leads to the \emph{logistic equation}, a nonlinear first‑order ordinary differential equation.

We will explore how this model is built and how to solve it explicitly.

\smallskip

Consider a population $P(t)$ at time $t$, where $P(t)$ is measured in, say, thousands of individuals.

\smallskip

(a) As a warm‑up, recall the exponential growth model
\[
\frac{dP}{dt} = rP, \qquad r>0.
\]
Solve this differential equation with initial condition $P(0) = P_0>0$. Explain why this model predicts unbounded growth as $t\to\infty$, and briefly discuss why that may be unrealistic for a real population.

\medskip

Now suppose that the environment can support at most a population of size $K>0$. We assume that the growth rate is proportional both to the current population $P(t)$ and to the remaining ``room'' $(K - P(t))$ before the carrying capacity is reached.

\smallskip

(b) Translate this modeling assumption into an ordinary differential equation for $P(t)$ of the form
\[
\frac{dP}{dt} = \text{(constant)}\times P(t)\times (K - P(t)).
\]
Choose a convenient positive constant, usually denoted by $r>0$, and write the resulting equation in the standard \emph{logistic} form
\[
\frac{dP}{dt} = r P(t)\Bigl(1 - \frac{P(t)}{K}\Bigr).
\]
Identify all constant (equilibrium) solutions and interpret them in terms of the population. 

\emph{Hint:} An equilibrium solution is a constant $P(t)\equiv P_\ast$ for which $dP/dt=0$ for all $t$.

\medskip

(c) Show that the logistic equation
\[
\frac{dP}{dt} = r P(t)\Bigl(1 - \frac{P(t)}{K}\Bigr),
\qquad r>0,\ K>0,
\]
is \emph{separable}. Rewrite it in the form
\[
\frac{dP}{P(t)\left(1 - \frac{P(t)}{K}\right)} = r\,dt.
\]
Then, write the left-hand side as a sum of simpler fractions in $P$ that you know how to integrate:
\[
\frac{1}{P\left(1 - \frac{P}{K}\right)} 
= \frac{K}{P(K-P)}
= \frac{A}{P} + \frac{B}{K-P}
\]
for suitable constants $A$ and $B$.

Determine $A$ and $B$, and write the separated equation with the left-hand side expressed as a sum of two integrable terms.

\emph{Hint:} Multiply both sides of
\[
\frac{K}{P(K-P)} = \frac{A}{P} + \frac{B}{K-P}
\]
by $P(K-P)$ and compare coefficients of $P$.

\medskip

(d) Now solve the logistic initial value problem
\[
\frac{dP}{dt} = r P(t)\Bigl(1 - \frac{P(t)}{K}\Bigr), 
\qquad P(0) = P_0,\quad 0<P_0\neq K.
\]
Integrate the separated equation from part (c) to obtain an implicit relation between $P$ and $t$, then solve for $P(t)$ explicitly. Your final answer should have the form
\[
P(t) = \frac{K}{1 + C e^{-rt}}
\]
for some constant $C$ that depends on $P_0$, $K$, and $r$. Determine $C$ in terms of $P_0$ and $K$.

\emph{Hint:} After integrating, you should obtain an equation of the form
\[
\ln\left|\frac{P}{K-P}\right| = rt + C_1.
\]
Exponentiate both sides and solve algebraically for $P$.

\smallskip

Once you have $P(t)$, analyze its long‑time behavior. What is $\displaystyle\lim_{t\to\infty} P(t)$, and how does the sign of $P_0-K$ affect whether $P(t)$ increases or decreases toward that limit?

\medskip

(e) Extensions and ``what if'' questions:

\begin{enumerate}
    \item Suppose a constant harvesting term $h>0$ is added, giving
    \[
    \frac{dP}{dt} = rP\Bigl(1 - \frac{P}{K}\Bigr) - h.
    \]
    Without solving this new equation explicitly, find the equilibrium values by solving a quadratic equation in $P$. For which values of $h$ (in terms of $r$ and $K$) are there two distinct positive equilibria, one positive equilibrium, or no positive equilibria?

    \emph{Hint:} Solve $rP(1-P/K) - h = 0$ and discuss the discriminant.

    \item (Non-dimensionalization.) Let $x(t) = P(t)/K$ be the fraction of the carrying capacity that is occupied. Rewrite the logistic equation in terms of $x(t)$ and a rescaled time variable $\tau = rt$. Show that in these units the equation takes the simpler dimensionless form
    \[
    \frac{dx}{d\tau} = x(1-x).
    \]
    What is the general solution $x(\tau)$ in this form, and how does it relate to your earlier expression for $P(t)$?
\end{enumerate}

\end{problem}

% ===== Example 3: Logistic Population Growth (full solution) =====
\begin{problem}[Logistic Population Growth]
Consider the logistic differential equation
\[
\frac{dP}{dt} = r P(t)\Bigl(1 - \frac{P(t)}{K}\Bigr),
\qquad r>0,\ K>0,
\]
with initial condition $P(0) = P_0>0$.

\begin{enumerate}
    \item[(a)] Find all equilibrium (constant) solutions and classify their stability using a one‑dimensional phase line.
    \item[(b)] Solve the initial value problem explicitly by the method of separation of variables and express $P(t)$ in terms of $r$, $K$, and $P_0$.
    \item[(c)] Determine $\displaystyle\lim_{t\to\infty} P(t)$ and describe how the solution behaves over time when $0<P_0<K$ and when $P_0>K$.
\end{enumerate}
\end{problem}

\begin{solution}
We are given the nonlinear first‑order ordinary differential equation
\[
\frac{dP}{dt} = r P(t)\Bigl(1 - \frac{P(t)}{K}\Bigr),
\]
with parameters $r>0$, $K>0$, and an initial condition $P(0)=P_0>0$. This is the standard logistic equation, a prototypical example of a separable but nonlinear equation in the ``simple cases'' class of ODEs.

\medskip

\noindent\textbf{(a) Equilibria and their stability.}
An equilibrium solution is a constant $P(t)\equiv P_\ast$ such that $dP/dt=0$ for all $t$. Substituting $P(t)=P_\ast$ into the right‑hand side gives
\[
0 = r P_\ast\Bigl(1 - \frac{P_\ast}{K}\Bigr).
\]
Since $r>0$, this product vanishes if and only if
\[
P_\ast = 0 \quad\text{or}\quad 1 - \frac{P_\ast}{K} = 0 \;\Longleftrightarrow\; P_\ast=K.
\]
Thus there are two equilibrium solutions: $P(t)\equiv 0$ and $P(t)\equiv K$.

To classify their stability, we examine the sign of $dP/dt$ as a function of $P$ using a one‑dimensional phase line. Note that the right‑hand side is
\[
f(P) := rP\Bigl(1 - \frac{P}{K}\Bigr).
\]
The sign of $f(P)$ is determined by the sign of $P$ and of $(1-P/K)$.

\begin{itemize}
    \item For $0<P<K$, we have $P>0$ and $1-P/K>0$, so $f(P)>0$. Thus, when the population is between $0$ and $K$, we have $dP/dt>0$ and $P$ increases with time.
    \item For $P>K$, we have $P>0$ but $1-P/K<0$, so $f(P)<0$. Thus, when the population exceeds $K$, we have $dP/dt<0$ and $P$ decreases with time.
    \item For $P<0$, the model is not biologically relevant, but mathematically $P<0$ and $1-P/K>1>0$, so $f(P)<0$ there.
\end{itemize}

On the phase line, the arrows point to the right (increasing $P$) on the interval $(0,K)$ and to the left (decreasing $P$) on $(K,\infty)$. Arrows from both sides point toward $P=K$, so $P=K$ is a \emph{stable} (asymptotically stable) equilibrium. At $P=0$, the arrows on $(0,K)$ point away from $0$; thus $P=0$ is an \emph{unstable} equilibrium.

\medskip

\noindent\textbf{(b) Solving the initial value problem.}
We next solve
\[
\frac{dP}{dt} = r P(t)\Bigl(1 - \frac{P(t)}{K}\Bigr), \qquad P(0)=P_0.
\]
This equation is separable. For $P(t)\neq 0$ and $P(t)\neq K$ we can write
\[
\frac{dP}{dt} = rP\Bigl(1-\frac{P}{K}\Bigr)
\quad\Longrightarrow\quad
\frac{dP}{P\left(1-\frac{P}{K}\right)} = r\,dt.
\]
We now express the left‑hand side as a sum of simpler rational functions of $P$. First,
\[
\frac{1}{P\left(1-\frac{P}{K}\right)}
= \frac{1}{P\cdot\frac{K-P}{K}}
= \frac{K}{P(K-P)}.
\]
We seek constants $A$ and $B$ such that
\[
\frac{K}{P(K-P)} = \frac{A}{P} + \frac{B}{K-P}.
\]
Multiplying both sides by $P(K-P)$ gives
\[
K = A(K-P) + B P = AK + (B-A)P.
\]
Since this identity must hold for all $P$, the constant and linear terms must match:
\[
AK = K \quad\Longrightarrow\quad A = 1,
\]
\[
B - A = 0 \quad\Longrightarrow\quad B = 1.
\]
Hence,
\[
\frac{K}{P(K-P)} = \frac{1}{P} + \frac{1}{K-P}.
\]
The separated equation becomes
\[
\left(\frac{1}{P} + \frac{1}{K-P}\right)\,dP = r\,dt.
\]

We now integrate both sides. Integrating with respect to $t$ is equivalent to integrating with respect to $P$ on the left:
\[
\int\left(\frac{1}{P} + \frac{1}{K-P}\right)\,dP = \int r\,dt.
\]
Compute each integral:
\[
\int \frac{1}{P}\,dP = \ln|P| + C_1,
\]
\[
\int \frac{1}{K-P}\,dP = -\ln|K-P| + C_2,
\]
because $\frac{d}{dP}(K-P) = -1$. Ignoring the intermediate constants and combining, we obtain
\[
\ln|P| - \ln|K-P| = rt + C,
\]
for some constant $C\in\mathbb{R}$. Using properties of logarithms, we write
\[
\ln\left|\frac{P}{K-P}\right| = rt + C.
\]
Exponentiating both sides yields
\[
\left|\frac{P}{K-P}\right| = e^{rt + C} = C_1 e^{rt},
\]
where $C_1 = \pm e^C$ is a nonzero constant. For $0<P<K$ (the biologically relevant regime), the ratio $P/(K-P)$ is positive, so we can drop the absolute value and simply write
\[
\frac{P}{K-P} = C_1 e^{rt}.
\]

Now we solve this algebraic equation for $P$ in terms of $t$. Multiply both sides by $(K-P)$:
\[
P = C_1 e^{rt}(K-P).
\]
Distribute on the right:
\[
P = C_1 K e^{rt} - C_1 e^{rt} P.
\]
Collect the terms involving $P$ on one side:
\[
P + C_1 e^{rt} P = C_1 K e^{rt},
\]
\[
P\bigl(1 + C_1 e^{rt}\bigr) = C_1 K e^{rt}.
\]
Thus
\[
P(t) = \frac{C_1 K e^{rt}}{1 + C_1 e^{rt}}.
\]
It is more convenient to rewrite the constant. Multiply numerator and denominator by $e^{-rt}$:
\[
P(t) = \frac{C_1 K}{e^{-rt} + C_1}
= \frac{K}{e^{-rt}/C_1 + 1}
= \frac{K}{1 + C e^{-rt}},
\]
where $C = 1/C_1$ is a new nonzero constant.

We determine $C$ from the initial condition $P(0)=P_0$. Substituting $t=0$ gives
\[
P_0 = P(0) = \frac{K}{1 + C e^{0}} = \frac{K}{1 + C}.
\]
Solving for $C$,
\[
1 + C = \frac{K}{P_0}
\quad\Longrightarrow\quad
C = \frac{K}{P_0} - 1 = \frac{K - P_0}{P_0}.
\]
Therefore, the explicit solution is
\[
P(t) = \frac{K}{1 + \displaystyle\left(\frac{K-P_0}{P_0}\right)e^{-rt}},
\qquad P_0>0,\ P_0\neq K.
\]
This formula is valid for all $t$ for which the denominator is nonzero; for positive parameters and $P_0>0$ it stays positive, so the solution exists for all $t\in\mathbb{R}$.

Note that the equilibrium solutions $P(t)\equiv 0$ and $P(t)\equiv K$ correspond to the special initial conditions $P_0=0$ and $P_0=K$, respectively. In these cases, the separation step above would have involved dividing by zero, which is why we treated equilibrium solutions separately.

\medskip

\noindent\textbf{(c) Long‑time behavior.}
We now compute the limit of $P(t)$ as $t\to\infty$ and describe the qualitative behavior. The explicit solution is
\[
P(t) = \frac{K}{1 + \left(\dfrac{K-P_0}{P_0}\right)e^{-rt}}.
\]
Since $r>0$, we have $e^{-rt}\to 0$ as $t\to\infty$. Therefore,
\[
\lim_{t\to\infty} P(t) 
= \frac{K}{1 + \left(\dfrac{K-P_0}{P_0}\right)\cdot 0}
= \frac{K}{1+0} = K.
\]
Thus, for every initial population $P_0>0$, $P_0\neq 0$, the solution tends to the carrying capacity $K$ as $t\to\infty$. This agrees with the phase line analysis, which indicated that $P=K$ is a stable equilibrium.

To understand whether $P(t)$ increases or decreases toward $K$, examine the denominator at $t=0$:
\[
P(0) = \frac{K}{1 + \left(\dfrac{K-P_0}{P_0}\right)} = P_0.
\]
\begin{itemize}
    \item If $0<P_0<K$, then $K-P_0>0$, so $C = (K-P_0)/P_0>0$. For $t>0$, the factor $e^{-rt}$ is positive and decreasing to $0$, so the denominator $1 + C e^{-rt}$ decreases from $1+C>1$ toward $1$. A decreasing denominator leads to an increasing $P(t)$. Thus $P(t)$ increases monotonically from $P_0$ up to $K$ as $t\to\infty$.
    \item If $P_0>K$, then $K-P_0<0$, so $C<0$. Thus $1 + C e^{-rt}$ increases from $1+C<1$ up to $1$ as $t\to\infty$. An increasing denominator leads to a decreasing $P(t)$. Hence $P(t)$ decreases monotonically from $P_0$ down to $K$ as $t\to\infty$.
\end{itemize}

In either case, the carrying capacity $K$ acts as an attracting equilibrium, and the population approaches $K$ from below if it starts below, and from above if it starts above.

\medskip

\noindent\textbf{Conceptual remarks.}
This example illustrates several central ideas from the section on ``simple cases'' of ordinary differential equations:

\begin{itemize}
    \item The logistic equation is a \emph{separable} first‑order ODE, so the main technique is to rearrange it into the form $g(P)\,dP = h(t)\,dt$ and integrate both sides.
    \item Even though the equation is nonlinear, partial fraction decomposition reduces the integral to standard logarithmic functions.
    \item Qualitative methods, such as phase line analysis and equilibrium classification, give global information about stability and long‑time behavior that is consistent with and complements the explicit solution.
\end{itemize}

Thus the logistic model serves as a prototypical nonlinear but explicitly solvable equation that bridges basic solution techniques with qualitative analysis of ODEs.
\end{solution}

% ===== Example 4: Mixing in a Well-Stirred Tank (inquiry-based) =====
\begin{problem}[Mixing in a Well-Stirred Tank]
A standard model in chemical and environmental engineering describes how a dissolved substance mixes in a tank of liquid. A solution with given concentration flows into the tank, the mixture (assumed perfectly stirred) flows out at the same rate, and the total volume in the tank remains constant. This situation leads to a first-order linear differential equation for the amount of solute in the tank as a function of time. In this problem you will build the model step by step, solve it, and interpret the answer.

A tank initially contains $100$ liters of liquid in which $10$ kilograms of a chemical are dissolved. A solution containing the same chemical flows into the tank at a rate of $5$ liters per minute, with concentration $0.4$ kilograms per liter. The mixture is kept perfectly stirred and flows out at the same rate of $5$ liters per minute.

\smallskip

(a) Let $Q(t)$ denote the amount of chemical (in kilograms) in the tank at time $t$ (in minutes). Explain, in words, how the quantity $Q(t)$ changes over a short time interval. Then write a verbal ``balance law'' for $Q(t)$ of the form:
\[
\text{rate of change of amount in tank} 
= \text{rate in} - \text{rate out}.
\]
Be explicit about what each of these three terms means for this mixing problem.

\medskip

(b) Now translate your verbal balance law into a differential equation for $Q(t)$.

\begin{itemize}
  \item What is the numerical value of the \emph{rate in} (in kilograms per minute)? Express it in terms of the flow rate and the incoming concentration.
  \item For the \emph{rate out}, first express the concentration in the tank at time $t$ in terms of $Q(t)$ and the constant volume.
  \item Use this to write $\text{rate out}$ as a function of $Q(t)$.
\end{itemize}

Combine these pieces to obtain an explicit first-order linear ODE for $Q(t)$. What is the initial condition for this equation?

\medskip

(c) Solve the differential equation for $Q(t)$.

Hint: First rewrite your ODE in the standard linear form
\[
\frac{dQ}{dt} + a Q = b,
\]
for appropriate constants $a$ and $b$. You may solve it either by using an integrating factor or by recognizing it as a separable equation. Be sure to use the initial condition to determine any integration constants.

\medskip

(d) Use your explicit formula for $Q(t)$ to answer the following questions.

\begin{itemize}
  \item[(i)] What is the limiting amount of chemical in the tank as $t \to \infty$? What is the corresponding limiting concentration (in kilograms per liter)?
  
  \item[(ii)] After how many minutes will the concentration in the tank reach $0.35$ kilograms per liter? Set up an equation for $t$ using your formula for $Q(t)$, and then solve for $t$.
\end{itemize}

Hint: For part (ii), remember that concentration equals $Q(t)$ divided by the tank volume, and use logarithms to solve for time.

\medskip

(e) Explore some variations of the model.

\begin{itemize}
  \item[(i)] Suppose now that pure water flows into the tank (incoming concentration $0$ kilograms per liter), still at $5$ liters per minute, and the tank initially contains $40$ kilograms of chemical in $100$ liters. Write down and solve the new differential equation for $Q(t)$, and describe in words what happens to the concentration as $t \to \infty$.

  \item[(ii)] Suppose instead that the inflow remains as in the original problem, but the outflow rate is only $4$ liters per minute, so that the volume in the tank increases over time. Describe how the balance law in part (a) must be modified. In particular, which quantities that were previously constant now depend on time? (You do not have to solve the new equation; focus on formulating it correctly.)
\end{itemize}

\end{problem}

% ===== Example 4: Mixing in a Well-Stirred Tank (full solution) =====
\begin{problem}[Mixing in a Well-Stirred Tank]
A tank initially contains $100$ liters of liquid with $10$ kilograms of a dissolved chemical. A solution flows in at $5$ liters per minute with concentration $0.4$ kilograms per liter. The mixture is perfectly stirred and flows out at the same rate of $5$ liters per minute, so the tank volume remains $100$ liters.

Let $Q(t)$ be the amount of chemical (in kilograms) in the tank at time $t$ (in minutes).

\begin{enumerate}
  \item Derive a differential equation for $Q(t)$ together with the initial condition.
  \item Solve this differential equation for $Q(t)$.
  \item Find the limiting concentration in the tank as $t \to \infty$.
  \item Determine how long it takes until the concentration in the tank reaches $0.35$ kilograms per liter.
\end{enumerate}
\end{problem}

\begin{solution}
We model the system by tracking the amount of chemical in the tank as a function of time. The key modeling principle is a balance (or conservation) law: the rate of change of the amount of chemical in the tank equals the rate at which chemical enters minus the rate at which chemical leaves.

\medskip

\noindent\textbf{1. Deriving the differential equation.}

Let $Q(t)$ denote the amount of chemical in the tank at time $t$, measured in kilograms. We work with the generic balance law
\[
\frac{dQ}{dt} = \text{(rate in)} - \text{(rate out)}.
\]

\emph{Rate in.} The inflow rate of liquid is $5$ liters per minute, and the incoming concentration is $0.4$ kilograms per liter. Thus the incoming mass rate is
\[
\text{rate in} = 5 \,\frac{\text{L}}{\text{min}} \times 0.4 \,\frac{\text{kg}}{\text{L}}
= 2 \,\frac{\text{kg}}{\text{min}}.
\]

\emph{Rate out.} The outflow rate of liquid is also $5$ liters per minute. The concentration of chemical in the tank at time $t$ is the amount divided by the volume. The volume of liquid remains constant at $100$ liters, so the concentration in the tank is
\[
\text{concentration in tank at time } t
= \frac{Q(t)}{100} \,\frac{\text{kg}}{\text{L}}.
\]
The outflow carries liquid at $5$ liters per minute with this concentration, so
\[
\text{rate out} = 5 \,\frac{\text{L}}{\text{min}} \times \frac{Q(t)}{100} \,\frac{\text{kg}}{\text{L}}
= \frac{5}{100} Q(t) \,\frac{\text{kg}}{\text{min}}
= \frac{1}{20} Q(t)\,\frac{\text{kg}}{\text{min}}.
\]

Substituting these expressions into the balance law gives
\[
\frac{dQ}{dt} = 2 - \frac{1}{20} Q(t).
\]
Rewriting,
\[
\frac{dQ}{dt} + \frac{1}{20} Q = 2.
\]

The initial condition comes from the initial amount of chemical. Initially there are $10$ kilograms dissolved in the $100$ liters, so
\[
Q(0) = 10.
\]

Thus the mathematical model is the initial value problem
\[
\frac{dQ}{dt} + \frac{1}{20} Q = 2, \qquad Q(0) = 10.
\]

This is a first-order linear ordinary differential equation, one of the “simple cases” studied in elementary ODE theory.

\medskip

\noindent\textbf{2. Solving the differential equation.}

We solve
\[
\frac{dQ}{dt} + \frac{1}{20} Q = 2
\]
using the integrating factor method. The standard linear form is
\[
\frac{dQ}{dt} + a Q = b
\]
with $a = \tfrac{1}{20}$ and $b = 2$.

An integrating factor is
\[
\mu(t) = e^{\int a\,dt} = e^{\int \frac{1}{20}\,dt} = e^{t/20}.
\]
Multiplying the equation by $\mu(t)$ gives
\[
e^{t/20} \frac{dQ}{dt} + \frac{1}{20} e^{t/20} Q = 2 e^{t/20}.
\]
The left-hand side is the derivative of the product $e^{t/20} Q(t)$:
\[
\frac{d}{dt}\bigl(e^{t/20} Q(t)\bigr) = 2 e^{t/20}.
\]

We integrate both sides with respect to $t$:
\[
\int \frac{d}{dt}\bigl(e^{t/20} Q(t)\bigr)\,dt
= \int 2 e^{t/20}\,dt.
\]
The left integral simply returns $e^{t/20} Q(t)$. For the right integral, we have
\[
\int 2 e^{t/20}\,dt = 2 \cdot 20 e^{t/20} + C = 40 e^{t/20} + C,
\]
where $C$ is an integration constant. Thus
\[
e^{t/20} Q(t) = 40 e^{t/20} + C.
\]

Dividing both sides by $e^{t/20}$ yields
\[
Q(t) = 40 + C e^{-t/20}.
\]

We now use the initial condition $Q(0) = 10$:
\[
Q(0) = 40 + C e^{0} = 40 + C = 10.
\]
Therefore $C = 10 - 40 = -30$, and the solution is
\[
Q(t) = 40 - 30 e^{-t/20}.
\]

This is the explicit formula for the amount of chemical in the tank at time $t$.

\medskip

\noindent\textbf{3. Limiting concentration as $t \to \infty$.}

The concentration in the tank at time $t$ is
\[
\text{concentration}(t) = \frac{Q(t)}{100}
= \frac{40 - 30 e^{-t/20}}{100}
= 0.4 - 0.3 e^{-t/20} \quad \text{kilograms per liter}.
\]

As $t \to \infty$, the exponential term $e^{-t/20}$ tends to $0$, so
\[
\lim_{t \to \infty} Q(t) = 40
\quad\text{and}\quad
\lim_{t \to \infty} \text{concentration}(t) = \frac{40}{100} = 0.4 \;\text{kg/L}.
\]

Thus the tank approaches a steady state in which the concentration of the chemical in the tank equals the concentration in the incoming solution. This is typical for such mixing problems: with constant volume and perfect stirring, the tank concentration exponentially approaches the inflow concentration.

\medskip

\noindent\textbf{4. Time to reach a given concentration.}

We are asked for the time at which the concentration reaches $0.35$ kilograms per liter. This corresponds to
\[
\frac{Q(t)}{100} = 0.35
\quad\Longleftrightarrow\quad
Q(t) = 35.
\]
We set our solution equal to $35$ and solve for $t$:
\[
40 - 30 e^{-t/20} = 35.
\]
Rearranging,
\[
-30 e^{-t/20} = 35 - 40 = -5,
\]
so
\[
30 e^{-t/20} = 5,
\qquad
e^{-t/20} = \frac{5}{30} = \frac{1}{6}.
\]

Taking natural logarithms,
\[
-\frac{t}{20} = \ln\!\left(\frac{1}{6}\right) = -\ln 6,
\]
which implies
\[
\frac{t}{20} = \ln 6,
\quad\text{so}\quad
t = 20 \ln 6 \;\text{minutes}.
\]

Numerically, $\ln 6 \approx 1.7918$, so
\[
t \approx 20 \times 1.7918 \approx 35.8 \;\text{minutes}.
\]

Therefore, it takes approximately $36$ minutes for the concentration in the tank to reach $0.35$ kilograms per liter.

\medskip

\noindent\textbf{Conceptual remarks.}

This example illustrates the core ideas from the section on ``ODEs: Simple Cases.'' The modeling step turns a physical conservation principle into a first-order linear differential equation with constant coefficients. The solution exhibits exponential behavior: the amount of chemical approaches a steady-state value determined by the balance of inflow and outflow, and the approach to this equilibrium is governed by an exponential decay factor $e^{-t/20}$. Problems of this type show how simple linear ODEs naturally arise in real-world mixing processes and how their solutions describe both transient behavior and long-term steady states. 

\end{solution}

% ===== Example 5: Simple Harmonic Oscillator (inquiry-based) =====
\begin{problem}[Simple Harmonic Oscillator]
A mass attached to a spring on a frictionless horizontal surface provides one of the simplest and most important models in mechanics. When the mass is displaced from its equilibrium position and released, it moves back and forth in a motion that we call \emph{simple harmonic}. In this problem you will derive the governing differential equation from physical principles, solve it using techniques for linear ODEs with constant coefficients, and interpret the resulting sinusoidal motion. You will also briefly explore how changes in the system alter the behavior of the solution.

Assume a mass $m>0$ is attached to an ideal spring with spring constant $k>0$ on a frictionless horizontal surface. Let $x(t)$ denote the displacement of the mass from its equilibrium position at time $t$, with $x>0$ meaning that the spring is stretched.

\medskip

(a) Using Newton's second law and Hooke's law, derive the differential equation satisfied by $x(t)$. Carefully choose a sign convention and write down the equation of motion.  
Hint: Newton's second law says ``mass $\times$ acceleration $=$ sum of forces.'' Hooke's law says the restoring force of the spring is proportional to the displacement and points toward the equilibrium.

\medskip

(b) Rewrite your equation from part (a) in the standard form
\[
x''(t) + \omega^2 x(t) = 0,
\]
and identify the constant $\omega$ in terms of $m$ and $k$. Physically, $\omega$ is called the \emph{angular frequency}.  
Now, to solve this equation, suppose that a solution has the exponential form $x(t) = e^{rt}$ for some constant $r$. Substitute this guess into the differential equation and find the algebraic equation (the \emph{characteristic equation}) that $r$ must satisfy.  
% Hint: Compute $x''(t)$ when $x(t) = e^{rt}$ and simplify.

\medskip

(c) Solve the characteristic equation you found in part (b). Show that the roots are purely imaginary, and denote them by $r = \pm i\omega$.  
Next, explain why the general \emph{real-valued} solution of the differential equation can be written in the form
\[
x(t) = A\cos(\omega t) + B\sin(\omega t),
\]
for some real constants $A$ and $B$.  
Hint: Use Euler's formula $e^{i\theta} = \cos\theta + i\sin\theta$, and remember that real and imaginary parts of complex exponentials solve the same real linear ODE.

\medskip

(d) Suppose that at time $t=0$ the mass is displaced to $x(0) = x_0$ and given initial velocity $x'(0) = v_0$.  
\begin{enumerate}
\item[(i)] Use these initial conditions to determine $A$ and $B$ in terms of $x_0$, $v_0$, and $\omega$.  
% Hint: Evaluate $x(t)$ and $x'(t)$ at $t=0$ and solve the resulting $2\times 2$ linear system.
\item[(ii)] Show that the same solution can also be written in the \emph{amplitude--phase} form
\[
x(t) = R \cos(\omega t - \phi),
\]
for suitable constants $R>0$ and $\phi\in\mathbb{R}$. Express $R$ and $\phi$ in terms of $x_0$ and $v_0$.  
Hint: Use the identity $\cos(\omega t - \phi) = \cos\phi\,\cos(\omega t) + \sin\phi\,\sin(\omega t)$ and compare coefficients with the $A\cos(\omega t)+B\sin(\omega t)$ form.
\end{enumerate}

\medskip

(e) Extensions and ``what if'' questions.
\begin{enumerate}
\item[(i)] Define the (mechanical) energy of the mass--spring system by
\[
E(t) = \frac{1}{2}m\bigl(x'(t)\bigr)^2 + \frac{1}{2}k\bigl(x(t)\bigr)^2.
\]
Show, by direct differentiation and use of the differential equation, that $E'(t) = 0$ for all $t$. What does this say about the motion physically?  
Hint: Differentiate $E(t)$ and substitute $x''(t)$ from the equation of motion.
\item[(ii)] Now imagine adding a small linear damping force $-b x'(t)$ with $b>0$, so that the equation becomes
\[
m x''(t) + b x'(t) + k x(t) = 0.
\]
Without solving this new equation in detail, discuss qualitatively how you expect the motion of the mass to change compared with the undamped case. In particular, what happens to the amplitude and the energy as $t\to\infty$?
\end{enumerate}

\end{problem}

% ===== Example 5: Simple Harmonic Oscillator (full solution) =====
\begin{problem}[Simple Harmonic Oscillator]
A mass $m>0$ is attached to an ideal spring with spring constant $k>0$ on a frictionless horizontal surface. Let $x(t)$ denote the displacement from equilibrium. 

\begin{enumerate}
\item[(i)] Use Newton's second law and Hooke's law to derive the equation of motion for $x(t)$, and rewrite it in the form
\[
x''(t) + \omega^2 x(t) = 0
\]
for a suitable constant $\omega$ expressed in terms of $m$ and $k$.
\item[(ii)] Solve this differential equation and show that all real solutions can be written as
\[
x(t) = A\cos(\omega t) + B\sin(\omega t)
\]
for constants $A,B\in\mathbb{R}$.
\item[(iii)] Given initial conditions $x(0)=x_0$ and $x'(0)=v_0$, determine $A$ and $B$, and rewrite the solution in the amplitude--phase form
\[
x(t) = R\cos(\omega t - \phi)
\]
by expressing $R>0$ and $\phi\in\mathbb{R}$ in terms of $x_0$ and $v_0$.
\item[(iv)] Define
\[
E(t) = \frac{1}{2}m\bigl(x'(t)\bigr)^2 + \frac{1}{2}k\bigl(x(t)\bigr)^2.
\]
Show that $E'(t) = 0$ along any solution, and interpret this physically.
\end{enumerate}
\end{problem}

\begin{solution}
We proceed step by step, beginning from the physical modeling and ending with a qualitative interpretation of the motion.

\medskip

\emph{(i) Derivation of the equation of motion and identification of $\omega$.}

We choose a coordinate axis along the direction of motion, with $x(t)$ denoting displacement from the equilibrium position, and we take $x>0$ when the spring is stretched. Newton's second law states that
\[
m x''(t) = \text{(sum of forces acting on the mass)}.
\]
For an ideal spring obeying Hooke's law, the restoring force is proportional to the displacement and points toward equilibrium. Thus the spring force is
\[
F_{\text{spring}} = -k x(t),
\]
where the minus sign indicates that the force is opposite in direction to $x(t)$.

Assuming there are no other horizontal forces (no friction or external forcing), Newton's law becomes
\[
m x''(t) = -k x(t).
\]
Rewriting, we obtain the second-order linear ordinary differential equation
\[
m x''(t) + k x(t) = 0.
\]

It is convenient to divide by $m>0$ and introduce the constant
\[
\omega^2 = \frac{k}{m}.
\]
Then the equation of motion takes the standard form
\[
x''(t) + \omega^2 x(t) = 0.
\]
The constant $\omega>0$ is called the \emph{angular frequency} of the oscillator.

\medskip

\emph{(ii) Solving the ODE via the characteristic equation.}

The equation
\[
x''(t) + \omega^2 x(t) = 0
\]
is a linear, homogeneous, constant-coefficient ODE of order two. A standard method for such equations is to look for solutions of exponential form
\[
x(t) = e^{rt},
\]
where $r$ is a constant to be determined. Substituting this trial solution, we compute
\[
x'(t) = r e^{rt}, \qquad x''(t) = r^2 e^{rt}.
\]
Plugging into the differential equation gives
\[
r^2 e^{rt} + \omega^2 e^{rt} = 0.
\]
Since $e^{rt}\neq 0$ for all $t$, we can divide by $e^{rt}$ and obtain the \emph{characteristic equation}
\[
r^2 + \omega^2 = 0.
\]
Solving for $r$ yields
\[
r^2 = -\omega^2 \quad \Rightarrow \quad r = \pm i\omega.
\]
Thus the characteristic roots are purely imaginary.

For a second-order linear homogeneous equation with distinct complex conjugate roots $r_1 = i\omega$ and $r_2 = -i\omega$, the general complex-valued solution is
\[
x(t) = C_1 e^{i\omega t} + C_2 e^{-i\omega t},
\]
where $C_1$ and $C_2$ may be complex constants. However, our physical displacement $x(t)$ must be real-valued. We can therefore use Euler's formula
\[
e^{i\theta} = \cos\theta + i\sin\theta,
\]
to express $e^{\pm i\omega t}$ in terms of sine and cosine. One way to proceed is to note that the real and imaginary parts of $e^{i\omega t}$ each satisfy the differential equation, because it has real coefficients and is linear. Consequently, both
\[
\cos(\omega t) \quad \text{and} \quad \sin(\omega t)
\]
are real-valued solutions. By linearity, any linear combination
\[
x(t) = A\cos(\omega t) + B\sin(\omega t),
\]
where $A$ and $B$ are real constants, is again a solution.

Moreover, since we have found two linearly independent real solutions $\cos(\omega t)$ and $\sin(\omega t)$, and the equation is of order two, the general real solution is precisely
\[
x(t) = A\cos(\omega t) + B\sin(\omega t),
\]
for some $A,B\in\mathbb{R}$.

\medskip

\emph{(iii) Imposing initial conditions and amplitude--phase form.}

We now incorporate the initial conditions
\[
x(0) = x_0, \qquad x'(0) = v_0.
\]
Starting from the general solution
\[
x(t) = A\cos(\omega t) + B\sin(\omega t),
\]
we first compute its derivative:
\[
x'(t) = -A\omega \sin(\omega t) + B\omega \cos(\omega t).
\]

Evaluating at $t=0$ gives
\[
x(0) = A\cos(0) + B\sin(0) = A,
\]
so
\[
A = x_0.
\]
Similarly,
\[
x'(0) = -A\omega\sin(0) + B\omega\cos(0) = B\omega,
\]
so
\[
B = \frac{v_0}{\omega}.
\]

Thus the unique solution with the given initial data is
\[
x(t) = x_0 \cos(\omega t) + \frac{v_0}{\omega}\sin(\omega t).
\]

Next we rewrite this in amplitude--phase form. We seek $R>0$ and $\phi\in\mathbb{R}$ such that
\[
x(t) = R\cos(\omega t - \phi).
\]
Using the cosine subtraction identity,
\[
\cos(\omega t - \phi) = \cos\phi \,\cos(\omega t) + \sin\phi \,\sin(\omega t),
\]
we obtain
\[
R\cos(\omega t - \phi)
= R\cos\phi\,\cos(\omega t) + R\sin\phi\,\sin(\omega t).
\]
Comparing this with
\[
x(t) = x_0 \cos(\omega t) + \frac{v_0}{\omega}\sin(\omega t),
\]
we see that we must have
\[
R\cos\phi = x_0, \qquad R\sin\phi = \frac{v_0}{\omega}.
\]

We can solve for $R$ by squaring and adding:
\[
R^2 = (R\cos\phi)^2 + (R\sin\phi)^2
= x_0^2 + \left(\frac{v_0}{\omega}\right)^2,
\]
so
\[
R = \sqrt{x_0^2 + \left(\frac{v_0}{\omega}\right)^2}.
\]
Since $R>0$, this determines $R$ uniquely.

To find $\phi$, we can use
\[
\cos\phi = \frac{x_0}{R}, \qquad \sin\phi = \frac{v_0}{\omega R},
\]
so that, for example,
\[
\phi = \arctan\!\left(\frac{v_0}{\omega x_0}\right),
\]
with the understanding that the correct quadrant for $\phi$ is chosen using the signs of $x_0$ and $v_0$ (equivalently, using the two-argument arctangent function). In any case, $(R,\phi)$ determined in this way yields the same motion as the $(A,B)$ description.

The amplitude--phase form
\[
x(t) = R\cos(\omega t - \phi)
\]
makes it clear that the mass executes sinusoidal motion of fixed amplitude $R$, oscillating with angular frequency $\omega$ and period $T = 2\pi/\omega$. The phase $\phi$ encodes where in its cycle the oscillator is at time $t=0$.

\medskip

\emph{(iv) Conservation of energy.}

We now define the mechanical energy of the mass--spring system by
\[
E(t) = \frac{1}{2}m\bigl(x'(t)\bigr)^2 + \frac{1}{2}k\bigl(x(t)\bigr)^2.
\]
The first term represents kinetic energy, and the second represents potential energy stored in the spring.

We differentiate $E(t)$ with respect to $t$:
\[
E'(t)
= m x'(t)x''(t) + k x(t)x'(t),
\]
where we used the chain rule on each term.

Now we use the equation of motion $m x''(t) + k x(t) = 0$, which can be rewritten as
\[
m x''(t) = -k x(t).
\]
Substituting this into the expression for $E'(t)$, we obtain
\[
E'(t) = x'(t)\, \bigl(m x''(t) + k x(t)\bigr) = x'(t)\cdot 0 = 0.
\]
Therefore $E(t)$ is constant in time along any solution:
\[
E(t) = E(0) \quad \text{for all } t.
\]

Physically, this means that in the idealized undamped mass--spring system, the total mechanical energy is conserved. Energy is continuously exchanged between kinetic and potential forms: when the mass passes through equilibrium, all energy is kinetic; when it reaches an extreme displacement, all energy is potential. But the sum remains constant, and as a result the oscillations persist forever with constant amplitude.

\medskip

\emph{Connection to ``ODEs: Simple Cases.''}

This example illustrates how a fundamental physical principle (Newton's law plus Hooke's law) naturally leads to a second-order linear ODE with constant coefficients. The solution method—postulating exponential solutions, deriving the characteristic equation, and interpreting complex conjugate roots in terms of sine and cosine—embodies the central technique for solving such equations. The simple harmonic oscillator also shows how qualitative features of the solution (oscillation frequency, amplitude, and energy conservation) are encoded in the parameters and structure of the ODE, providing a model example of how mathematical analysis illuminates physical behavior.
\end{solution}

\section{Direct Methods for Solving Linear ODEs}
% --- Narrative plan (auto-generated) ---
% This section develops the core "direct" techniques for solving linear ordinary differential equations: methods that convert an equation and its data into an explicit formula for the solution. We focus on constant-coefficient equations and low-dimensional systems, where algebraic tools such as characteristic polynomials, eigenvalues, and particular-solution ansätze are especially effective. Along the way, we connect the algebraic structure of a differential equation to the qualitative behavior of its solutions, such as exponential growth and decay, oscillations, and resonance.
%
% These ideas are central throughout applied mathematics. Many basic models in mechanics, electrical circuits, population dynamics, and simple control systems reduce to linear ODEs that can be solved directly. In turn, the same solution forms reappear when separating variables in linear partial differential equations, for instance in the heat or wave equation, and when analyzing linearized stability of nonlinear dynamical systems. The characteristic-exponential viewpoint also links naturally to topics such as complex analysis, Fourier methods, and spectral theory, where exponentials and eigenfunctions play a unifying role.

% ===== Example 1: Linear First-Order Equation for Exponential Growth and Decay (inquiry-based) =====
\begin{problem}[Linear First-Order Equation for Exponential Growth and Decay]
Many physical and biological processes have the feature that the rate of change of a quantity is proportional to the quantity itself. A standard example is radioactive decay: each atom has the same chance of decaying per unit time, so the overall decay rate is proportional to how many atoms remain. A contrasting example is population growth in an idealized, unlimited environment: the more individuals there are, the more births occur per unit time. In both cases, the same kind of differential equation appears, and its solutions are exponential functions.

In this problem you will discover how this model leads to a first-order linear ordinary differential equation, how to solve it explicitly, and how the familiar notions of half-life and doubling time arise from the solution.

\smallskip

(a) Let $Q(t)$ denote the amount of a substance (or the size of a population) at time $t$. Suppose we are told that the rate of change of $Q$ is \emph{proportional} to the current amount $Q(t)$, with proportionality constant $k$.

\begin{enumerate}
\item[(i)] Write a differential equation for $Q(t)$ that encodes this statement. Be explicit about the sign of $k$ in the cases of growth and decay.
\item[(ii)] What are the physical units of the constant $k$ if $Q$ is measured in grams and time is measured in years? Explain briefly.
\end{enumerate}

Hint: ``Rate of change'' means a derivative with respect to time, and ``proportional to'' means a constant multiple of.

\smallskip

(b) Now treat $k$ as a given real constant, and assume $Q(t)$ satisfies the differential equation you wrote in part (a). 

\begin{enumerate}
\item[(i)] Rewrite the equation in a form where all occurrences of $Q$ are on one side and all occurrences of $t$ are on the other side.
\item[(ii)] Integrate both sides with respect to $t$ to obtain an equation involving $\ln |Q(t)|$ and $t$.

Hint: You should obtain an integral of the form $\displaystyle \int \frac{1}{Q}\, dQ$ on one side. Recall that $\displaystyle \int \frac{1}{Q}\, dQ = \ln|Q| + C$.
\end{enumerate}

\smallskip

(c) From your integrated equation in part (b), solve explicitly for $Q(t)$.

\begin{enumerate}
\item[(i)] Show that the general solution can be written in the form
\[
Q(t) = Ce^{kt}
\]
for some constant $C$. Explain how the constant of integration from part (b) is related to $C$.
\item[(ii)] Suppose that $Q(0) = Q_0$ is known. Determine $C$ in terms of $Q_0$ and write the solution $Q(t)$ entirely in terms of $Q_0$, $k$, and $t$.
\end{enumerate}

Hint: Evaluate the general solution at $t=0$ and use the fact that $e^{0} = 1$.

\smallskip

(d) Let $Q(t)$ be the solution with initial condition $Q(0) = Q_0 > 0$.

\begin{enumerate}
\item[(i)] Assume $k < 0$ (radioactive decay). Define the \emph{half-life} $T_{1/2}$ to be the time at which $Q(T_{1/2}) = \tfrac12 Q_0$. Use your formula for $Q(t)$ to solve for $T_{1/2}$ in terms of $k$.
\item[(ii)] Assume $k > 0$ (unrestricted population growth). Define the \emph{doubling time} $T_{2}$ to be the time at which $Q(T_{2}) = 2 Q_0$. Use your formula for $Q(t)$ to solve for $T_2$ in terms of $k$.
\item[(iii)] In each case, does the characteristic time ($T_{1/2}$ or $T_2$) depend on the initial amount $Q_0$? Explain why this is or is not reasonable from the modeling point of view.
\end{enumerate}

Hint: You will need to solve equations of the form $Q_0 e^{kT} = \alpha Q_0$ for $T$, where $\alpha$ is a constant such as $\tfrac12$ or $2$. Divide both sides by $Q_0$ and take logarithms.

\smallskip

(e) Explore the limitations and possible extensions of this model.

\begin{enumerate}
\item[(i)] In reality, a population cannot grow forever without bound. Suppose we modify the model so that the growth rate slows down when $Q(t)$ becomes large, for instance by introducing a carrying capacity $K>0$. One classical model is
\[
\frac{dQ}{dt} = k Q(t)\left(1 - \frac{Q(t)}{K}\right).
\]
Compare this with your original equation. In what sense is the original exponential growth model a special case of this more complicated model?
\item[(ii)] In radioactive decay, $k$ is essentially constant in time. In some population models, however, the effective growth rate may vary seasonally, so that it is more realistic to write
\[
\frac{dQ}{dt} = k(t)\, Q(t),
\]
where $k(t)$ is a known function of time. Based on the separation-of-variables step you used in part (b), sketch (in words or symbols) how you would attempt to solve this more general equation.
\end{enumerate}

Hint: For part (e)(ii), think about repeating the step where you placed all $Q$'s on one side and all $t$'s on the other. What changes if $k$ depends on $t$?
\end{problem}

% ===== Example 1: Linear First-Order Equation for Exponential Growth and Decay (full solution) =====
\begin{problem}[Linear First-Order Equation for Exponential Growth and Decay]
Consider a quantity $Q(t)$ that changes in time so that its rate of change is proportional to its current value:
\[
\frac{dQ}{dt} = k Q(t),
\]
where $k$ is a real constant. 

\begin{enumerate}
\item[(a)] Solve this differential equation to obtain the general solution.
\item[(b)] Impose the initial condition $Q(0) = Q_0$ with $Q_0>0$ and write the corresponding solution $Q(t)$ explicitly.
\item[(c)] Assume $k < 0$ and define the half-life $T_{1/2}$ to be the time such that $Q(T_{1/2}) = \tfrac12 Q_0$. Express $T_{1/2}$ in terms of $k$.
\item[(d)] Assume $k > 0$ and define the doubling time $T_2$ to be the time such that $Q(T_2) = 2 Q_0$. Express $T_2$ in terms of $k$.
\item[(e)] In each of the cases (c) and (d), state whether the characteristic time ($T_{1/2}$ or $T_2$) depends on the initial amount $Q_0$, and briefly explain why this is consistent with the model.
\end{enumerate}
\end{problem}

\begin{solution}
We are given the first-order linear ordinary differential equation
\[
\frac{dQ}{dt} = k Q(t),
\]
where $k$ is a real constant, and we are asked to solve it and interpret parameters such as half-life and doubling time. This is the simplest nontrivial example of a linear ODE and illustrates that linear constant-coefficient equations produce exponential solutions.

\medskip

\noindent\textbf{(a) General solution.}
We solve the equation directly by separation of variables. For all $t$ where $Q(t)\neq 0$, we can write
\[
\frac{dQ}{dt} = k Q(t)
\quad\Longrightarrow\quad
\frac{1}{Q(t)}\,\frac{dQ}{dt} = k.
\]
Interpreting $\dfrac{dQ}{dt}\,dt$ as $dQ$, we separate variables:
\[
\frac{1}{Q}\, dQ = k\, dt.
\]
We now integrate both sides with respect to $t$ (equivalently, with respect to $Q$ on the left):
\[
\int \frac{1}{Q}\, dQ = \int k\, dt.
\]
The antiderivatives are
\[
\ln|Q| = kt + C,
\]
where $C$ is a constant of integration.

To solve for $Q$, we exponentiate both sides:
\[
|Q| = e^{kt + C} = e^C e^{kt}.
\]
Because $e^C>0$, we can absorb both the absolute value and the positive multiplicative constant into a single nonzero constant $C_1\in\mathbb{R}$, writing
\[
Q(t) = C_1 e^{kt}.
\]
Thus, the general nontrivial solution has the exponential form
\[
Q(t) = C e^{kt},
\]
where $C$ is an arbitrary real constant. In addition, $Q(t)\equiv 0$ is also a solution, obtained by taking $C=0$. So the full family of solutions is $Q(t) = C e^{kt}$ for $C\in\mathbb{R}$.

\medskip

\noindent\textbf{(b) Solution with initial condition.}
We now impose the initial condition $Q(0) = Q_0$ with $Q_0>0$. Substituting $t=0$ into the general solution gives
\[
Q(0) = C e^{k\cdot 0} = C e^{0} = C.
\]
Therefore $C = Q_0$, and the unique solution satisfying this initial condition is
\[
Q(t) = Q_0 e^{kt}.
\]

\medskip

\noindent\textbf{(c) Half-life for $k<0$.}
Assume $k<0$, corresponding to exponential decay such as radioactive decay. By definition, the half-life $T_{1/2}$ is the time at which the quantity has decreased to one half of its initial value:
\[
Q(T_{1/2}) = \frac{1}{2} Q_0.
\]
Using the solution $Q(t) = Q_0 e^{kt}$, we substitute $t = T_{1/2}$ and obtain
\[
Q_0 e^{k T_{1/2}} = \frac{1}{2} Q_0.
\]
Because $Q_0>0$, we can divide both sides by $Q_0$ to get
\[
e^{k T_{1/2}} = \frac{1}{2}.
\]
Taking the natural logarithm of both sides yields
\[
k T_{1/2} = \ln\left(\frac{1}{2}\right) = -\ln 2.
\]
Solving for $T_{1/2}$ gives
\[
T_{1/2} = \frac{-\ln 2}{k}.
\]
Since $k<0$, the quotient $-\ln 2/k$ is positive, as expected for a time. It is often convenient to rewrite this as
\[
T_{1/2} = \frac{\ln 2}{|k|},
\]
because $|k| = -k$ when $k<0$.

\medskip

\noindent\textbf{(d) Doubling time for $k>0$.}
Now assume $k>0$, corresponding to exponential growth. By definition, the doubling time $T_2$ is the time at which the quantity has increased to twice its initial value:
\[
Q(T_2) = 2 Q_0.
\]
Again using the solution $Q(t) = Q_0 e^{kt}$, we set $t = T_2$ and obtain
\[
Q_0 e^{k T_2} = 2 Q_0.
\]
Dividing both sides by $Q_0$ (which is positive) gives
\[
e^{k T_2} = 2.
\]
Taking natural logarithms,
\[
k T_2 = \ln 2,
\]
and hence
\[
T_2 = \frac{\ln 2}{k}.
\]
Here $k>0$, so $T_2$ is again positive. Note that the same constant $\ln 2$ appears, with the sign of $k$ distinguishing decay from growth.

\medskip

\noindent\textbf{(e) Dependence on $Q_0$ and modeling interpretation.}
In parts (c) and (d), the characteristic times $T_{1/2}$ and $T_2$ are given by
\[
T_{1/2} = \frac{\ln 2}{|k|}, 
\qquad
T_2 = \frac{\ln 2}{k}.
\]
Neither formula involves the initial amount $Q_0$. Therefore, the half-life and the doubling time are independent of the initial condition.

This independence is consistent with the modeling assumption that the rate of change is \emph{proportional} to $Q(t)$ with a fixed proportionality constant $k$. In the decay case, each individual atom has the same probability per unit time of decaying, regardless of how many atoms there are, so the characteristic time scale is determined solely by $k$ and not by $Q_0$. In the growth case, if each individual reproduces at the same rate, it takes the same amount of time for any starting population to double; what changes with $Q_0$ is the absolute size of the population, not the fraction by which it increases over a given characteristic time.

\medskip

\noindent\textbf{Connection to direct methods for linear ODEs.}
This example illustrates the main idea of the section on direct methods for solving linear ODEs: when we have a first-order linear equation with constant coefficients, such as $Q' = kQ$, we can often solve it explicitly by straightforward manipulations. Here, the equation is both linear and separable, and the solution is obtained by simple integration after separating variables. The result is an exponential function, showing that linear constant-coefficient equations naturally produce exponential responses. This pattern persists in more complicated linear systems and higher-order linear ODEs, where eigenvalues and exponentials play a central role.
\end{solution}

% ===== Example 2: Forced Mass–Spring System and Undetermined Coefficients (inquiry-based) =====
\begin{problem}[Forced Mass–Spring System and Undetermined Coefficients]
A mass attached to a spring and subject to an external periodic force is a standard model for vibrations in engineering and physics. Neglecting friction, the displacement $x(t)$ from equilibrium satisfies a second-order linear ordinary differential equation with constant coefficients. When the forcing is sinusoidal, the solution typically consists of transient oscillations plus a steady-state oscillation at the forcing frequency. In some special situations, called \emph{resonance}, the amplitude of the oscillations can grow very large.

In this problem we explore how to solve such equations using the method of undetermined coefficients, and we see how resonance appears naturally in the mathematics.

Consider a unit mass attached to a spring with spring constant $k = 4$. An external horizontal force $F(t)$ acts on the mass. Neglect all friction or damping. Then the displacement $x(t)$ satisfies
\[
x''(t) + 4 x(t) = F(t),
\]
where $F(t)$ is measured in suitable units.

\smallskip

\noindent\textbf{(a) Natural oscillations (no forcing).}  
First suppose there is no external force, so $F(t) \equiv 0$ and the equation reduces to
\[
x''(t) + 4 x(t) = 0.
\]

\begin{enumerate}
\item[(i)] Solve this homogeneous equation by the characteristic equation method and write the general solution $x_h(t)$.
\item[(ii)] What is the natural (angular) frequency of the undamped mass–spring system? How is it related to the eigenvalues you found?
\end{enumerate}

Hint: Recall that for $x'' + \omega_0^2 x = 0$, the general solution is a combination of $\cos(\omega_0 t)$ and $\sin(\omega_0 t)$.

\smallskip

\noindent\textbf{(b) A first forced problem: nonresonant sinusoidal forcing.}  
Now let the external force be
\[
F(t) = 2 \cos(3t),
\]
so the equation becomes
\[
x''(t) + 4 x(t) = 2 \cos(3t).
\]

\begin{enumerate}
\item[(i)] Explain why the general solution should be written in the form
\[
x(t) = x_h(t) + x_p(t),
\]
where $x_h$ solves the homogeneous equation from part (a), and $x_p$ is a particular solution of the forced equation.
\item[(ii)] Using the method of undetermined coefficients, propose a form for $x_p(t)$ for this nonresonant problem.  
What combination of sines and cosines with frequency $3$ should you try?
\item[(iii)] Substitute your proposed $x_p(t)$ into the differential equation and solve for the unknown coefficients.  
Write down one particular solution $x_p(t)$ explicitly.
\end{enumerate}

Hint: Try $x_p(t) = A \cos(3t) + B \sin(3t)$ and determine $A$ and $B$.

\smallskip

\noindent\textbf{(c) Resonant forcing: what goes wrong with the first guess?}  
Now change the forcing frequency to match the natural frequency of the system:
\[
F(t) = 2 \cos(2t),
\]
so that
\[
x''(t) + 4 x(t) = 2 \cos(2t).
\]

\begin{enumerate}
\item[(i)] If you repeat the same guess as in part (b),
\[
x_p(t) = A \cos(2t) + B \sin(2t),
\]
what happens when you substitute this into the equation? Do you get equations that determine $A$ and $B$, or do you run into a difficulty? Describe precisely what fails.
\item[(ii)] How is the failure in (i) related to the homogeneous solution $x_h(t)$ from part (a)?
\end{enumerate}

Hint: Compare your guessed $x_p$ with the terms already present in $x_h(t)$.

\smallskip

\noindent\textbf{(d) Fixing the resonance and finding the full solution.}  
To repair the method of undetermined coefficients in the resonant case, you modify your guess so that it is not a solution of the homogeneous equation.

\begin{enumerate}
\item[(i)] Propose a new form for $x_p(t)$ by multiplying your old guess by a suitable power of $t$. What form should you try here?
\item[(ii)] Substitute your new $x_p(t)$ into the equation $x'' + 4x = 2\cos(2t)$ and solve for the coefficients.  
Obtain an explicit particular solution $x_p(t)$.
\item[(iii)] Combine $x_h(t)$ and your $x_p(t)$ to form the general solution
\[
x(t) = x_h(t) + x_p(t).
\]
Now impose the initial conditions
\[
x(0) = 0, \qquad x'(0) = 0
\]
and determine the unique solution satisfying these conditions.
\item[(iv)] Study the behavior of your solution as $t \to \infty$. Does the amplitude remain bounded, or does it grow without bound? How fast does it grow?  
Explain how this is related to the physical phenomenon of resonance.
\end{enumerate}

Hint: When the forcing frequency equals the natural frequency, the method of undetermined coefficients prescribes multiplying by $t$; look for growth of order $t$ in the amplitude.

\smallskip

\noindent\textbf{(e) Extensions and variations.}  
In real systems there is often some damping and sometimes other types of forcing.

\begin{enumerate}
\item[(i)] Suppose we add a damping term $2x'(t)$ to the model and consider
\[
x''(t) + 2x'(t) + 4x(t) = 2\cos(2t).
\]
Without solving this new equation in full detail, explain how you would modify the method of undetermined coefficients to find a particular solution. What form would you guess for $x_p(t)$? Do you expect unbounded growth of the amplitude as $t \to \infty$ in this damped case? Briefly justify your answer.
\item[(ii)] Imagine instead that the forcing is not sinusoidal but exponential,
\[
F(t) = e^{t},
\]
so that the equation becomes
\[
x''(t) + 4x(t) = e^t.
\]
What form of $x_p(t)$ would you try using undetermined coefficients? How would your guess change if the homogeneous solution already contained terms proportional to $e^t$?
\end{enumerate}

Hint: For exponential forcing $e^{\lambda t}$, try $x_p(t) = Ce^{\lambda t}$, unless $e^{\lambda t}$ already solves the homogeneous equation, in which case multiply by a suitable power of $t$.
\end{problem}

% ===== Example 2: Forced Mass–Spring System and Undetermined Coefficients (full solution) =====
\begin{problem}[Forced Mass–Spring System and Undetermined Coefficients]
Consider the undamped mass–spring system
\[
x''(t) + 4 x(t) = F(t),
\]
where $x(t)$ is the displacement from equilibrium.

\begin{enumerate}
\item[(i)] Solve the homogeneous equation $x'' + 4x = 0$ and identify the natural angular frequency of the system.
\item[(ii)] Use the method of undetermined coefficients to find a particular solution of
\[
x''(t) + 4x(t) = 2\cos(3t),
\]
and hence write the general solution.
\item[(iii)] Now consider the resonant forcing
\[
x''(t) + 4x(t) = 2\cos(2t).
\]
Explain why the naive guess $x_p(t) = A\cos(2t) + B\sin(2t)$ fails, and then find a correct particular solution by modifying the guess appropriately.
\item[(iv)] For the resonant equation in (iii), impose the initial conditions $x(0) = 0$ and $x'(0) = 0$. Find the unique solution and describe its long-time behavior as $t \to \infty$. Interpret this behavior in terms of resonance.
\item[(v)] Briefly indicate how the method of undetermined coefficients would be applied if the equation were
\[
x''(t) + 2x'(t) + 4x(t) = 2\cos(2t)
\quad\text{or}\quad
x''(t) + 4x(t) = e^t.
\]
State the trial form you would use for $x_p(t)$ in each case.
\end{enumerate}
\end{problem}

\begin{solution}
We analyze a standard forced mass–spring system governed by a linear second-order ordinary differential equation with constant coefficients. The main idea of the \emph{direct method} used here is to decompose the solution into the sum of the homogeneous solution (describing natural or transient behavior) and a particular solution (describing the forced or steady-state response). The method of undetermined coefficients provides a systematic way to guess the form of the particular solution for many common forcing terms.

\medskip

\noindent\textbf{(i) Homogeneous solution and natural frequency.}  
The homogeneous equation is
\[
x''(t) + 4x(t) = 0.
\]
We solve this by the characteristic equation. Assume $x(t) = e^{rt}$, which gives
\[
r^2 + 4 = 0 \quad\Longrightarrow\quad r = \pm 2i.
\]
Thus the general real solution is
\[
x_h(t) = C_1\cos(2t) + C_2\sin(2t),
\]
where $C_1$ and $C_2$ are arbitrary constants. The natural angular frequency of the oscillation is therefore $\omega_0 = 2$.

\medskip

\noindent\textbf{(ii) Nonresonant forcing $2\cos(3t)$.}  
We now consider
\[
x''(t) + 4x(t) = 2\cos(3t).
\]
The general solution is of the form
\[
x(t) = x_h(t) + x_p(t),
\]
where $x_h(t)$ is the homogeneous solution found above, and $x_p(t)$ is any particular solution of the nonhomogeneous equation.

Because the forcing term is sinusoidal with angular frequency $3$, and $3$ is \emph{not} equal to the natural frequency $2$, a standard undetermined-coefficients guess is
\[
x_p(t) = A\cos(3t) + B\sin(3t),
\]
where $A$ and $B$ are constants to be determined.

We compute derivatives:
\[
x_p'(t) = -3A\sin(3t) + 3B\cos(3t),
\]
\[
x_p''(t) = -9A\cos(3t) - 9B\sin(3t).
\]
Substitute into the differential equation:
\[
x_p'' + 4x_p
= \bigl(-9A\cos(3t) - 9B\sin(3t)\bigr) + 4\bigl(A\cos(3t) + B\sin(3t)\bigr).
\]
Combine like terms:
\[
x_p'' + 4x_p
= (-9A + 4A)\cos(3t) + (-9B + 4B)\sin(3t)
= (-5A)\cos(3t) + (-5B)\sin(3t).
\]
We require this to equal the forcing term $2\cos(3t)$:
\[
-5A\cos(3t) - 5B\sin(3t) = 2\cos(3t) + 0\cdot\sin(3t).
\]
Equating coefficients of $\cos(3t)$ and $\sin(3t)$ gives the system
\[
-5A = 2, \qquad -5B = 0.
\]
Thus
\[
A = -\frac{2}{5}, \qquad B = 0.
\]
Therefore one particular solution is
\[
x_p(t) = -\frac{2}{5}\cos(3t).
\]

The general solution of the nonresonant problem is then
\[
x(t) = C_1\cos(2t) + C_2\sin(2t) - \frac{2}{5}\cos(3t).
\]
Here the first two terms represent natural oscillations at frequency $2$ (which will depend on initial conditions), and the last term is a forced oscillation at frequency $3$, with fixed amplitude determined by the forcing.

\medskip

\noindent\textbf{(iii) Resonant forcing $2\cos(2t)$.}  
Now we consider
\[
x''(t) + 4x(t) = 2\cos(2t),
\]
where the forcing frequency $2$ \emph{equals} the natural frequency of the system.

\smallskip

\emph{Failure of the naive guess.}  
If we try the same form as in the nonresonant case,
\[
x_p(t) = A\cos(2t) + B\sin(2t),
\]
we immediately see that this is contained in the homogeneous solution
\[
x_h(t) = C_1\cos(2t) + C_2\sin(2t).
\]
Any linear combination $A\cos(2t) + B\sin(2t)$ solves the homogeneous equation $x'' + 4x = 0$. If we substitute this $x_p$ into $x'' + 4x$, we obtain
\[
x_p'' + 4x_p = 0,
\]
which can never equal $2\cos(2t)$ for all $t$. There is no choice of $A$ and $B$ that makes $x_p'' + 4x_p$ equal to the nonzero forcing term. Thus the naive guess fails because it produces only homogeneous solutions.

\smallskip

\emph{Corrected guess by multiplying by $t$.}  
The method of undetermined coefficients instructs us that when the forcing term is a solution of the homogeneous equation (or is of a type that overlaps with it), we should multiply our usual trial function by a suitable power of $t$ to obtain a linearly independent form.

Since $\cos(2t)$ and $\sin(2t)$ already appear in $x_h$, we try
\[
x_p(t) = t\bigl(A\cos(2t) + B\sin(2t)\bigr).
\]
We differentiate:
\[
x_p'(t) 
= A\cos(2t) + B\sin(2t)
+ t\bigl(-2A\sin(2t) + 2B\cos(2t)\bigr),
\]
\[
x_p''(t)
= \underbrace{-2A\sin(2t) + 2B\cos(2t)}_{\text{derivative of the first part}}
+ \underbrace{\bigl(-2A\sin(2t) + 2B\cos(2t)\bigr)}_{\text{derivative of the second part without $t$}}
+ t\bigl(-4A\cos(2t) - 4B\sin(2t)\bigr).
\]
Collecting terms, we obtain
\[
x_p''(t) = (-4A\sin(2t) + 4B\cos(2t)) + t(-4A\cos(2t) - 4B\sin(2t)).
\]

Now compute $x_p'' + 4x_p$:
\begin{align*}
x_p''(t) + 4x_p(t)
&= \bigl(-4A\sin(2t) + 4B\cos(2t)\bigr) 
+ t(-4A\cos(2t) - 4B\sin(2t)) \\
&\quad + 4t\bigl(A\cos(2t) + B\sin(2t)\bigr).
\end{align*}
The $t$-dependent terms cancel:
\[
t(-4A\cos(2t) - 4B\sin(2t)) + 4t(A\cos(2t) + B\sin(2t)) = 0.
\]
So we are left with
\[
x_p''(t) + 4x_p(t) = -4A\sin(2t) + 4B\cos(2t).
\]
We want this to equal the forcing term $2\cos(2t)$. Thus
\[
-4A\sin(2t) + 4B\cos(2t) = 2\cos(2t) + 0\cdot \sin(2t).
\]
Equating coefficients of $\cos(2t)$ and $\sin(2t)$ gives
\[
4B = 2, \qquad -4A = 0,
\]
so
\[
A = 0, \qquad B = \frac{1}{2}.
\]
Therefore a valid particular solution is
\[
x_p(t) = t\cdot \frac{1}{2}\sin(2t) = \frac{t}{2}\sin(2t).
\]

\medskip

\noindent\textbf{(iv) Solution with initial conditions and long-time behavior.}  
The general solution of the resonant equation is
\[
x(t) = x_h(t) + x_p(t)
= C_1\cos(2t) + C_2\sin(2t) + \frac{t}{2}\sin(2t).
\]

We now impose $x(0) = 0$ and $x'(0) = 0$.

First, use $x(0) = 0$:
\[
x(0) = C_1\cos(0) + C_2\sin(0) + \frac{0}{2}\sin(0) = C_1.
\]
Hence $C_1 = 0$.

Next, compute $x'(t)$:
\[
x'(t) = C_1(-2\sin(2t)) + C_2(2\cos(2t)) + \frac{1}{2}\sin(2t) + \frac{t}{2}\cdot 2\cos(2t).
\]
Substitute $C_1 = 0$:
\[
x'(t) = 2C_2\cos(2t) + \frac{1}{2}\sin(2t) + t\cos(2t).
\]
Now impose $x'(0) = 0$:
\[
x'(0) = 2C_2\cos(0) + \frac{1}{2}\sin(0) + 0\cdot \cos(0) = 2C_2.
\]
Thus $C_2 = 0$.

Therefore the unique solution satisfying the given initial conditions is
\[
x(t) = \frac{t}{2}\sin(2t).
\]

To analyze the long-time behavior, observe that $\sin(2t)$ remains bounded between $-1$ and $1$ for all $t$, but it is multiplied by the factor $t/2$. Thus the envelope of the oscillation grows linearly in time:
\[
\lvert x(t)\rvert \leq \frac{t}{2}\quad\text{for all }t\ge 0.
\]
Hence the amplitude grows without bound as $t \to \infty$; more precisely, it grows on the order of $t$.

Physically, this is the phenomenon of \emph{resonance}. The forcing frequency matches the natural frequency of the undamped system, so each push from the external force constructively adds energy to the system. The transient contribution from the homogeneous solution has decayed (in this undamped case, it simply reflects initial conditions), but the forced component continues to build, and the amplitude increases indefinitely.

\medskip

\noindent\textbf{(v) Other forcings and damping: trial forms.}  

\smallskip

\emph{(a) Damped, sinusoidally forced system.}  
Consider
\[
x''(t) + 2x'(t) + 4x(t) = 2\cos(2t).
\]
The homogeneous equation $x'' + 2x' + 4x = 0$ has characteristic equation
\[
r^2 + 2r + 4 = 0,
\]
with roots $r = -1 \pm i\sqrt{3}$. Thus the homogeneous solution is
\[
x_h(t) = e^{-t}\bigl(C_1\cos(\sqrt{3}t) + C_2\sin(\sqrt{3}t)\bigr),
\]
which oscillates at frequency $\sqrt{3}$ and is exponentially damped.

The forcing is still $2\cos(2t)$, a sinusoid with frequency $2$. This is not a solution of the homogeneous equation; the homogeneous terms involve a factor $e^{-t}$ and a different frequency. Therefore, the method of undetermined coefficients prescribes the same trial form as in a nonresonant undamped case:
\[
x_p(t) = A\cos(2t) + B\sin(2t).
\]
Substituting this into the damped equation and solving for $A$ and $B$ would yield a bounded particular solution.

In the presence of damping, even when the forcing frequency coincides with the undamped natural frequency, the amplitude remains bounded in time. Energy is continually supplied by the forcing but also continually dissipated by damping, leading to a steady-state oscillation of finite amplitude rather than unbounded growth.

\smallskip

\emph{(b) Exponential forcing.}  
Consider instead
\[
x''(t) + 4x(t) = e^t.
\]
Here the forcing is the exponential $e^t = e^{1\cdot t}$. A standard undetermined-coefficients trial function for a forcing of the form $e^{\lambda t}$ is
\[
x_p(t) = Ce^{\lambda t},
\]
provided that $e^{\lambda t}$ is not already part of the homogeneous solution.

In our original homogeneous solution, $x_h(t) = C_1\cos(2t) + C_2\sin(2t)$, there is no $e^t$ term. Thus we would try
\[
x_p(t) = Ce^t.
\]
Substituting into $x'' + 4x = e^t$ would give an algebraic equation for $C$.

If instead the homogeneous equation had a root $r = 1$, so that $e^t$ were already part of $x_h(t)$, then the naive guess $Ce^t$ would fail just as in the resonant sinusoidal case. The method then prescribes multiplying by $t$ to obtain a linearly independent trial function:
\[
x_p(t) = Ct e^t,
\]
or, if necessary (for higher multiplicity), even higher powers of $t$.

\medskip

In summary, this example illustrates the main ideas of the section on \emph{Direct Methods for Solving Linear ODEs}. We:

\begin{itemize}
\item solved the homogeneous linear equation via its characteristic polynomial to find the natural oscillatory behavior;
\item used the method of undetermined coefficients to construct particular solutions for standard forcing terms (sinusoids and exponentials);
\item saw how resonance arises when the forcing term overlaps with the homogeneous solution, requiring a modification of the trial function (multiplication by $t$);
\item and interpreted the long-time behavior of solutions in physical terms, distinguishing between bounded steady-state oscillations and unbounded growth due to resonance.
\end{itemize}
These techniques form a central part of the toolkit for solving linear ordinary differential equations with constant coefficients in applied mathematics.
\end{solution}

% ===== Example 3: RLC Electrical Circuit as a Second-Order Linear ODE (inquiry-based) =====
\begin{problem}[RLC Electrical Circuit as a Second-Order Linear ODE]
An ideal series RLC circuit consists of a resistor of resistance $R>0$, an inductor of inductance $L>0$, and a capacitor of capacitance $C>0$, all in series with a voltage source $E(t)$. The state of the circuit can be described by the charge $q(t)$ on the capacitor plates or by the current $i(t)$ in the loop. Kirchhoff's voltage law tells us that the sum of voltage drops across the three elements equals the supplied voltage $E(t)$, which will lead to a second-order linear ordinary differential equation for $q(t)$. Depending on the values of $R$, $L$, and $C$, the circuit can behave like a damped oscillator with qualitatively different transient responses, and when driven by a time-dependent source $E(t)$ it can act as a frequency-selective filter.

(a) Let $q(t)$ denote the charge on the capacitor and $i(t)$ the current in the circuit. Recall that the voltage drops across the elements are given by
\[
V_R = R i(t), \quad V_L = L\,\dfrac{di}{dt}(t), \quad V_C = \dfrac{1}{C} q(t),
\]
and that $i(t) = \dfrac{dq}{dt}(t)$. Use Kirchhoff's voltage law
\[
V_R + V_L + V_C = E(t)
\]
to derive a single second-order linear ordinary differential equation for the charge $q(t)$ when the circuit is driven by a source $E(t)$. Write your final answer in the form
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = E(t).
\]
Explain briefly how each physical law enters your derivation.

% Hint: First write the KVL equation in terms of $i$ and $q$, then use $i = q'$ to eliminate $i$.

(b) For the remainder of parts (b)--(d), suppose the source is turned off, so $E(t) \equiv 0$. Then the charge satisfies
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = 0.
\]
Divide this equation by $L$ to obtain a standard form with leading coefficient $1$, and write down the characteristic polynomial for this homogeneous linear ODE. Express its roots in terms of $R$, $L$, and $C$. Introduce the discriminant
\[
\Delta = R^2 - 4\frac{L}{C},
\]
and explain why the sign of $\Delta$ leads to three qualitatively distinct types of solutions.

% Hint: You should obtain a quadratic characteristic equation in $\lambda$, solve it with the quadratic formula, and then analyze the possibilities $\Delta>0$, $\Delta=0$, and $\Delta<0$.

(c) Consider first the underdamped case $\Delta<0$. Show that in this case the general solution can be written in the form
\[
q(t) = e^{-\alpha t} \bigl( A \cos(\omega t) + B \sin(\omega t)\bigr)
\]
for suitable constants $\alpha>0$, $\omega>0$ depending on $R$, $L$, and $C$, and arbitrary real constants $A$ and $B$. Determine $\alpha$ and $\omega$ explicitly in terms of $R$, $L$, and $C$. Then, assuming initial conditions $q(0)=q_0$ and $q'(0)=i(0)=i_0$ are given, express $A$ and $B$ in terms of $q_0$ and $i_0$.

% Hint: Start from the complex-conjugate roots of the characteristic equation and rewrite the complex exponential solution in terms of real sines and cosines.

(d) Now consider the driven circuit with a sinusoidal source
\[
E(t) = E_0 \cos(\omega t), \qquad E_0>0,\ \omega>0,
\]
and assume $R>0$ so that all transients decay for large $t$. Return to the full inhomogeneous equation
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = E_0 \cos(\omega t).
\]
(i) Use the method of undetermined coefficients to seek a particular solution of the form
\[
q_p(t) = a \cos(\omega t) + b \sin(\omega t).
\]
Derive the linear system of equations that $a$ and $b$ must satisfy, and solve it to obtain $a$ and $b$ in terms of $R$, $L$, $C$, $E_0$, and $\omega$.

(ii) Show that the steady-state (long-time) oscillation of the charge has the form
\[
q_{\text{ss}}(t) = A(\omega)\cos\bigl(\omega t - \phi(\omega)\bigr),
\]
for some amplitude $A(\omega)\ge 0$ and phase shift $\phi(\omega)$ depending on $\omega$. Write an explicit expression for $A(\omega)$, and briefly describe how $A(\omega)$ behaves as $\omega\to 0$ and as $\omega\to\infty$.

% Hint: From $a$ and $b$, use the identity $a\cos(\omega t)+b\sin(\omega t) = A\cos(\omega t-\phi)$ with $A = \sqrt{a^2+b^2}$.

(e) \emph{Explorations and extensions.}
\begin{enumerate}
    \item Suppose $R=0$ (no resistor). What do your formulas predict for the homogeneous solutions when $E(t)\equiv 0$? How does this relate to the idea of an undamped oscillator and electrical resonance?
    \item In many applications one is interested not in the charge $q(t)$, but in the voltage across a single component, for instance across the resistor. Explain qualitatively, using your expression for $A(\omega)$ from part (d), why a series RLC circuit can be used as a frequency-selective filter in signal processing.
\end{enumerate}

% Hint: Think about which frequencies lead to large steady-state amplitudes, and which frequencies are strongly suppressed.
\end{problem}

% ===== Example 3: RLC Electrical Circuit as a Second-Order Linear ODE (full solution) =====
\begin{problem}[RLC Electrical Circuit as a Second-Order Linear ODE]
Consider a series RLC circuit with resistance $R>0$, inductance $L>0$, and capacitance $C>0$, driven by a voltage source $E(t)$. Let $q(t)$ denote the charge on the capacitor and $i(t)=q'(t)$ the current.

\begin{enumerate}
    \item Derive the differential equation satisfied by $q(t)$ using Kirchhoff's voltage law and the constitutive laws
    \[
    V_R = R i(t),\quad V_L = L i'(t),\quad V_C = \frac{1}{C}q(t).
    \]
    \item For the unforced circuit $E(t)\equiv 0$, write the homogeneous equation in standard form, find the characteristic equation, and classify the behavior of solutions as overdamped, critically damped, or underdamped according to the sign of the discriminant.
    \item In the underdamped case, show that the general solution can be written as
    \[
    q(t) = e^{-\alpha t}\bigl(A\cos(\omega t)+B\sin(\omega t)\bigr),
    \]
    and determine $\alpha$ and $\omega$ in terms of $R$, $L$, and $C$.
    \item For a sinusoidal forcing $E(t)=E_0\cos(\omega t)$ with $E_0>0$ and $R>0$, use the method of undetermined coefficients to find a particular solution of the form $q_p(t)=a\cos(\omega t)+b\sin(\omega t)$, and hence express the steady-state solution as
    \[
    q_{\mathrm{ss}}(t) = A(\omega)\cos\bigl(\omega t - \phi(\omega)\bigr).
    \]
    Obtain an explicit formula for the amplitude $A(\omega)$ and comment briefly on its dependence on the driving frequency $\omega$.
\end{enumerate}
\end{problem}

\begin{solution}
We begin by translating the physical laws of the circuit into an ordinary differential equation for the charge on the capacitor.

\medskip
\noindent\textbf{(1) Derivation of the ODE.}
In a series circuit, the same current $i(t)$ flows through all components. The voltage drops across the resistor, inductor, and capacitor are given by
\[
V_R = R i(t),\qquad V_L = L\,\frac{di}{dt}(t),\qquad V_C = \frac{1}{C}q(t),
\]
and the current and charge are related by $i(t) = \dfrac{dq}{dt}(t)$.

Kirchhoff's voltage law states that the sum of the voltage drops equals the applied source voltage $E(t)$:
\[
V_R + V_L + V_C = E(t).
\]
Substituting the expressions above, we obtain
\[
R i(t) + L\,\frac{di}{dt}(t) + \frac{1}{C}q(t) = E(t).
\]
Now use $i(t)=q'(t)$ and $i'(t)=q''(t)$ to eliminate the current in favor of $q$:
\[
R q'(t) + L q''(t) + \frac{1}{C} q(t) = E(t).
\]
Reordering the terms yields the second-order linear ODE for the charge:
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = E(t).
\]
This is a linear equation with constant coefficients and a (possibly) time-dependent forcing $E(t)$.

\medskip
\noindent\textbf{(2) Homogeneous equation and classification.}
For the unforced circuit with $E(t)\equiv 0$, the charge satisfies
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = 0.
\]
Dividing through by $L$ puts the equation into standard form:
\[
q''(t) + \frac{R}{L} q'(t) + \frac{1}{LC} q(t) = 0.
\]
We seek exponential solutions $q(t) = e^{\lambda t}$, which leads to the characteristic equation
\[
\lambda^2 + \frac{R}{L} \lambda + \frac{1}{LC} = 0.
\]
Equivalently, multiplying by $L$:
\[
L\lambda^2 + R\lambda + \frac{1}{C} = 0.
\]
The roots are given by the quadratic formula:
\[
\lambda_{1,2} = \frac{-R \pm \sqrt{R^2 - 4\frac{L}{C}}}{2L}.
\]
It is convenient to define the discriminant
\[
\Delta = R^2 - 4\frac{L}{C}.
\]
Then we have three cases.

\begin{itemize}
    \item \emph{Overdamped} ($\Delta>0$): The roots $\lambda_1$ and $\lambda_2$ are distinct real numbers, both negative (since $R>0$, $L>0$, and $1/C>0$). The general solution is
    \[
    q(t) = c_1 e^{\lambda_1 t} + c_2 e^{\lambda_2 t},
    \]
    which decays to zero as $t\to\infty$ without oscillating.

    \item \emph{Critically damped} ($\Delta=0$): The roots coincide, $\lambda_1=\lambda_2=\lambda=-R/(2L)$. The general solution is
    \[
    q(t) = (c_1 + c_2 t) e^{\lambda t},
    \]
    which again decays to zero but is the borderline case between oscillatory and nonoscillatory behavior.

    \item \emph{Underdamped} ($\Delta<0$): The roots are complex conjugates,
    \[
    \lambda_{1,2} = -\frac{R}{2L} \pm i\sqrt{\frac{1}{LC} - \frac{R^2}{4L^2}},
    \]
    with negative real part and nonzero imaginary part. The solution is a decaying oscillation.
\end{itemize}
Thus, the sign of $\Delta$ divides the parameter space into three qualitatively different regimes of transient behavior.

\medskip
\noindent\textbf{(3) Solution in the underdamped case.}
Assume $\Delta<0$, that is,
\[
R^2 < 4\frac{L}{C}.
\]
The characteristic roots are
\[
\lambda_{1,2} = -\frac{R}{2L} \pm i \sqrt{\frac{1}{LC} - \frac{R^2}{4L^2}}.
\]
Set
\[
\alpha = \frac{R}{2L} > 0, \qquad \omega = \sqrt{\frac{1}{LC} - \frac{R^2}{4L^2}} > 0.
\]
Then the roots are $-\alpha \pm i\omega$. The general complex solution is
\[
q(t) = \tilde{c}_1 e^{(-\alpha + i\omega)t} + \tilde{c}_2 e^{(-\alpha - i\omega)t}.
\]
Using Euler's formula and combining complex conjugate terms, this can be written as a real linear combination of $e^{-\alpha t}\cos(\omega t)$ and $e^{-\alpha t}\sin(\omega t)$:
\[
q(t) = e^{-\alpha t}\bigl(A \cos(\omega t) + B \sin(\omega t)\bigr),
\]
where $A$ and $B$ are real constants determined by the initial conditions.

To relate $A$ and $B$ to $q(0)$ and $q'(0)$, first evaluate at $t=0$:
\[
q(0) = A\cos 0 + B\sin 0 = A.
\]
Thus $A = q(0) = q_0$.

Next, differentiate:
\[
q'(t) = e^{-\alpha t}\bigl(-\alpha A\cos(\omega t) - \alpha B\sin(\omega t)\bigr)
      + e^{-\alpha t}\bigl(-A\omega\sin(\omega t) + B\omega\cos(\omega t)\bigr).
\]
Evaluating at $t=0$, where $\cos 0 = 1$ and $\sin 0 = 0$, gives
\[
q'(0) = -\alpha A + B\omega.
\]
Since $q'(0) = i(0) = i_0$, we obtain
\[
i_0 = -\alpha q_0 + \omega B,
\]
so
\[
B = \frac{i_0 + \alpha q_0}{\omega}.
\]
Therefore, in the underdamped case the unique solution satisfying $q(0)=q_0$ and $q'(0)=i_0$ is
\[
q(t) = e^{-\alpha t}\left(q_0\cos(\omega t)
+ \frac{i_0 + \alpha q_0}{\omega}\sin(\omega t)\right),
\]
with $\alpha$ and $\omega$ as above. This is a damped oscillation of angular frequency $\omega$ and exponential decay rate $\alpha$.

\medskip
\noindent\textbf{(4) Sinusoidal forcing and steady-state response.}
Now consider the forced equation
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = E_0 \cos(\omega t),
\]
with $E_0>0$ and $R>0$. The general solution is the sum of a homogeneous (transient) part, which we have already analyzed, and a particular (forced) part. Because $R>0$, the homogeneous solution decays to zero as $t\to\infty$, so the long-time behavior is governed by any bounded particular solution.

By the method of undetermined coefficients, we look for a particular solution of the form
\[
q_p(t) = a\cos(\omega t) + b\sin(\omega t),
\]
where $a$ and $b$ are constants to be determined. We compute the derivatives:
\[
q_p'(t) = -a\omega\sin(\omega t) + b\omega\cos(\omega t),
\]
\[
q_p''(t) = -a\omega^2\cos(\omega t) - b\omega^2\sin(\omega t).
\]
Substituting into the differential equation gives
\[
L\bigl(-a\omega^2\cos\omega t - b\omega^2\sin\omega t\bigr)
+ R\bigl(-a\omega\sin\omega t + b\omega\cos\omega t\bigr)
+ \frac{1}{C}\bigl(a\cos\omega t + b\sin\omega t\bigr)
= E_0\cos\omega t.
\]
Group the cosine and sine terms:
\[
\bigl(-L a \omega^2 + R b \omega + \tfrac{1}{C}a\bigr)\cos(\omega t)
+ \bigl(-L b \omega^2 - R a \omega + \tfrac{1}{C}b\bigr)\sin(\omega t)
= E_0\cos(\omega t) + 0\cdot\sin(\omega t).
\]
For this identity to hold for all $t$, the coefficients of $\cos(\omega t)$ and $\sin(\omega t)$ on both sides must agree, giving the linear system
\[
\begin{cases}
- L a \omega^2 + R b \omega + \dfrac{1}{C}a = E_0, \\[0.5em]
- L b \omega^2 - R a \omega + \dfrac{1}{C}b = 0.
\end{cases}
\]
It is convenient to write
\[
K(\omega) = \frac{1}{C} - L\omega^2.
\]
Then the system becomes
\[
\begin{cases}
K(\omega) a + R\omega b = E_0, \\[0.3em]
- R\omega a + K(\omega) b = 0.
\end{cases}
\]
Solving, for example by Cramer's rule, we find
\[
a = \frac{E_0 K(\omega)}{K(\omega)^2 + (R\omega)^2},\qquad
b = \frac{E_0 R\omega}{K(\omega)^2 + (R\omega)^2}.
\]
The particular solution can be rewritten in amplitude–phase form. Note that
\[
q_p(t) = a\cos(\omega t) + b\sin(\omega t)
= A(\omega)\cos\bigl(\omega t - \phi(\omega)\bigr),
\]
where the amplitude $A(\omega)$ and phase shift $\phi(\omega)$ satisfy
\[
A(\omega) = \sqrt{a^2 + b^2}, \qquad
\cos\phi = \frac{a}{A(\omega)},\quad \sin\phi = \frac{b}{A(\omega)}.
\]
We compute the amplitude:
\[
A(\omega)^2 = a^2 + b^2
= \frac{E_0^2\bigl(K(\omega)^2 + (R\omega)^2\bigr)}{\bigl(K(\omega)^2 + (R\omega)^2\bigr)^2}
= \frac{E_0^2}{K(\omega)^2 + (R\omega)^2}.
\]
Hence
\[
A(\omega) = \frac{E_0}{\sqrt{\bigl(\frac{1}{C} - L\omega^2\bigr)^2 + (R\omega)^2}}.
\]
The full solution is the sum of the homogeneous and particular parts. Because the homogeneous part decays to zero when $R>0$, the long-time (steady-state) behavior is given by
\[
q_{\mathrm{ss}}(t) = A(\omega)\cos\bigl(\omega t - \phi(\omega)\bigr),
\]
with amplitude $A(\omega)$ as above and some phase shift $\phi(\omega)$ determined by $a$ and $b$.

We can now examine the dependence of $A(\omega)$ on $\omega$. For very low frequencies, $\omega\to 0$, we have $K(\omega)\to 1/C$ and $R\omega\to 0$, so
\[
A(\omega) \to \frac{E_0}{|1/C|} = E_0 C.
\]
Thus, the steady-state charge has a finite nonzero amplitude as $\omega\to 0$. For very high frequencies, $\omega\to\infty$, the term $-L\omega^2$ dominates in $K(\omega)$, so $K(\omega)^2\sim L^2\omega^4$ and
\[
A(\omega) \sim \frac{E_0}{|L|\omega^2} \to 0.
\]
Therefore, the amplitude of the charge oscillations decays to zero for large driving frequency. This frequency-dependent response is one manifestation of the circuit's filtering behavior.

\medskip
\noindent\textbf{Conceptual remarks.}
This example neatly illustrates the main ideas of direct methods for solving linear ODEs with constant coefficients. The homogeneous equation is solved via the characteristic polynomial, whose roots determine whether the motion is overdamped, critically damped, or underdamped. The physical damping corresponds mathematically to roots with negative real part. For the forced problem, the method of undetermined coefficients provides a particular solution tailored to the sinusoidal input. The solution decomposes into a transient part (the homogeneous solution) and a steady-state part (the particular solution). The frequency dependence of the steady-state amplitude $A(\omega)$ explains the use of RLC circuits as frequency-selective filters in signal processing.
\end{solution}

% ===== Example 4: Coupled Linear Populations and 2×2 Systems (inquiry-based) =====
\begin{problem}[Coupled Linear Populations and 2×2 Systems]
Many models for interacting populations, such as predator--prey or two competing species, lead to nonlinear systems of differential equations. However, near an equilibrium point, these nonlinear systems are often well approximated by a linear system. Analyzing this linear system already reveals a great deal about stability, oscillations, and how perturbations decay or grow. In this problem you will explore a specific $2\times 2$ linear system and see how eigenvalues and eigenvectors determine the behavior of the two populations.

Suppose that $(X^\ast,Y^\ast)$ is an equilibrium of a two-species model, and let
\[
u(t) = X(t) - X^\ast, \qquad v(t) = Y(t) - Y^\ast
\]
denote the deviations of the populations from equilibrium. A linearization of the nonlinear model near $(X^\ast,Y^\ast)$ leads to the system
\[
\begin{cases}
u'(t) = -u(t) - 2v(t),\\[0.3em]
v'(t) = 2u(t) - v(t).
\end{cases}
\tag{$\ast$}
\]

\smallskip

(a) Rewrite the system $(\ast)$ in matrix form
\[
\mathbf{z}'(t) = A\,\mathbf{z}(t),
\]
where $\mathbf{z}(t) = \begin{pmatrix}u(t)\\[0.2em]v(t)\end{pmatrix}$ and $A$ is a $2\times 2$ matrix. What is the equilibrium point of this \emph{linearized} system in terms of $u$ and $v$? How does it relate to the original equilibrium $(X^\ast,Y^\ast)$?

\medskip

(b) Compute the eigenvalues and (complex) eigenvectors of the matrix $A$.

\quad(i) Write down the characteristic polynomial of $A$ and solve for its roots.

\quad(ii) For one of the eigenvalues, find a corresponding eigenvector (you may work over $\mathbb{C}^2$ if necessary).

Based on the real parts of the eigenvalues, decide whether the equilibrium is linearly stable or unstable. Based on the imaginary parts, discuss whether you expect oscillations in $(u(t),v(t))$.

\emph{Hint:} Recall that for a $2\times 2$ matrix $A$, the characteristic polynomial has the form
\[
\lambda^2 - (\operatorname{tr}A)\,\lambda + \det(A) = 0.
\]

\medskip

(c) Use your eigenvalue calculation to write down the general (complex-valued) solution of the system in the form
\[
\mathbf{z}(t) = c_1 e^{\lambda_1 t}\mathbf{v}_1 + c_2 e^{\lambda_2 t}\mathbf{v}_2,
\]
where $\lambda_1,\lambda_2$ are the eigenvalues and $\mathbf{v}_1,\mathbf{v}_2$ are corresponding eigenvectors.

Then convert this complex expression into a real-valued general solution of the form
\[
\begin{pmatrix}u(t)\\[0.2em]v(t)\end{pmatrix}
= e^{at}\Big[ C_1\,\mathbf{p}(t) + C_2\,\mathbf{q}(t)\Big],
\]
where $a$ is a real constant, and $\mathbf{p}(t)$ and $\mathbf{q}(t)$ are real vector-valued functions involving sines and cosines.

\emph{Hint:} If $\lambda = a+bi$ and $\mathbf{v} = \mathbf{p} + i\mathbf{q}$ with $\mathbf{p},\mathbf{q}$ real vectors, then
\[
e^{\lambda t}\mathbf{v}
= e^{at}\Big[(\mathbf{p}\cos bt - \mathbf{q}\sin bt)
+ i(\mathbf{p}\sin bt + \mathbf{q}\cos bt)\Big].
\]
The real and imaginary parts give two linearly independent real solutions.

\medskip

(d) Now impose a specific initial condition. Suppose a perturbation initially displaces only the first population:
\[
u(0) = 1, \qquad v(0) = 0.
\]
Use your real general solution from part (c) to determine $u(t)$ and $v(t)$ explicitly for this initial condition.

Then:

\quad(i) Describe qualitatively what happens to $(u(t),v(t))$ as $t\to\infty$. Does it approach the equilibrium? Does it oscillate while doing so?

\quad(ii) Determine the angular frequency and period of these oscillations. How does the exponential factor affect the amplitude over time?

\emph{Hint:} You may find it useful to look at $u(t)^2 + v(t)^2$ to see how the ``radius'' of the trajectory in the $(u,v)$-plane changes with $t$.

\medskip

(e) Exploring variations of the model.

\quad(i) Consider the one-parameter family of matrices
\[
A_k = \begin{pmatrix} -1 & -k \\[0.2em] 2 & -1 \end{pmatrix},
\qquad k>0.
\]
Repeat (without detailed computation) the eigenvalue analysis in part (b) using the trace and determinant of $A_k$ to decide whether the eigenvalues are real or complex, and whether their real parts are positive or negative. For which values of $k$ does the system exhibit decaying oscillations?

\emph{Hint:} For $2\times 2$ matrices, complex conjugate eigenvalues occur when $(\operatorname{tr}A)^2 - 4\det(A) < 0$, and the real part of the eigenvalues is $\operatorname{tr}(A)/2$.

\smallskip

\quad(ii) Suppose instead that the linearization produced eigenvalues $\lambda = +1 \pm 2i$. Describe how the phase portrait and time evolution of $(u(t),v(t))$ would differ from the case you analyzed in parts (b)--(d). What would this suggest about the stability of the original equilibrium $(X^\ast,Y^\ast)$ in the nonlinear model?

\end{problem}

% ===== Example 4: Coupled Linear Populations and 2×2 Systems (full solution) =====
\begin{problem}[Coupled Linear Populations and 2×2 Systems]
Consider deviations $u(t)$ and $v(t)$ from an equilibrium of a two-species population model, governed by the linear system
\[
\begin{cases}
u'(t) = -u(t) - 2v(t),\\[0.3em]
v'(t) = 2u(t) - v(t).
\end{cases}
\]
\begin{enumerate}
\item Rewrite this system in matrix form $\mathbf{z}' = A\mathbf{z}$, where $\mathbf{z} = (u,v)^{\mathsf T}$. Identify the equilibrium point of the linear system in $(u,v)$-coordinates.
\item Find the eigenvalues and (complex) eigenvectors of $A$, and classify the equilibrium as a node, saddle, spiral, etc. Determine whether it is linearly stable or unstable, and whether solutions oscillate.
\item Using the eigenvalues and eigenvectors, derive the real general solution $(u(t),v(t))$ and then find the unique solution satisfying the initial condition $u(0)=1$, $v(0)=0$.
\item Describe the long-time behavior of this solution, including the decay rate, the presence or absence of oscillations, and the angular frequency (and period) of any oscillations.
\item For the one-parameter family
\[
A_k = \begin{pmatrix} -1 & -k \\[0.2em] 2 & -1 \end{pmatrix}, \quad k>0,
\]
express the trace and determinant of $A_k$ in terms of $k$, and determine for which $k>0$ the linear system has (i) decaying oscillations, (ii) non-oscillatory decay.
\end{enumerate}
\end{problem}

\begin{solution}
We analyze this linear $2\times 2$ system using the standard direct method of eigenvalues and eigenvectors, which leads to an explicit solution via the matrix exponential and allows a clear interpretation in the phase plane.

\medskip

\noindent\textbf{(1) Matrix form and equilibrium.}
We write
\[
\mathbf{z}(t) = \begin{pmatrix}u(t)\\[0.2em]v(t)\end{pmatrix},
\]
so the system becomes
\[
\mathbf{z}'(t)
=
\begin{pmatrix}
-1 & -2\\[0.2em]
2 & -1
\end{pmatrix}
\begin{pmatrix}u(t)\\[0.2em]v(t)\end{pmatrix}.
\]
Thus
\[
A = \begin{pmatrix} -1 & -2 \\[0.2em] 2 & -1 \end{pmatrix},
\qquad
\mathbf{z}' = A\mathbf{z}.
\]

The equilibrium of the linear system in $(u,v)$-space is obtained by setting $\mathbf{z}'=0$, that is,
\[
A\mathbf{z} = 0.
\]
Since $A$ is invertible (as will be clear from the eigenvalue computation), the only solution is $\mathbf{z} = \mathbf{0}$, so
\[
(u,v) = (0,0)
\]
is the unique equilibrium of the linear system. In terms of the original populations, $(u,v)=(0,0)$ corresponds exactly to $(X,Y)=(X^\ast,Y^\ast)$, the equilibrium of the nonlinear model.

\medskip

\noindent\textbf{(2) Eigenvalues, eigenvectors, and classification.}
We next compute the eigenvalues of $A$. The characteristic polynomial is
\[
\det(A - \lambda I)
=
\begin{vmatrix}
-1-\lambda & -2\\[0.2em]
2 & -1-\lambda
\end{vmatrix}
= (-1-\lambda)^2 + 4.
\]
Setting this equal to zero gives
\[
(-1-\lambda)^2 + 4 = 0
\quad\Longleftrightarrow\quad
(\lambda+1)^2 = -4
\quad\Longleftrightarrow\quad
\lambda + 1 = \pm 2i.
\]
Hence the eigenvalues are
\[
\lambda_{1,2} = -1 \pm 2i.
\]

To find an eigenvector for, say, $\lambda_1 = -1 + 2i$, we solve $(A - \lambda_1 I)\mathbf{v}_1 = 0$. We have
\[
A - \lambda_1 I =
\begin{pmatrix}
-1 - (-1+2i) & -2\\[0.2em]
2 & -1 - (-1+2i)
\end{pmatrix}
=
\begin{pmatrix}
-2i & -2\\[0.2em]
2 & -2i
\end{pmatrix}.
\]
Let $\mathbf{v}_1 = \begin{pmatrix}x\\[0.2em]y\end{pmatrix}$. One row gives
\[
-2i\,x - 2y = 0
\quad\Longleftrightarrow\quad
i\,x + y = 0
\quad\Longleftrightarrow\quad
y = -i x.
\]
We may choose $x=1$, so one eigenvector is
\[
\mathbf{v}_1 = \begin{pmatrix}1\\[0.2em]-i\end{pmatrix}
= \mathbf{p} + i\mathbf{q}
\quad\text{with}\quad
\mathbf{p} = \begin{pmatrix}1\\[0.2em]0\end{pmatrix},
\quad
\mathbf{q} = \begin{pmatrix}0\\[0.2em]-1\end{pmatrix}.
\]
An eigenvector for $\lambda_2 = -1 - 2i$ is the complex conjugate $\mathbf{v}_2 = \overline{\mathbf{v}}_1 = \begin{pmatrix}1\\[0.2em]i\end{pmatrix}$.

The eigenvalues have negative real part $-1$ and nonzero imaginary part $\pm 2i$. Therefore the equilibrium at the origin in $(u,v)$-space is a \emph{stable spiral} (also called a spiral sink). Solutions spiral into the origin as $t\to\infty$, with oscillations whose angular frequency is $2$.

\medskip

\noindent\textbf{(3) Real general solution and specific initial condition.}

\emph{Complex form.}
For a $2\times 2$ system with distinct complex conjugate eigenvalues, a complex fundamental solution has the form
\[
\mathbf{z}(t)
= c_1 e^{\lambda_1 t}\mathbf{v}_1
+ c_2 e^{\lambda_2 t}\mathbf{v}_2,
\]
where $c_1,c_2\in\mathbb{C}$. However, for a real system we prefer a real basis of solutions.

\emph{Conversion to real solutions.}
We use the standard construction. Write
\[
\lambda_1 = a+bi = -1 + 2i,
\qquad
a=-1,\quad b=2,
\]
and
\[
\mathbf{v}_1 = \mathbf{p} + i\mathbf{q},
\qquad
\mathbf{p} = \begin{pmatrix}1\\[0.2em]0\end{pmatrix},
\quad
\mathbf{q} = \begin{pmatrix}0\\[0.2em]-1\end{pmatrix}.
\]
Then
\[
e^{\lambda_1 t}\mathbf{v}_1
= e^{(a+bi)t}(\mathbf{p} + i\mathbf{q})
= e^{at}\big[(\mathbf{p}\cos bt - \mathbf{q}\sin bt)
+ i(\mathbf{p}\sin bt + \mathbf{q}\cos bt)\big].
\]
The real and imaginary parts are real solutions. Thus,
\[
\mathbf{z}_1(t)
= e^{at}\big(\mathbf{p}\cos bt - \mathbf{q}\sin bt\big),
\qquad
\mathbf{z}_2(t)
= e^{at}\big(\mathbf{p}\sin bt + \mathbf{q}\cos bt\big)
\]
form a real fundamental set of solutions.

We compute $\mathbf{z}_1(t)$ and $\mathbf{z}_2(t)$ explicitly. Since $a=-1$ and $b=2$,
\[
\mathbf{p}\cos(2t) - \mathbf{q}\sin(2t)
=
\begin{pmatrix}1\\[0.2em]0\end{pmatrix}\cos(2t)
-
\begin{pmatrix}0\\[0.2em]-1\end{pmatrix}\sin(2t)
=
\begin{pmatrix}\cos(2t)\\[0.2em]\sin(2t)\end{pmatrix},
\]
and
\[
\mathbf{p}\sin(2t) + \mathbf{q}\cos(2t)
=
\begin{pmatrix}1\\[0.2em]0\end{pmatrix}\sin(2t)
+
\begin{pmatrix}0\\[0.2em]-1\end{pmatrix}\cos(2t)
=
\begin{pmatrix}\sin(2t)\\[0.2em]-\cos(2t)\end{pmatrix}.
\]
Therefore,
\[
\mathbf{z}_1(t)
= e^{-t}\begin{pmatrix}\cos(2t)\\[0.2em]\sin(2t)\end{pmatrix},
\qquad
\mathbf{z}_2(t)
= e^{-t}\begin{pmatrix}\sin(2t)\\[0.2em]-\cos(2t)\end{pmatrix}
\]
are two real independent solutions. The real general solution is a linear combination:
\[
\mathbf{z}(t)
= C_1 \mathbf{z}_1(t) + C_2 \mathbf{z}_2(t)
= e^{-t}\left[
C_1 \begin{pmatrix}\cos(2t)\\[0.2em]\sin(2t)\end{pmatrix}
+
C_2 \begin{pmatrix}\sin(2t)\\[0.2em]-\cos(2t)\end{pmatrix}
\right],
\]
with real constants $C_1,C_2$.

Equivalently, in component form,
\begin{align*}
u(t) &= e^{-t}\big(C_1\cos(2t) + C_2\sin(2t)\big),\\
v(t) &= e^{-t}\big(C_1\sin(2t) - C_2\cos(2t)\big).
\end{align*}

\emph{Imposing the initial condition.}
We now enforce $u(0)=1$, $v(0)=0$. At $t=0$ we have $\cos(0)=1$ and $\sin(0)=0$, so
\[
u(0) = e^{0}(C_1\cdot 1 + C_2\cdot 0) = C_1 = 1,
\]
and
\[
v(0) = e^{0}(C_1\cdot 0 - C_2\cdot 1) = -C_2 = 0
\quad\Longrightarrow\quad C_2 = 0.
\]
Thus $C_1=1$, $C_2=0$, and the specific solution is
\[
\mathbf{z}(t) = \begin{pmatrix}u(t)\\[0.2em]v(t)\end{pmatrix}
= e^{-t}
\begin{pmatrix}\cos(2t)\\[0.2em]\sin(2t)\end{pmatrix}.
\]
In components,
\[
u(t) = e^{-t}\cos(2t),
\qquad
v(t) = e^{-t}\sin(2t).
\]

\medskip

\noindent\textbf{(4) Long-time behavior, decay rate, and oscillations.}
We now describe the qualitative behavior as $t\to\infty$.

First, both $u(t)$ and $v(t)$ are multiplied by the common factor $e^{-t}$, which tends to zero as $t\to\infty$. Therefore,
\[
\lim_{t\to\infty}u(t) = 0,
\qquad
\lim_{t\to\infty}v(t) = 0,
\]
so the solution tends to the equilibrium $(u,v)=(0,0)$ in the linear system, and hence returns to $(X^\ast,Y^\ast)$ in the original variables. The convergence is exponentially fast with rate $1$, because of the factor $e^{-t}$.

Second, the trigonometric factors $\cos(2t)$ and $\sin(2t)$ produce oscillations. The angular frequency is $2$ (radians per unit time), so the period $T$ is
\[
T = \frac{2\pi}{\text{angular frequency}} = \frac{2\pi}{2} = \pi.
\]
Thus the populations cycle around the equilibrium with period $\pi$, while each oscillation is damped by the factor $e^{-t}$.

To see the spiral structure in the phase plane, we can look at
\[
u(t)^2 + v(t)^2
= e^{-2t}(\cos^2(2t) + \sin^2(2t))
= e^{-2t}.
\]
This shows that the distance from the origin in the $(u,v)$-plane decays like $e^{-t}$; the trajectory moves along a spiral of shrinking radius, making one full rotation in angle per time interval $\pi$.

Hence the equilibrium is a spiral sink: trajectories spiral inward with exponentially decaying radius and oscillatory behavior determined by the imaginary part of the eigenvalues.

\medskip

\noindent\textbf{(5) The family $A_k$ and the trace--determinant viewpoint.}
Finally, consider
\[
A_k = \begin{pmatrix} -1 & -k \\[0.2em] 2 & -1 \end{pmatrix},\qquad k>0.
\]
The characteristic polynomial of a $2\times 2$ matrix is
\[
\lambda^2 - (\operatorname{tr}A_k)\lambda + \det(A_k)=0.
\]
We compute
\[
\operatorname{tr}(A_k) = -1 + (-1) = -2,
\]
and
\[
\det(A_k) = (-1)(-1) - (-k)(2) = 1 + 2k.
\]
Thus the eigenvalues satisfy
\[
\lambda^2 + 2\lambda + (1 + 2k) = 0.
\]
The discriminant of this quadratic is
\[
\Delta_k = (\operatorname{tr}A_k)^2 - 4\det(A_k)
= (-2)^2 - 4(1+2k)
= 4 - 4 - 8k
= -8k.
\]

For all $k>0$, we have $\Delta_k < 0$, so the eigenvalues are complex conjugates. Their real part is
\[
\Re(\lambda) = \frac{\operatorname{tr}(A_k)}{2} = \frac{-2}{2} = -1 < 0
\]
for all $k>0$. Therefore, for every positive $k$, the equilibrium is a \emph{stable spiral}, and the solutions exhibit \emph{decaying oscillations}.

There is no value of $k>0$ for which the eigenvalues are real (since the discriminant is strictly negative for $k>0$), so in this parameter range there is no non-oscillatory pure node. The imaginary part of the eigenvalues depends on $k$; in fact
\[
\lambda_{1,2} = -1 \pm i\sqrt{8k}\!/2 = -1 \pm i\sqrt{2k},
\]
so the angular frequency of oscillation is $\sqrt{2k}$, and the period is $2\pi/\sqrt{2k}$, which decreases as $k$ increases. The decay rate, determined by the real part $-1$, is independent of $k$ in this family.

\medskip

\noindent\textbf{Conceptual summary.}
This example illustrates how direct methods for solving linear ODEs—specifically, writing the system in matrix form, computing eigenvalues and eigenvectors, and forming the matrix exponential—provide both explicit formulas for solutions and a clear qualitative picture of the dynamics. In the $2\times 2$ case, the eigenvalues immediately reveal stability (via the real part) and the presence of oscillations (via the imaginary part), and the phase-plane behavior (node, saddle, spiral) follows directly. Near an equilibrium of a nonlinear population model, this linear analysis already captures the essential features of small perturbations: whether they decay or grow, and whether they do so monotonically or in an oscillatory fashion.
\end{solution}

% ===== Example 5: Steady States from the Heat Equation as a Boundary-Value ODE (inquiry-based) =====
\begin{problem}[Steady States from the Heat Equation as a Boundary-Value ODE]
A long, thin, homogeneous rod of length $L$ is insulated along its sides, so heat can flow only along the $x$–direction. The temperature $u(x,t)$ along the rod evolves according to the one-dimensional heat equation. At steady state, one observes that the temperature no longer changes in time, and the time-dependent partial differential equation reduces to a spatial ordinary differential equation with boundary conditions at the ends of the rod. This example shows how a steady-state profile arises as the solution of a second-order linear boundary-value problem, and how the values at the boundary determine the entire interior profile.

Assume that the rod occupies the interval $0 < x < L$, that there is no internal heat source, and that the long sides are perfectly insulated. Let $u(x,t)$ be the temperature at position $x$ and time $t$, and let $k>0$ be the thermal diffusivity.

\medskip

(a) Write down the standard one-dimensional heat equation (with no internal heat sources) that $u(x,t)$ satisfies under these assumptions, together with Dirichlet boundary conditions
\[
u(0,t) = T_0, \qquad u(L,t) = T_L
\]
for fixed constants $T_0$ and $T_L$. Explain in words what it means for the rod to reach a \emph{steady state}, and translate this into a mathematical condition on $u(x,t)$.

\medskip

(b) Use your steady-state condition from part (a) to show that any steady-state temperature profile $u_{\mathrm{ss}}(x)$ must satisfy a second-order ordinary differential equation on $0<x<L$ together with boundary conditions at $x=0$ and $x=L$. Write this boundary-value problem explicitly.  

Hint: At steady state, $u_{\mathrm{ss}}$ no longer depends on $t$, so its time derivative should vanish.

\medskip

(c) Now solve the ordinary differential equation you obtained in part (b).  

(i) First solve the homogeneous equation
\[
u''(x) = 0
\]
on $(0,L)$, and find the general solution in terms of two constants $C_1$ and $C_2$.  

(ii) Then impose the boundary conditions at $x=0$ and $x=L$ to determine $C_1$ and $C_2$ in terms of $T_0$, $T_L$, and $L$.  

Write your final formula for $u_{\mathrm{ss}}(x)$ in a simple and transparent form (for instance, in a way that makes it clear how $u_{\mathrm{ss}}(x)$ depends on $T_0$ and $T_L$).  

% Hint: Integrate $u''(x)=0$ twice, or recognize that a function with zero second derivative must be linear.

\medskip

(d) In this part, you will explore uniqueness and interpretation.  

(i) Suppose $v(x)$ is \emph{any} function satisfying
\[
v''(x) = 0, \qquad v(0)=0,\qquad v(L)=0.
\]
Show that $v(x)\equiv 0$ for all $x \in [0,L]$.  

% Hint: Use your general solution of $v''(x)=0$ from part (c) and apply the two boundary conditions.

(ii) Explain how the result in (i) implies that there is at most one steady-state solution to the boundary-value problem you found in part (b). In other words, explain why the temperatures at the ends of the rod uniquely determine the entire steady-state temperature profile inside.

(iii) Give a physical interpretation of the shape of $u_{\mathrm{ss}}(x)$ that you found in part (c). For example, how does the graph of $u_{\mathrm{ss}}(x)$ look if $T_0 < T_L$ or if $T_0 > T_L$? Why does a linear profile make sense physically in the absence of internal heat sources?

\medskip

(e) Now consider two extensions of the model.

\medskip\noindent
\emph{(e1) Uniform internal heating.}  
Suppose that, in addition to conduction, the rod is heated uniformly along its length by a constant heat source of strength $Q>0$ (per unit length). Then the heat equation takes the form
\[
u_t(x,t) = k\,u_{xx}(x,t) + Q.
\]
(i) Derive the steady-state ordinary differential equation and boundary conditions that $u_{\mathrm{ss}}$ must satisfy.  

(ii) Solve this new boundary-value problem and find an explicit formula for $u_{\mathrm{ss}}(x)$.  

(iii) How does the shape of this new steady-state profile differ from the no-source case? Sketch or describe the qualitative shape when $T_0=T_L$ and $Q>0$.

% Hint: The steady-state ODE is now inhomogeneous. Integrate twice and then use the boundary conditions as before.

\medskip\noindent
\emph{(e2) An insulated end.}  
Instead of fixing the temperature at $x=0$, suppose the left end is perfectly insulated, while the right end is held at temperature $T_R$. An insulated end means that there is no heat flux through that end, which translates to the Neumann boundary condition
\[
u_x(0,t) = 0,
\]
while at the right end we keep $u(L,t) = T_R$.

(i) Write down the steady-state boundary-value problem for $u_{\mathrm{ss}}(x)$ in this new scenario.  

(ii) Solve this boundary-value problem explicitly.  

(iii) How does this new steady-state profile compare to the original case with two fixed end temperatures? What physical effect does the insulation at $x=0$ have on the temperature profile?

\end{problem}

% ===== Example 5: Steady States from the Heat Equation as a Boundary-Value ODE (full solution) =====
\begin{problem}[Steady States from the Heat Equation as a Boundary-Value ODE]
A thin homogeneous rod of length $L$ lies along $0<x<L$. Its sides are insulated, and there are no internal heat sources. The temperature $u(x,t)$ satisfies the one-dimensional heat equation
\[
u_t = k\,u_{xx}, \quad 0<x<L,\ t>0,
\]
with Dirichlet boundary conditions $u(0,t)=T_0$ and $u(L,t)=T_L$, where $T_0$ and $T_L$ are fixed constants.

\begin{enumerate}
\item[(a)] Define what it means for the rod to be in a steady state, and show that any steady-state temperature profile $u_{\mathrm{ss}}(x)$ must satisfy the boundary-value problem
\[
u_{\mathrm{ss}}''(x) = 0,\quad 0<x<L,\qquad
u_{\mathrm{ss}}(0)=T_0,\quad u_{\mathrm{ss}}(L)=T_L.
\]
\item[(b)] Solve this boundary-value problem explicitly and express $u_{\mathrm{ss}}(x)$ in terms of $T_0$, $T_L$, and $L$.
\item[(c)] Prove that this steady-state solution is unique: if $v''(x)=0$ for $0<x<L$ and $v(0)=v(L)=0$, then $v(x)\equiv 0$.
\item[(d)] Now suppose there is a uniform internal heat source of strength $Q>0$, so that the heat equation becomes
\[
u_t = k\,u_{xx} + Q.
\]
Derive the steady-state boundary-value problem and solve it explicitly under the same Dirichlet boundary conditions $u(0,t)=T_0$, $u(L,t)=T_L$.
\end{enumerate}
\end{problem}

\begin{solution}
We are asked to understand steady states of the one-dimensional heat equation and to see how these are described by a second-order linear ordinary differential equation with boundary conditions. This is a typical example of a \emph{boundary-value problem} for a linear ODE which can be solved directly by integration and by using the boundary data to determine the constants of integration.

\medskip

\textbf{(a) Steady-state reduction to an ODE.}  
The rod is in a \emph{steady state} when the temperature no longer changes in time. Mathematically, this means that for each fixed $x$, the function $t\mapsto u(x,t)$ is constant in $t$. If we denote the steady-state temperature by $u_{\mathrm{ss}}(x)$, this condition is expressed as
\[
\frac{\partial}{\partial t} u_{\mathrm{ss}}(x) = 0
\quad\text{for all }x\in(0,L).
\]
Substituting $u(x,t)=u_{\mathrm{ss}}(x)$, which depends only on $x$, into the heat equation
\[
u_t = k\,u_{xx}
\]
gives
\[
0 = k\,u_{\mathrm{ss}}''(x),
\]
since $u_t=0$ at steady state and $u_{xx}$ becomes the ordinary second derivative $u_{\mathrm{ss}}''(x)$ with respect to $x$. Because $k>0$, we can divide by $k$ and obtain the ordinary differential equation
\[
u_{\mathrm{ss}}''(x) = 0,\quad 0<x<L.
\]

The boundary conditions on $u$ carry over directly to $u_{\mathrm{ss}}$. Since $u(0,t)=T_0$ and $u(L,t)=T_L$ for all $t$, any time-independent solution must satisfy
\[
u_{\mathrm{ss}}(0)=T_0,\qquad u_{\mathrm{ss}}(L)=T_L.
\]
Thus the steady-state profile solves the boundary-value problem
\[
u_{\mathrm{ss}}''(x) = 0,\quad 0<x<L,\qquad
u_{\mathrm{ss}}(0)=T_0,\quad u_{\mathrm{ss}}(L)=T_L.
\]

\medskip

\textbf{(b) Solving the homogeneous boundary-value problem.}  
We now solve the ordinary differential equation
\[
u_{\mathrm{ss}}''(x) = 0.
\]
This is a linear ODE with constant coefficients. The characteristic equation is $r^2 = 0$, which has the double root $r=0$. Hence, the general solution of $u''=0$ is
\[
u_{\mathrm{ss}}(x) = C_1 x + C_2,
\]
where $C_1$ and $C_2$ are constants of integration.

We use the boundary conditions to determine $C_1$ and $C_2$. At $x=0$,
\[
u_{\mathrm{ss}}(0) = C_1\cdot 0 + C_2 = C_2 = T_0,
\]
so $C_2 = T_0$. At $x=L$,
\[
u_{\mathrm{ss}}(L) = C_1 L + C_2 = C_1 L + T_0 = T_L.
\]
Solving for $C_1$ gives
\[
C_1 = \frac{T_L - T_0}{L}.
\]
Therefore
\[
u_{\mathrm{ss}}(x) = \frac{T_L - T_0}{L}\,x + T_0.
\]
A more revealing way to write this is
\[
u_{\mathrm{ss}}(x)
= T_0 + \frac{T_L - T_0}{L}\,x,
\]
which shows that the steady-state temperature varies linearly from $T_0$ at $x=0$ to $T_L$ at $x=L$.

\medskip

\textbf{(c) Uniqueness of the steady state.}  
To establish uniqueness, we consider the associated homogeneous boundary-value problem
\[
v''(x) = 0,\quad 0<x<L,\qquad v(0)=0,\quad v(L)=0.
\]
We must show that $v(x)\equiv 0$ is the only solution.

We already solved the equation $v''(x)=0$ in part (b). Its general solution is
\[
v(x) = A x + B,
\]
for constants $A$ and $B$. Applying the boundary condition at $x = 0$ gives
\[
v(0) = B = 0,
\]
so $B = 0$. Applying the boundary condition at $x = L$ gives
\[
v(L) = A L + B = A L = 0.
\]
Since $L>0$, this implies $A = 0$. Hence $v(x) = 0$ for all $x$, as required.

Now suppose that $u_1(x)$ and $u_2(x)$ are two steady-state solutions of the original boundary-value problem
\[
u'' = 0,\quad 0<x<L,\qquad u(0)=T_0,\quad u(L)=T_L.
\]
Consider their difference $w(x) = u_1(x) - u_2(x)$. Then
\[
w''(x) = u_1''(x)-u_2''(x) = 0-0 = 0,
\]
and
\[
w(0) = u_1(0)-u_2(0) = T_0 - T_0 = 0,\qquad
w(L) = u_1(L)-u_2(L) = T_L - T_L = 0.
\]
So $w$ satisfies the homogeneous boundary-value problem of the previous paragraph and therefore must be identically zero. That is, $u_1(x) = u_2(x)$ for all $x$. Hence the steady-state solution $u_{\mathrm{ss}}(x)$ is unique.

Physically, this means that once the end temperatures $T_0$ and $T_L$ are fixed, there is only one possible steady temperature distribution inside the rod.

\medskip

\textbf{(d) Steady state with a uniform source.}  
We now consider the heat equation with a uniform internal heat source of strength $Q>0$:
\[
u_t = k\,u_{xx} + Q,\quad 0<x<L.
\]
As in part (a), a steady-state profile $u_{\mathrm{ss}}(x)$ is time-independent, so $u_t=0$, and we obtain
\[
0 = k\,u_{\mathrm{ss}}''(x) + Q.
\]
Dividing through by $k>0$ gives the inhomogeneous ordinary differential equation
\[
u_{\mathrm{ss}}''(x) = -\frac{Q}{k},\quad 0<x<L.
\]
The boundary conditions remain Dirichlet:
\[
u_{\mathrm{ss}}(0) = T_0,\qquad u_{\mathrm{ss}}(L) = T_L.
\]
Thus the steady-state problem is
\[
u_{\mathrm{ss}}''(x) = -\frac{Q}{k},\quad 0<x<L,\qquad
u_{\mathrm{ss}}(0)=T_0,\quad u_{\mathrm{ss}}(L)=T_L.
\]

We solve the ODE by direct integration. First integrate once:
\[
u_{\mathrm{ss}}'(x) = -\frac{Q}{k}\,x + C_1,
\]
for some constant $C_1$. Integrating again,
\[
u_{\mathrm{ss}}(x) = -\frac{Q}{2k}\,x^2 + C_1 x + C_2,
\]
for some constant $C_2$.

We determine $C_1$ and $C_2$ from the boundary conditions. At $x=0$,
\[
u_{\mathrm{ss}}(0) = -\frac{Q}{2k}\cdot 0^2 + C_1\cdot 0 + C_2 = C_2 = T_0,
\]
so $C_2 = T_0$. At $x=L$,
\[
u_{\mathrm{ss}}(L) = -\frac{Q}{2k}L^2 + C_1 L + T_0 = T_L.
\]
Solving for $C_1$,
\[
C_1 L = T_L - T_0 + \frac{Q}{2k}L^2,
\qquad\text{so}\qquad
C_1 = \frac{T_L - T_0}{L} + \frac{Q}{2k}L.
\]
Substituting back, we find
\[
u_{\mathrm{ss}}(x)
= -\frac{Q}{2k}x^2
+ \left(\frac{T_L - T_0}{L} + \frac{Q}{2k}L\right)x
+ T_0.
\]
It is often helpful to separate the effects of the boundary temperatures and the internal source. To do that, we can rewrite the expression as
\[
u_{\mathrm{ss}}(x)
= T_0 + \frac{T_L - T_0}{L}x + \frac{Q}{2k}\big(Lx - x^2\big).
\]
Indeed, expanding the right-hand side gives
\[
T_0 + \frac{T_L - T_0}{L}x + \frac{Q}{2k}Lx - \frac{Q}{2k}x^2,
\]
which matches the previous expression.

The first two terms,
\[
T_0 + \frac{T_L - T_0}{L}x,
\]
are precisely the no-source steady-state profile from part (b). The last term,
\[
\frac{Q}{2k}(Lx - x^2),
\]
is the additional contribution caused by the uniform heating. Notice that $u_{\mathrm{ss}}''(x) = -Q/k < 0$, so the profile is \emph{concave down}. When $T_0 = T_L$, the temperature is highest in the interior of the rod and lower at the ends, which matches the intuition that internal heating adds extra heat in the middle that must escape through the ends.

\medskip

\textbf{Connection to direct methods for linear ODEs.}  
This example illustrates several central ideas from the section on direct methods for solving linear ordinary differential equations:

\begin{itemize}
\item A physical partial differential equation (the heat equation) reduces at steady state to a linear second-order ODE in space, together with boundary conditions. This is a typical way that boundary-value problems for ODEs arise.
\item The resulting homogeneous and inhomogeneous equations ($u''=0$ and $u''=-Q/k$) are solved by direct integration and by applying boundary conditions to determine integration constants. This is the basic direct method for constant-coefficient second-order equations.
\item Uniqueness of the boundary-value problem is established by studying the associated homogeneous problem and showing that only the trivial solution satisfies the homogeneous boundary conditions. This use of the homogeneous equation is a recurring theme in linear theory.
\item The explicit formulas show how boundary data determine the interior state, and how additional forcing (the internal source) adds a predictable, explicitly computable correction to the homogeneous steady-state solution.
\end{itemize}

Thus, this boundary-value problem serves both as a concrete physical model and as a clear demonstration of direct solution techniques for linear second-order ODEs with boundary conditions.

\end{solution}

\section{Linear Dynamics via the Green Function}
% --- Narrative plan (auto-generated) ---
% This section introduces the Green function as a tool for understanding linear ordinary differential equations, especially those that model forced or driven systems. Instead of solving each new inhomogeneous equation from scratch, we learn to describe a system by its response to an idealized impulse and then build the response to general forcing by superposition and convolution. In doing so, we uncover a unifying viewpoint on linear dynamics: the Green function encodes how information propagates through the system over time or space.
%
% The Green function perspective matters across applied mathematics because it generalizes naturally from simple ODE models to partial differential equations, where it appears as a fundamental solution or kernel. It connects directly with the Laplace transform, Fourier methods, and complex analysis, all of which can be used to construct or analyze Green functions. Throughout this section we will move back and forth between concrete physical models (such as mechanical oscillators and circuits) and abstract linear operators, preparing for later chapters on boundary value problems, Fourier series, and Green functions for PDEs.

% ===== Example 1: Impulse Response of a Damped Harmonic Oscillator (inquiry-based) =====
\begin{problem}[Impulse Response of a Damped Harmonic Oscillator]
A mass–spring–dashpot system is a standard model for a damped harmonic oscillator. A mass $m$ is attached to a spring of stiffness $k$ and a dashpot (damper) with damping coefficient $c$. Its displacement $x(t)$ from equilibrium obeys a linear second-order ODE. In this problem, you will discover how a very short “impulse” force, idealized mathematically by a Dirac delta function, gives rise to the Green function of the system. Once this impulse response is known, any small external force can be built up as a superposition of impulses in time, leading to a convolution formula for the solution.

We assume throughout that the system is \emph{underdamped}, in the sense that $c^2 < 4mk$, so that the homogeneous solutions are decaying oscillations.

Consider the initial value problem
\[
m x''(t) + c x'(t) + k x(t) = f(t)
\]
with given initial conditions $x(0) = x_0$, $x'(0) = v_0$.

\medskip

(a) First, recall the unforced problem. Set $f(t) = 0$ and consider the homogeneous equation
\[
m x''(t) + c x'(t) + k x(t) = 0.
\]
Solve this homogeneous ODE under the underdamped assumption $c^2 < 4mk$.

\quad(i) Write down the characteristic equation and its roots. Show that the general solution has the form
\[
x_h(t) = e^{-\gamma t} \left( A \cos(\omega_d t) + B \sin(\omega_d t) \right),
\]
for suitable constants $\gamma$ and $\omega_d$ that you should identify in terms of $m$, $c$, and $k$.

\quad(ii) Interpret $\gamma$ and $\omega_d$ physically or qualitatively (for example, in terms of decay and oscillation).

% Hint: Introduce $\gamma = \dfrac{c}{2m}$ and $\omega_0 = \sqrt{\dfrac{k}{m}}$, and then compute $\omega_d$ from the characteristic roots.

\medskip

(b) Now we idealize a very short, sharp “kick” applied to the mass at some time $s > 0$ by writing the forcing as a Dirac delta distribution:
\[
m x''(t) + c x'(t) + k x(t) = \delta(t - s).
\]
We are interested in the response to this impulse when the system is initially at rest. Thus we impose
\[
x(t) = 0 \quad \text{for } t < s.
\]

\quad(i) Explain why it is natural to require $x(t)$ to be continuous at $t = s$. That is, argue that $x(s^-) = x(s^+)$.

\quad(ii) Integrate the differential equation from $t = s - \varepsilon$ to $t = s + \varepsilon$, and then let $\varepsilon \to 0^+$. Use this to derive a “jump condition” describing how $x'(t)$ behaves at $t = s$, that is, a formula for $x'(s^+) - x'(s^-)$.

% Hint: Only one term in the ODE produces a nonzero contribution as $\varepsilon \to 0$, because $\delta(t - s)$ has integral $1$ and the solution stays bounded.

\medskip

(c) We now define the Green function $G(t,s)$ for the damped oscillator as the solution of
\[
m \frac{\partial^2 G}{\partial t^2}(t,s) + c \frac{\partial G}{\partial t}(t,s) + k G(t,s) = \delta(t - s),
\]
which satisfies
\[
G(t,s) = 0 \quad \text{for } t < s,
\]
and is continuous at $t = s$.

\quad(i) Use the result from part (b) to determine the initial conditions for the function of $t$
\[
t \mapsto G(t,s) \quad \text{at } t = s^+,
\]
namely $G(s^+,s)$ and $\dfrac{\partial G}{\partial t}(s^+,s)$.

\quad(ii) For $t > s$, the right-hand side is zero, so $G(t,s)$ satisfies the homogeneous equation in $t$. Set $\tau = t - s$ and consider the function $g(\tau) = G(s + \tau,s)$ for $\tau > 0$. Use your homogeneous solution from part (a) and the initial conditions from part (c)(i) to solve for $g(\tau)$ explicitly.

\quad(iii) Show that your answer can be written in the compact form
\[
G(t,s) = \frac{1}{m \omega_d} e^{-\gamma (t - s)} \sin\bigl(\omega_d (t - s)\bigr) H(t - s),
\]
where $H$ is the Heaviside step function.

% Hint: You should find that $G(t,s)$ has the form $C e^{-\gamma (t-s)} \sin(\omega_d (t-s))$ for $t > s$, and you must choose the constant $C$ using your jump condition.

\medskip

(d) Now consider a general forcing function $f(t)$ (assume it is continuous and sufficiently nice) with homogeneous initial conditions
\[
x(0) = 0, \qquad x'(0) = 0.
\]
We claim that the solution can be expressed as a time-convolution of the Green function with the forcing:
\[
x(t) = \int_0^t G(t,s) f(s)\,ds.
\]
Explain, using the linearity of the ODE and the interpretation of $\delta(t-s)$ as an “idealized unit impulse at time $s$,” why this integral formula represents the solution. In your explanation, you may think in terms of approximating $f$ by a sum of short pulses and then passing to a limit.

% Hint: View $f$ as a superposition of many small impulses at different times and use superposition (linearity) of the system's response.

\medskip

(e) Extensions and “what if” questions.

\quad(i) How would your formula for $G(t,s)$ change if the system were \emph{critically damped}, that is, if $c^2 = 4mk$? Sketch (but do not fully compute) how you would solve the homogeneous problem and match the jump conditions to find the Green function in this case.

\quad(ii) Suppose the system does not start from rest, but instead has initial conditions $x(0) = x_0$, $x'(0) = v_0$. Sketch how the general solution can be written as a sum of a homogeneous solution satisfying the initial data and a convolution of $G(t,s)$ with $f(s)$. What extra terms appear, compared with the formula in part (d)?
\end{problem}

% ===== Example 1: Impulse Response of a Damped Harmonic Oscillator (full solution) =====
\begin{problem}[Impulse Response of a Damped Harmonic Oscillator]
Consider the damped harmonic oscillator
\[
m x''(t) + c x'(t) + k x(t) = f(t),
\]
with $m>0$, $c>0$, $k>0$, and assume the system is underdamped, that is, $c^2 < 4mk$.

(a) For the impulsively forced problem
\[
m x''(t) + c x'(t) + k x(t) = \delta(t - s),
\]
with $x(t) = 0$ for $t < s$, define the Green function $G(t,s)$ as the solution satisfying
\[
m \frac{\partial^2 G}{\partial t^2}(t,s) + c \frac{\partial G}{\partial t}(t,s) + k G(t,s) = \delta(t - s), 
\qquad G(t,s) = 0 \text{ for } t < s,
\]
and $G$ continuous at $t = s$. Derive an explicit formula for $G(t,s)$.

(b) Use the Green function to show that the solution to
\[
m x''(t) + c x'(t) + k x(t) = f(t), \qquad x(0) = 0,\quad x'(0) = 0,
\]
can be written as
\[
x(t) = \int_0^t G(t,s)\,f(s)\,ds.
\]
Briefly explain how this representation illustrates the Green function viewpoint on linear dynamics.
\end{problem}

\begin{solution}
We study the forced damped oscillator
\[
m x''(t) + c x'(t) + k x(t) = f(t)
\]
and its response to an idealized impulse. Throughout we assume the underdamped condition $c^2 < 4mk$, so that the homogeneous motion is a decaying oscillation.

\medskip

\textbf{Step 1: Homogeneous solution and notation.}

We begin by solving the homogeneous equation
\[
m x''(t) + c x'(t) + k x(t) = 0.
\]
The characteristic polynomial is
\[
m r^2 + c r + k = 0,
\]
which has roots
\[
r = \frac{-c \pm \sqrt{c^2 - 4mk}}{2m}.
\]
Under the assumption $c^2 < 4mk$ these roots are complex with negative real part. It is convenient to set
\[
\gamma = \frac{c}{2m}, 
\qquad 
\omega_0 = \sqrt{\frac{k}{m}},
\qquad
\omega_d = \sqrt{\omega_0^2 - \gamma^2} = \sqrt{\frac{k}{m} - \frac{c^2}{4m^2}},
\]
so that
\[
r = -\gamma \pm i \omega_d.
\]
Therefore the general homogeneous solution is
\[
x_h(t) = e^{-\gamma t} \bigl( A \cos(\omega_d t) + B \sin(\omega_d t) \bigr),
\]
for constants $A$ and $B$ determined by initial conditions. The factor $e^{-\gamma t}$ describes exponential decay due to damping, and the factor involving $\cos$ and $\sin$ describes oscillation at the damped frequency $\omega_d$.

\medskip

\textbf{Step 2: Impulse forcing and the jump condition.}

We now consider the impulsively forced problem
\[
m x''(t) + c x'(t) + k x(t) = \delta(t - s),
\]
with the requirement that the system is at rest before the impulse:
\[
x(t) = 0 \quad \text{for } t < s.
\]
Physically it is natural to assume that the displacement $x(t)$ does not change abruptly at the moment of the impulse; the mass cannot move a finite distance instantaneously. Thus $x$ is continuous at $t=s$, so
\[
x(s^-) = x(s^+).
\]
Since $x(t) = 0$ for $t<s$ we have $x(s^-) = 0$, and therefore
\[
x(s^+) = 0.
\]

The time derivative $x'(t)$, however, is allowed to change abruptly, because an impulse corresponds to a finite, instantaneous change in momentum. To quantify this, we integrate the differential equation across a small interval containing $s$. For $\varepsilon > 0$, integrate from $t = s - \varepsilon$ to $t = s + \varepsilon$:
\[
\int_{s-\varepsilon}^{s+\varepsilon} \left( m x''(t) + c x'(t) + k x(t) \right) \, dt
= \int_{s-\varepsilon}^{s+\varepsilon} \delta(t - s)\,dt.
\]
The right-hand side is equal to $1$, because the delta function integrates to $1$ over any interval containing $s$.

On the left-hand side, we evaluate each term. For the second derivative term,
\[
\int_{s-\varepsilon}^{s+\varepsilon} m x''(t)\, dt
= m \bigl( x'(s+\varepsilon) - x'(s-\varepsilon) \bigr).
\]
For the other terms, we note that $x$ and $x'$ remain bounded as $\varepsilon \to 0$, so
\[
\int_{s-\varepsilon}^{s+\varepsilon} c x'(t)\, dt 
\to 0, \qquad
\int_{s-\varepsilon}^{s+\varepsilon} k x(t)\, dt
\to 0
\quad \text{as } \varepsilon \to 0^+.
\]
Thus, taking the limit $\varepsilon \to 0^+$, we obtain the jump condition
\[
m\bigl(x'(s^+) - x'(s^-)\bigr) = 1,
\]
or equivalently
\[
x'(s^+) - x'(s^-) = \frac{1}{m}.
\]

Since the system is at rest for $t < s$, we have $x'(t) = 0$ and hence $x'(s^-) = 0$, so
\[
x'(s^+) = \frac{1}{m}.
\]
In summary, the impulsively forced solution satisfies
\[
x(s^+) = 0, \qquad x'(s^+) = \frac{1}{m}.
\]

\medskip

\textbf{Step 3: Definition and construction of the Green function.}

We define the Green function $G(t,s)$ to be the solution of
\[
m \frac{\partial^2 G}{\partial t^2}(t,s) + c \frac{\partial G}{\partial t}(t,s) + k G(t,s) = \delta(t - s),
\]
with the conditions
\[
G(t,s) = 0 \quad \text{for } t < s,
\]
and $G$ continuous at $t = s$. For each fixed $s$, $G(t,s)$ as a function of $t$ describes the displacement response to a unit impulse applied at time $t = s$.

From the previous step, we know that
\[
G(s^-,s) = G(s^+,s), \quad\text{and}\quad
\frac{\partial G}{\partial t}(s^+,s) - \frac{\partial G}{\partial t}(s^-,s) = \frac{1}{m}.
\]
Because $G(t,s) = 0$ for $t < s$, we have
\[
G(s^-,s) = 0, \qquad \frac{\partial G}{\partial t}(s^-,s) = 0.
\]
Thus the initial conditions at $t = s^+$ are
\[
G(s^+,s) = 0, \qquad \frac{\partial G}{\partial t}(s^+,s) = \frac{1}{m}.
\]

For $t > s$, the right-hand side of the defining equation for $G$ vanishes, so $G(t,s)$ satisfies the homogeneous equation in $t$:
\[
m \frac{\partial^2 G}{\partial t^2}(t,s) + c \frac{\partial G}{\partial t}(t,s) + k G(t,s) = 0, \quad t > s.
\]
It is convenient to shift time so that the impulse occurs at the new time origin. Let
\[
\tau = t - s, \qquad g(\tau) = G(s + \tau, s), \quad \tau \ge 0.
\]
Then $g$ satisfies the homogeneous equation
\[
m g''(\tau) + c g'(\tau) + k g(\tau) = 0, \quad \tau > 0,
\]
with initial conditions at $\tau = 0^+$ given by
\[
g(0^+) = G(s^+,s) = 0, \qquad g'(0^+) = \frac{\partial G}{\partial t}(s^+,s) = \frac{1}{m}.
\]

From Step 1, the general homogeneous solution has the form
\[
g(\tau) = e^{-\gamma \tau} \bigl( A \cos(\omega_d \tau) + B \sin(\omega_d \tau) \bigr).
\]
Applying the condition $g(0) = 0$ gives
\[
0 = g(0) = e^{0}(A \cos 0 + B \sin 0) = A,
\]
so $A = 0$ and
\[
g(\tau) = B e^{-\gamma \tau} \sin(\omega_d \tau).
\]
We determine $B$ from $g'(0) = 1/m$. Differentiating, we obtain
\[
g'(\tau) = B e^{-\gamma \tau} \bigl( \omega_d \cos(\omega_d \tau) - \gamma \sin(\omega_d \tau) \bigr),
\]
so
\[
g'(0) = B \omega_d = \frac{1}{m}.
\]
Therefore $B = 1/(m \omega_d)$, and hence
\[
g(\tau) = \frac{1}{m \omega_d} e^{-\gamma \tau} \sin(\omega_d \tau), \quad \tau > 0.
\]

Translating back to $t$ and $s$, this means that for $t > s$,
\[
G(t,s) = \frac{1}{m \omega_d} e^{-\gamma (t - s)} \sin\bigl(\omega_d (t - s)\bigr).
\]
For $t < s$ we have $G(t,s) = 0$ by definition. It is common to express this “causality” in terms of the Heaviside step function $H$:
\[
G(t,s) = \frac{1}{m \omega_d} e^{-\gamma (t - s)} \sin\bigl(\omega_d (t - s)\bigr) H(t - s).
\]
This is the impulse response, or Green function, of the underdamped harmonic oscillator.

\medskip

\textbf{Step 4: Solution representation by convolution.}

We now use $G$ to represent the solution of the forced problem
\[
m x''(t) + c x'(t) + k x(t) = f(t), \qquad x(0) = 0,\quad x'(0) = 0.
\]

The key idea comes from linearity and the interpretation of the delta function. For each fixed $s$, the Green function $G(t,s)$ solves
\[
m \frac{\partial^2 G}{\partial t^2}(t,s) + c \frac{\partial G}{\partial t}(t,s) + k G(t,s) = \delta(t - s),
\]
with zero “initial” data for $t < s$. Thus $G(t,s)$ is the response at time $t$ to an idealized unit impulse applied at time $s$.

If we apply instead a force $\alpha\,\delta(t-s)$ with strength $\alpha$, the response is simply $\alpha G(t,s)$ by linearity. More generally, if we imagine approximating a continuous forcing function $f(t)$ as a superposition of many small impulses in time,
\[
f(t) \approx \sum_j f(s_j)\,\Delta s_j\,\delta(t - s_j),
\]
then the response is approximately
\[
x(t) \approx \sum_j f(s_j) \Delta s_j\, G(t,s_j).
\]
In the limit as the time step $\Delta s_j$ tends to zero, this Riemann sum becomes the integral
\[
x(t) = \int_0^t G(t,s) f(s)\,ds.
\]
Here we have integrated up to $t$ because impulses applied at times $s>t$ cannot affect the solution at earlier time $t$; causality is reflected in the factor $H(t-s)$ built into $G$.

To verify the formula more directly, one can differentiate under the integral sign and check that $x(t)$ satisfies the ODE and the initial conditions. However, from the Green function viewpoint, the essential reasoning is that $G(t,s)$ captures the effect of a single impulse at time $s$, and by linear superposition over all times $s$ where a “small” impulse $f(s)\,ds$ acts, we obtain the full response.

Thus the solution with homogeneous initial data is
\[
x(t) = \int_0^t G(t,s) f(s)\,ds
= \int_0^t \frac{1}{m \omega_d} e^{-\gamma (t - s)} 
    \sin\bigl(\omega_d (t - s)\bigr)\, f(s)\, ds.
\]

\medskip

\textbf{Conceptual remark.}
This example illustrates the core idea of the section “Linear Dynamics via the Green Function.” For a linear time-invariant system, the delta impulse response $G(t,s)$ encapsulates the dynamics: how an instantaneous kick at time $s$ affects the displacement at later times $t$. Once $G$ is known, the response to an arbitrary forcing $f$ is obtained by a time-convolution of $G$ with $f$. The structure of the homogeneous solution (damped oscillation in this case) is imprinted directly onto the Green function and hence onto the behavior of solutions for general forcing.
\end{solution}

% ===== Example 2: Step Forcing in a First-Order Linear ODE (inquiry-based) =====
\begin{problem}[Step Forcing in a First-Order Linear ODE]
A large, well-mixed tank is being filled with fluid. At first, the inlet pipe is closed and only draining and mixing are happening. After some time $T>0$, a pump is switched on and a constant inflow begins. A simple model for the volume (or temperature, or concentration) $y(t)$ in the tank is a first-order linear ordinary differential equation with a forcing term that turns on at time $T$ like a step function.

In this problem, you will connect the standard integrating-factor solution formula for first-order linear equations with the Green function, and you will see how a step input can be understood as the accumulation of many small ``impulses.''

We fix a constant $a>0$ and consider the differential operator
\[
L[y](t) := y'(t) + a\,y(t).
\]

\medskip

(a) \textbf{Warm-up: constant forcing and steady state.}  
First ignore any switching and consider the initial-value problem
\[
y'(t) + a\,y(t) = 1, \qquad y(0)=0.
\]
\begin{enumerate}
  \item[(i)] Solve this equation using the integrating-factor method.
  \item[(ii)] Compute $\displaystyle \lim_{t\to\infty} y(t)$. Interpret this limit as a steady state of the system.
\end{enumerate}
% Hint: The integrating factor is $e^{at}$.  

\medskip

(b) \textbf{Step forcing: turning the source on at $t=T$.}  
Now suppose the source is switched on at time $T>0$, instead of at $t=0$. Let $H$ be the Heaviside step function,
\[
H(t-T) = \begin{cases}
0, & t<T,\\
1, & t\ge T.
\end{cases}
\]
Consider the initial-value problem
\[
y'(t) + a\,y(t) = H(t-T), \qquad y(0)=0.
\]

\begin{enumerate}
  \item[(i)] Solve the equation for $0 \le t < T$. What condition do you use at $t=0$?
  \item[(ii)] Now solve the equation for $t>T$, treating it as an equation with constant forcing $1$ but an unknown initial condition at $t=T^+$. What condition do you impose at $t=T$ to glue the two pieces together?
  \item[(iii)] Combine your work to obtain a single piecewise formula for $y(t)$ for all $t\ge 0$. Sketch $y(t)$ qualitatively and describe in words what happens at time $T$.
\end{enumerate}
Hint: The solution should be a decaying exponential before $T$, and after $T$ it should relax toward the same steady state you found in part (a), but starting from the value reached at $t=T$.

\medskip

(c) \textbf{The integrating-factor formula as an integral kernel.}  
For the general forced problem
\[
y'(t) + a\,y(t) = f(t), \qquad y(0) = y_0,
\]
the integrating-factor method gives a formula of the form
\[
y(t) = e^{-at} y_0 + \int_0^t K(t,s)\, f(s)\, ds,
\]
for some kernel $K(t,s)$ that depends on $a$.  

\begin{enumerate}
  \item[(i)] Derive this formula and identify $K(t,s)$ explicitly.
  \item[(ii)] Specialize this formula to the step forcing $f(t)=H(t-T)$ and $y_0=0$, and show that your expression reduces to the piecewise solution you found in part (b). (You may find it helpful to split the integral at $s=T$.)
\end{enumerate}
Hint: Look carefully at $e^{-at} \int_0^t e^{as}f(s)\,ds$ and rewrite it as an integral with integrand $e^{-a(t-s)} f(s)$.

\medskip

(d) \textbf{Introducing the Green function.}  
We now reinterpret the kernel $K(t,s)$ as a Green function.

\begin{enumerate}
  \item[(i)] For each fixed $s\ge 0$, consider the function $G(\,\cdot\,,s)$ of $t$ defined by
  \[
  G(t,s) :=
  \begin{cases}
  0, & t<s,\\
  e^{-a(t-s)}, & t\ge s.
  \end{cases}
  \]
  Show that $G$ satisfies the homogeneous equation $L[G(\,\cdot\,,s)](t)=0$ for all $t\neq s$, together with the initial condition $G(0,s)=0$.
  \item[(ii)] Show that $G$ has a unit jump in its derivative at $t=s$:
  \[
  \lim_{t\downarrow s} \frac{d}{dt}G(t,s) - \lim_{t\uparrow s} \frac{d}{dt}G(t,s) = 1.
  \]
  Explain briefly (in words) why this jump condition encodes the presence of a delta source at time $s$, that is, $L[G(\,\cdot\,,s)](t) = \delta(t-s)$ in the sense of distributions.
  \item[(iii)] Using your work in part (c), write the solution of
  \[
  L[y](t) = f(t), \qquad y(0)=0,
  \]
  in the compact Green-function form
  \[
  y(t) = \int_0^t G(t,s)\, f(s)\, ds.
  \]
  Identify $G(t,s)$ in your formula with the function defined in part (i).
\end{enumerate}
Hint: Compare the kernel $K(t,s)$ from part (c) with $G(t,s)$ above.

\medskip

(e) \textbf{Extensions and ``what if'' questions.}
\begin{enumerate}
  \item[(i)] Suppose now that the source turns on to a different constant level $A\neq 1$ at time $T$, so that $f(t) = A\, H(t-T)$. Without re-solving the differential equation from scratch, use your Green-function formula to write down $y(t)$ explicitly.
  \item[(ii)] Consider a piecewise constant forcing built from several step functions; for example,
  \[
  f(t) = H(t-T_1) - H(t-T_2), \qquad 0<T_1<T_2.
  \]
  Describe qualitatively (and, if you wish, quantitatively) how the solution behaves in time. How does viewing $f$ as a linear combination of steps, and $y$ as an integral against $G(t,s)$, help you understand this behavior?
  \item[(iii)] (Conceptual) The step function $H(t-T)$ can be thought of as the integral over time of many tiny impulses that start at $t=T$. Explain, using your Green-function representation, how this viewpoint is reflected in the formula
  \[
  y(t) = \int_0^t G(t,s)\, H(s-T)\, ds.
  \]
  In your explanation, highlight the role of $G(t,s)$ as the response to an impulse at time $s$.
\end{enumerate}
\end{problem}

% ===== Example 2: Step Forcing in a First-Order Linear ODE (full solution) =====
\begin{problem}[Step Forcing in a First-Order Linear ODE]
Let $a>0$ and $T>0$ be fixed, and let $H$ denote the Heaviside step function,
\[
H(t-T) =
\begin{cases}
0, & t<T,\\
1, & t\ge T.
\end{cases}
\]
Consider the initial-value problem
\[
y'(t) + a\,y(t) = H(t-T), \qquad y(0)=0.
\]
\begin{enumerate}
  \item[(a)] Solve this equation explicitly, giving a piecewise formula for $y(t)$.
  \item[(b)] For the general forced problem
  \[
  y'(t) + a\,y(t) = f(t), \qquad y(0)=y_0,
  \]
  derive the integrating-factor solution and show that it can be written in the Green-function form
  \[
  y(t) = e^{-at}y_0 + \int_0^t G(t,s)\, f(s)\,ds
  \quad\text{with}\quad
  G(t,s) = \begin{cases}
  0, & t<s,\\
  e^{-a(t-s)}, & t\ge s.
  \end{cases}
  \]
  \item[(c)] Specialize this Green-function formula to $f(t)=H(t-T)$ and $y_0=0$, and verify that it reproduces your explicit solution from part (a).
\end{enumerate}
Briefly explain how this example illustrates the interpretation of a step forcing as an accumulation of impulse responses and how it fits into the viewpoint of linear dynamics via the Green function.
\end{problem}

\begin{solution}
We proceed in stages, starting with the explicit solution of the step-forced equation and then connecting it to the general Green-function formula.

\medskip

\textbf{(a) Explicit solution for step forcing.}

We consider
\[
y'(t) + a\,y(t) = H(t-T), \qquad y(0)=0.
\]
Because $H(t-T)$ is piecewise constant, it is natural to solve the equation separately on the intervals $[0,T)$ and $(T,\infty)$ and then impose continuity of $y$ at $t=T$.

\emph{Region 1: $0\le t<T$.}  
Here $H(t-T)=0$, so $y$ satisfies the homogeneous equation
\[
y'(t) + a\,y(t) = 0, \qquad y(0)=0.
\]
The general solution of $y' + a y = 0$ is $y(t)=C e^{-at}$. Imposing $y(0)=0$ gives $C=0$, so
\[
y(t)=0 \quad \text{for } 0\le t<T.
\]

\emph{Region 2: $t>T$.}  
Here $H(t-T)=1$, so $y$ satisfies the inhomogeneous equation
\[
y'(t) + a\,y(t) = 1, \qquad t>T.
\]
We do not know $y(T^+)$ a priori, but we expect the solution to be continuous, so we will determine $y(T^+)$ from the left-hand limit.

We solve $y'+a y = 1$ for $t>T$ using an integrating factor. Multiplying by $e^{at}$, we obtain
\[
\frac{d}{dt}\bigl(e^{at} y(t)\bigr) = e^{at}.
\]
Integrating from $T$ to $t$ (with $t>T$) yields
\[
e^{at} y(t) - e^{aT} y(T^+) = \int_T^t e^{as}\,ds = \frac{1}{a}\left(e^{at}-e^{aT}\right).
\]
Solving for $y(t)$ gives
\[
y(t) = e^{-at} e^{aT} y(T^+) + \frac{1}{a}\Bigl(1 - e^{-a(t-T)}\Bigr).
\]

By continuity of $y$, we set $y(T^+) = y(T^-)$, and from the first region we have $y(T^-)=0$. Thus $y(T^+)=0$, and the first term in the expression above vanishes. Therefore, for $t\ge T$ we obtain
\[
y(t) = \frac{1}{a}\left(1 - e^{-a(t-T)}\right).
\]

Combining the two regions, the solution is
\[
y(t) =
\begin{cases}
0, & 0\le t<T,\\[4pt]
\dfrac{1}{a}\bigl(1 - e^{-a(t-T)}\bigr), & t\ge T.
\end{cases}
\]
This solution is continuous at $t=T$ (it equals $0$ there) and, for $t>T$, relaxes exponentially from $0$ toward the steady state $1/a$.

\medskip

\textbf{(b) Integrating-factor solution and Green function.}

Now consider the general forced equation
\[
y'(t) + a\,y(t) = f(t), \qquad y(0)=y_0.
\]
We apply the integrating factor $e^{at}$. Multiplying both sides by $e^{at}$ gives
\[
e^{at} y'(t) + a e^{at} y(t) = e^{at} f(t),
\]
or, recognizing the left-hand side as a product derivative,
\[
\frac{d}{dt}\bigl(e^{at} y(t)\bigr) = e^{at} f(t).
\]
Integrating from $0$ to $t$ yields
\[
e^{at} y(t) - e^{a\cdot 0} y(0) = \int_0^t e^{as} f(s)\,ds.
\]
Using $y(0)=y_0$, we obtain
\[
e^{at} y(t) = y_0 + \int_0^t e^{as} f(s)\,ds.
\]
Solving for $y(t)$,
\[
y(t) = e^{-at} y_0 + e^{-at}\int_0^t e^{as} f(s)\,ds.
\]
We can rewrite the integral term as
\[
e^{-at}\int_0^t e^{as} f(s)\,ds = \int_0^t e^{-a(t-s)} f(s)\,ds,
\]
since $e^{-at}e^{as} = e^{-a(t-s)}$. Thus the solution takes the kernel form
\[
y(t) = e^{-at} y_0 + \int_0^t e^{-a(t-s)} f(s)\,ds.
\]

To express this in Green-function language, we define
\[
G(t,s) =
\begin{cases}
0, & t<s,\\
e^{-a(t-s)}, & t\ge s.
\end{cases}
\]
Then, for $t\ge 0$,
\[
\int_0^t e^{-a(t-s)} f(s)\,ds = \int_0^t G(t,s) f(s)\,ds,
\]
since on the interval of integration $0\le s\le t$ we have $t\ge s$ and hence $G(t,s)=e^{-a(t-s)}$. Outside this interval we set $G(t,s)=0$ to emphasize causality: the response at time $t$ does not depend on future forcing.

Therefore, the solution can be written as
\[
y(t) = e^{-at} y_0 + \int_0^t G(t,s) f(s)\,ds,
\]
with
\[
G(t,s) = \begin{cases}
0, & t<s,\\
e^{-a(t-s)}, & t\ge s.
\end{cases}
\]
This function $G$ is the Green function for the operator $L[y]=y'+ay$ with the initial condition $y(0)=0$. For each fixed $s$, $G(\cdot,s)$ satisfies the homogeneous equation $G_t + a G = 0$ away from $t=s$, vanishes for $t<s$, and has a unit jump in its derivative at $t=s$, corresponding formally to $L[G(\cdot,s)](t) = \delta(t-s)$ in the sense of distributions.

\medskip

\textbf{(c) Specialization to step forcing.}

We now specialize the Green-function formula to the step forcing $f(t)=H(t-T)$ and the initial condition $y_0=0$.

From part (b), with $y_0=0$ we have
\[
y(t) = \int_0^t G(t,s) H(s-T)\,ds.
\]
Since $G(t,s)=e^{-a(t-s)}$ for $0\le s\le t$, this becomes
\[
y(t) = \int_0^t e^{-a(t-s)} H(s-T)\,ds.
\]

To evaluate this integral explicitly, we use the definition of $H(s-T)$. If $0\le t<T$, then for all $0\le s\le t$ we have $s<T$, so $H(s-T)=0$. Thus the integral vanishes and
\[
y(t) = 0, \qquad 0\le t<T,
\]
which matches the earlier computation.

If $t\ge T$, then $H(s-T)=0$ for $s<T$ and $H(s-T)=1$ for $s\ge T$. Therefore we can write
\[
y(t) = \int_0^t e^{-a(t-s)} H(s-T)\,ds
= \int_T^t e^{-a(t-s)}\,ds,
\]
where the lower limit becomes $T$ because the integrand is zero for $s<T$. Evaluating the integral,
\[
\int_T^t e^{-a(t-s)}\,ds
= \int_T^t e^{-a(t-s)}\,ds
= \left[ -\frac{1}{a} e^{-a(t-s)}\right]_{s=T}^{s=t}
= \frac{1}{a}\left(1 - e^{-a(t-T)}\right).
\]
Thus
\[
y(t) = \frac{1}{a}\left(1 - e^{-a(t-T)}\right), \qquad t\ge T.
\]

Combining the two cases, we recover exactly the piecewise solution found in part (a):
\[
y(t) =
\begin{cases}
0, & 0\le t<T,\\[4pt]
\dfrac{1}{a}\bigl(1 - e^{-a(t-T)}\bigr), & t\ge T.
\end{cases}
\]

\medskip

\textbf{Interpretation and connection to linear dynamics via the Green function.}

This example illustrates several key ideas from the viewpoint of linear dynamics and Green functions:

\begin{itemize}
  \item The kernel $G(t,s)=e^{-a(t-s)}$ (for $t\ge s$) is the system's response at time $t$ to a unit impulse applied at time $s$. It encodes the natural decay rate $a$ of the homogeneous dynamics.
  \item The solution for arbitrary forcing $f$ is a time-convolution of $f$ with this kernel: $y(t)=e^{-at}y_0+\int_0^t G(t,s)f(s)\,ds$. In words, the total response at time $t$ is the superposition of the decayed effects of all past inputs.
  \item A step forcing $H(t-T)$ can be understood as turning on a constant input at time $T$. In Green-function language, the step can be viewed as the accumulation of infinitesimal impulses beginning at $T$. The integral representation
  \[
  y(t) = \int_0^t G(t,s) H(s-T)\,ds = \int_T^t G(t,s)\,ds
  \]
  makes this explicit: the system's state at time $t$ is the integral over all impulse responses from $s\in[T,t]$.
\end{itemize}

Thus the familiar integrating-factor method for first-order linear equations is naturally interpreted as constructing a Green function and then expressing the solution as an integral against this kernel. This rephrasing is the simplest example of the general Green-function approach to linear dynamics.
\end{solution}

% ===== Example 3: Driven RLC Circuit and Convolution with the Green Function (inquiry-based) =====
\begin{problem}[Driven RLC Circuit and Convolution with the Green Function]
A series RLC circuit with resistance \(R>0\), inductance \(L>0\), and capacitance \(C>0\) is driven by a time-dependent voltage source \(V(t)\). If \(q(t)\) denotes the charge on the capacitor, Kirchhoff's voltage law leads to a linear second-order differential equation relating \(q\) and \(V\). In this problem you will build, step by step, the Green function for this circuit, and then use it to represent the response to more complicated drivings by convolution. Along the way you will see how the transient and steady-state behaviors are both encoded in the same Green function.

Throughout, assume the \emph{underdamped} case
\[
R^2 < \frac{4L}{C},
\]
so that the natural response of the circuit oscillates with exponentially decaying amplitude.

\smallskip

(a) \textbf{Setting up the model.}  
Use Kirchhoff's voltage law to show that the charge \(q(t)\) on the capacitor in a driven series RLC circuit satisfies
\[
L\,q''(t) + R\,q'(t) + \frac{1}{C}\,q(t) \;=\; V(t).
\]
Explain briefly how each term in this equation arises from the voltage drops across the inductor, resistor, and capacitor.

\smallskip

(b) \textbf{Homogeneous dynamics and natural frequencies.}  
Consider the associated homogeneous equation
\[
L\,q''(t) + R\,q'(t) + \frac{1}{C}\,q(t) = 0.
\]
\begin{enumerate}
\item[(i)] Write down the characteristic equation and its roots. Show that, under the underdamped assumption \(R^2 < 4L/C\), the roots are
\[
r_{1,2} = -\alpha \pm i\omega_d,
\]
for suitable real constants \(\alpha>0\) and \(\omega_d>0\), which you should express in terms of \(R\), \(L\), and \(C\).

\item[(ii)] Write the general solution of the homogeneous equation in the real form
\[
q_{\mathrm{hom}}(t) = e^{-\alpha t}\,\big(A\cos(\omega_d t) + B\sin(\omega_d t)\big),
\]
and explain briefly the qualitative behavior of this solution.
\end{enumerate}
% Hint: Recall that a pair of complex conjugate roots \(-\alpha \pm i\omega_d\) leads to a decaying oscillation.

\smallskip

(c) \textbf{Defining and determining the Green function.}  
We now introduce the causal Green function \(G(t)\) for the operator
\[
L\frac{d^2}{dt^2} + R\frac{d}{dt} + \frac{1}{C}.
\]
By definition, \(G\) satisfies
\[
L\,G''(t) + R\,G'(t) + \frac{1}{C}\,G(t) = \delta(t),
\]
and \(G(t) = 0\) for all \(t<0\).

\begin{enumerate}
\item[(i)] Integrate the Green function equation across a small interval \((-\varepsilon,\varepsilon)\) around \(t=0\), and let \(\varepsilon \to 0^+\), to derive the jump condition that \(G'\) must satisfy at \(t=0\). What are \(G(0^+)\) and \(G'(0^+)\)?

\emph{Hint:} Use \(\displaystyle \int_{-\varepsilon}^{\varepsilon}\delta(t)\,dt = 1\) and argue that \(\int_{-\varepsilon}^{\varepsilon} G(t)\,dt\) and \(\int_{-\varepsilon}^{\varepsilon} G'(t)\,dt\) vanish as \(\varepsilon\to 0\).

\item[(ii)] For \(t>0\), \(G\) satisfies the homogeneous equation of part (b). Using your general homogeneous solution and the initial conditions you just found at \(t=0^+\), solve for an explicit formula for \(G(t)\) for \(t>0\). You may introduce the Heaviside step function \(H(t)\) if you wish to write a single formula valid for all \(t\).
% Hint: Set \(G(t) = e^{-\alpha t}\left(a\cos(\omega_d t) + b\sin(\omega_d t)\right)\) for \(t>0\) and use the initial conditions at \(0^+\) to find \(a\) and \(b\).
\end{enumerate}

\smallskip

(d) \textbf{Convolution representation of the driven response.}  
Now consider the driven equation with zero initial charge and current:
\[
L\,q''(t) + R\,q'(t) + \frac{1}{C}\,q(t) = V(t), \qquad q(0)=0,\quad q'(0)=0.
\]
\begin{enumerate}
\item[(i)] Using the Green function you have just found, propose an integral formula of the form
\[
q(t) = \int_0^t G(t-s)\,V(s)\,ds
\]
for the solution with zero initial data, and explain in words why this formula should be plausible physically.

\item[(ii)] Verify directly, by differentiating under the integral sign and using the defining properties of \(G\), that your formula for \(q(t)\) indeed satisfies the differential equation and the initial conditions.
% Hint: Differentiate \(q(t) = \int_0^t G(t-s)V(s)\,ds\) once and twice; be careful with the upper limit \(t\). Use the equation satisfied by \(G\) and recall that \(\int_0^t \delta(t-s) V(s)\,ds = V(t)\).
\end{enumerate}

\smallskip

(e) \textbf{Two drivings: sinusoidal and pulsed.}  
We now explore how the same Green function encodes different kinds of behavior.

\begin{enumerate}
\item[(i)] Let the driving voltage be sinusoidal for \(t>0\):
\[
V(t) = V_0 \cos(\omega t)\,H(t),
\]
where \(V_0\) and \(\omega\) are constants. Use the convolution formula to write \(q(t)\) explicitly as a single integral in terms of \(G\), \(V_0\), and \(\omega\). Then, by evaluating or otherwise analyzing this integral, separate the solution into a transient part and a steady-state part oscillating at frequency \(\omega\). What happens to the transient part as \(t\to\infty\)?

\emph{Hint:} You may find it easier to recognize that any solution can be written as ``homogeneous part \(+\) particular part,'' and compare your convolution formula with the particular solution obtained by the method of undetermined coefficients.

\item[(ii)] Now take a \emph{voltage pulse}:
\[
V(t) = 
\begin{cases}
V_0, & 0 < t < T,\\[4pt]
0, & t\ge T,
\end{cases}
\]
with \(V(t)=0\) for \(t<0\). Use the convolution representation to express \(q(t)\) for all \(t\ge 0\) in terms of integrals of \(G\). You do not need to carry out the integrals explicitly, but you should clearly indicate the integration limits in the cases \(0<t<T\) and \(t\ge T\).

\item[(iii)] Qualitatively compare the behavior of the circuit for the sinusoidal driving in part (i) and the pulsed driving in part (ii). In each case, how are the transient and long-time behaviors visible from your expressions involving the same Green function \(G\)?
\end{enumerate}

\end{problem}

% ===== Example 3: Driven RLC Circuit and Convolution with the Green Function (full solution) =====
\begin{problem}[Driven RLC Circuit and Convolution with the Green Function]
Consider a series RLC circuit with resistance \(R>0\), inductance \(L>0\), and capacitance \(C>0\), driven by a voltage source \(V(t)\). Let \(q(t)\) be the charge on the capacitor. Assume the underdamped regime \(R^2 < 4L/C\).

\begin{enumerate}
\item[(a)] Derive the differential equation relating \(q(t)\) and \(V(t)\), and write down the homogeneous characteristic roots and corresponding homogeneous solution.
\item[(b)] Define the causal Green function \(G(t)\) for the operator
\[
L\frac{d^2}{dt^2} + R\frac{d}{dt} + \frac{1}{C},
\]
that is, the solution of
\[
L\,G''(t) + R\,G'(t) + \frac{1}{C}\,G(t) = \delta(t), \qquad G(t)=0 \text{ for } t<0.
\]
Derive the jump conditions at \(t=0\) and obtain an explicit formula for \(G(t)\).
\item[(c)] Show that the solution of
\[
L\,q''(t) + R\,q'(t) + \frac{1}{C}\,q(t) = V(t), \qquad q(0)=0,\quad q'(0)=0,
\]
is given by the convolution
\[
q(t) = \int_0^t G(t-s)\,V(s)\,ds.
\]
\item[(d)] Specialize to the sinusoidal driving \(V(t)=V_0\cos(\omega t)\,H(t)\) and write \(q(t)\) as a convolution integral. By evaluating this integral, or by comparing with a particular solution found by the method of undetermined coefficients, decompose \(q(t)\) into a transient part and a steady-state oscillation at frequency \(\omega\), and describe their long-time behavior.
\item[(e)] For the pulsed voltage
\[
V(t) = 
\begin{cases}
V_0, & 0<t<T,\\[4pt]
0, & t\ge T,
\end{cases}
\quad (V(t)=0 \text{ for } t<0),
\]
express \(q(t)\) for \(t\ge 0\) in terms of integrals of \(G\) with appropriate limits, and briefly interpret the transient behavior of the circuit in this case.

\end{enumerate}
\end{problem}

\begin{solution}
\textbf{(a) Governing equation and homogeneous dynamics.}  
In a series RLC circuit, the current is \(i(t) = q'(t)\). The voltage across the resistor is \(V_R = R i(t) = R q'(t)\), across the inductor is \(V_L = L\,di/dt = L q''(t)\), and across the capacitor is \(V_C = q(t)/C\). Kirchhoff's voltage law asserts that the sum of the voltage drops equals the applied source voltage \(V(t)\), so
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = V(t).
\]

The associated homogeneous equation is
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = 0.
\]
The characteristic polynomial is
\[
L r^2 + R r + \frac{1}{C} = 0,
\]
which has roots
\[
r_{1,2} = \frac{-R \pm \sqrt{R^2 - 4L/C}}{2L}.
\]
Under the underdamped assumption \(R^2<4L/C\), the discriminant is negative, so we define
\[
\alpha = \frac{R}{2L} > 0,\qquad
\omega_0 = \frac{1}{\sqrt{LC}},\qquad
\omega_d = \sqrt{\omega_0^2 - \alpha^2} = \frac{1}{2L}\sqrt{4L/C - R^2} > 0.
\]
Then
\[
r_{1,2} = -\alpha \pm i\omega_d.
\]

Thus the homogeneous solution can be written in real form as
\[
q_{\mathrm{hom}}(t) = e^{-\alpha t}\big(A\cos(\omega_d t) + B\sin(\omega_d t)\big),
\]
where \(A\) and \(B\) are arbitrary constants determined by initial data. This is an exponentially decaying oscillation: the factor \(e^{-\alpha t}\) produces damping, while \(\omega_d\) is the damped oscillation frequency.

\medskip

\textbf{(b) The causal Green function and its explicit form.}

The causal Green function \(G\) for the operator
\[
\mathcal{L} = L\frac{d^2}{dt^2} + R\frac{d}{dt} + \frac{1}{C}
\]
is defined by
\[
L G''(t) + R G'(t) + \frac{1}{C} G(t) = \delta(t),\qquad G(t) = 0 \text{ for } t<0.
\]

\emph{Jump condition at \(t=0\).}  
Integrate this equation from \(-\varepsilon\) to \(\varepsilon\):
\[
\int_{-\varepsilon}^{\varepsilon} \Big(L G''(t) + R G'(t) + \frac{1}{C} G(t)\Big)\,dt
= \int_{-\varepsilon}^{\varepsilon}\delta(t)\,dt = 1.
\]
The left-hand side splits into
\[
L\int_{-\varepsilon}^{\varepsilon} G''(t)\,dt
+ R\int_{-\varepsilon}^{\varepsilon} G'(t)\,dt
+ \frac{1}{C}\int_{-\varepsilon}^{\varepsilon} G(t)\,dt.
\]

By the fundamental theorem of calculus,
\[
\int_{-\varepsilon}^{\varepsilon} G''(t)\,dt = G'(\varepsilon) - G'(-\varepsilon),
\qquad
\int_{-\varepsilon}^{\varepsilon} G'(t)\,dt = G(\varepsilon) - G(-\varepsilon).
\]
Since \(G(t)=0\) for \(t<0\), we have \(G(-\varepsilon)=0\). Moreover, because \(G\) is zero for negative times, its left derivative at zero is also zero, so \(G'(-\varepsilon)\to G'(0^-)=0\) as \(\varepsilon\to 0^+\). Finally, as \(\varepsilon\to 0^+\), the integral of \(G\) itself over a shrinking interval tends to zero. Passing to the limit \(\varepsilon\to 0^+\), we obtain
\[
L\big(G'(0^+) - 0\big) + R\big(G(0^+) - 0\big) + 0 = 1.
\]

We also know that \(G\) must be continuous at \(t=0\) for this second-order equation with \(\delta\) forcing, and since \(G(0^-)=0\), we get \(G(0^+)=0\). Therefore the jump condition simplifies to
\[
L G'(0^+) = 1,\qquad G(0^+) = 0.
\]

\emph{Solving for \(G(t)\) for \(t>0\).}  
For \(t>0\), \(G\) satisfies the homogeneous equation
\[
L G''(t) + R G'(t) + \frac{1}{C} G(t) = 0,
\]
so \(G\) has the same form as the homogeneous solution:
\[
G(t) = e^{-\alpha t}\big(a\cos(\omega_d t) + b\sin(\omega_d t)\big), \qquad t>0,
\]
for some constants \(a\) and \(b\).

We impose the initial data at \(t=0^+\). First,
\[
G(0^+) = a = 0.
\]
Next, compute \(G'(t)\) and evaluate at \(t=0^+\):
\begin{align*}
G'(t) &= e^{-\alpha t}\big(-\alpha a\cos(\omega_d t) - \alpha b\sin(\omega_d t)\big)
       + e^{-\alpha t}\big(-a\omega_d\sin(\omega_d t) + b\omega_d\cos(\omega_d t)\big),\\
G'(0^+) &= -\alpha a + b\omega_d.
\end{align*}
Using \(a=0\), we get \(G'(0^+) = b\omega_d\). The jump condition \(L G'(0^+) = 1\) then gives
\[
L b\omega_d = 1 \quad\Rightarrow\quad b = \frac{1}{L\omega_d}.
\]

Therefore, for \(t>0\),
\[
G(t) = e^{-\alpha t}\,\frac{1}{L\omega_d}\,\sin(\omega_d t),
\]
and for \(t<0\), \(G(t)=0\). We may summarize this compactly using the Heaviside step function \(H(t)\):
\[
G(t) = \frac{1}{L\omega_d}\,e^{-\alpha t}\,\sin(\omega_d t)\,H(t).
\]

This function is the impulse response of the circuit: it is the charge response to a unit-voltage impulse applied at time \(t=0\).

\medskip

\textbf{(c) Convolution formula for the driven solution.}

We now consider
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t) = V(t),\qquad q(0)=0,\quad q'(0)=0.
\]
We claim that
\[
q(t) = \int_0^t G(t-s)\,V(s)\,ds
\]
solves this initial value problem.

\emph{Idea of the formula.}  
The Green function \(G(t-s)\) is the response at time \(t\) to a unit impulse applied at time \(s\). If the actual input at time \(s\) is \(V(s)\,ds\), thought of as a scaled impulse, then the contribution to the charge at time \(t\) is approximately \(G(t-s)V(s)\,ds\). Integrating over all past times \(0\le s\le t\) sums up all such contributions, which is the intuitive origin of the convolution integral.

\emph{Verification by differentiation.}  
Define
\[
q(t) = \int_0^t G(t-s)\,V(s)\,ds.
\]
We first check the initial conditions. At \(t=0\),
\[
q(0) = \int_0^0 G(0-s)\,V(s)\,ds = 0.
\]
For the first derivative, we differentiate under the integral sign using the Leibniz rule:
\[
q'(t) = G(t-t)V(t) + \int_0^t \frac{\partial}{\partial t}G(t-s)\,V(s)\,ds
      = G(0)V(t) + \int_0^t G'(t-s)\,V(s)\,ds.
\]
Since \(G(0)=0\), this simplifies to
\[
q'(t) = \int_0^t G'(t-s)\,V(s)\,ds.
\]
Thus
\[
q'(0) = \int_0^0 G'(0-s)\,V(s)\,ds = 0,
\]
so the initial conditions are satisfied.

Next compute the second derivative:
\begin{align*}
q''(t) &= \frac{d}{dt}\left(\int_0^t G'(t-s)\,V(s)\,ds\right)\\
       &= G'(t-t)V(t) + \int_0^t \frac{\partial}{\partial t}G'(t-s)\,V(s)\,ds\\
       &= G'(0)V(t) + \int_0^t G''(t-s)\,V(s)\,ds.
\end{align*}
Now apply the operator \(\mathcal{L}\) to \(q\):
\begin{align*}
L q''(t) + R q'(t) + \frac{1}{C}q(t)
&= L\Big(G'(0)V(t) + \int_0^t G''(t-s)V(s)\,ds\Big)
   + R\int_0^t G'(t-s)V(s)\,ds
   + \frac{1}{C}\int_0^t G(t-s)V(s)\,ds\\
&= L G'(0)V(t)
   + \int_0^t \Big(L G''(t-s) + R G'(t-s) + \tfrac{1}{C}G(t-s)\Big)V(s)\,ds.
\end{align*}
By the defining equation for \(G\),
\[
L G''(t-s) + R G'(t-s) + \frac{1}{C} G(t-s) = \delta(t-s),
\]
so the integral becomes
\[
\int_0^t \delta(t-s)V(s)\,ds.
\]
This evaluates to \(V(t)\). The term \(L G'(0)V(t)\) can be recognized from the jump condition: \(L G'(0^+)=1\). In the integrals above we always evaluate \(G'(0)\) from the right, so \(L G'(0) = 1\). Therefore
\[
L q''(t) + R q'(t) + \frac{1}{C} q(t)
= \big(LG'(0)\big) V(t) + \int_0^t \delta(t-s)V(s)\,ds
= V(t) + V(t) = 2V(t).
\]

This suggests double-counting, so we examine more carefully: the \(\delta\) contribution already incorporates the jump in \(G'\). A cleaner way is to avoid splitting off \(G'(0)V(t)\) and instead use the Green function equation in its integral form.

An efficient alternative is the following: start directly from
\[
q(t) = \int_0^t G(t-s)\,V(s)\,ds
\]
and apply \(\mathcal{L}\) under the integral sign, without isolating boundary terms. Observe that for fixed \(s\in(0,t)\), as a function of \(t\),
\[
\mathcal{L}_t\big(G(t-s)\big) = \delta(t-s),
\]
where \(\mathcal{L}_t\) denotes the operator acting on the \(t\)-variable. Then, under mild regularity assumptions on \(V\),
\begin{align*}
\mathcal{L}q(t)
&= \mathcal{L}_t\left(\int_0^t G(t-s)V(s)\,ds\right)\\
&= \int_0^t \mathcal{L}_t\big(G(t-s)\big)V(s)\,ds \quad\text{(no boundary term because \(G(t-s)=0\) at \(s=t\))}\\
&= \int_0^t \delta(t-s)V(s)\,ds = V(t).
\end{align*}
This shows that the convolution formula indeed produces a solution of the inhomogeneous equation with the desired initial conditions (the vanishing of \(q(0)\) and \(q'(0)\) follows from the causality and the continuity of \(G\) at \(0\)). Thus, the solution with zero initial data is
\[
q(t) = \int_0^t G(t-s)V(s)\,ds.
\]

The moral is that the impulse response \(G\) completely determines how the system responds to any driving via convolution. This is the key perspective of “linear dynamics via the Green function.”

\medskip

\textbf{(d) Sinusoidal driving: transient and steady state.}

Let
\[
V(t) = V_0\cos(\omega t)\,H(t),
\]
where \(H\) is the Heaviside step function. Then, for \(t>0\),
\[
q(t) = \int_0^t G(t-s)\,V_0\cos(\omega s)\,ds
     = V_0\int_0^t G(t-s)\cos(\omega s)\,ds.
\]
Substituting the explicit Green function,
\[
G(t-s) = \frac{1}{L\omega_d}e^{-\alpha (t-s)}\sin\big(\omega_d(t-s)\big),\qquad 0<s<t,
\]
gives
\[
q(t)
= \frac{V_0}{L\omega_d}\int_0^t e^{-\alpha (t-s)}\sin\big(\omega_d(t-s)\big)\cos(\omega s)\,ds.
\]

One can evaluate this integral explicitly by, for example, writing everything in terms of complex exponentials, but the algebra is lengthy. Instead, we exploit the linearity of the equation and compare with the standard method of finding a particular solution.

We know from the theory of linear ODEs that the general solution has the form
\[
q(t) = q_{\mathrm{hom}}(t) + q_{\mathrm{part}}(t),
\]
where \(q_{\mathrm{hom}}(t)\) is a solution of the homogeneous equation and \(q_{\mathrm{part}}\) is any one particular solution of the forced equation. For sinusoidal forcing, a natural particular solution ansatz is
\[
q_{\mathrm{part}}(t) = A\cos(\omega t) + B\sin(\omega t),
\]
with constants \(A\) and \(B\) depending on \(R\), \(L\), \(C\), \(V_0\), and \(\omega\).

Substituting into
\[
L q'' + R q' + \frac{1}{C}q = V_0\cos(\omega t)
\]
and solving for \(A\) and \(B\) yields a steady-state oscillation at frequency \(\omega\). One convenient way is to use complex notation, seeking \(q_{\mathrm{part}}(t) = \Re\left( \tilde{Q} e^{i\omega t}\right)\), with \(\tilde{Q}\) complex. Then
\[
L(-\omega^2)\tilde{Q}e^{i\omega t} + R(i\omega)\tilde{Q}e^{i\omega t} + \frac{1}{C}\tilde{Q}e^{i\omega t}
= V_0 e^{i\omega t},
\]
so
\[
\big(-L\omega^2 + iR\omega + \tfrac{1}{C}\big)\tilde{Q} = V_0
\quad\Rightarrow\quad
\tilde{Q} = \frac{V_0}{\frac{1}{C} - L\omega^2 + iR\omega}.
\]
Thus \(q_{\mathrm{part}}(t)\) is a cosine at frequency \(\omega\) with some amplitude and phase shift:
\[
q_{\mathrm{part}}(t) = Q(\omega)\cos\big(\omega t - \phi(\omega)\big),
\]
where \(Q(\omega) = |\tilde{Q}|\) and \(\phi(\omega) = \arg(\tilde{Q})\).

The homogeneous part is
\[
q_{\mathrm{hom}}(t) = e^{-\alpha t}\big(A_0\cos(\omega_d t) + B_0\sin(\omega_d t)\big),
\]
for some constants \(A_0\) and \(B_0\). These are determined by the requirement that \(
\(q(0)=0\) and \(q'(0)=0\). Explicitly,
\[
q(0) = q_{\mathrm{hom}}(0)+q_{\mathrm{part}}(0)
      = A_0 + q_{\mathrm{part}}(0) = 0,
\]
\[
q'(0) = q_{\mathrm{hom}}'(0)+q_{\mathrm{part}}'(0)
      = (-\alpha A_0 + \omega_d B_0) + q_{\mathrm{part}}'(0) = 0,
\]
which determine \(A_0\) and \(B_0\) uniquely in terms of the forcing parameters. Their exact values are not important for the qualitative behavior: the homogeneous part is always of the form
\[
q_{\mathrm{hom}}(t) = e^{-\alpha t}\big(A_0\cos(\omega_d t) + B_0\sin(\omega_d t)\big),
\]
so it is a \emph{transient} decaying oscillation at the natural frequency \(\omega_d\).

Thus the full solution can be written as
\[
q(t) = q_{\mathrm{trans}}(t) + q_{\mathrm{ss}}(t),
\]
where
\[
q_{\mathrm{trans}}(t) = q_{\mathrm{hom}}(t)
     = e^{-\alpha t}\big(A_0\cos(\omega_d t) + B_0\sin(\omega_d t)\big),
\]
and
\[
q_{\mathrm{ss}}(t) = q_{\mathrm{part}}(t)
     = Q(\omega)\cos\big(\omega t - \phi(\omega)\big)
\]
is the \emph{steady-state} response at the driving frequency \(\omega\). Because of the exponential factor,
\[
\lim_{t\to\infty} q_{\mathrm{trans}}(t) = 0,
\]
so in the long-time limit the circuit settles into a pure sinusoidal oscillation at frequency \(\omega\), with amplitude and phase determined by the complex transfer function
\[
\tilde{Q}(\omega) = \frac{V_0}{\frac{1}{C} - L\omega^2 + iR\omega}.
\]

\medskip

\textbf{(e) Pulsed driving and comparison.}

\emph{(i) Piecewise integral representation for a pulse.}

Now take
\[
V(t) =
\begin{cases}
V_0, & 0<t<T,\\[4pt]
0, & t\ge T,
\end{cases}
\quad (V(t)=0 \text{ for } t<0).
\]
Using the convolution formula
\[
q(t) = \int_0^t G(t-s)V(s)\,ds,
\]
we distinguish the cases \(0<t<T\) and \(t\ge T\).

\emph{Case \(0<t<T\).}  
Here the integration interval \(0\le s\le t\) lies entirely inside the region where \(V(s)=V_0\), so
\[
q(t) = \int_0^t G(t-s)V_0\,ds
     = V_0\int_0^t G(t-s)\,ds,\qquad 0<t<T.
\]
Equivalently, with the change of variable \(\tau = t-s\),
\[
q(t) = V_0\int_0^t G(\tau)\,d\tau,\qquad 0<t<T.
\]

\emph{Case \(t\ge T\).}  
Now the integration interval splits: \(V(s)=V_0\) only for \(0<s<T\), and \(V(s)=0\) for \(T\le s\le t\). Thus
\[
q(t) = \int_0^T G(t-s)V_0\,ds + \int_T^t G(t-s)\cdot 0\,ds
     = V_0\int_0^T G(t-s)\,ds,\qquad t\ge T.
\]
With \(\tau = t-s\), this can be written as
\[
q(t) = V_0\int_{t-T}^{t} G(\tau)\,d\tau,\qquad t\ge T.
\]

Summarizing,
\[
q(t) =
\begin{cases}
V_0\displaystyle\int_0^t G(\tau)\,d\tau, & 0<t<T,\\[10pt]
V_0\displaystyle\int_{t-T}^{t} G(\tau)\,d\tau, & t\ge T.
\end{cases}
\]

\medskip

\emph{(ii) Qualitative comparison: sinusoidal vs pulsed driving.}

In both drivings, the same Green function
\[
G(t) = \dfrac{1}{L\omega_d}e^{-\alpha t}\sin(\omega_d t)\,H(t)
\]
governs the response; the different behaviors arise entirely from how \(G\) is weighted and integrated against \(V(t)\).

\begin{itemize}
\item \textbf{Sinusoidal driving.}  
The convolution integral continuously accumulates contributions from the ongoing oscillatory input. At each time \(t\), the integral includes values of \(G(\tau)\) for \(\tau\) from \(0\) up to \(t\); older contributions are exponentially suppressed by the factor \(e^{-\alpha \tau}\), but new contributions at the driving frequency \(\omega\) keep arriving. As a result, after the transient dies out, the system settles into a non-decaying steady-state oscillation at frequency \(\omega\), with amplitude and phase determined by the frequency-domain factor \(1/(\frac{1}{C}-L\omega^2+iR\omega)\).

\item \textbf{Pulsed driving.}  
For \(0<t<T\), the situation resembles a constant-voltage drive switched on at \(t=0\): the charge builds up via the integral \(\int_0^t G(\tau)\,d\tau\), mixing the natural oscillation of \(G\) with the step-like input. Once the pulse ends at \(t=T\), the input vanishes and, for \(t\ge T\), the solution is given by an integral of \(G(\tau)\) over a finite, sliding time window \([t-T,t]\). Equivalently, for \(t\ge T\), \(q(t)\) solves the homogeneous equation with initial data \(q(T)\), \(q'(T)\); therefore \(q(t)\) is a purely transient response:
\[
q(t) = e^{-\alpha (t-T)}\big(\tilde{A}\cos(\omega_d (t-T)) + \tilde{B}\sin(\omega_d (t-T))\big),
\]
for some constants \(\tilde{A},\tilde{B}\) determined by the history up to \(T\). Because of the damping, this “ring-down” oscillation decays to zero:
\[
\lim_{t\to\infty} q(t) = 0.
\]
No steady oscillation remains, since the source is off for large times.
\end{itemize}

Thus, in both cases the transient behavior is encoded in the same decaying oscillatory kernel \(G(t)\). When the driving persists indefinitely (sinusoidal case), the convolution yields a superposition of a decaying transient plus a non-decaying steady-state oscillation at the driving frequency. When the driving is of finite duration (pulse), the convolution produces only transient behavior: after the pulse, the circuit simply rings at its natural frequency \(\omega_d\) with exponentially decreasing amplitude, eventually returning to equilibrium.

\end{solution}

% ===== Example 4: Green Functions for a Two-Point Boundary Value Problem (inquiry-based) =====
\begin{problem}[Green Functions for a Two-Point Boundary Value Problem]
Consider a slender elastic rod of length $L$ stretched along the $x$-axis from $x=0$ to $x=L$. In static equilibrium, its (scaled) transverse deflection $u(x)$ under a distributed load $f(x)$ can be modeled by a second-order linear ordinary differential equation together with boundary conditions at both ends. In this problem we construct a Green function for such a boundary value problem and see how it encodes the rod's response to arbitrary load distributions. The same ideas appear throughout linear dynamics and partial differential equations.

We study the boundary value problem
\[
u''(x) \;=\; f(x), \qquad 0<x<L, \qquad u(0) = 0, \quad u(L) = 0.
\]
Throughout, assume $f$ is continuous on $[0,L]$.

\smallskip

(a) Interpret physically what the boundary conditions $u(0)=u(L)=0$ mean for the rod. In particular, describe what is being held fixed and what is allowed to move. Then, solve the homogeneous equation
\[
u''(x) = 0
\]
and write down the general solution. How many free constants does this general solution contain, and how does this relate to the order of the differential equation?

\smallskip

(b) In order to build the Green function, it is convenient to choose homogeneous solutions adapted to the two boundary points. 

Find two linearly independent solutions $y_1$ and $y_2$ of $y''=0$ such that
\[
y_1(0) = 0, \qquad y_2(L) = 0.
\]
(You may also choose a simple normalization, such as $y_1'(0)=1$ or $y_2'(L)=-1$, to make the formulas nice.) Verify that $y_1$ and $y_2$ are linearly independent.

\smallskip

(c) Fix a point $\xi$ with $0<\xi<L$, which you should think of as the location of a ``point load'' on the rod. The Green function $G(x,\xi)$ for this problem is defined to be the solution (as a function of $x$) of
\[
G''(x,\xi) = \delta(x-\xi), \qquad 0<x<L,
\]
together with the boundary conditions
\[
G(0,\xi) = 0, \qquad G(L,\xi) = 0.
\]
Here $\delta$ is the Dirac delta distribution. The idea is that $G(x,\xi)$ gives the deflection at $x$ caused by a unit point force applied at $\xi$.

\begin{enumerate}
\item[(i)] Argue that for $x\neq \xi$, the equation $G''(x,\xi)=\delta(x-\xi)$ reduces to the homogeneous equation $G''(x,\xi)=0$. Explain why this implies that, for fixed $\xi$,
\[
G(x,\xi) =
\begin{cases}
A(\xi)\, x + B(\xi), & 0 \le x < \xi,\\[4pt]
C(\xi)\, x + D(\xi), & \xi < x \le L,
\end{cases}
\]
for some coefficients $A(\xi),B(\xi),C(\xi),D(\xi)$ depending on $\xi$.

\item[(ii)] Use the boundary conditions $G(0,\xi)=0$ and $G(L,\xi)=0$ to simplify this form and reduce the number of unknown coefficient functions.

\item[(iii)] Now impose that $G(x,\xi)$ is continuous at $x=\xi$. Write down the resulting condition relating the coefficients for the left piece and the right piece.

\item[(iv)] Finally, derive the \emph{jump condition} for the first derivative of $G$ at $x=\xi$. Integrate the equation $G''(x,\xi) = \delta(x-\xi)$ over a small interval $(\xi-\varepsilon,\xi+\varepsilon)$ and then let $\varepsilon\to 0^+$. Show that
\[
G_x(\xi^+,\xi) - G_x(\xi^-,\xi) = 1,
\]
where $G_x(\xi^\pm,\xi)$ denote the right and left derivatives at $x=\xi$.

\end{enumerate}

Use all these conditions to solve for the coefficients and obtain an explicit formula for $G(x,\xi)$ in terms of $x$, $\xi$, and $L$.

\emph{Hint:} After you have used the boundary conditions, you should be left with only two unknown coefficient functions. The continuity and jump conditions at $x=\xi$ will then form a $2\times 2$ linear system for these remaining unknowns.

\smallskip

(d) Suppose now that $f$ is an arbitrary continuous load distribution on $[0,L]$. Show formally that the function
\[
u(x) = \int_0^L G(x,\xi)\, f(\xi)\, d\xi
\]
solves the boundary value problem
\[
u''(x) = f(x), \qquad 0<x<L, \qquad u(0)=u(L)=0.
\]
Proceed in three steps:
\begin{enumerate}
\item[(i)] Differentiate under the integral sign (formally) to compute $u''(x)$, using the defining equation for $G$.
\item[(ii)] Check that the boundary conditions $u(0)=u(L)=0$ hold.
\item[(iii)] Briefly explain why the construction is linear in $f$ (that is, why the map $f\mapsto u$ is a linear operator).
\end{enumerate}

\emph{Hint:} For (i), think in distributional terms: the equation $G''(\cdot,\xi) = \delta(\cdot-\xi)$ says that when you differentiate $G$ twice in $x$ and integrate against a test function of $x$, you get evaluation at $x=\xi$.

\smallskip

(e) Extensions and variations.

\begin{enumerate}
\item[(i)] What changes in the construction of the Green function if the boundary conditions are replaced by
\[
u'(0) = 0, \qquad u(L) = 0,
\]
modeling, for instance, one end clamped horizontally and the other end held at zero displacement? Sketch how you would adapt steps (b) and (c) to this modified problem. You do not need to carry out all the algebra.

\item[(ii)] Suppose now that the load is itself a point load at $x=a$, modeled by $f(x)=F_0\,\delta(x-a)$ with $0<a<L$. Using your Green function $G(x,\xi)$, write down the corresponding deflection $u(x)$ explicitly. What does this say about the physical meaning of $G(x,\xi)$?

\end{enumerate}

\end{problem}

% ===== Example 4: Green Functions for a Two-Point Boundary Value Problem (full solution) =====
\begin{problem}[Green Functions for a Two-Point Boundary Value Problem]
Consider the boundary value problem
\[
u''(x) = f(x), \qquad 0<x<L, \qquad u(0)=0, \quad u(L)=0,
\]
where $f$ is continuous on $[0,L]$.

(a) Construct the Green function $G(x,\xi)$ satisfying
\[
G''(x,\xi) = \delta(x-\xi), \qquad 0<x<L, \qquad G(0,\xi)=0,\quad G(L,\xi)=0,
\]
together with continuity at $x=\xi$ and the jump condition
\[
\frac{\partial G}{\partial x}(\xi^+,\xi) - \frac{\partial G}{\partial x}(\xi^-,\xi) = 1.
\]
Find an explicit formula for $G(x,\xi)$ in terms of $x$, $\xi$, and $L$.

(b) Show that the function
\[
u(x) = \int_0^L G(x,\xi)\, f(\xi)\, d\xi
\]
solves the boundary value problem, that is, $u''(x)=f(x)$ for $0<x<L$ and $u(0)=u(L)=0$.

Briefly comment on how this example illustrates the idea of describing linear dynamics via a Green function.
\end{problem}

\begin{solution}
We begin by constructing the Green function for the operator $\dfrac{d^2}{dx^2}$ with homogeneous Dirichlet boundary conditions at $x=0$ and $x=L$. The central idea is that the Green function represents the response of the system to a unit point source, and hence any other forcing can be represented as a superposition of such point sources.

\medskip

\textbf{(a) Construction of the Green function.}

Fix a point $\xi$ with $0<\xi<L$. For this fixed $\xi$, we view $G(\cdot,\xi)$ as a function of $x$ which solves
\[
G''(x,\xi) = \delta(x-\xi), \qquad 0<x<L,
\]
with
\[
G(0,\xi)=0, \qquad G(L,\xi)=0.
\]

\smallskip

\emph{Step 1: General form away from $x=\xi$.}

For $x\neq \xi$, the right-hand side is zero, so $G$ satisfies the homogeneous equation
\[
G''(x,\xi)=0 \quad\text{for } x\in(0,\xi)\cup(\xi,L).
\]
The general solution of $y''=0$ is linear, $y(x)=\alpha x + \beta$. Therefore, for fixed $\xi$ we may write
\[
G(x,\xi) =
\begin{cases}
A(\xi)\, x + B(\xi), & 0 \le x < \xi,\\[4pt]
C(\xi)\, x + D(\xi), & \xi < x \le L,
\end{cases}
\]
where $A,B,C,D$ are functions of the parameter $\xi$ only.

\smallskip

\emph{Step 2: Boundary conditions.}

The boundary condition at $x=0$ gives
\[
G(0,\xi)=0 \quad \Longrightarrow \quad B(\xi)=0.
\]
Thus on the left side we have
\[
G(x,\xi) = A(\xi)\, x, \qquad 0\le x<\xi.
\]

The boundary condition at $x=L$ gives
\[
G(L,\xi)=0 \quad \Longrightarrow \quad C(\xi)\,L + D(\xi) = 0,
\]
so
\[
D(\xi) = -C(\xi)\,L,
\]
and hence on the right side we may write
\[
G(x,\xi) = C(\xi)\,x - C(\xi)\,L = C(\xi)\,(x-L), \qquad \xi<x\le L.
\]

At this point the Green function has the form
\[
G(x,\xi) =
\begin{cases}
A(\xi)\, x, & 0 \le x < \xi,\\[4pt]
C(\xi)\,(x-L), & \xi < x \le L.
\end{cases}
\]

\smallskip

\emph{Step 3: Continuity at $x=\xi$.}

By definition, $G(x,\xi)$ should be continuous at $x=\xi$, even though its derivative will have a jump there. Thus
\[
\lim_{x\uparrow \xi} G(x,\xi) = \lim_{x\downarrow \xi} G(x,\xi).
\]
Substituting from the left and right formulas, this gives
\[
A(\xi)\,\xi = C(\xi)\,(\xi - L).
\]
We may rewrite this relation as
\begin{equation}\label{eq:continuity}
A(\xi)\,\xi = C(\xi)\,(\xi - L).
\end{equation}

\smallskip

\emph{Step 4: Jump condition for the derivative.}

The defining equation $G''(x,\xi)=\delta(x-\xi)$ implies a jump condition for the first derivative at $x=\xi$. Integrate both sides over a small interval $(\xi-\varepsilon,\xi+\varepsilon)$:
\[
\int_{\xi-\varepsilon}^{\xi+\varepsilon} G''(x,\xi)\,dx
=
\int_{\xi-\varepsilon}^{\xi+\varepsilon} \delta(x-\xi)\,dx
= 1.
\]
The left-hand side can be evaluated by the Fundamental Theorem of Calculus:
\[
\int_{\xi-\varepsilon}^{\xi+\varepsilon} G''(x,\xi)\,dx
=
G_x(\xi+\varepsilon,\xi) - G_x(\xi-\varepsilon,\xi).
\]
Now let $\varepsilon\to 0^+$. Since $G''$ is zero away from $x=\xi$, the one-sided derivatives $G_x(\xi^+,\xi)$ and $G_x(\xi^-,\xi)$ exist as limits from the right and left, and we obtain
\[
G_x(\xi^+,\xi) - G_x(\xi^-,\xi) = 1.
\]

Next, we compute these derivatives from our piecewise-linear representation. From the left we have
\[
G(x,\xi) = A(\xi)\,x \quad\Longrightarrow\quad G_x(x,\xi) = A(\xi)\quad (x<\xi),
\]
so
\[
G_x(\xi^-,\xi) = A(\xi).
\]
From the right we have
\[
G(x,\xi) = C(\xi)\,(x-L) \quad\Longrightarrow\quad G_x(x,\xi) = C(\xi)\quad (x>\xi),
\]
so
\[
G_x(\xi^+,\xi) = C(\xi).
\]
The jump condition therefore becomes
\begin{equation}\label{eq:jump}
C(\xi) - A(\xi) = 1.
\end{equation}

\smallskip

\emph{Step 5: Solving for the coefficients.}

We now have two equations in the two unknowns $A(\xi)$ and $C(\xi)$:
\[
\begin{cases}
A(\xi)\,\xi = C(\xi)\,(\xi - L),\\[4pt]
C(\xi) - A(\xi) = 1.
\end{cases}
\]
We solve this linear system. From the continuity condition \eqref{eq:continuity},
\[
A(\xi) = C(\xi)\,\dfrac{\xi - L}{\xi}.
\]
Substitute into the jump condition \eqref{eq:jump}:
\[
C(\xi) - C(\xi)\,\dfrac{\xi - L}{\xi} = 1
\quad\Longrightarrow\quad
C(\xi)\left(1 - \frac{\xi - L}{\xi}\right) = 1.
\]
Inside the parentheses we simplify:
\[
1 - \frac{\xi - L}{\xi} = \frac{\xi}{\xi} - \frac{\xi - L}{\xi}
= \frac{L}{\xi}.
\]
Thus
\[
C(\xi)\,\frac{L}{\xi} = 1
\quad\Longrightarrow\quad
C(\xi) = \frac{\xi}{L}.
\]
Then
\[
A(\xi) = C(\xi)\,\frac{\xi - L}{\xi}
= \frac{\xi}{L} \cdot \frac{\xi - L}{\xi}
= \frac{\xi - L}{L}.
\]

Therefore the Green function is
\[
G(x,\xi) =
\begin{cases}
\dfrac{\xi - L}{L}\,x, & 0 \le x < \xi,\\[6pt]
\dfrac{\xi}{L}\,(x-L), & \xi < x \le L.
\end{cases}
\]
It is straightforward to check that this expression is continuous at $x=\xi$, satisfies the prescribed boundary conditions at $x=0$ and $x=L$, and exhibits the correct unit jump in the first derivative at $x=\xi$.

It is also instructive to note that $G$ is symmetric:
\[
G(x,\xi)=G(\xi,x),
\]
which one can verify directly by considering the cases $x<\xi$ and $x>\xi$. This symmetry reflects the self-adjoint nature of the operator $d^2/dx^2$ with homogeneous Dirichlet boundary conditions, but that observation is not needed for the present computation.

\medskip

\textbf{(b) Representation of the solution by the Green function.}

We now show that
\[
u(x) = \int_0^L G(x,\xi)\, f(\xi)\, d\xi
\]
solves the boundary value problem
\[
u''(x) = f(x), \qquad 0<x<L, \qquad u(0)=u(L)=0.
\]

\smallskip

\emph{Step 1: Verifying the differential equation.}

Formally differentiating under the integral sign with respect to $x$, we obtain
\[
u'(x) = \int_0^L \frac{\partial G}{\partial x}(x,\xi)\, f(\xi)\, d\xi,
\]
and
\[
u''(x) = \int_0^L \frac{\partial^2 G}{\partial x^2}(x,\xi)\, f(\xi)\, d\xi.
\]
By the defining property of the Green function, for each fixed $\xi$ we have
\[
\frac{\partial^2 G}{\partial x^2}(x,\xi) = \delta(x-\xi)
\]
in the sense of distributions. Substituting this into the expression for $u''(x)$ gives
\[
u''(x) = \int_0^L \delta(x-\xi)\, f(\xi)\, d\xi.
\]
The integral of a function against a Dirac delta evaluates the integrand at the point where the delta is centered, so
\[
u''(x) = f(x), \qquad 0<x<L.
\]
This is precisely the differential equation we seek to solve.

A more formal justification of the differentiation under the integral sign can be given by approximating the delta distribution with smooth functions or by working in the framework of weak solutions. For the purposes of this example, it is enough to treat the calculation at a formal level.

\smallskip

\emph{Step 2: Verifying the boundary conditions.}

We next compute the boundary values of $u$. Using the explicit form of $G(x,\xi)$, note that for any fixed $\xi$,
\[
G(0,\xi) = 0 \quad\text{and}\quad G(L,\xi)=0.
\]
Therefore
\[
u(0) = \int_0^L G(0,\xi)\, f(\xi)\, d\xi
= \int_0^L 0\cdot f(\xi)\, d\xi = 0,
\]
and similarly
\[
u(L) = \int_0^L G(L,\xi)\, f(\xi)\, d\xi
= \int_0^L 0\cdot f(\xi)\, d\xi = 0.
\]
Thus the function $u$ defined by the Green function integral satisfies the prescribed boundary conditions.

\smallskip

\emph{Step 3: Linearity and interpretation.}

The map that takes $f$ to $u$ is linear because the integral defining $u$ is linear in $f$:
\[
\int_0^L G(x,\xi)\, (\alpha f_1(\xi) + \beta f_2(\xi))\, d\xi
= \alpha \int_0^L G(x,\xi)\, f_1(\xi)\, d\xi
 + \beta \int_0^L G(x,\xi)\, f_2(\xi)\, d\xi.
\]
Thus, the Green function encapsulates the complete linear response of the system: it is the response to a unit point source at $\xi$, and any general forcing $f$ is built up as a superposition of such point sources, weighted by $f(\xi)$, integrated over $\xi$.

\medskip

\textbf{Connection to linear dynamics via the Green function.}

This example illustrates a fundamental strategy in the study of linear dynamical systems, whether governed by ordinary or partial differential equations. Instead of solving the boundary value problem separately for each forcing function $f$, we solve a single, canonical problem: find the Green function, the response to a unit impulse. Once the Green function is known, the solution for any forcing is obtained by a single integral, expressing the principle of superposition. The Green function therefore serves as the kernel of a linear integral operator which is the inverse of the differential operator (subject to the given boundary conditions). In this way, the linear dynamics of the system are encoded completely in the Green function.

\end{solution}

% ===== Example 5: Preview: From ODE Green Functions to the Heat Equation Kernel (inquiry-based) =====
\begin{problem}[Preview: From ODE Green Functions to the Heat Equation Kernel]
On an infinitely long, thin wire, let $u(x,t)$ denote the temperature at position $x\in\mathbb{R}$ and time $t>0$. If the wire is perfectly insulated from its surroundings and heat diffuses along the wire with constant diffusivity $\kappa>0$, then $u$ satisfies the one-dimensional heat equation
\[
u_t = \kappa u_{xx}, \qquad x\in\mathbb{R},\ t>0.
\]
In this problem we use ideas from the Green function for simple ordinary differential equations to derive, somewhat formally, the fundamental solution (or heat kernel) for this partial differential equation. The key idea is to transform in the spatial variable so that the heat equation becomes, for each spatial frequency, a first-order ODE in time whose Green function we already understand.

We fix a source point $\xi\in\mathbb{R}$ and source time $\tau\in\mathbb{R}$, and we look for a function $G(x,t;\xi,\tau)$ (the heat kernel) that solves
\[
G_t(x,t;\xi,\tau) \;=\; \kappa\,G_{xx}(x,t;\xi,\tau) \;+\; \delta(x-\xi)\,\delta(t-\tau),
\]
for $x\in\mathbb{R}$ and $t\in\mathbb{R}$, together with $G(x,t;\xi,\tau)=0$ for $t<\tau$. Intuitively, $G$ is the temperature at $(x,t)$ produced by a unit instantaneous heat source at $(\xi,\tau)$.

\medskip

(a) As a warm-up, recall the Green function for a simple first-order ODE in time. Consider
\[
y'(t) = f(t), \qquad y(t_0)=0.
\]
(i) Write down the solution $y(t)$ in integral form, starting from the fundamental theorem of calculus.  

(ii) Show that there is a function $g(t,s)$ such that
\[
y(t) = \int_{-\infty}^{\infty} g(t,s)\,f(s)\,ds,
\]
and identify $g(t,s)$.  

Hint: Think about when the forcing $f(s)$ occurring at time $s$ can influence the solution at time $t$.

\medskip

(b) Now recall the Green function for the constant-coefficient ODE
\[
y'(t) + a\,y(t) = f(t), \qquad y(t_0)=0, \qquad a>0,
\]
which you studied earlier in this chapter.

(i) Using the integrating factor $e^{at}$, solve this ODE with $y(t_0)=0$ to express $y(t)$ as an integral involving $f$.  

(ii) Show that $y$ can be written in the convolution form
\[
y(t) = \int_{-\infty}^{\infty} G_a(t,s)\,f(s)\,ds,
\]
and identify the Green function $G_a(t,s)$.  

(iii) Describe in words how $G_a(t,s)$ functions as an impulse response in time.

\medskip

(c) We now turn to the heat equation Green function. To take advantage of translation invariance, it is convenient to work in the shifted spatial variable $y = x-\xi$ and define
\[
H(y,t;\tau) := G(y+\xi,t;\xi,\tau).
\]
Then $H$ satisfies
\[
H_t(y,t;\tau) = \kappa H_{yy}(y,t;\tau) + \delta(y)\,\delta(t-\tau), \qquad H(y,t;\tau)=0 \text{ for } t<\tau.
\]

We will take the Fourier transform in $y$. For a sufficiently nice function $\phi(y)$, define its Fourier transform by
\[
\widehat{\phi}(k) := \int_{-\infty}^{\infty} e^{-iky}\,\phi(y)\,dy.
\]

(i) Compute the Fourier transform in $y$ of $H_t$ and of $H_{yy}$. That is, express $\widehat{H_t}(k,t;\tau)$ and $\widehat{H_{yy}}(k,t;\tau)$ in terms of $\widehat{H}(k,t;\tau)$.  

(ii) Compute the Fourier transform in $y$ of $\delta(y)\,\delta(t-\tau)$ and show that it equals $\delta(t-\tau)$ (as a function of $t$ and $\tau$) for every $k$.  

(iii) Conclude that, for each fixed spatial frequency $k\in\mathbb{R}$, the transform $\widehat{H}$ satisfies the ODE
\[
\partial_t \widehat{H}(k,t;\tau) + \kappa k^2\,\widehat{H}(k,t;\tau)
= \delta(t-\tau), \qquad \widehat{H}(k,t;\tau)=0\ \text{for } t<\tau.
\]
Explain why this is now a \emph{time-dependent} ODE with parameter $k$ that we can solve using part~(b).  

% Hint: You should recognize this as the ODE from part (b) with $a = \kappa k^2$ and forcing $f(t) = \delta(t-\tau)$.

\medskip

(d) Use your work in part~(b) to solve the ODE for $\widehat{H}$.

(i) For fixed $k$, solve
\[
\partial_t \widehat{H}(k,t;\tau) + \kappa k^2\,\widehat{H}(k,t;\tau)
= \delta(t-\tau), \qquad \widehat{H}(k,t;\tau)=0\ \text{for } t<\tau,
\]
and show that
\[
\widehat{H}(k,t;\tau) = e^{-\kappa k^2 (t-\tau)}\,H(t-\tau),
\]
where $H$ on the right-hand side is now the Heaviside step function in time.  

(ii) Invert the Fourier transform in $y$ to recover $H(y,t;\tau)$:
\[
H(y,t;\tau) = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{iky}\,\widehat{H}(k,t;\tau)\,dk.
\]
Substitute your formula for $\widehat{H}$ and write $H(y,t;\tau)$ as an integral in $k$.  

(iii) Evaluate the resulting integral in $k$ using the standard Gaussian integral
\[
\int_{-\infty}^{\infty} e^{-\alpha k^2 + i\beta k}\,dk
= \sqrt{\frac{\pi}{\alpha}}\,e^{-\beta^2/(4\alpha)}, \qquad \alpha>0.
\]
Conclude that, for $t>\tau$,
\[
G(x,t;\xi,\tau)
= \frac{1}{\sqrt{4\pi\kappa (t-\tau)}}
\exp\!\left(-\frac{(x-\xi)^2}{4\kappa (t-\tau)}\right),
\]
and that $G(x,t;\xi,\tau)=0$ for $t<\tau$.  

(iv) Briefly interpret this formula: how does the spatial profile at fixed $t-\tau$ look, and what happens to that profile as $t-\tau$ increases?

% Hint: Recognize the $x$-dependence as a Gaussian whose variance grows linearly in time.

\medskip

(e) Finally, connect this PDE Green function back to the idea of convolution and superposition.

(i) Suppose now that the initial temperature at time $t=0$ is some given function $u_0(x)$, and there are no sources for $t>0$. By thinking of $u_0$ as a continuous superposition of point sources at time $0$, write a formal representation of the solution $u(x,t)$ for $t>0$ in terms of $G$ and $u_0$.  

(ii) Compare your formula with the convolution representation for ODEs from parts~(a)–(b). In what sense is the heat equation solution a \emph{spatial} convolution rather than a \emph{temporal} convolution?  

(iii) (What if / extension.) How do you expect the heat kernel to change if we work on a periodic domain, say $x\in[0,2\pi]$ with periodic boundary conditions? Describe, at an informal level, how the spatial Fourier series temporarily replaces the Fourier transform, and what you think the corresponding Green function would look like. Hint: Think about summing up periodically shifted copies of the Gaussian.
\end{problem}

% ===== Example 5: Preview: From ODE Green Functions to the Heat Equation Kernel (full solution) =====
\begin{problem}[Preview: From ODE Green Functions to the Heat Equation Kernel]
Consider the one-dimensional heat equation on the whole line,
\[
u_t = \kappa u_{xx}, \qquad x\in\mathbb{R},\ t>0,\ \kappa>0.
\]
Let $G(x,t;\xi,\tau)$ denote the fundamental solution (heat kernel), defined as the solution of
\[
G_t = \kappa G_{xx} + \delta(x-\xi)\,\delta(t-\tau), \qquad G(\cdot,t;\xi,\tau)=0\ \text{for } t<\tau.
\]
  
(a) Take the Fourier transform in $x$ (or, more conveniently, in $y=x-\xi$) to show that, for each fixed spatial frequency $k$, the transform $\widehat{G}$ satisfies the ODE
\[
\partial_t \widehat{G}(k,t;\xi,\tau) + \kappa k^2\,\widehat{G}(k,t;\xi,\tau) = \delta(t-\tau), \qquad \widehat{G}(k,t;\xi,\tau)=0\ \text{for } t<\tau.
\]
  
(b) Using the Green function for the ODE $y'(t) + a y(t) = \delta(t-\tau)$ with $a>0$, solve explicitly for $\widehat{G}(k,t;\xi,\tau)$ and then invert the Fourier transform to obtain a formula for $G(x,t;\xi,\tau)$.  

(c) Show that
\[
G(x,t;\xi,\tau)
= \frac{1}{\sqrt{4\pi\kappa (t-\tau)}}
\exp\!\left(-\frac{(x-\xi)^2}{4\kappa (t-\tau)}\right) H(t-\tau),
\]
where $H$ is the Heaviside step function in $t-\tau$.  

(d) Finally, for given initial data $u(x,0)=u_0(x)$ on $\mathbb{R}$, write the solution $u(x,t)$ for $t>0$ as a spatial convolution of $u_0$ with the heat kernel, and briefly explain how this representation is analogous to the convolution formulas for linear ODEs with a Green function.
\end{problem}

\begin{solution}
We begin by recalling the strategy: for linear constant-coefficient equations, the Green function is the response to a unit impulse. In the ODE setting, this Green function often appears as the kernel of a convolution integral in time. For the heat equation, we will transfer the PDE to the Fourier side in space, where each Fourier mode solves a first-order ODE in time, and then use the known ODE Green function to recover the heat kernel.

\medskip

\emph{Step 1: Fourier transform in space and reduction to an ODE in time.}

It is convenient to exploit translation invariance in $x$ by introducing the shifted variable
\[
y = x-\xi
\]
and defining
\[
H(y,t;\tau) := G(y+\xi,t;\xi,\tau).
\]
In terms of $H$, the defining equation for $G$ becomes
\[
H_t(y,t;\tau) = \kappa H_{yy}(y,t;\tau) + \delta(y)\,\delta(t-\tau),
\]
with $H(y,t;\tau)=0$ for $t<\tau$.

We now take the Fourier transform in the spatial variable $y$. For a sufficiently nice function $\phi(y)$, we define
\[
\widehat{\phi}(k) = \int_{-\infty}^{\infty} e^{-iky}\,\phi(y)\,dy.
\]
We use the standard properties of the Fourier transform:

\begin{itemize}
  \item Differentiation in $y$ corresponds to multiplication by $ik$:
  \[
  \widehat{\phi_y}(k) = ik\,\widehat{\phi}(k), \qquad \widehat{\phi_{yy}}(k) = -k^2\,\widehat{\phi}(k).
  \]
  \item The Fourier transform of the Dirac delta $\delta(y)$ is the constant function $1$:
  \[
  \int_{-\infty}^{\infty} e^{-iky}\,\delta(y)\,dy = 1
  \]
  for every $k$.
\end{itemize}

Applying the Fourier transform in $y$ to the equation for $H$ gives
\[
\widehat{H_t}(k,t;\tau)
= \kappa\,\widehat{H_{yy}}(k,t;\tau) + \widehat{\delta(y)\,\delta(t-\tau)}.
\]
Because the transform is with respect to $y$ only, time $t$ is just a parameter in this operation. Thus
\[
\widehat{H_t}(k,t;\tau) = \partial_t \widehat{H}(k,t;\tau),
\]
while
\[
\widehat{H_{yy}}(k,t;\tau) = -k^2 \widehat{H}(k,t;\tau).
\]
For the source term, we obtain
\[
\widehat{\delta(y)\,\delta(t-\tau)} = \delta(t-\tau)\,\widehat{\delta(y)} = \delta(t-\tau)\cdot 1 = \delta(t-\tau),
\]
since $\delta(t-\tau)$ does not depend on $y$ and the Fourier transform of $\delta(y)$ is $1$.

Putting these pieces together, we obtain, for each fixed $k\in\mathbb{R}$,
\[
\partial_t \widehat{H}(k,t;\tau)
= \kappa\left(-k^2 \widehat{H}(k,t;\tau)\right) + \delta(t-\tau),
\]
or equivalently
\[
\partial_t \widehat{H}(k,t;\tau) + \kappa k^2\,\widehat{H}(k,t;\tau)
= \delta(t-\tau).
\]
Moreover, the condition $H(\cdot,t;\tau)=0$ for $t<\tau$ implies
\[
\widehat{H}(k,t;\tau) = 0 \quad \text{for } t<\tau.
\]
Since $H$ and $G$ differ only by a spatial translation, the same ODE holds for the Fourier transform of $G$ in the spatial variable $(x-\xi)$; we will continue to use the notation $\widehat{H}$ because the calculation is simpler in $y$.

Thus, for each frequency $k$, we have reduced the PDE to a first-order linear ODE in time with a delta-function forcing and an initial condition at $t=\tau$.

\medskip

\emph{Step 2: Solve the ODE using the ODE Green function.}

We consider, for fixed $k$,
\[
\partial_t \widehat{H}(k,t;\tau) + \kappa k^2\,\widehat{H}(k,t;\tau)
= \delta(t-\tau), \qquad \widehat{H}(k,t;\tau) = 0 \text{ for } t<\tau.
\]
This is exactly of the form
\[
y'(t) + a\,y(t) = \delta(t-\tau), \qquad y(t)=0 \text{ for } t<\tau,
\]
with $a = \kappa k^2 > 0$.

From our earlier study of first-order linear ODEs with constant coefficient $a>0$, we know the Green function $G_a(t,s)$ for such an equation. Solving
\[
y'(t) + a y(t) = f(t), \qquad y(t_0)=0,
\]
by the integrating-factor method, we obtain
\[
y(t) = \int_{t_0}^{t} e^{-a(t-s)} f(s)\,ds.
\]
If we extend $f$ by zero for $s<t_0$, we may also write
\[
y(t) = \int_{-\infty}^{\infty} e^{-a(t-s)} H(t-s)\,f(s)\,ds,
\]
where $H$ is the Heaviside function
\[
H(t-s) =
\begin{cases}
0, & t<s,\\[4pt]
1, & t\ge s.
\end{cases}
\]
Thus the ODE Green function is
\[
G_a(t,s) = e^{-a(t-s)} H(t-s).
\]

In our present situation, $f(t) = \delta(t-\tau)$ and $a = \kappa k^2$. Therefore,
\[
\widehat{H}(k,t;\tau) = \int_{-\infty}^{\infty} e^{-\kappa k^2 (t-s)} H(t-s)\,\delta(s-\tau)\,ds.
\]
The integral collapses by the sifting property of the delta distribution, yielding
\[
\widehat{H}(k,t;\tau)
= e^{-\kappa k^2 (t-\tau)} H(t-\tau).
\]
This formula incorporates both the exponential decay in time (for each fixed nonzero $k$) and the causality condition that there is no response for $t<\tau$.

\medskip

\emph{Step 3: Invert the Fourier transform to find $H$ and hence $G$.}

We now invert the spatial Fourier transform. By the usual inversion formula,
\[
H(y,t;\tau) = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{iky}\,\widehat{H}(k,t;\tau)\,dk.
\]
Substituting the expression found above for $\widehat{H}(k,t;\tau)$, we obtain
\[
H(y,t;\tau) = \frac{H(t-\tau)}{2\pi} \int_{-\infty}^{\infty} e^{iky}\,e^{-\kappa k^2 (t-\tau)}\,dk.
\]
We focus on the case $t>\tau$, for which $H(t-\tau)=1$. Then
\[
H(y,t;\tau) = \frac{1}{2\pi} \int_{-\infty}^{\infty} \exp\!\bigl(-\kappa (t-\tau) k^2 + i y k\bigr)\,dk.
\]

This is a classical Gaussian integral. For $\alpha>0$ and real $\beta$, one has
\[
\int_{-\infty}^{\infty} e^{-\alpha k^2 + i\beta k}\,dk
= \sqrt{\frac{\pi}{\alpha}}\,\exp\!\left(-\frac{\beta^2}{4\alpha}\right).
\]
Here $\alpha = \kappa (t-\tau) >0$ (since $t>\tau$) and $\beta = y$. Applying the formula, we find
\[
\int_{-\infty}^{\infty} e^{-\kappa (t-\tau) k^2 + i y k}\,dk
= \sqrt{\frac{\pi}{\kappa (t-\tau)}}\,
\exp\!\left(-\frac{y^2}{4\kappa (t-\tau)}\right).
\]
Therefore,
\[
H(y,t;\tau)
= \frac{1}{2\pi}\,\sqrt{\frac{\pi}{\kappa (t-\tau)}}\,
\exp\!\left(-\frac{y^2}{4\kappa (t-\tau)}\right),
\]
for $t>\tau$. Simplifying the prefactor,
\[
\frac{1}{2\pi} \sqrt{\frac{\pi}{\kappa (t-\tau)}}
= \frac{1}{\sqrt{4\pi\kappa (t-\tau)}}.
\]
Thus,
\[
H(y,t;\tau)
= \frac{1}{\sqrt{4\pi\kappa (t-\tau)}}
\exp\!\left(-\frac{y^2}{4\kappa (t-\tau)}\right),
\qquad t>\tau.
\]
For $t<\tau$, we have $H(y,t;\tau)=0$.

Recalling that $y=x-\xi$ and $H(y,t;\tau) = G(y+\xi,t;\xi,\tau)$, we obtain the desired formula for the heat kernel:
\[
G(x,t;\xi,\tau)
= \frac{1}{\sqrt{4\pi\kappa (t-\tau)}}
\exp\!\left(-\frac{(x-\xi)^2}{4\kappa (t-\tau)}\right) H(t-\tau),
\]
where we have now explicitly reintroduced the Heaviside function $H(t-\tau)$ to encode the causality condition $G=0$ for $t<\tau$.

This establishes part (c). For each fixed $t>\tau$, $G(\cdot,t;\xi,\tau)$ is a Gaussian in $x$ centered at $\xi$ with variance $2\kappa (t-\tau)$. As $t-\tau$ increases, the Gaussian flattens and spreads out, representing the diffusion of heat away from the initial impulse.

\medskip

\emph{Step 4: Solution for general initial data as a spatial convolution.}

Let us now solve the homogeneous heat equation with initial data $u(x,0)=u_0(x)$ and no sources:
\[
u_t = \kappa u_{xx}, \qquad u(x,0) = u_0(x).
\]
The initial data $u_0$ may be regarded as a continuous superposition of point sources at time $\tau=0$, with strength $u_0(\xi)$ at point $\xi$. By linearity and superposition, the solution at later time $t>0$ is obtained by integrating the responses to each point source:
\[
u(x,t) = \int_{-\infty}^{\infty} G(x,t;\xi,0)\,u_0(\xi)\,d\xi.
\]
Substituting the explicit expression for $G$ at $\tau=0$, we obtain
\[
u(x,t) = \int_{-\infty}^{\infty}
\frac{1}{\sqrt{4\pi\kappa t}}
\exp\!\left(-\frac{(x-\xi)^2}{4\kappa t}\right) u_0(\xi)\,d\xi,
\qquad t>0.
\]

This formula may be written more compactly as a spatial convolution:
\[
u(\cdot,t) = (G(\cdot,t;\cdot,0) * u_0)(x),
\]
where the convolution is in the spatial variable and the kernel is the Gaussian heat kernel.

Conceptually, this is directly analogous to the convolution formulas for linear ODEs with Green functions. There, the solution $y(t)$ of a forced ODE is expressed as a time convolution of the forcing $f(s)$ with a temporal Green function $G(t,s)$, which represents the impulse response at time $t$ to a unit forcing at time $s$. Here, for the heat equation, the solution $u(x,t)$ is expressed as a spatial convolution of the initial data with a spatial Green function (the heat kernel), which represents the temperature at $x$ at time $t$ due to a unit impulse initially (at $t=0$) at location $\xi$.

In both settings, the central ideas of this chapter—impulse response, superposition, and convolution mediated by a Green function—reappear. The only difference is that, for the PDE, the superposition is over spatial locations (and, in more general problems with time-dependent forcing, also over time), while for the ODE it is solely over past times. This example thus previews how the Green function approach to linear dynamics extends naturally from ordinary to partial differential equations.
\end{solution}

\section{Linear Static Problems}
% --- Narrative plan (auto-generated) ---
% In this section we study linear static problems, that is, models in which all transient or time-dependent behaviors have settled and the system rests in equilibrium. Mathematically, these arise when we set time derivatives equal to zero in linear ordinary differential equations and obtain algebraic or reduced differential relations between the unknown quantities. The focus shifts from describing how solutions evolve to identifying which configurations can persist indefinitely under given forces, constraints, or inputs.
%
% Linear static problems matter throughout applied mathematics because many phenomena of interest are controlled not by how fast things change, but by how they ultimately balance. The temperature distribution in a thick wall that has been held at fixed boundary temperatures for a long time, the constant current levels in an electrical network under direct current (DC) forcing, and the deflection of a beam bolted at its ends and loaded by a static weight are all examples of linear static problems. These models are often the starting point for stability and bifurcation analysis in dynamical systems and for understanding the long-time behavior of solutions to time-dependent PDEs.
%
% Conceptually, linear static problems form a bridge between linear ODE theory, linear algebra, and the boundary value problems that appear in partial differential equations. Setting derivatives to zero reduces many dynamical equations to systems of linear equations, which brings in matrix methods, eigenvalues, and the structure of solution spaces. At the same time, steady-state or equilibrium equations for PDEs, such as Laplace’s or Poisson’s equation, share the same themes of balance and superposition that we explore here. The tools developed in this section—to identify, interpret, and compute equilibria—will reappear when we solve time-dependent ODEs, use Fourier methods, and study harmonic and potential problems in higher dimensions.

% ===== Example 1: Static equilibrium of a mass–spring system (inquiry-based) =====
\begin{problem}[Static equilibrium of a mass--spring system]
A common starting point for modeling mechanical systems is a single mass attached to one or more ideal (linear) springs. If we are only interested in the resting position under a constant force, such as gravity or a steady push, the motion eventually dies out and the system reaches a static equilibrium. In that regime, the usual differential equation from Newton's second law simplifies to an algebraic balance of forces. This example explores how that simplification works and how the resulting equation can be viewed as a very simple linear system.

Consider a block of mass $m$ that can move without friction along a horizontal line. It is attached to a rigid wall at $x=0$ by one or more ideal springs, and it is pulled to the right by a constant force $F>0$. Let $x(t)$ denote the displacement of the block from its \emph{natural} (unstretched) position at time $t$.

\smallskip

(a) Suppose first that there is a \emph{single} spring of stiffness $k>0$ between the wall and the block, and that there is also a linear damping force of the form $-b\,x'(t)$ with $b>0$. Using Newton's second law, write down the differential equation for $x(t)$ that balances all forces on the mass. State clearly which forces you include and what sign you assign to each.

\emph{Hint:} Count forces in the horizontal direction. Take displacements to the right as positive. Recall that an ideal spring of stiffness $k$ exerts a restoring force $-k\,x$ when stretched by an amount $x$.

\smallskip

(b) We now focus on the \emph{static equilibrium} of this system. In words, what does ``static equilibrium'' mean for the motion $x(t)$? Translate this verbal description into conditions on $x'(t)$ and $x''(t)$. Then impose these conditions on your differential equation from part (a) and obtain a \emph{purely algebraic} equation for the equilibrium displacement $x_\ast$.

\emph{Hint:} At equilibrium, the block is at rest and not accelerating.

\smallskip

(c) Solve the algebraic equation from part (b) to express the equilibrium displacement $x_\ast$ in terms of $F$ and $k$. What is the physical meaning of the sign of $x_\ast$? How does $x_\ast$ change if you double the stiffness $k$ while keeping $F$ fixed?

\emph{Hint:} You should obtain a linear relation between $x_\ast$ and $F$.

\smallskip

Now keep the same setup, but change the spring arrangement.

\smallskip

(d) Suppose that instead of a single spring, the block is attached to the wall by \emph{two} ideal springs in parallel, with stiffnesses $k_1>0$ and $k_2>0$. Both springs are attached between the same wall and the same block, so they stretch by the same amount $x(t)$ when the block moves.

(i) Using Hooke's law for each spring, write an expression for the force exerted by each spring on the block. Then show that the total restoring force exerted by the two springs together can be written in the form $-k_{\text{eff}}\,x$, and find the effective stiffness $k_{\text{eff}}$.

\emph{Hint:} The forces from the two springs add, because they act in the same direction on the block.

(ii) Specialize your dynamic equation from part (a) to this two-spring case (you may ignore damping if you wish), and then find the corresponding algebraic equation for the static equilibrium displacement $x_\ast$ under the constant force $F$.

(iii) Solve for $x_\ast$ in terms of $F$, $k_1$, and $k_2$. Compare it with your answer from part (c), and interpret the effect of adding an extra spring in parallel.

\smallskip

(e) Generalize the previous part to $n$ parallel springs with stiffnesses $k_1,\dots,k_n>0$ between the wall and the block.

(i) Write down the total restoring force as a function of $x$, and deduce a formula for the effective stiffness $k_{\text{eff}}$ in terms of the $k_i$.

(ii) Write the static equilibrium condition as a \emph{linear} equation of the form
\[
K\,x_\ast = F,
\]
where $K$ is a number that you should express in terms of the $k_i$. How is this a $1\times 1$ example of the matrix equation ``stiffness matrix times displacement vector equals load vector'' that appears in more complicated static problems?

\smallskip

(f) (Extensions and ``what if'' questions.)

(i) Suppose that the external force depends linearly on the displacement, so that $F_{\text{ext}} = F_0 + \alpha x$ for some constants $F_0$ and $\alpha$. How does your static equilibrium equation change in this case, and under what condition on $\alpha$ and the stiffnesses does a unique equilibrium still exist?

\emph{Hint:} Move all terms involving $x$ to the left-hand side and compare with your previous expression for $K$.

(ii) Imagine instead that the block is attached to a \emph{vertical} spring and is pulled downward by gravity $mg$ as well as by the spring. Which parts of your analysis need to be modified, and which parts stay the same? What would the static equilibrium equation look like in that case?
\end{problem}

% ===== Example 1: Static equilibrium of a mass–spring system (full solution) =====
\begin{problem}[Static equilibrium of a mass--spring system]
A block of mass $m$ moves without friction along a horizontal line and is attached to a rigid wall by two ideal springs in parallel, with stiffnesses $k_1>0$ and $k_2>0$. The block is pulled to the right by a constant horizontal force $F>0$. Let $x(t)$ be the displacement of the block from its natural (unstretched) position at time $t$, with $x>0$ to the right. 

(a) Using Newton's second law and Hooke's law, write down the differential equation governing $x(t)$ if there is also a linear damping force $-b\,x'(t)$ with $b\ge 0$.  

(b) Define what is meant by a \emph{static equilibrium} position $x_\ast$ for this system, and derive the algebraic equation that $x_\ast$ must satisfy.  

(c) Solve explicitly for $x_\ast$ in terms of $F$, $k_1$, and $k_2$, and interpret your answer in terms of an \emph{effective stiffness}.  

(d) Briefly explain how this example illustrates the general idea of a linear static problem, and how the equilibrium equation can be viewed as a $1\times 1$ linear system $Kx_\ast = F$.
\end{problem}

\begin{solution}
We begin by identifying all the forces acting on the mass and then applying Newton's second law in the horizontal direction.

\medskip

\textbf{(a) Dynamic equation.}  
The displacement $x(t)$ is measured from the position where both springs are at their natural (unstretched) length. When the block is displaced by an amount $x$, each spring stretches (or compresses) by the same amount $x$.

By Hooke's law, the restoring force from the first spring on the block is $-k_1 x$, and from the second spring is $-k_2 x$. Both forces act to the left when $x>0$. The total spring force is therefore the sum
\[
F_{\text{springs}} = -k_1 x - k_2 x = -(k_1 + k_2)\,x.
\]
The damping force, assumed linear in velocity, is $F_{\text{damp}} = -b\,x'(t)$, acting opposite to the direction of motion. The external applied force is $F_{\text{ext}} = F$, acting to the right and taken as positive.

Newton's second law says that mass times acceleration equals the sum of all forces:
\[
m\,x''(t) = F_{\text{springs}} + F_{\text{damp}} + F_{\text{ext}}.
\]
Substituting the expressions for the forces, we obtain
\[
m\,x''(t) = -(k_1 + k_2)\,x(t) - b\,x'(t) + F.
\]
Rewriting this in the standard form with all terms on the left-hand side gives
\[
m\,x''(t) + b\,x'(t) + (k_1 + k_2)\,x(t) = F.
\]
This is a linear second-order ordinary differential equation with constant coefficients and a constant right-hand side.

\medskip

\textbf{(b) Static equilibrium and the algebraic equation.}  
A \emph{static equilibrium} is a position at which the block can remain forever if placed there carefully, with no subsequent motion. In terms of $x(t)$, this means that the position is constant in time, so the velocity and acceleration both vanish:
\[
x(t) \equiv x_\ast,\qquad x'(t) = 0,\qquad x''(t) = 0.
\]

To find $x_\ast$, we substitute these conditions into the differential equation. From
\[
m\,x''(t) + b\,x'(t) + (k_1 + k_2)\,x(t) = F,
\]
we set $x''(t)=0$ and $x'(t)=0$ and replace $x(t)$ by the constant $x_\ast$. This yields
\[
0 + 0 + (k_1 + k_2)\,x_\ast = F,
\]
or simply
\[
(k_1 + k_2)\,x_\ast = F.
\]
The equilibrium position $x_\ast$ therefore satisfies a purely algebraic linear equation, obtained from the dynamic equation by dropping the time-derivative terms. Physically, this expresses the balance between the total spring force and the applied load $F$ in the absence of motion.

\medskip

\textbf{(c) Solving for the equilibrium and effective stiffness.}  
The algebraic equilibrium equation
\[
(k_1 + k_2)\,x_\ast = F
\]
is a linear equation in the unknown $x_\ast$. Since $k_1>0$ and $k_2>0$, we have $k_1 + k_2 > 0$, so there is a unique solution:
\[
x_\ast = \frac{F}{k_1 + k_2}.
\]
It is natural to interpret $k_{\text{eff}} = k_1 + k_2$ as the \emph{effective stiffness} of the two parallel springs, because the static displacement under a given load $F$ is exactly what it would be for a single spring with stiffness $k_{\text{eff}}$. In words, adding a second spring in parallel makes the combined system stiffer and therefore reduces the equilibrium displacement under the same force.

One can also see this as a linear input--output relation: the applied load $F$ is proportional to the displacement $x_\ast$, with proportionality constant $k_{\text{eff}}$. The fact that $x_\ast$ is proportional to $F$ is a hallmark of linear static behavior.

\medskip

\textbf{(d) Relation to linear static problems and a $1\times 1$ system.}  
The original dynamic model
\[
m\,x''(t) + b\,x'(t) + (k_1 + k_2)\,x(t) = F
\]
is a linear ordinary differential equation with constant coefficients and a constant right-hand side. When we restrict attention to static equilibrium, we assume that the time derivatives $x'(t)$ and $x''(t)$ are zero, leaving only the terms that do not involve derivatives:
\[
(k_1 + k_2)\,x_\ast = F.
\]
This is the simplest example of a \emph{linear static problem}: the governing differential operator is linear, and when all time derivatives are set to zero, we obtain a linear algebraic equation relating the unknown (here, the equilibrium displacement) to the data (here, the applied load and the spring parameters).

In the language of linear algebra, the static equilibrium equation can be written as
\[
K\,x_\ast = F,
\]
where $K = k_1 + k_2$ is a $1\times 1$ \emph{stiffness matrix} and $x_\ast$ and $F$ are $1\times 1$ column vectors (that is, scalars). In more complicated mechanical systems with many degrees of freedom, $K$ becomes a larger matrix, $x_\ast$ a vector of displacements, and $F$ a vector of loads, but the basic form ``stiffness times displacement equals load'' remains the same. Thus this mass--spring example provides a concrete, one-dimensional illustration of the main idea behind linear static problems: equilibrium configurations are found by solving linear algebraic equations derived from a linearized balance of forces.
\end{solution}

% ===== Example 2: Direct-current equilibrium in a resistor network (inquiry-based) =====
\begin{problem}[Direct-current equilibrium in a resistor network]
A simple direct-current (DC) circuit can be modeled as a network of ideal resistors and sources. In the static regime, after all transients have died out, the currents and voltages no longer depend on time. At this stage, Kirchhoff's current and voltage laws express conservation of charge and energy and give linear relations among the unknown node potentials and branch currents. In this problem you will set up and solve such a linear system for a small network, and then relate the result to the general idea of ``static'' solutions of differential equations.

Consider the following resistor network. There is a distinguished reference node (ground) whose potential is defined to be zero. An ideal DC voltage source of $12\ \mathrm{V}$ connects ground to a node $S$, with its positive terminal at $S$, so that $V_S = 12\ \mathrm{V}$. From node $S$ a resistor of resistance $R_1 = 6\ \Omega$ leads to node $A$. From node $A$ there are two connections: a resistor $R_2 = 3\ \Omega$ leads to node $B$, and a resistor $R_4 = 6\ \Omega$ leads to ground. Finally, from node $B$ a resistor $R_3 = 6\ \Omega$ leads to ground. All components are ideal. 

\medskip

(a) Introduce unknowns for the electrical state of the network. Take ground as the zero of potential. Let $V_A$ and $V_B$ denote the node voltages at $A$ and $B$ relative to ground. Let $I_1$, $I_2$, $I_3$, and $I_4$ denote the steady currents through $R_1$, $R_2$, $R_3$, and $R_4$, with the convention that each current flows from the higher-potential node to the lower-potential node.  

Describe in words how $V_A$, $V_B$, and the branch currents are related by Ohm's law and by Kirchhoff's current law (KCL). Which conservation principle does KCL represent in this context?

\medskip

(b) Now translate your verbal description into equations.  

\quad(i) Use Ohm's law to express each current $I_k$ in terms of the node voltages at the ends of the corresponding resistor. For example, write $I_1$ in terms of $V_S = 12\ \mathrm{V}$ and $V_A$.  

\quad(ii) Apply Kirchhoff's current law at node $A$ and at node $B$. For each node, write an equation stating that the sum of currents \emph{leaving} the node is zero.  

Hint: For node $A$, include the currents through $R_1$ (between $S$ and $A$), $R_2$ (between $A$ and $B$), and $R_4$ (between $A$ and ground). For node $B$, include the currents through $R_2$ and $R_3$.

\medskip

(c) Eliminate the currents $I_1,\dots,I_4$ from your equations so that you obtain a closed system of linear equations for the unknown node voltages $V_A$ and $V_B$ only.  

Write this system explicitly in the form
\[
A
\begin{pmatrix}
V_A \\[4pt]
V_B
\end{pmatrix}
=
\begin{pmatrix}
b_1 \\[4pt]
b_2
\end{pmatrix},
\]
and identify the $2\times 2$ coefficient matrix $A$ and the right-hand side vector $b$.  

Hint: Carefully substitute your Ohm's law expressions for the currents into the KCL equations and simplify.

\medskip

(d) Solve your $2\times 2$ linear system for $V_A$ and $V_B$. Then recover the currents $I_1,\dots,I_4$ from your Ohm's law formulas. Provide numerical values for $V_A$, $V_B$, and for each current.  

Check your work by verifying Kirchhoff's current law at node $A$ and at node $B$ using your computed currents. As an additional consistency check, compute the power delivered by the source and the total power dissipated in the four resistors, and compare them.

\medskip

(e) Explore two extensions that connect this static network problem to broader modeling ideas.

\begin{itemize}
  \item[(i)] Suppose the resistor $R_4$ between node $A$ and ground is removed from the circuit (this corresponds to replacing $R_4$ by an infinite resistance, or an ``open circuit''). How do the KCL equations at node $A$ and at node $B$ change? Without doing all the algebra, predict qualitatively how $V_A$ and $V_B$ compare to the values you found in part~(d). Do they increase, decrease, or stay the same? Explain your reasoning in terms of current paths and effective resistance.

  \item[(ii)] In more general circuits, capacitors and inductors introduce time dependence and lead to differential equations. An ideal capacitor of capacitance $C$ obeys $I_C = C\,\frac{dV}{dt}$ relating its current and voltage, and an ideal inductor of inductance $L$ obeys $V_L = L\,\frac{dI}{dt}$ relating its voltage and current. Explain, using these relations, why in a \emph{steady} DC regime (where all currents and voltages are constant in time) an ideal capacitor behaves like an open circuit and an ideal inductor behaves like a short circuit. How does this observation justify ignoring capacitors and inductors in the static analysis you performed above?
\end{itemize}

\end{problem}

% ===== Example 2: Direct-current equilibrium in a resistor network (full solution) =====
\begin{problem}[Direct-current equilibrium in a resistor network]
Consider a DC resistor network with a reference node (ground, $0\ \mathrm{V}$) and an ideal $12\ \mathrm{V}$ voltage source whose positive terminal is at node $S$, so that $V_S = 12\ \mathrm{V}$. From $S$ a resistor $R_1 = 6\ \Omega$ leads to node $A$. From $A$ a resistor $R_2 = 3\ \Omega$ leads to node $B$, and another resistor $R_4 = 6\ \Omega$ leads to ground. From $B$ a resistor $R_3 = 6\ \Omega$ leads to ground. All components are ideal, and the circuit has reached a time-independent (DC) equilibrium.

Using node-voltage analysis:
\begin{enumerate}
  \item Set up the Kirchhoff current law (KCL) equations at nodes $A$ and $B$ in terms of the node voltages $V_A$ and $V_B$.
  \item Reduce these equations to a $2\times 2$ linear system for $(V_A, V_B)$ and solve for $V_A$ and $V_B$.
  \item Compute the steady currents through each resistor and the current supplied by the source. Verify KCL at both nodes and show that the power delivered by the source equals the total power dissipated in the resistors.
\end{enumerate}
\end{problem}

\begin{solution}
We analyze the network using the node-voltage method with ground as the reference. The unknown node potentials are $V_A$ and $V_B$ (relative to ground). The source node has fixed potential $V_S = 12\ \mathrm{V}$.

\medskip

\textbf{Step 1: Express currents using Ohm's law.}

Each branch current is determined by the voltage difference across the corresponding resistor. With the convention that current flows from higher potential to lower potential, we have:
\begin{align*}
I_1 &= \frac{V_S - V_A}{R_1} = \frac{12 - V_A}{6}, &&\text{(from $S$ to $A$ through $R_1$)}, \\
I_2 &= \frac{V_A - V_B}{R_2} = \frac{V_A - V_B}{3}, &&\text{(from $A$ to $B$ through $R_2$)}, \\
I_3 &= \frac{V_B - 0}{R_3} = \frac{V_B}{6}, &&\text{(from $B$ to ground through $R_3$)}, \\
I_4 &= \frac{V_A - 0}{R_4} = \frac{V_A}{6}, &&\text{(from $A$ to ground through $R_4$)}.
\end{align*}
These are direct applications of Ohm's law, $I = \dfrac{V_\text{drop}}{R}$.

\medskip

\textbf{Step 2: Apply Kirchhoff's current law at each node.}

Kirchhoff's current law (KCL) is a mathematical statement of conservation of charge at each node: in the steady state, no net charge accumulates, so the algebraic sum of currents leaving a node is zero.

\emph{Node $A$.} Currents can leave node $A$ along three paths:
\begin{itemize}
  \item through $R_1$ toward node $S$, with current $(V_A - V_S)/R_1 = (V_A - 12)/6$,
  \item through $R_2$ toward node $B$, with current $(V_A - V_B)/3$,
  \item through $R_4$ toward ground, with current $V_A/6$.
\end{itemize}
Summing these currents and setting the total to zero gives
\[
\frac{V_A - 12}{6} + \frac{V_A - V_B}{3} + \frac{V_A}{6} = 0.
\]

\emph{Node $B$.} Currents can leave node $B$ along two paths:
\begin{itemize}
  \item through $R_2$ toward node $A$, with current $(V_B - V_A)/3$,
  \item through $R_3$ toward ground, with current $V_B/6$.
\end{itemize}
Thus KCL at $B$ gives
\[
\frac{V_B - V_A}{3} + \frac{V_B}{6} = 0.
\]

These two equations express charge conservation at the internal nodes of the network.

\medskip

\textbf{Step 3: Simplify to a $2\times 2$ linear system.}

We now simplify the KCL equations to obtain a standard linear system in $V_A$ and $V_B$.

For node $A$:
\[
\frac{V_A - 12}{6} + \frac{V_A - V_B}{3} + \frac{V_A}{6} = 0.
\]
Multiply by $6$ to clear denominators:
\[
(V_A - 12) + 2(V_A - V_B) + V_A = 0.
\]
Collect terms:
\[
V_A - 12 + 2V_A - 2V_B + V_A = 0
\quad\Longrightarrow\quad
4V_A - 2V_B = 12.
\]

For node $B$:
\[
\frac{V_B - V_A}{3} + \frac{V_B}{6} = 0.
\]
Multiply by $6$:
\[
2(V_B - V_A) + V_B = 0
\quad\Longrightarrow\quad
2V_B - 2V_A + V_B = 0
\quad\Longrightarrow\quad
-2V_A + 3V_B = 0.
\]

Thus, in matrix form,
\[
\begin{pmatrix}
4 & -2 \\[4pt]
-2 & 3
\end{pmatrix}
\begin{pmatrix}
V_A \\[4pt]
V_B
\end{pmatrix}
=
\begin{pmatrix}
12 \\[4pt]
0
\end{pmatrix}.
\]
This is a symmetric positive definite system characteristic of many linear static equilibrium problems (including resistor networks and discretized elliptic equations).

\medskip

\textbf{Step 4: Solve for the node voltages.}

We solve the system
\[
\begin{cases}
4V_A - 2V_B = 12, \\
-2V_A + 3V_B = 0.
\end{cases}
\]

From the second equation,
\[
-2V_A + 3V_B = 0
\quad\Longrightarrow\quad
3V_B = 2V_A
\quad\Longrightarrow\quad
V_B = \dfrac{2}{3} V_A.
\]

Substitute this expression into the first equation:
\[
4V_A - 2\left(\frac{2}{3}V_A\right) = 12
\quad\Longrightarrow\quad
4V_A - \frac{4}{3}V_A = 12
\quad\Longrightarrow\quad
\left(\frac{12}{3} - \frac{4}{3}\right)V_A = 12
\quad\Longrightarrow\quad
\frac{8}{3}V_A = 12.
\]
Hence
\[
V_A = 12\cdot \frac{3}{8} = \frac{36}{8} = 4.5\ \mathrm{V}.
\]
Then
\[
V_B = \frac{2}{3}V_A = \frac{2}{3}\cdot 4.5 = 3.0\ \mathrm{V}.
\]

So the node voltages are
\[
V_A = 4.5\ \mathrm{V},
\qquad
V_B = 3.0\ \mathrm{V}.
\]

\medskip

\textbf{Step 5: Compute currents and verify KCL.}

Using the previously derived Ohm's law expressions, we obtain:
\begin{align*}
I_1 &= \frac{12 - V_A}{6}
= \frac{12 - 4.5}{6}
= \frac{7.5}{6} = 1.25\ \mathrm{A}, \\
I_2 &= \frac{V_A - V_B}{3}
= \frac{4.5 - 3.0}{3}
= \frac{1.5}{3} = 0.50\ \mathrm{A}, \\
I_3 &= \frac{V_B}{6}
= \frac{3.0}{6}
= 0.50\ \mathrm{A}, \\
I_4 &= \frac{V_A}{6}
= \frac{4.5}{6}
= 0.75\ \mathrm{A}.
\end{align*}

We now check KCL numerically.

At node $A$, the currents \emph{leaving} are:
\[
I_{A\to S} = \frac{V_A - 12}{6} = -1.25\ \mathrm{A},
\quad
I_{A\to B} = 0.50\ \mathrm{A},
\quad
I_{A\to \text{ground}} = 0.75\ \mathrm{A}.
\]
Summing gives
\[
(-1.25) + 0.50 + 0.75 = 0,
\]
so KCL holds at node $A$. Interpreted physically, $1.25\ \mathrm{A}$ enters node $A$ from the source, and the same total current $0.50 + 0.75 = 1.25\ \mathrm{A}$ leaves toward node $B$ and ground.

At node $B$, the currents leaving are:
\[
I_{B\to A} = \frac{V_B - V_A}{3} = -0.50\ \mathrm{A},
\quad
I_{B\to \text{ground}} = 0.50\ \mathrm{A}.
\]
Thus
\[
(-0.50) + 0.50 = 0,
\]
so KCL holds at node $B$ as well. Here, $0.50\ \mathrm{A}$ enters node $B$ from node $A$ and the same amount flows to ground.

The source current is precisely $I_1 = 1.25\ \mathrm{A}$, since the only connection from node $S$ to the rest of the circuit is through $R_1$.

\medskip

\textbf{Step 6: Check power balance (energy conservation).}

The power delivered by the ideal voltage source is
\[
P_\text{source} = V_S \cdot I_\text{source} = 12\ \mathrm{V} \cdot 1.25\ \mathrm{A} = 15\ \mathrm{W}.
\]

We now compute the power dissipated in each resistor using $P = I^2 R$:
\begin{align*}
P_{R_1} &= I_1^2 R_1 = (1.25)^2 \cdot 6 = 1.5625 \cdot 6 = 9.375\ \mathrm{W}, \\
P_{R_2} &= I_2^2 R_2 = (0.50)^2 \cdot 3 = 0.25 \cdot 3 = 0.75\ \mathrm{W}, \\
P_{R_3} &= I_3^2 R_3 = (0.50)^2 \cdot 6 = 0.25 \cdot 6 = 1.5\ \mathrm{W}, \\
P_{R_4} &= I_4^2 R_4 = (0.75)^2 \cdot 6 = 0.5625 \cdot 6 = 3.375\ \mathrm{W}.
\end{align*}
The total resistive power dissipation is
\[
P_{R_1} + P_{R_2} + P_{R_3} + P_{R_4}
= 9.375 + 0.75 + 1.5 + 3.375 = 15\ \mathrm{W}.
\]

Thus
\[
P_\text{source} = 15\ \mathrm{W} = P_{R_1} + P_{R_2} + P_{R_3} + P_{R_4},
\]
which is a statement of energy conservation in the static regime: all power delivered by the source is dissipated as heat in the resistors.

\medskip

\textbf{Conceptual remark: relation to linear static problems.}

This example illustrates the idea of a linear static problem as used in the study of ordinary differential equations. A more complete dynamical model of an electrical circuit with capacitors and inductors leads to a system of linear ODEs in time for the node voltages and branch currents. The DC equilibrium corresponds to a \emph{steady state} of that system, where all time derivatives vanish. Algebraically, setting the derivatives to zero in the ODE system yields exactly a linear system of the form
\[
A\,u = b,
\]
where $u$ collects the steady voltages and currents. Our resistor network problem is precisely such a static equilibrium: the matrix
\[
A =
\begin{pmatrix}
4 & -2 \\
-2 & 3
\end{pmatrix}
\]
encodes the network connectivity and conductances, and solving $A u = b$ gives the unique DC state consistent with charge and energy conservation. This is typical of linear static problems arising as the equilibrium limit of more general dynamical systems.

\end{solution}

% ===== Example 3: Steady concentrations in a simple reaction network (inquiry-based) =====
\begin{problem}[Steady concentrations in a simple reaction network]
In a well-mixed chemical reactor, several species may be created, transformed into one another, and removed. When the reactor has been running for a long time under constant conditions, the concentrations may approach a steady state in which they no longer change in time. Mathematically, this steady state corresponds to setting all time derivatives equal to zero in the governing differential equations. The resulting equations form a linear \emph{static} problem, which we can analyze using tools from linear algebra.

Consider three chemical species $A$, $B$, and $C$ with time-dependent concentrations $a(t)$, $b(t)$, and $c(t)$ (for $t \ge 0$). The reactor is fed from an external reservoir that produces species $A$ at a constant rate $\alpha > 0$ (for instance, in units of moles per liter per second). Inside the reactor, the following reactions occur:
\[
A \xrightarrow{k_1} B,\qquad
B \xrightarrow{k_2} C,\qquad
C \xrightarrow{k_3} \varnothing,
\]
where $k_1, k_2, k_3 > 0$ are rate constants, and $\varnothing$ denotes removal of $C$ from the system (for example, by decay or outflow).

\smallskip

(a) Using the usual well-mixed, linear mass-action assumptions, write down the system of ordinary differential equations satisfied by $a(t)$, $b(t)$, and $c(t)$.  
Explain in words how you obtain each term in each equation from the reaction network.

% Hint: Each reaction contributes a term proportional to the concentration of the reactant. Inflow adds a positive constant term; conversion or decay removes mass from one species and may add it to another.

\smallskip

(b) Rewrite your system compactly in matrix form
\[
\mathbf{x}'(t) = M\,\mathbf{x}(t) + \mathbf{f},
\]
where $\mathbf{x}(t) = \begin{pmatrix} a(t) \\[2pt] b(t) \\[2pt] c(t) \end{pmatrix}$.  
Identify the $3\times 3$ matrix $M$ and the vector $\mathbf{f}$ explicitly.

% Hint: The entries of $M$ come from coefficients multiplying $a$, $b$, and $c$ in your equations; the constant inflow $\alpha$ becomes part of $\mathbf{f}$.

\smallskip

(c) A steady state (or equilibrium) is a vector $\mathbf{x}^* = \begin{pmatrix} a^* \\ b^* \\ c^* \end{pmatrix}$ such that the concentrations do not change in time when $\mathbf{x}(t) \equiv \mathbf{x}^*$.  
Derive the algebraic system of equations that $\mathbf{x}^*$ must satisfy.  
Express this system both componentwise and in matrix form.

% Hint: Set $\mathbf{x}'(t) = \mathbf{0}$ in your matrix equation and simplify.

\smallskip

(d) Solve the steady-state equations you found in part (c) and obtain explicit formulas for $a^*$, $b^*$, and $c^*$ in terms of $\alpha$, $k_1$, $k_2$, and $k_3$.  
Explain briefly why your solution is unique in this model.  
Interpret your formulas: what simple pattern do you notice relating each steady concentration to the corresponding rate constant?

% Hint: The system is triangular: $a^*$ appears only in the first equation, then $b^*$ in terms of $a^*$, and so on. For uniqueness, think about whether the linear system $M \mathbf{x}^* + \mathbf{f} = \mathbf{0}$ can have more than one solution when $k_1,k_2,k_3>0$.

\smallskip

(e) Explore two ``what if'' modifications of the model.

\quad (i) Suppose the removal of $C$ is very slow or absent, so that $k_3 = 0$ while $\alpha > 0$ and $k_1,k_2>0$ remain fixed.  
Write down the new steady-state equations and discuss whether a steady state with finite concentrations $a^*,b^*,c^*$ can exist. How does this reflect the physical behavior of the reactor?

\quad (ii) Suppose instead that there is no external feed, so $\alpha = 0$, while all rate constants $k_1,k_2,k_3$ are positive.  
Determine all steady states of the system in this case.  
How does this differ qualitatively from the case $\alpha > 0$?

% Hint: For (i), think about the equation that previously defined $c^*$. For (ii), you now have a homogeneous linear system $M \mathbf{x}^* = \mathbf{0}$.

\end{problem}

% ===== Example 3: Steady concentrations in a simple reaction network (full solution) =====
\begin{problem}[Steady concentrations in a simple reaction network]
In a well-mixed reactor, three chemical species $A$, $B$, and $C$ have concentrations $a(t)$, $b(t)$, and $c(t)$, respectively. Species $A$ is supplied from an external source at a constant rate $\alpha > 0$, and the species undergo the linear reactions
\[
A \xrightarrow{k_1} B,\qquad
B \xrightarrow{k_2} C,\qquad
C \xrightarrow{k_3} \varnothing,
\]
with rate constants $k_1, k_2, k_3 > 0$.  

(a) Derive the system of linear ODEs for $a(t)$, $b(t)$, and $c(t)$, and write it in matrix form $\mathbf{x}'(t) = M \mathbf{x}(t) + \mathbf{f}$, where $\mathbf{x}(t) = (a(t), b(t), c(t))^{\mathsf{T}}$.  

(b) Find the steady-state concentrations $a^*$, $b^*$, and $c^*$ for $\alpha > 0$ and $k_1,k_2,k_3>0$, and show that the steady state is unique.  

(c) Discuss what happens to the existence of a finite steady state if $k_3 = 0$ while $\alpha>0$ remains fixed.

\end{problem}

\begin{solution}
We begin by translating the reaction description into differential equations. Under the standard well-mixed, linear mass-action assumption, each reaction contributes a term proportional to the concentration of its reactant.

\medskip

\textbf{(a) Derivation of the ODE system and matrix form.}

First consider species $A$.  
There is a constant inflow at rate $\alpha$, which adds the term $+\alpha$.  
Species $A$ is lost only through the reaction $A \to B$ at rate $k_1 a(t)$, which contributes the term $-k_1 a(t)$.  
Thus
\[
a'(t) = \alpha - k_1 a(t).
\]

Next consider species $B$.  
It is produced from $A$ at rate $k_1 a(t)$, and it is consumed in the reaction $B \to C$ at rate $k_2 b(t)$.  
Therefore
\[
b'(t) = k_1 a(t) - k_2 b(t).
\]

Finally consider species $C$.  
It is produced from $B$ at rate $k_2 b(t)$ and is removed from the system at rate $k_3 c(t)$ by the reaction $C \to \varnothing$.  
Hence
\[
c'(t) = k_2 b(t) - k_3 c(t).
\]

Collecting these, we obtain the linear system of ODEs
\[
\begin{cases}
a'(t) = \alpha - k_1 a(t),\\[4pt]
b'(t) = k_1 a(t) - k_2 b(t),\\[4pt]
c'(t) = k_2 b(t) - k_3 c(t).
\end{cases}
\]

To write this in matrix form, we set
\[
\mathbf{x}(t) = \begin{pmatrix} a(t) \\[2pt] b(t) \\[2pt] c(t) \end{pmatrix}.
\]
Then we can read off the coefficients of $a(t)$, $b(t)$, and $c(t)$ in each equation. The terms involving the concentrations form the matrix $M$, and the constant inflow forms the vector $\mathbf{f}$:
\[
M =
\begin{pmatrix}
-\,k_1 & 0 & 0 \\
k_1 & -\,k_2 & 0 \\
0 & k_2 & -\,k_3
\end{pmatrix},
\qquad
\mathbf{f} =
\begin{pmatrix}
\alpha \\[2pt] 0 \\[2pt] 0
\end{pmatrix}.
\]
Thus the system can be written compactly as
\[
\mathbf{x}'(t) = M\,\mathbf{x}(t) + \mathbf{f}.
\]

This form makes it clear that we are dealing with a linear, nonhomogeneous system of ODEs.

\medskip

\textbf{(b) Steady-state concentrations and uniqueness.}

A steady state (or equilibrium) is a time-independent solution $\mathbf{x}(t) \equiv \mathbf{x}^*$ for which $\mathbf{x}'(t) = \mathbf{0}$. Substituting $\mathbf{x}(t) = \mathbf{x}^*$ into the matrix equation gives
\[
\mathbf{0} = M\,\mathbf{x}^* + \mathbf{f},
\]
or equivalently
\[
M\,\mathbf{x}^* = -\,\mathbf{f}.
\]
In component form, this means that $a^*$, $b^*$, and $c^*$ must satisfy
\[
\begin{cases}
0 = \alpha - k_1 a^*,\\[4pt]
0 = k_1 a^* - k_2 b^*,\\[4pt]
0 = k_2 b^* - k_3 c^*.
\end{cases}
\]

This system is triangular, so we can solve it sequentially.

From the first equation we obtain
\[
\alpha - k_1 a^* = 0
\quad\Longrightarrow\quad
a^* = \dfrac{\alpha}{k_1}.
\]

Substituting this into the second equation gives
\[
k_1 a^* - k_2 b^* = 0
\quad\Longrightarrow\quad
k_1 \left(\dfrac{\alpha}{k_1}\right) - k_2 b^* = 0
\quad\Longrightarrow\quad
b^* = \dfrac{\alpha}{k_2}.
\]

Finally, substituting $b^*$ into the third equation yields
\[
k_2 b^* - k_3 c^* = 0
\quad\Longrightarrow\quad
k_2 \left(\dfrac{\alpha}{k_2}\right) - k_3 c^* = 0
\quad\Longrightarrow\quad
c^* = \dfrac{\alpha}{k_3}.
\]

Therefore, for $\alpha > 0$ and $k_1,k_2,k_3>0$, the unique steady state is
\[
\boxed{
a^* = \dfrac{\alpha}{k_1}, \quad
b^* = \dfrac{\alpha}{k_2}, \quad
c^* = \dfrac{\alpha}{k_3}.
}
\]

A simple pattern emerges: each steady concentration equals the input rate $\alpha$ divided by the effective removal rate for that species. Although species $A$ and $B$ are removed only by conversion and not by direct decay, in steady state the conversion acts exactly like a removal process with rate constants $k_1$ and $k_2$, respectively.

To see why this steady state is unique, observe that $M$ is a lower triangular matrix with diagonal entries $-k_1$, $-k_2$, and $-k_3$. Since each $k_i>0$, none of the diagonal entries is zero, so
\[
\det M = (-k_1)(-k_2)(-k_3) \neq 0.
\]
Thus $M$ is invertible, and the linear system $M \mathbf{x}^* = -\mathbf{f}$ has exactly one solution. In terms of the broader theme of \emph{linear static problems}, we see that the steady state is found by solving a linear algebraic system, and invertibility of the coefficient matrix guarantees a unique equilibrium.

\medskip

\textbf{(c) Effect of removing the sink for $C$ ($k_3 = 0$).}

Now suppose that $k_3 = 0$ while $\alpha > 0$ and $k_1,k_2>0$ remain fixed. Physically, this means that the final species $C$ is no longer removed from the system. The ODE for $c(t)$ becomes
\[
c'(t) = k_2 b(t) - 0\cdot c(t) = k_2 b(t).
\]
The steady-state equations are then
\[
\begin{cases}
0 = \alpha - k_1 a^*,\\[4pt]
0 = k_1 a^* - k_2 b^*,\\[4pt]
0 = k_2 b^*.
\end{cases}
\]

From the third equation we obtain $k_2 b^* = 0$, so $b^* = 0$ because $k_2>0$.  
However, the second equation says $k_1 a^* - k_2 b^* = 0$, hence $k_1 a^* = k_2 b^* = 0$, so $a^* = 0$ as well.  
This contradicts the first equation, which requires
\[
0 = \alpha - k_1 a^* = \alpha - 0 = \alpha > 0.
\]
Therefore, there is no triple $(a^*,b^*,c^*)$ with finite components that satisfies all three equations simultaneously when $k_3 = 0$ and $\alpha > 0$.

In matrix terms, when $k_3 = 0$ the coefficient matrix $M$ loses invertibility: its last diagonal entry is zero, and $\det M = 0$. The linear static problem $M \mathbf{x}^* = -\mathbf{f}$ has no solution. Physically, if there is a constant inflow of material at rate $\alpha$ and no ultimate sink for $C$, then mass continually accumulates in the reactor, and the system cannot reach a time-independent steady state with bounded concentrations.

\medskip

\textbf{Connection to linear static problems.}

This example illustrates the general principle that steady states of linear ODE systems
\[
\mathbf{x}'(t) = A \mathbf{x}(t) + \mathbf{b}
\]
are obtained by solving the \emph{static} linear system
\[
A \mathbf{x}^* + \mathbf{b} = 0.
\]
Here the reaction network yields a specific matrix $M$ and source vector $\mathbf{f}$, and the existence and uniqueness of the steady state are governed by algebraic properties of $M$ (in particular, whether $M$ is invertible). Thus, analyzing equilibria of dynamical systems naturally leads to linear static problems in linear algebra.

\end{solution}

% ===== Example 4: Static temperature profile in a one-dimensional bar (inquiry-based) =====
\begin{problem}[Static temperature profile in a one-dimensional bar]
Consider a long, thin, homogeneous metal bar of length $L$. Its lateral surface is perfectly insulated, so that heat can flow only along the length of the bar. The left end of the bar is held at a fixed temperature $T_0$ and the right end at a fixed temperature $T_L$. After a long time, the temperature no longer changes in time and the bar reaches a \emph{steady state}. In this regime, the temperature depends only on the spatial coordinate $x$ along the bar and satisfies a simpler static equation.

We will derive and solve the resulting ordinary differential equation for the steady-state temperature $T(x)$.

\medskip

(a) Let $u(x,t)$ denote the temperature at position $x\in[0,L]$ and time $t\ge 0$. The one-dimensional heat equation with constant thermal diffusivity $\kappa>0$ is
\[
u_t(x,t) \;=\; \kappa\, u_{xx}(x,t), \qquad 0<x<L,\ t>0.
\]
Explain in words what it means for the bar to be in a \emph{steady state}. Translate this into a mathematical condition on $u(x,t)$, and use it to derive a differential equation satisfied by the steady-state temperature $T(x)$.

% Hint: In steady state, the temperature no longer changes in time at any point.

\medskip

(b) You should have found that the steady-state temperature $T(x)$ satisfies a second-order ordinary differential equation of the form
\[
T''(x) = 0.
\]
Solve this ordinary differential equation explicitly. Write down the most general twice-differentiable function $T(x)$ whose second derivative is identically zero on the interval $[0,L]$.

Hint: If the second derivative of a function is zero everywhere, what does that say about its first derivative? And what does that say about the function itself?

\medskip

(c) Now incorporate the physical boundary conditions. The ends of the bar are held at fixed temperatures
\[
T(0) = T_0, \qquad T(L) = T_L,
\]
where $T_0$ and $T_L$ are given constants. Use these boundary conditions to determine the constants in your general solution from part (b), and obtain an explicit formula for $T(x)$ in terms of $T_0$, $T_L$, $x$, and $L$.

% Hint: You will obtain two linear equations for the two unknown constants.

\medskip

(d) Show that your formula can be written in the form
\[
T(x) \;=\; T_0 \left(1 - \frac{x}{L}\right) \;+\; T_L \frac{x}{L}.
\]
Interpret this expression in words. In particular:
\begin{itemize}
    \item How does the temperature vary along the bar?
    \item What is the temperature at the midpoint $x = L/2$ in terms of $T_0$ and $T_L$?
\end{itemize}

Hint: Rewrite your solution so that at $x=0$ you obtain $T_0$ and at $x=L$ you obtain $T_L$ by inspection, and notice that the coefficients of $T_0$ and $T_L$ add up to $1$.

\medskip

(e) (Exploration and extensions.)

\begin{enumerate}
    \item Suppose now that $T_0 = T_L = T^\ast$ for some fixed temperature $T^\ast$. What is the steady-state temperature profile $T(x)$ in this case? Is this consistent with your physical intuition? Briefly explain.
    \item Imagine instead that there is a uniform internal heat source in the bar, so that the steady-state equation becomes
    \[
    -k\, T''(x) = q, \qquad 0<x<L,
    \]
    where $k>0$ is the thermal conductivity and $q>0$ is the (constant) rate of heat generation per unit length. Using only qualitative reasoning (you do not need to solve this equation completely), describe the \emph{shape} of the steady-state temperature profile in this case. Will it be linear, concave up, or concave down? How does this relate to the sign of $T''(x)$?
    % Hint: If $T''$ is a positive constant, think about the graph of a quadratic function.
\end{enumerate}

\end{problem}

% ===== Example 4: Static temperature profile in a one-dimensional bar (full solution) =====
\begin{problem}[Static temperature profile in a one-dimensional bar]
A homogeneous bar of length $L$ is insulated on its sides so that heat flows only along its length. The left end at $x=0$ is held at temperature $T_0$ and the right end at $x=L$ is held at temperature $T_L$. In steady state, the temperature $T(x)$ along the bar satisfies
\[
T''(x) = 0, \qquad 0<x<L,
\]
with boundary conditions $T(0) = T_0$ and $T(L) = T_L$.

(a) Solve this boundary value problem and obtain an explicit formula for $T(x)$.

(b) Briefly describe the qualitative behavior of this steady-state temperature profile and how it reflects the nature of a linear static problem.
\end{problem}

\begin{solution}
The situation described is a prototypical \emph{linear static problem}. The temperature no longer depends on time, and the governing equation reduces to a linear ordinary differential equation with boundary conditions prescribed at the ends of the interval.

\medskip

\noindent\textbf{(a) Solving the boundary value problem.}

We are given the second-order linear ordinary differential equation
\[
T''(x) = 0,\qquad 0<x<L.
\]
This equation is especially simple because the second derivative of $T$ is identically zero. Integrating once with respect to $x$ shows that the first derivative must be constant:
\[
T''(x) = 0 \quad \Longrightarrow \quad T'(x) = C_1
\]
for some constant $C_1$. Integrating a second time gives
\[
T(x) = C_1 x + C_2,
\]
where $C_2$ is another constant. Thus every twice-differentiable solution of $T''(x)=0$ on $[0,L]$ is an affine (that is, linear plus constant) function of $x$.

We now apply the boundary conditions to determine $C_1$ and $C_2$. The left boundary condition $T(0)=T_0$ gives
\[
T(0) = C_1\cdot 0 + C_2 = C_2 = T_0,
\]
so $C_2 = T_0$. The right boundary condition $T(L)=T_L$ gives
\[
T(L) = C_1 L + C_2 = C_1 L + T_0 = T_L,
\]
hence
\[
C_1 = \frac{T_L - T_0}{L}.
\]
Substituting these constants into the general solution yields
\[
T(x) = \frac{T_L - T_0}{L}\,x + T_0.
\]

It is often convenient to rewrite this in a symmetric form that highlights the contribution of each boundary temperature. We can write
\[
T(x)
= T_0 + \frac{T_L - T_0}{L}\,x
= T_0\left(1 - \frac{x}{L}\right) + T_L \frac{x}{L}.
\]
This expression makes it clear that $T(0)=T_0$ and $T(L)=T_L$, and that for each $x$ the temperature is a weighted average of the two end temperatures.

\medskip

\noindent\textbf{(b) Qualitative behavior and relation to linear static problems.}

From the formula
\[
T(x) = T_0\left(1 - \frac{x}{L}\right) + T_L \frac{x}{L},
\]
we see that $T(x)$ varies \emph{linearly} from $T_0$ at $x=0$ to $T_L$ at $x=L$. In particular, the temperature at the midpoint is the arithmetic mean of the end temperatures:
\[
T\!\left(\frac{L}{2}\right)
= T_0\left(1 - \frac{1}{2}\right) + T_L \frac{1}{2}
= \frac{T_0 + T_L}{2}.
\]
The constant derivative
\[
T'(x) = \frac{T_L - T_0}{L}
\]
means that the temperature gradient, and hence the heat flux in Fourier's law, is uniform along the bar in steady state. Physically, the same amount of heat per unit time flows through every cross-section, so there is no accumulation of heat inside the bar.

This example illustrates several key features of linear static problems:
\begin{itemize}
    \item A time-dependent partial differential equation (the heat equation) reduces in steady state to a spatial ordinary differential equation, here $T''(x)=0$.
    \item The resulting equation is linear with constant coefficients, leading to a simple general solution determined by integration and then fixed by boundary conditions.
    \item The boundary value nature of the problem is essential: the two end temperatures uniquely determine the linear profile in between.
\end{itemize}
More complicated static problems in this chapter will involve higher-order or variable-coefficient linear equations and more intricate boundary conditions, but the basic structure is the same: a linear differential operator acting on an unknown function, together with physical boundary data that determine a unique static solution.
\end{solution}

% ===== Example 5: Static deflection of an elastic beam under uniform load (inquiry-based) =====
\begin{problem}[Static deflection of an elastic beam under uniform load]
In elementary beam theory (Euler--Bernoulli theory), the transverse deflection $u(x)$ of a slender elastic beam is modeled by an ordinary differential equation. The variable $x$ denotes position along the beam, while $u(x)$ measures the vertical displacement of the centerline. When the beam is loaded by a uniform load, the internal bending moment and shear force balance the external loading in static equilibrium. This leads to a linear, fourth-order boundary-value problem for $u(x)$.

Consider a straight, horizontal beam of length $L$, with constant bending stiffness $EI$ (where $E$ is Young's modulus and $I$ is the second moment of area of the cross section). The beam is subjected to a uniform distributed load of intensity $q>0$ (force per unit length), acting downward along its entire length $0<x<L$. We choose the sign convention so that downward deflections $u(x)$ are positive.

\smallskip

(a) Let $M(x)$ denote the internal bending moment and $V(x)$ the internal shear force in the beam at position $x$. In statics, the balance of forces and moments on a small slice of the beam leads to the one-dimensional equilibrium equations
\[
V'(x) + q = 0, 
\qquad
M'(x) = V(x).
\]
Explain in words what each of these equations means physically, and why the signs are chosen as written. 

\smallskip

(b) In Euler--Bernoulli beam theory, the curvature of the beam is proportional to the bending moment:
\[
M(x) = -EI\,u''(x),
\]
where primes denote derivatives with respect to $x$. 

\begin{itemize}
  \item[(i)] Differentiate the relation $M(x) = -EI\,u''(x)$ once to express $V(x)$ in terms of $u'''(x)$. 
  \item[(ii)] Then use the force balance equation for $V'(x)$ to derive a single differential equation involving only $u$ (and its derivatives) and the given load $q$. Simplify this equation as much as possible.
\end{itemize}

Hint: You should arrive at a fourth-order linear ODE for $u(x)$ with a constant right-hand side.

\smallskip

(c) Now solve the differential equation you obtained in part (b), without yet imposing boundary conditions. 

\begin{itemize}
  \item[(i)] Integrate the equation step by step to obtain an explicit expression for $u(x)$ as a polynomial in $x$, containing four unknown constants of integration $C_1,C_2,C_3,C_4$.
  \item[(ii)] Write your answer in the form
  \[
  u(x) = A x^4 + B x^3 + C x^2 + D x + E
  \]
  and determine $A,B,C,D,E$ in terms of $q$, $EI$, and the constants of integration.
\end{itemize}

Hint: Because the load $q$ is constant, each integration increases the degree of the polynomial by one.

\smallskip

(d) Suppose the beam is \emph{simply supported} at both ends. This means that the displacement is zero at the ends and the bending moment vanishes at the supports. Mathematically, the boundary conditions are
\[
u(0) = 0, 
\quad u(L) = 0,
\quad u''(0) = 0,
\quad u''(L) = 0.
\]
\begin{itemize}
  \item[(i)] Impose these four boundary conditions on your general polynomial solution from part (c), and solve for the four constants of integration.
  \item[(ii)] Write the resulting \emph{particular} solution $u(x)$, simplified as much as you can. 
  \item[(iii)] By differentiating your final expression, find the location $x_{\max}$ and value $u_{\max}$ of the maximum deflection of the beam.
\end{itemize}

Hint: It may be helpful to notice that the loading and the boundary conditions are symmetric with respect to the midpoint $x = L/2$.

\smallskip

(e) Extensions and variations.

\begin{itemize}
  \item[(i)] Suppose instead that the beam is \emph{clamped} (built-in) at both ends. In this case, both the displacement and the slope vanish at $x=0$ and $x=L$, so
  \[
  u(0)=u'(0)=u(L)=u'(L)=0.
  \]
  Without doing all algebraic steps in detail, outline how you would solve for $u(x)$ in this case. Which parts of your work from the simply supported case still apply, and where do the calculations differ?

  \item[(ii)] Finally, think conceptually: how would the differential equation change if the load were not constant, but a prescribed function $q(x)$? How would it change if the stiffness $EI$ varied with $x$? Describe in words what remains linear and what becomes more complicated in the mathematical model.
\end{itemize}

\end{problem}

% ===== Example 5: Static deflection of an elastic beam under uniform load (full solution) =====
\begin{problem}[Static deflection of an elastic beam under uniform load]
A prismatic Euler--Bernoulli beam of length $L$ has constant bending stiffness $EI$ and is subjected to a uniform distributed load of intensity $q>0$ (force per unit length) acting downward along $0<x<L$. Let $u(x)$ denote the vertical deflection of the beam, measured positive downward. 

\begin{enumerate}
  \item Using the relations
  \[
  V'(x) + q = 0, \qquad M'(x)=V(x), \qquad M(x) = -EI\,u''(x),
  \]
  derive a single differential equation for $u(x)$.
  \item Solve this equation for a \emph{simply supported} beam, for which
  \[
  u(0)=u(L)=0, \qquad u''(0)=u''(L)=0.
  \]
  Find the resulting deflection $u(x)$ explicitly.
  \item Determine the location and magnitude of the maximum deflection.  
\end{enumerate}
\end{problem}

\begin{solution}
We proceed in three steps: derivation of the governing ordinary differential equation, solution of the boundary-value problem for the simply supported beam, and determination of the point of maximum deflection. Throughout we emphasize that this is a linear static boundary-value problem of fourth order.

\medskip

\textbf{1. Derivation of the governing differential equation.}

The internal shear force $V(x)$ and bending moment $M(x)$ in a slender beam in static equilibrium satisfy the one-dimensional balance laws
\[
V'(x) + q = 0, 
\qquad 
M'(x) = V(x),
\]
where $q>0$ is the constant distributed load (force per unit length). The first equation states that the change in internal shear along the beam balances the applied distributed load; the second states that the change in internal moment equals the shear force.

In Euler--Bernoulli theory, the curvature of the deflected centerline is proportional to the bending moment,
\[
M(x) = -EI\,u''(x),
\]
where $E$ is Young's modulus, $I$ is the second moment of area, and $u''(x)$ is the second derivative of the deflection $u(x)$ with respect to $x$. The minus sign reflects the convention that a positive bending moment produces a concave-up shape (negative second derivative) when downward deflections are taken as positive.

Differentiating the constitutive relation once gives
\[
M'(x) = -EI\,u'''(x).
\]
But from equilibrium we also have $M'(x) = V(x)$, so
\[
V(x) = -EI\,u'''(x).
\]
Differentiating this with respect to $x$ yields
\[
V'(x) = -EI\,u^{(4)}(x).
\]
Finally, the shear equilibrium equation $V'(x) + q = 0$ gives
\[
-EI\,u^{(4)}(x) + q = 0,
\]
or, equivalently,
\[
EI\,u^{(4)}(x) = q.
\]
Thus the deflection $u(x)$ satisfies the fourth-order linear ordinary differential equation
\[
u^{(4)}(x) = \frac{q}{EI},
\]
with constant right-hand side. This is a typical example of a linear static problem: the operator $L[u] = EI\,u^{(4)}$ is linear, the data $q$ are time-independent, and we will impose boundary conditions at the ends of the domain.

\medskip

\textbf{2. General solution and simply supported boundary conditions.}

To solve
\[
u^{(4)}(x) = \frac{q}{EI},
\]
we integrate successively. For convenience, set
\[
a := \frac{q}{EI},
\]
so the equation is $u^{(4)}(x) = a$, with $a$ a constant.

Integrating once,
\[
u^{(3)}(x) = a x + C_1,
\]
where $C_1$ is an integration constant. Integrating again,
\[
u''(x) 
= \int (a x + C_1)\,dx 
= \frac{a}{2}x^2 + C_1 x + C_2.
\]
Integrating a third time,
\[
u'(x) 
= \int\left(\frac{a}{2}x^2 + C_1 x + C_2\right)\,dx
= \frac{a}{6}x^3 + \frac{C_1}{2}x^2 + C_2 x + C_3.
\]
Finally, integrating a fourth time,
\[
u(x) 
= \int\left(\frac{a}{6}x^3 + \frac{C_1}{2}x^2 + C_2 x + C_3\right)\,dx
= \frac{a}{24}x^4 + \frac{C_1}{6}x^3 + \frac{C_2}{2}x^2 + C_3 x + C_4.
\]
Thus the general solution is the quartic polynomial
\[
u(x) = \frac{a}{24}x^4 + \frac{C_1}{6}x^3 + \frac{C_2}{2}x^2 + C_3 x + C_4,
\]
with four constants $C_1,C_2,C_3,C_4$ to be determined from the boundary conditions.

For a simply supported beam at $x=0$ and $x=L$, we impose:
\[
u(0) = 0,\quad u(L) = 0, \qquad u''(0) = 0,\quad u''(L) = 0.
\]
The conditions $u(0)=0$ and $u(L)=0$ express that the beam passes through the supports and cannot deflect there. The conditions $u''(0)=u''(L)=0$ express that the bending moment vanishes at the simple supports, since
\[
M(x) = -EI\,u''(x).
\]

We now apply these conditions.

\emph{First}, from $u(0)=0$ we have
\[
u(0) = \frac{a}{24}\cdot 0 + \frac{C_1}{6}\cdot 0 + \frac{C_2}{2}\cdot 0 + C_3\cdot 0 + C_4 = C_4 = 0.
\]
Thus $C_4 = 0$.

\emph{Second}, from $u''(0)=0$ and the expression
\[
u''(x) = \frac{a}{2}x^2 + C_1 x + C_2,
\]
we obtain
\[
u''(0) = C_2 = 0.
\]
So $C_2 = 0$.

\emph{Third}, from $u''(L) = 0$ we have
\[
0 = u''(L) = \frac{a}{2}L^2 + C_1 L + C_2
= \frac{a}{2}L^2 + C_1 L,
\]
since $C_2=0$. Solving for $C_1$ gives
\[
C_1 = -\frac{a}{2}L.
\]

\emph{Fourth}, from $u(L)=0$ we compute
\[
0 = u(L) 
= \frac{a}{24}L^4 + \frac{C_1}{6}L^3 + \frac{C_2}{2}L^2 + C_3 L + C_4
= \frac{a}{24}L^4 + \frac{C_1}{6}L^3 + C_3 L,
\]
since $C_2 = C_4 = 0$. Substituting $C_1 = -\frac{a}{2}L$ yields
\[
0 = \frac{a}{24}L^4 + \frac{1}{6}\left(-\frac{a}{2}L\right)L^3 + C_3 L
= \frac{a}{24}L^4 - \frac{a}{12}L^4 + C_3 L
= -\frac{a}{24}L^4 + C_3 L.
\]
Hence
\[
C_3 L = \frac{a}{24}L^4
\quad\Rightarrow\quad
C_3 = \frac{a}{24}L^3.
\]

We now substitute all constants back into $u(x)$:
\[
u(x)
= \frac{a}{24}x^4 + \frac{C_1}{6}x^3 + \frac{C_2}{2}x^2 + C_3 x + C_4
= \frac{a}{24}x^4 + \frac{1}{6}\left(-\frac{a}{2}L\right)x^3 + 0 + \frac{a}{24}L^3 x + 0.
\]
Simplifying,
\[
u(x)
= \frac{a}{24}\left(x^4 - 2L x^3 + L^3 x\right).
\]
Recalling that $a=q/(EI)$, we obtain the explicit deflection of the simply supported beam:
\[
u(x) 
= \frac{q}{24\,EI}\,\bigl(x^4 - 2L x^3 + L^3 x\bigr), 
\qquad 0 \le x \le L.
\]

This function is a quartic polynomial in $x$; its form reflects the fact that the operator $u \mapsto EI\,u^{(4)}$ is linear, the right-hand side $q$ is constant, and the boundary conditions are linear. Together, these ingredients make this a prototypical linear static boundary-value problem for an ordinary differential equation.

\medskip

\textbf{3. Location and magnitude of the maximum deflection.}

To find the location of the maximum deflection, we differentiate $u(x)$ and look for critical points. Differentiating,
\[
u'(x) 
= \frac{q}{24\,EI}\,\bigl(4x^3 - 6L x^2 + L^3\bigr).
\]
We solve $u'(x)=0$:
\[
4x^3 - 6L x^2 + L^3 = 0.
\]
By inspection, $x = L/2$ is a root:
\[
4\left(\frac{L}{2}\right)^3 - 6L\left(\frac{L}{2}\right)^2 + L^3
= 4\frac{L^3}{8} - 6L\frac{L^2}{4} + L^3
= \frac{L^3}{2} - \frac{3}{2}L^3 + L^3 = 0.
\]
Thus $x = L/2$ is a stationary point. One can factor the polynomial to find the remaining roots, but physically we know the beam, load, and supports are symmetric about $x = L/2$, so this point must correspond to either a maximum or a minimum deflection in the interior. Since the beam sags downward under the load, $u(L/2)$ is a maximum.

We now compute the deflection at $x = L/2$:
\[
u\!\left(\frac{L}{2}\right) 
= \frac{q}{24\,EI}\left[\left(\frac{L}{2}\right)^4 - 2L\left(\frac{L}{2}\right)^3 + L^3\left(\frac{L}{2}\right)\right].
\]
We simplify each term:
\[
\left(\frac{L}{2}\right)^4 = \frac{L^4}{16},\quad
2L\left(\frac{L}{2}\right)^3 = 2L\cdot\frac{L^3}{8} = \frac{L^4}{4},\quad
L^3\left(\frac{L}{2}\right) = \frac{L^4}{2}.
\]
Thus
\[
u\!\left(\frac{L}{2}\right)
= \frac{q}{24\,EI}\left(\frac{L^4}{16} - \frac{L^4}{4} + \frac{L^4}{2}\right)
= \frac{qL^4}{24\,EI}\left(\frac{1}{16} - \frac{4}{16} + \frac{8}{16}\right)
= \frac{qL^4}{24\,EI}\cdot\frac{5}{16}
= \frac{5qL^4}{384\,EI}.
\]
Therefore, the maximum deflection occurs at the midpoint $x_{\max} = L/2$, with magnitude
\[
u_{\max} = u\!\left(\frac{L}{2}\right) = \frac{5 q L^4}{384\,EI}.
\]

\medskip

\textbf{4. Remarks on linear static problems and boundary conditions.}

This example illustrates several central ideas of \emph{linear static problems}:

\begin{itemize}
  \item The governing equation $EI\,u^{(4)}(x) = q$ is linear in $u$, and the data $q$ are independent of time. The operator $L[u] = EI\,u^{(4)}$ is a linear differential operator of fourth order.
  \item The boundary conditions at $x=0$ and $x=L$ are linear in $u$ and its derivatives. Changing the physical support conditions corresponds to modifying these boundary conditions. For example, if the beam were clamped at both ends, we would impose $u(0)=u'(0)=u(L)=u'(L)=0$ instead. Solving with these clamped conditions leads to
  \[
  u(x) = \frac{q}{24\,EI}\,x^2(x-L)^2,
  \]
  whose maximum deflection is $qL^4/(384\,EI)$, notably smaller than in the simply supported case.
  \item The method of solution consists of integrating the differential equation to obtain a general solution with arbitrary constants and then using the boundary conditions to determine those constants. This pattern recurs throughout the study of linear boundary-value problems for ordinary differential equations.
\end{itemize}

In summary, the static deflection of a simply supported elastic beam under uniform load is described by a quartic polynomial, obtained by solving a linear fourth-order ordinary differential equation with appropriate boundary conditions that encode the support behavior at the ends.
\end{solution}

\section{Sturm–Liouville (Spectral) Theory}
% --- Narrative plan (auto-generated) ---
% This section develops Sturm–Liouville theory, which studies a special class of second-order linear differential operators equipped with boundary conditions. These operators admit real eigenvalues and orthogonal eigenfunctions, much like symmetric matrices in linear algebra. The main goal is to understand how such operators arise from physical models, how to systematically find their eigenvalues and eigenfunctions, and how to use these eigenfunctions as building blocks to represent more complicated functions.
%
% Sturm–Liouville problems lie at the heart of separation of variables for partial differential equations such as the heat, wave, and Laplace equations. When you separate variables in a PDE, the spatial part almost always becomes a Sturm–Liouville problem, and its spectrum determines the time evolution and qualitative behavior of solutions. This spectral viewpoint connects directly to Fourier series and Fourier transforms: orthogonal eigenfunctions generalize sines and cosines, and expansions in these eigenfunctions allow you to solve boundary value and initial value problems.
%
% Conceptually, Sturm–Liouville theory brings together ideas from ordinary differential equations, linear algebra, and functional analysis. It prepares the ground for modern spectral theory of operators, and it links to complex analysis through contour integral representations and analytic properties of eigenfunctions. In applied mathematics, mastering these tools is essential for modeling vibrating systems, diffusion, quantum mechanics, and for understanding how spatial structure shapes dynamical behavior.

% ===== Example 1: The vibrating string and the simplest Sturm–Liouville problem (inquiry-based) =====
\begin{problem}[The vibrating string and the simplest Sturm--Liouville problem]
A taut string of length $L$ fixed at both ends can be modeled, to a good approximation, by the one-dimensional wave equation. The transverse displacement $u(x,t)$ of the string at position $x \in [0,L]$ and time $t \ge 0$ satisfies a partial differential equation together with appropriate boundary conditions. In this problem you will rediscover how separation of variables leads to a simple Sturm--Liouville problem, whose eigenvalues determine the natural frequencies of vibration of the string. Along the way you will see how a very concrete physical system gives rise to the abstract spectral theory discussed in this chapter.

Assume a string of uniform density under constant tension, and let $u(x,t)$ denote the vertical displacement from equilibrium.

\smallskip

(a) Write down the one-dimensional wave equation for $u(x,t)$ with constant wave speed $c>0$, and impose the boundary conditions corresponding to the string being fixed at both ends $x=0$ and $x=L$.

\begin{itemize}
  \item[(i)] State the partial differential equation (PDE) that $u$ must satisfy.
  \item[(ii)] State the boundary conditions at $x=0$ and $x=L$ in terms of $u$.
\end{itemize}

Hint: The standard wave equation in one space dimension relates $u_{tt}$ and $u_{xx}$.

\smallskip

(b) Use separation of variables to reduce the wave equation to ordinary differential equations.

Assume a separated solution of the form
\[
u(x,t) = X(x)\,T(t),
\]
where $X$ depends only on $x$ and $T$ depends only on $t$.

\begin{itemize}
  \item[(i)] Substitute $u(x,t)=X(x)T(t)$ into your PDE from part (a), and divide by $X(x)T(t)$ (assuming $X$ and $T$ are not identically zero) to obtain an equation of the form
  \[
  \frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)}.
  \]
  Explain why the left-hand side depends only on $t$ and the right-hand side depends only on $x$, and why this forces both sides to be equal to the same constant, say $-\lambda$.
  \item[(ii)] Write down the resulting ordinary differential equations for $X$ and $T$.
  \item[(iii)] Translate the boundary conditions from part (a) into boundary conditions for $X(x)$.
\end{itemize}

Hint: Think about what it means for $u(0,t)$ and $u(L,t)$ to be zero for all $t$ in terms of $X$ and $T$.

\smallskip

(c) You should now have arrived at a boundary value problem for $X$ of the form
\[
- X''(x) = \lambda X(x), \qquad 0<x<L,
\]
together with boundary conditions $X(0)=0$ and $X(L)=0$.

\begin{itemize}
  \item[(i)] This is the simplest example of a Sturm--Liouville eigenvalue problem. Briefly explain why one calls $\lambda$ an ``eigenvalue'' and $X$ an ``eigenfunction'' in this context.
  \item[(ii)] Solve this boundary value problem by considering three cases for the parameter $\lambda$: (A) $\lambda<0$, (B) $\lambda=0$, and (C) $\lambda>0$.
  
  For each case:
  \begin{itemize}
    \item Write down the general solution of $-X''=\lambda X$.
    \item Impose the boundary condition at $x=0$ to restrict the constants in your general solution.
    \item Impose the boundary condition at $x=L$ and determine for which values of $\lambda$ there exist nontrivial (not identically zero) solutions $X$.
  \end{itemize}
\end{itemize}

Hint: When $\lambda>0$, it is convenient to write $\lambda = \mu^2$ with $\mu>0$, and when $\lambda<0$, write $\lambda=-\mu^2$.

\smallskip

(d) Collect your results from part (c).

\begin{itemize}
  \item[(i)] List all eigenvalues $\lambda_n$ and corresponding eigenfunctions $X_n(x)$ that satisfy $-X''=\lambda X$ with $X(0)=X(L)=0$.
  \item[(ii)] For each allowed eigenvalue $\lambda_n$, solve the time equation
  \[
  T''(t) + c^2 \lambda_n T(t) = 0
  \]
  and write down the corresponding separated solutions $u_n(x,t) = X_n(x)T_n(t)$.
  \item[(iii)] Interpret the parameters $\lambda_n$ and the corresponding angular frequencies
  \[
  \omega_n = c\sqrt{\lambda_n}
  \]
  in terms of the ``natural modes'' or ``harmonics'' of the vibrating string.
\end{itemize}

Hint: The ODE for $T$ is a familiar harmonic oscillator equation when $\lambda_n>0$.

\smallskip

(e) Finally, explore how this picture changes under slight modifications.

\begin{itemize}
  \item[(i)] Suppose instead that the left end of the string is fixed and the right end is free. A free end corresponds (under reasonable modeling assumptions) to the slope being zero there, that is $u_x(L,t) = 0$. Write the new boundary conditions for $X(x)$, and sketch how the eigenvalue problem and its solutions would change. In particular, what trigonometric functions do you expect to appear as eigenfunctions in this case?

  \item[(ii)] In our derivation, the spatial equation took the form
  \[
  -X''(x) = \lambda X(x),
  \]
  which can be written in Sturm--Liouville form
  \[
  -\frac{d}{dx}\Bigl(p(x)\,X'(x)\Bigr) + q(x)\,X(x) = \lambda\,w(x)\,X(x)
  \]
  with appropriate choices of $p$, $q$, and $w$. Identify $p(x)$, $q(x)$, and $w(x)$ in the vibrating string example, and briefly speculate how a non-uniform string (for instance, with variable density) might lead to a different weight function $w(x)$.
\end{itemize}

Hint: For (ii), compare your equation to the Sturm--Liouville template and match coefficients term by term.
\end{problem}

% ===== Example 1: The vibrating string and the simplest Sturm–Liouville problem (full solution) =====
\begin{problem}[The vibrating string and the simplest Sturm--Liouville problem]
Consider a taut string of length $L$ with fixed ends, whose transverse displacement $u(x,t)$ satisfies the one-dimensional wave equation
\[
u_{tt} = c^2 u_{xx}, \qquad 0<x<L,\ t>0,
\]
with boundary conditions
\[
u(0,t) = 0, \qquad u(L,t) = 0, \qquad t\ge 0.
\]

(a) Using a separated solution $u(x,t)=X(x)T(t)$, derive the ordinary differential equations for $X$ and $T$, and show that $X$ must satisfy the boundary value problem
\[
- X''(x) = \lambda X(x), \qquad X(0)=0,\ X(L)=0,
\]
for some constant $\lambda$.

(b) Solve this Sturm--Liouville eigenvalue problem: determine all eigenvalues $\lambda_n$ and corresponding eigenfunctions $X_n(x)$ (up to nonzero scalar multiples).

(c) For each eigenvalue $\lambda_n$, solve the time equation for $T_n(t)$, and write down the corresponding separated solutions
\[
u_n(x,t) = X_n(x) T_n(t).
\]
Identify the natural angular frequencies $\omega_n$ of vibration.

(d) Briefly explain how this example fits into the general Sturm--Liouville framework, and indicate the functions $p(x)$, $q(x)$, and $w(x)$ in the standard form
\[
-\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + q(x) y(x) = \lambda\, w(x)\, y(x).
\]
\end{problem}

\begin{solution}
We begin from the physical model: a taut, uniform string of length $L$ with displacement $u(x,t)$ satisfies the wave equation
\[
u_{tt} = c^2 u_{xx}, \qquad 0<x<L,\ t>0,
\]
where $c>0$ is the wave speed, and the fixed ends impose
\[
u(0,t)=0,\qquad u(L,t)=0,\qquad t\ge 0.
\]

\medskip

\textbf{(a) Separation of variables and the spatial eigenvalue problem.}
We look for separated solutions of the form
\[
u(x,t) = X(x)\,T(t),
\]
where $X$ is a function of $x$ alone and $T$ is a function of $t$ alone. Substituting into the wave equation gives
\[
X(x) T''(t) = c^2 X''(x) T(t).
\]
Assuming $X$ and $T$ are not identically zero, we may divide both sides by $c^2 X(x) T(t)$:
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)}.
\]
The left-hand side depends only on $t$ and the right-hand side depends only on $x$. The only way a function of $t$ can be equal to a function of $x$ for all $x$ and $t$ is for both functions to be constant. We denote this common constant by $-\lambda$:
\[
\frac{T''(t)}{c^2 T(t)} = \frac{X''(x)}{X(x)} = -\lambda.
\]

This yields the system of ordinary differential equations
\[
T''(t) + c^2 \lambda\, T(t) = 0, \qquad -X''(x) = \lambda\, X(x).
\]

The boundary conditions $u(0,t)=0$ and $u(L,t)=0$ must hold for all $t$. In terms of $X$ and $T$, they read
\[
X(0) T(t) = 0,\qquad X(L) T(t)=0,\quad \text{for all $t$}.
\]
We are looking for nontrivial separated solutions, so we assume $T$ is not identically zero. Hence $X(0)=0$ and $X(L)=0$. Thus $X$ must satisfy the boundary value problem
\[
- X''(x) = \lambda X(x), \qquad 0<x<L,\qquad X(0)=0,\ X(L)=0.
\]
This is the simplest form of a Sturm--Liouville eigenvalue problem: we seek values of the parameter $\lambda$ (the eigenvalues) for which there exist nontrivial solutions $X$ (the eigenfunctions) satisfying both the differential equation and the boundary conditions.

\medskip

\textbf{(b) Solving the Sturm--Liouville problem.}
We now solve
\[
- X''(x) = \lambda X(x),\qquad X(0)=0,\ X(L)=0.
\]
We distinguish three cases for $\lambda$.

\emph{Case 1: $\lambda<0$.} Write $\lambda = -\mu^2$ with $\mu>0$. The equation becomes
\[
- X''(x) = -\mu^2 X(x) \quad\Longleftrightarrow\quad X''(x) = \mu^2 X(x),
\]
whose general solution is
\[
X(x) = A e^{\mu x} + B e^{-\mu x}.
\]
Imposing $X(0)=0$ gives $A+B=0$, so $B=-A$ and
\[
X(x) = A\bigl(e^{\mu x} - e^{-\mu x}\bigr) = 2A \sinh(\mu x).
\]
Imposing $X(L)=0$ then yields $\sinh(\mu L)=0$, which forces $A=0$ since $\sinh(\mu L)\ne 0$ for $\mu>0$. Thus the only solution is the trivial one $X\equiv 0$. Therefore there are no negative eigenvalues.

\emph{Case 2: $\lambda=0$.} The equation becomes $-X''(x)=0$, that is,
\[
X''(x)=0,
\]
whose general solution is
\[
X(x) = A x + B.
\]
The condition $X(0)=0$ gives $B=0$, so $X(x)=Ax$. Then $X(L)=0$ implies $A L = 0$, hence $A=0$. Again, only the trivial solution exists. So $\lambda=0$ is not an eigenvalue.

\emph{Case 3: $\lambda>0$.} Write $\lambda=\mu^2$ with $\mu>0$. Then
\[
- X''(x) = \mu^2 X(x) \quad\Longleftrightarrow\quad X''(x) + \mu^2 X(x)=0,
\]
whose general solution is
\[
X(x) = A \cos(\mu x) + B \sin(\mu x).
\]
Imposing $X(0)=0$ gives $A \cos(0) + B \sin(0) = A = 0$. Thus $A=0$ and
\[
X(x) = B \sin(\mu x).
\]
Now $X(L)=0$ requires
\[
B \sin(\mu L) = 0.
\]
For a nontrivial solution we must have $B\ne 0$, so $\sin(\mu L)=0$. This occurs precisely when $\mu L = n\pi$ for some positive integer $n$. Hence
\[
\mu_n = \frac{n\pi}{L},\qquad n=1,2,3,\dots,
\]
and the corresponding eigenvalues are
\[
\lambda_n = \mu_n^2 = \Bigl(\frac{n\pi}{L}\Bigr)^2.
\]
For each $n$, a corresponding eigenfunction is
\[
X_n(x) = \sin\Bigl(\frac{n\pi x}{L}\Bigr),
\]
up to a nonzero multiplicative constant. Scaling does not matter for eigenfunctions, so this choice is convenient.

Thus the Sturm--Liouville problem has a countable set of positive eigenvalues
\[
\lambda_n = \Bigl(\frac{n\pi}{L}\Bigr)^2,\quad n=1,2,\dots,
\]
with eigenfunctions $X_n(x)=\sin\bigl(\frac{n\pi x}{L}\bigr)$.

\medskip

\textbf{(c) Time dependence and normal modes.}
For each eigenvalue $\lambda_n$ we now solve the time equation
\[
T''(t) + c^2 \lambda_n T(t) = 0.
\]
Substituting $\lambda_n = (n\pi/L)^2$, this becomes
\[
T''(t) + \omega_n^2 T(t) = 0, \qquad \omega_n := c \frac{n\pi}{L}.
\]
This is the standard harmonic oscillator equation with angular frequency $\omega_n$. Its general real solution is
\[
T_n(t) = A_n \cos(\omega_n t) + B_n \sin(\omega_n t),
\]
where $A_n$ and $B_n$ are constants determined by initial conditions (such as the initial shape and initial velocity of the string).

Combining the spatial and temporal parts, the separated solutions (also called \emph{normal modes}) are
\[
u_n(x,t) = X_n(x)\,T_n(t)
= \sin\Bigl(\frac{n\pi x}{L}\Bigr)\,\Bigl[A_n \cos(\omega_n t) + B_n \sin(\omega_n t)\Bigr],
\quad n=1,2,\dots
\]
Each mode $u_n$ oscillates sinusoidally in time with fixed spatial shape $\sin(\frac{n\pi x}{L})$ and angular frequency
\[
\omega_n = c\sqrt{\lambda_n} = c\,\frac{n\pi}{L}.
\]
These $\omega_n$ are the \emph{natural frequencies} or \emph{harmonics} of the string. The fundamental frequency corresponds to $n=1$, and higher values of $n$ give overtones with spatial patterns possessing more interior nodes.

From Sturm--Liouville theory, the eigenfunctions $\{\sin(\frac{n\pi x}{L})\}_{n=1}^\infty$ form an orthogonal set in $L^2(0,L)$ with respect to the standard inner product
\[
\langle f,g\rangle = \int_0^L f(x) g(x)\,dx.
\]
Indeed, one can verify
\[
\int_0^L \sin\Bigl(\frac{n\pi x}{L}\Bigr)\sin\Bigl(\frac{m\pi x}{L}\Bigr)\,dx =
\begin{cases}
0, & n\ne m,\\[4pt]
\dfrac{L}{2}, & n=m.
\end{cases}
\]
This orthogonality allows one to expand a general initial displacement as a Fourier sine series in these eigenfunctions and hence superpose the normal modes to obtain the full solution. Although this expansion step is not asked for explicitly here, it is a central point of Sturm--Liouville spectral theory.

\medskip

\textbf{(d) Sturm--Liouville form.}
In general, a Sturm--Liouville problem has the form
\[
-\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + q(x) y(x) = \lambda\, w(x)\, y(x),
\]
together with suitable boundary conditions. In the vibrating string example, the spatial equation is
\[
- X''(x) = \lambda X(x),
\]
which can be written as
\[
-\frac{d}{dx}\bigl(1\cdot X'(x)\bigr) + 0\cdot X(x) = \lambda\, 1\cdot X(x).
\]
Thus we can identify
\[
p(x) \equiv 1,\qquad q(x) \equiv 0,\qquad w(x) \equiv 1.
\]
The boundary conditions $X(0)=0$ and $X(L)=0$ are homogeneous Dirichlet conditions, which fit the Sturm--Liouville framework.

This example illustrates the central themes of Sturm--Liouville (spectral) theory:
\begin{itemize}
  \item A second-order self-adjoint differential operator (here $L[X] = -X''$ with Dirichlet boundary conditions) has a discrete spectrum of real eigenvalues $\lambda_n$.
  \item There is an associated sequence of eigenfunctions $X_n$ which are orthogonal with respect to the weight $w(x)$ (here simply $1$).
  \item These eigenfunctions form a basis (in an appropriate sense) for expanding general functions, leading to Fourier series and modal decompositions of solutions to PDEs such as the wave equation.
\end{itemize}
The vibrating string with fixed ends is therefore the archetypal concrete realization of Sturm--Liouville spectral theory on a finite interval.
\end{solution}

% ===== Example 2: Heat flow in a rod and eigenfunction expansions (inquiry-based) =====
\begin{problem}[Heat flow in a rod and eigenfunction expansions]
Consider a thin, homogeneous rod of length $L$ lying along the $x$–axis from $x=0$ to $x=L$. Let $u(x,t)$ denote the temperature at position $x$ and time $t$. The rod is perfectly insulated along its sides, but its ends are held at zero temperature by contact with large heat baths. The temperature evolves according to the one-dimensional heat equation with thermal diffusivity $\kappa>0$,
\[
u_t = \kappa\,u_{xx},
\]
together with homogeneous Dirichlet boundary conditions $u(0,t)=u(L,t)=0$. We prescribe an initial temperature profile $u(x,0)=f(x)$ and seek to describe $u(x,t)$ for $t>0$.

\smallskip

(a) Write down the full initial–boundary value problem for $u(x,t)$ on $0<x<L$, $t>0$, including the partial differential equation, the boundary conditions, and the initial condition. Then assume a {\em separated} form
\[
u(x,t) = X(x)\,T(t),
\]
and substitute into the heat equation. Rearrange the resulting equation to isolate all dependence on $x$ on one side and all dependence on $t$ on the other side.

\begin{itemize}
  \item[(i)] What does this rearrangement tell you about the existence of a constant $\lambda$ (the {\em separation constant}) such that $X$ and $T$ satisfy
  \[
  X''(x) + \lambda X(x) = 0, \qquad T'(t) + \kappa \lambda T(t) = 0?
  \]
  \item[(ii)] What boundary conditions must $X(x)$ satisfy, and why?
\end{itemize}
Hint: Justify carefully why the left-hand side depends only on $t$ and the right-hand side only on $x$, and therefore they must both be equal to a constant.

\smallskip

(b) We now analyze the {\em spatial} Sturm–Liouville problem
\[
X''(x) + \lambda X(x) = 0, \qquad X(0)=0,\quad X(L)=0.
\]
Consider the three cases $\lambda<0$, $\lambda=0$, and $\lambda>0$.

\begin{itemize}
  \item[(i)] Show that if $\lambda\leq 0$, then the only solution satisfying the boundary conditions is the trivial solution $X\equiv 0$. (Thus no nonzero separated solution arises from these values of $\lambda$.)
  \item[(ii)] Solve the differential equation explicitly for $\lambda>0$ and impose the boundary conditions to determine the allowed values of $\lambda$.
  \item[(iii)] Show that for $\lambda>0$, the nontrivial solutions are (up to constant multiples) of the form
  \[
  X_n(x) = \sin\left(\frac{n\pi x}{L}\right) \quad\text{for } n=1,2,3,\dots,
  \]
  and find the corresponding eigenvalues $\lambda_n$.
\end{itemize}
Hint: For $\lambda>0$, write $\lambda=\mu^2$ and express the general solution in terms of $\sin(\mu x)$ and $\cos(\mu x)$. Use the two boundary conditions to force $\mu$ to take discrete values.

\smallskip

(c) The functions $\{X_n\}_{n=1}^\infty$ form an orthogonal family with respect to the inner product
\[
\langle \phi,\psi\rangle = \int_0^L \phi(x)\,\psi(x)\,dx.
\]
\begin{itemize}
  \item[(i)] Verify directly that
  \[
  \int_0^L \sin\left(\frac{m\pi x}{L}\right)\sin\left(\frac{n\pi x}{L}\right)\,dx =
  \begin{cases}
    0, & m\neq n,\\[4pt]
    \dfrac{L}{2}, & m=n.
  \end{cases}
  \]
  \item[(ii)] For each $n$, solve the time equation
  \[
  T_n'(t) + \kappa \lambda_n T_n(t) = 0
  \]
  and write the corresponding separated solution $u_n(x,t)=X_n(x)T_n(t)$.
\end{itemize}
Hint: For the integral, you may use the product-to-sum identity for sines, or integrate by parts. For the time equation, recall how to solve a first-order linear ODE with constant coefficients.

\smallskip

(d) We now assemble the separated solutions into a series that can match an arbitrary initial temperature profile $f(x)$.

\begin{itemize}
  \item[(i)] Argue (heuristically, using completeness of the sine functions) that we can seek a solution in the form
  \[
  u(x,t) = \sum_{n=1}^\infty b_n\,\sin\left(\frac{n\pi x}{L}\right) e^{-\kappa \lambda_n t},
  \]
  where the $\lambda_n$ and $\sin(n\pi x/L)$ are as found above.
  \item[(ii)] Impose the initial condition $u(x,0)=f(x)$ and show that $f$ must have a {\em sine series expansion}
  \[
  f(x) = \sum_{n=1}^\infty b_n\,\sin\left(\frac{n\pi x}{L}\right)
  \]
  on the interval $[0,L]$. Use the orthogonality from part (c) to derive a formula for the coefficients $b_n$ in terms of $f$.
  \item[(iii)] As a concrete example, take $f(x)\equiv 1$ (the rod starts at uniform temperature). Compute the coefficients $b_n$ explicitly and write the resulting series for $u(x,t)$.
\end{itemize}
Hint: Multiply the expansion for $f(x)$ by $\sin(m\pi x/L)$ and integrate from $0$ to $L$ to isolate $b_m$.

\smallskip

(e) Explore one or two variations of the model.

\begin{itemize}
  \item[(i)] Suppose instead that the rod is insulated at both ends, so that no heat flows through $x=0$ and $x=L$. This is modeled by Neumann boundary conditions $u_x(0,t)=u_x(L,t)=0$. Repeat the separation-of-variables step and write down the corresponding spatial eigenvalue problem for $X(x)$. What do you expect the eigenfunctions to look like in this case (sines, cosines, or something else)?
  \item[(ii)] The vibrating string with fixed ends is governed by the wave equation $v_{tt}=c^2 v_{xx}$ with $v(0,t)=v(L,t)=0$. If you repeat the same separation-of-variables procedure for that problem, which parts of the analysis above will be identical, and which will change? In particular, compare the time dependence you obtain here (exponential decay) with the time dependence in the wave equation (oscillatory behavior).
\end{itemize}
Hint: Think about which differential operator is the same in both problems, and which equation differs only by replacing a first time derivative by a second time derivative.
\end{problem}

% ===== Example 2: Heat flow in a rod and eigenfunction expansions (full solution) =====
\begin{problem}[Heat flow in a rod and eigenfunction expansions]
Consider the heat equation on a homogeneous rod of length $L$:
\[
u_t = \kappa u_{xx},\qquad 0<x<L,\ t>0,
\]
with homogeneous Dirichlet boundary conditions
\[
u(0,t)=0,\quad u(L,t)=0,\qquad t>0,
\]
and initial condition
\[
u(x,0)=f(x),\qquad 0<x<L,
\]
where $\kappa>0$ is a constant and $f$ is a given function.

\begin{enumerate}
  \item[(a)] Use separation of variables to find the eigenvalues $\lambda_n$ and eigenfunctions $X_n(x)$ of the associated spatial problem. Show that, up to scalar multiples,
  \[
  X_n(x)=\sin\!\left(\frac{n\pi x}{L}\right),\qquad \lambda_n = \left(\frac{n\pi}{L}\right)^2,\quad n=1,2,\dots.
  \]
  \item[(b)] Show that the solution can be written as an eigenfunction expansion
  \[
  u(x,t) = \sum_{n=1}^\infty b_n \sin\!\left(\frac{n\pi x}{L}\right)
  e^{-\kappa \left(\frac{n\pi}{L}\right)^2 t},
  \]
  and derive a formula for the coefficients $b_n$ in terms of $f$.
  \item[(c)] Specialize to the case of a uniformly heated rod with $f(x)\equiv 1$ on $(0,L)$. Compute the coefficients $b_n$ explicitly and write the resulting series for $u(x,t)$.
\end{enumerate}
\end{problem}

\begin{solution}
We solve the heat equation by the method of separation of variables, and in doing so we will see the role of the Sturm–Liouville eigenvalue problem associated with the spatial operator $-d^2/dx^2$ and homogeneous Dirichlet boundary conditions.

\medskip

\noindent\textbf{(a) Separation of variables and the spatial eigenvalue problem.}
We look for solutions of the form
\[
u(x,t) = X(x)\,T(t),
\]
where $X$ depends only on $x$ and $T$ depends only on $t$. Substituting into the heat equation $u_t=\kappa u_{xx}$ gives
\[
X(x)\,T'(t) = \kappa X''(x)\,T(t).
\]
Assuming $X$ and $T$ are not identically zero, we can divide both sides by $\kappa X(x)T(t)$ to separate the variables:
\[
\frac{T'(t)}{\kappa T(t)} = \frac{X''(x)}{X(x)}.
\]
The left-hand side depends only on $t$, while the right-hand side depends only on $x$. Hence both sides must be equal to the same constant, say $-\lambda$. Thus we obtain the two ordinary differential equations
\begin{equation}\label{eq:separated-ODEs}
\begin{aligned}
T'(t) + \kappa \lambda T(t) &= 0,\\
X''(x) + \lambda X(x) &= 0.
\end{aligned}
\end{equation}
The boundary conditions on $u$ translate to boundary conditions on $X$:
\[
u(0,t)=0 \implies X(0)T(t)=0,\quad
u(L,t)=0 \implies X(L)T(t)=0.
\]
Since we seek nontrivial products $X\,T$, we require that $T$ is not identically zero, so $X$ must satisfy
\[
X(0)=0,\quad X(L)=0.
\]
Thus the spatial factor $X$ solves the Sturm–Liouville problem
\[
X''(x) + \lambda X(x) = 0,\qquad X(0)=0,\quad X(L)=0.
\]

We now analyze this eigenvalue problem and determine the admissible values of $\lambda$ and the corresponding eigenfunctions.

\medskip

\noindent\textbf{Case $\lambda\le 0$.}
If $\lambda=0$, then $X''(x)=0$, so
\[
X(x)=A+Bx.
\]
The boundary condition $X(0)=0$ implies $A=0$, and then $X(L)=0$ implies $BL=0$, so $B=0$. Hence $X\equiv0$ is the only solution when $\lambda=0$.

If $\lambda<0$, write $\lambda=-\mu^2$ with $\mu>0$. Then the equation becomes
\[
X''(x)-\mu^2 X(x)=0,
\]
whose general solution is
\[
X(x)=A e^{\mu x}+B e^{-\mu x}.
\]
The boundary condition $X(0)=0$ gives $A+B=0$, so $B=-A$. Thus
\[
X(x)=A(e^{\mu x}-e^{-\mu x})=2A\sinh(\mu x).
\]
Applying $X(L)=0$ gives $2A\sinh(\mu L)=0$. Since $\mu>0$ and $\sinh(\mu L)\neq 0$, we must have $A=0$, so $X\equiv 0$ again. Thus there are no nontrivial solutions for $\lambda\le 0$.

\medskip

\noindent\textbf{Case $\lambda>0$.}
Let $\lambda=\mu^2$ with $\mu>0$. Then the spatial equation is
\[
X''(x)+\mu^2 X(x)=0,
\]
with general solution
\[
X(x)=A\cos(\mu x)+B\sin(\mu x).
\]
The boundary condition $X(0)=0$ gives $A\cos 0 + B\sin 0 = A=0$. Thus $X(x)=B\sin(\mu x)$. Applying $X(L)=0$ yields
\[
B\sin(\mu L)=0.
\]
For a nontrivial solution we must have $B\neq 0$, so $\sin(\mu L)=0$. Therefore $\mu L = n\pi$ for some positive integer $n$. Thus
\[
\mu = \frac{n\pi}{L},\qquad n=1,2,3,\dots,
\]
and hence
\[
\lambda_n = \mu^2 = \left(\frac{n\pi}{L}\right)^2,\qquad
X_n(x)=\sin\left(\frac{n\pi x}{L}\right),\quad n=1,2,3,\dots,
\]
up to multiplication by a nonzero constant. These are precisely the eigenvalues and eigenfunctions of the self-adjoint Sturm–Liouville operator $-d^2/dx^2$ with homogeneous Dirichlet boundary conditions on $(0,L)$.

\medskip

\noindent\textbf{(b) Time factors and eigenfunction expansion.}
For each eigenvalue $\lambda_n$, the corresponding time-dependent factor $T_n(t)$ satisfies
\[
T_n'(t)+\kappa\lambda_n T_n(t)=0,\qquad n=1,2,\dots.
\]
This is a first-order linear ordinary differential equation with constant coefficients. Its general solution is
\[
T_n(t)=C_n e^{-\kappa \lambda_n t}
= C_n \exp\!\left(-\kappa\left(\frac{n\pi}{L}\right)^2 t\right),
\]
where $C_n$ is a constant. Thus each mode produces a separated solution
\[
u_n(x,t) = X_n(x)T_n(t) 
= C_n \sin\left(\frac{n\pi x}{L}\right)
\exp\!\left(-\kappa\left(\frac{n\pi}{L}\right)^2 t\right).
\]

Because the heat equation is linear and homogeneous, any (finite) linear combination of such separated solutions is again a solution. Motivated by the completeness of the sine functions on $(0,L)$ (from Fourier series theory and Sturm–Liouville theory), we look for a general solution as an infinite series
\[
u(x,t) = \sum_{n=1}^\infty b_n \sin\left(\frac{n\pi x}{L}\right)
\exp\!\left(-\kappa\left(\frac{n\pi}{L}\right)^2 t\right),
\]
where the coefficients $b_n$ are to be determined from the initial condition.

At time $t=0$ this series becomes
\[
u(x,0) = \sum_{n=1}^\infty b_n \sin\left(\frac{n\pi x}{L}\right).
\]
Imposing the initial condition $u(x,0)=f(x)$, we arrive at the sine series expansion
\[
f(x) = \sum_{n=1}^\infty b_n \sin\left(\frac{n\pi x}{L}\right),
\qquad 0<x<L.
\]

The eigenfunctions $\sin(n\pi x/L)$ are orthogonal on $(0,L)$ with respect to the standard $L^2$ inner product:
\[
\int_0^L \sin\left(\frac{m\pi x}{L}\right) \sin\left(\frac{n\pi x}{L}\right)\,dx
=
\begin{cases}
0,& m\neq n,\\[4pt]
\dfrac{L}{2},& m=n.
\end{cases}
\]
This can be verified either by explicit integration or by using trigonometric identities. The orthogonality allows us to solve for the coefficients $b_n$ in the usual Fourier way. Multiply the expansion for $f(x)$ by $\sin(m\pi x/L)$ and integrate:
\[
\int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx
= \sum_{n=1}^\infty b_n \int_0^L 
\sin\left(\frac{n\pi x}{L}\right)\sin\left(\frac{m\pi x}{L}\right)\,dx.
\]
All terms with $n\neq m$ vanish by orthogonality, and the $n=m$ term gives
\[
\int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx
= b_m \cdot \frac{L}{2}.
\]
Therefore
\[
b_m = \frac{2}{L} \int_0^L f(x)\sin\left(\frac{m\pi x}{L}\right)\,dx,\qquad m=1,2,\dots.
\]
Hence the full solution of the initial–boundary value problem is
\[
u(x,t) =
\sum_{n=1}^\infty 
\left(\frac{2}{L} \int_0^L f(s)\sin\left(\frac{n\pi s}{L}\right)\,ds\right)
\sin\left(\frac{n\pi x}{L}\right)
\exp\!\left(-\kappa\left(\frac{n\pi}{L}\right)^2 t\right).
\]
This is an eigenfunction expansion in the orthogonal basis of eigenfunctions of the Sturm–Liouville operator $-d^2/dx^2$ with Dirichlet boundary conditions.

\medskip

\noindent\textbf{(c) Uniform initial temperature.}
Now we specialize to the initial data
\[
f(x)\equiv 1,\qquad 0<x<L.
\]
The coefficients become
\[
b_n = \frac{2}{L} \int_0^L 1\cdot \sin\left(\frac{n\pi x}{L}\right)\,dx.
\]
This integral is straightforward to compute:
\[
\int_0^L \sin\left(\frac{n\pi x}{L}\right)\,dx
= \left[-\frac{L}{n\pi}\cos\left(\frac{n\pi x}{L}\right)\right]_{x=0}^{x=L}
= -\frac{L}{n\pi}\Bigl(\cos(n\pi)-\cos 0\Bigr).
\]
Using $\cos(n\pi)=(-1)^n$ and $\cos 0 =1$, we obtain
\[
\int_0^L \sin\left(\frac{n\pi x}{L}\right)\,dx
= -\frac{L}{n\pi}\left((-1)^n-1\right)
= \frac{L}{n\pi}\left(1-(-1)^n\right).
\]
Therefore
\[
b_n = \frac{2}{L}\cdot \frac{L}{n\pi}\left(1-(-1)^n\right)
= \frac{2}{n\pi}\left(1-(-1)^n\right).
\]
If $n$ is even, then $(-1)^n=1$, so $b_n=0$. If $n$ is odd, write $n=2k+1$; then $(-1)^n=-1$, and
\[
b_n = \frac{2}{n\pi}(1-(-1)) = \frac{4}{n\pi}.
\]
Thus only the odd sine modes appear, and we can write
\[
b_n =
\begin{cases}
\dfrac{4}{n\pi},& n\ \text{odd},\\[4pt]
0,& n\ \text{even}.
\end{cases}
\]

The solution for a uniformly heated rod with ends clamped at zero temperature is therefore
\[
u(x,t) = \sum_{\substack{n=1\\ n\ \text{odd}}}^\infty 
\frac{4}{n\pi}\sin\left(\frac{n\pi x}{L}\right)
\exp\!\left(-\kappa\left(\frac{n\pi}{L}\right)^2 t\right).
\]
Equivalently, we may index over odd integers $n=2k+1$:
\[
u(x,t) = \sum_{k=0}^\infty \frac{4}{(2k+1)\pi}
\sin\left(\frac{(2k+1)\pi x}{L}\right)
\exp\!\left(-\kappa\
\left(\frac{(2k+1)\pi}{L}\right)^2 t\right).
\]

This series solution satisfies the heat equation, the homogeneous Dirichlet boundary conditions, and the uniform initial condition $u(x,0)=1$.

\end{solution}

% ===== Example 3: Legendre’s equation on an interval and nonconstant coefficients (inquiry-based) =====
\begin{problem}[Legendre’s equation on an interval and nonconstant coefficients]
The Legendre differential equation appears when solving Laplace’s equation in spherical coordinates for axisymmetric (no $\varphi$–dependence) potentials. After separating variables and making the change of variable $x = \cos \theta$, one arrives at an ordinary differential equation on the interval $-1 < x < 1$. On this interval, with suitable boundary conditions at $x = \pm 1$, the solutions turn out to be polynomials, the \emph{Legendre polynomials}, which arise as eigenfunctions of a Sturm–Liouville operator with variable coefficients. In this problem you will gradually uncover this structure and see how orthogonal polynomials fit naturally into the Sturm–Liouville framework.

Consider the differential equation
\[
(1 - x^2) y''(x) - 2 x\, y'(x) + \lambda\, y(x) = 0, \qquad -1 < x < 1,
\]
together with the requirement that $y(x)$ remain bounded as $x \to \pm 1$.

\medskip

(a) Rewrite the given equation in Sturm–Liouville form. That is, show that it can be written as
\[
\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + \bigl(\lambda\, w(x) - q(x)\bigr) y(x) = 0
\]
for suitable functions $p(x)$, $w(x)$, and $q(x)$ on $(-1,1)$. Identify $p$, $w$, and $q$ explicitly.

\emph{Hint:} Try to recognize the left-hand side of the original equation as a single derivative of the form $\dfrac{d}{dx}\bigl(\cdots y'(x)\bigr)$ plus the term involving $\lambda y$.

\medskip

(b) One important feature of a Sturm–Liouville problem is the associated self-adjoint differential operator. Define
\[
L[y](x) := -\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + q(x) y(x),
\]
with the $p$ and $q$ you found in part (a), acting on functions defined on $(-1,1)$.

\begin{enumerate}
\item[(i)] Write the Legendre equation in the form
\[
L[y] = \lambda\, w(x)\, y.
\]
\item[(ii)] Use Green’s identity for Sturm–Liouville operators,
\[
\int_{-1}^{1} \bigl(u\,L[v] - v\,L[u]\bigr)\,w(x)\,dx
= \bigl.p(x)\,\bigl(u(x)v'(x) - u'(x)v(x)\bigr)\bigr|_{x=-1}^{x=1},
\]
to explain what boundary behavior at $x = \pm 1$ is needed in order for $L$ to be formally self-adjoint on the space of \emph{bounded} functions on $[-1,1]$.

\end{enumerate}

\emph{Hint:} Observe that $p(x) = 1 - x^2$ vanishes at $x = \pm 1$. If $u$ and $v$ are bounded and not too wild near the endpoints, what happens to the boundary term?

\medskip

(c) Now look for power series solutions about $x = 0$. Assume
\[
y(x) = \sum_{k=0}^\infty a_k x^k,
\]
and substitute this into the differential equation to obtain a recurrence relation for the coefficients $a_k$.

\begin{enumerate}
\item[(i)] Find the recurrence relation expressing $a_{k+2}$ in terms of $a_k$ and $\lambda$.
\item[(ii)] Show that the even and odd coefficients decouple: if you choose $a_0$ and $a_1$ arbitrarily, then all even coefficients are determined from $a_0$, and all odd coefficients are determined from $a_1$.
\end{enumerate}

\emph{Hint:} After substituting the series into the ODE, carefully align powers of $x$ and compare coefficients of $x^k$ on both sides. You should obtain a relation of the form
\[
a_{k+2} = \frac{k(k+1) - \lambda}{(k+2)(k+1)}\,a_k.
\]

\medskip

(d) The requirement that the solution be bounded at $x = \pm 1$ forces a quantization of the parameter $\lambda$.

\begin{enumerate}
\item[(i)] Using your recurrence relation, argue that unless the series \emph{terminates}, the solution will not remain a polynomial and will typically fail to stay bounded as $x \to \pm 1$.
\item[(ii)] Show that the series terminates (so that $y$ is a polynomial) if and only if $\lambda = n(n+1)$ for some integer $n \ge 0$. In this case one obtains a polynomial of degree $n$, denoted $P_n(x)$, called the $n$th Legendre polynomial.
\item[(iii)] Compute $P_0(x)$, $P_1(x)$, and $P_2(x)$ explicitly by executing the recurrence up to the relevant degree.
\end{enumerate}

\emph{Hint:} For the series to terminate, there must be some index $k = n$ such that the numerator $k(k+1) - \lambda$ vanishes, making $a_{n+2} = 0$, and hence all higher $a_{n+2}, a_{n+4}, \dots$ vanish.

\medskip

(e) A central result in Sturm–Liouville theory is that eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to the weight function $w(x)$.

\begin{enumerate}
\item[(i)] Take two bounded solutions $y_n$ and $y_m$ of the Legendre equation with parameters $\lambda_n = n(n+1)$ and $\lambda_m = m(m+1)$, with $n \ne m$. Use the Sturm–Liouville form you found in part (a) to prove that
\[
\int_{-1}^{1} y_n(x)\,y_m(x)\,dx = 0.
\]
\item[(ii)] Conclude that the Legendre polynomials $\{P_n\}_{n = 0}^\infty$ form an orthogonal family in $L^2(-1,1)$ with respect to the inner product
\[
\langle f,g\rangle = \int_{-1}^{1} f(x)g(x)\,dx.
\]
\end{enumerate}

\emph{Hint:} Multiply the differential equation for $y_n$ by $y_m$, and the equation for $y_m$ by $y_n$, subtract them, and integrate over $[-1,1]$. Then use the boundary behavior identified in part (b) to handle the endpoint terms.

\medskip

(f) What if / extensions.

\begin{enumerate}
\item[(i)] The Legendre equation originally arises in the polar angle $\theta$ with a weight factor $\sin\theta$ in the inner product. If you start instead from the angular equation
\[
\frac{d}{d\theta}\Bigl(\sin\theta\,\frac{dY}{d\theta}\Bigr) + \lambda\,\sin\theta\, Y = 0, \qquad 0 < \theta < \pi,
\]
identify the coefficient $p(\theta)$ and weight $w(\theta)$ in Sturm–Liouville form. Then explain how the change of variable $x = \cos\theta$ transforms this into the Legendre equation with weight $w(x) = 1$.
\item[(ii)] Suppose we changed the interval to $[0,1]$ and imposed boundary conditions such as $y(0) = y(1) = 0$ instead of boundedness at $\pm 1$. How do you think the set of eigenvalues and eigenfunctions would change? Would the Legendre polynomials still appear in the same way?
\end{enumerate}

\end{problem}

% ===== Example 3: Legendre’s equation on an interval and nonconstant coefficients (full solution) =====
\begin{problem}[Legendre’s equation on an interval and nonconstant coefficients]
Consider the differential equation
\[
(1 - x^2)\,y''(x) - 2x\,y'(x) + \lambda\,y(x) = 0,\qquad -1 < x < 1,
\]
with the condition that $y(x)$ remains bounded as $x \to \pm 1$.

\begin{enumerate}
\item[(a)] Put this equation into Sturm–Liouville form, identify the functions $p(x)$, $w(x)$, and $q(x)$, and define the associated operator
\[
L[y] := -\dfrac{d}{dx}\bigl(p(x) y'(x)\bigr) + q(x) y(x).
\]
\item[(b)] By seeking a power series solution about $x=0$, derive the recurrence relation for the coefficients and show that bounded (polynomial) solutions exist only when $\lambda = n(n+1)$ for some integer $n \ge 0$. Denote these polynomial solutions by $P_n(x)$ and compute $P_0(x)$, $P_1(x)$, and $P_2(x)$ explicitly.
\item[(c)] Using the Sturm–Liouville structure, prove that if $m \ne n$, then
\[
\int_{-1}^{1} P_m(x)\,P_n(x)\,dx = 0.
\]
\end{enumerate}
Explain briefly how this example illustrates the general ideas of Sturm–Liouville spectral theory.

\end{problem}

\begin{solution}
We analyze the given equation
\[
(1 - x^2)\,y''(x) - 2x\,y'(x) + \lambda\,y(x) = 0,\qquad -1 < x < 1,
\]
under the requirement that $y$ remains bounded at $x = \pm 1$.

\medskip

\textbf{(a) Sturm–Liouville form and operator.}
A second-order Sturm–Liouville equation on an interval $(a,b)$ has the form
\[
\frac{d}{dx}\bigl(p(x)\,y'(x)\bigr) + \bigl(\lambda\,w(x) - q(x)\bigr)y(x) = 0,
\]
where $p$, $w$ are positive on $(a,b)$, and $q$ is real-valued.

We start from
\[
(1 - x^2)\,y'' - 2x\,y' + \lambda y = 0.
\]
Notice that
\[
\frac{d}{dx}\bigl((1 - x^2)\,y'(x)\bigr)
= (1 - x^2)\,y''(x) - 2x\,y'(x),
\]
so the equation can be rewritten as
\[
\frac{d}{dx}\bigl((1 - x^2)\,y'(x)\bigr) + \lambda\,y(x) = 0.
\]
This matches Sturm–Liouville form with
\[
p(x) = 1 - x^2,\qquad w(x) = 1,\qquad q(x) = 0.
\]
The associated Sturm–Liouville operator is therefore
\[
L[y](x) := -\frac{d}{dx}\bigl(p(x)\,y'(x)\bigr) + q(x)\,y(x)
= -\frac{d}{dx}\bigl((1 - x^2)\,y'(x)\bigr).
\]
In this notation, the Legendre equation becomes
\[
L[y] = \lambda\,w(x)\,y = \lambda\,y.
\]
Thus the Legendre equation is the eigenvalue problem
\[
L[y] = \lambda y,
\]
with $L$ a Sturm–Liouville operator having variable coefficient $p(x) = 1 - x^2$ and weight $w(x)=1$.

\medskip

\textbf{(b) Power series solution and eigenvalues $\lambda = n(n+1)$.}

\emph{Step 1: Derive the recurrence.}
We seek a solution as a power series about $x=0$:
\[
y(x) = \sum_{k=0}^\infty a_k x^k.
\]
Differentiating term by term,
\[
y'(x) = \sum_{k=1}^\infty k\,a_k x^{k-1},\qquad
y''(x) = \sum_{k=2}^\infty k(k-1)\,a_k x^{k-2}.
\]
Substitute these into the differential equation:
\[
(1 - x^2)\,y'' - 2x\,y' + \lambda y = 0.
\]
Compute each part:
\[
(1 - x^2)\,y'' = \sum_{k=2}^\infty k(k-1)a_k x^{k-2} - \sum_{k=2}^\infty k(k-1)a_k x^{k},
\]
and
\[
-2x\,y' = -2\sum_{k=1}^\infty k\,a_k x^{k}.
\]
Therefore
\[
(1 - x^2)\,y'' - 2x\,y'
= \sum_{k=2}^\infty k(k-1)a_k x^{k-2}
- \sum_{k=2}^\infty k(k-1)a_k x^{k}
- 2\sum_{k=1}^\infty k\,a_k x^{k}.
\]
Add $\lambda y = \lambda \sum_{k=0}^\infty a_k x^k$ and require that the total sum vanish.

To combine like powers, first reindex the term with $x^{k-2}$:
\[
\sum_{k=2}^\infty k(k-1)a_k x^{k-2}
= \sum_{j=0}^\infty (j+2)(j+1)\,a_{j+2} x^{j}.
\]
For the $x^k$–terms, we combine:
\[
-\sum_{k=2}^\infty k(k-1)a_k x^k - 2\sum_{k=1}^\infty k\,a_k x^k
= -\sum_{k=2}^\infty \bigl[k(k-1) + 2k\bigr]a_k x^k
= -\sum_{k=2}^\infty k(k+1)\,a_k x^k.
\]
Then including $\lambda y$,
\[
\lambda y = \lambda \sum_{k=0}^\infty a_k x^k.
\]

Thus the entire equation becomes
\[
\sum_{j=0}^\infty (j+2)(j+1)\,a_{j+2} x^{j}
+ \sum_{k=0}^\infty \bigl(\lambda - k(k+1)\bigr)a_k x^k = 0.
\]
We can rename $j$ as $k$ in the first sum. After that, combine the sums:
\[
\sum_{k=0}^\infty \Bigl[(k+2)(k+1)a_{k+2} + \bigl(\lambda - k(k+1)\bigr)a_k\Bigr]x^k = 0.
\]
For this power series to vanish identically, each coefficient must vanish, giving
\[
(k+2)(k+1)a_{k+2} + \bigl(\lambda - k(k+1)\bigr)a_k = 0,\qquad k=0,1,2,\dots
\]
or equivalently,
\[
a_{k+2} = \frac{k(k+1) - \lambda}{(k+2)(k+1)}\,a_k,\qquad k=0,1,2,\dots.
\]
This is the fundamental recurrence relation.

\emph{Step 2: Even and odd subsequences.}
The recurrence couples $a_{k+2}$ only to $a_k$. Thus the even coefficients
\[
a_0, a_2, a_4, \dots
\]
form a closed subsequence, and similarly the odd coefficients
\[
a_1, a_3, a_5, \dots
\]
form another. Concretely,
\[
a_2 \text{ is determined by } a_0,\quad
a_4 \text{ by } a_2,\quad \dots,
\]
and
\[
a_3 \text{ is determined by } a_1,\quad
a_5 \text{ by } a_3,\quad \dots.
\]
Thus, given $a_0$ and $a_1$, the entire solution is determined; the even part and odd part evolve independently.

\emph{Step 3: Termination of the series and quantization of $\lambda$.}
We now impose boundedness at $x = \pm 1$. If the power series does not terminate, the solution grows like an infinite series of powers $x^k$; one can show (for instance, by examining behavior near $x = \pm 1$ or comparing with known special functions) that generic infinite series solutions of Legendre’s equation develop singular behavior as $x \to \pm 1$. In contrast, if the series terminates, we obtain a polynomial, which is automatically bounded on $[-1,1]$.

From the recurrence
\[
a_{k+2} = \frac{k(k+1) - \lambda}{(k+2)(k+1)}\,a_k,
\]
we see that if for some integer $n \ge 0$ the numerator vanishes at $k = n$, namely
\[
n(n+1) - \lambda = 0,
\]
then $a_{n+2} = 0$. Once a coefficient in a subsequence is zero, all later ones in that subsequence vanish as well, because subsequent terms involve multiplication by finite factors. Hence, if
\[
\lambda = n(n+1),
\]
then either the even or the odd subsequence terminates (depending on whether $n$ is even or odd), and we obtain a polynomial solution of degree $n$. These are the \emph{Legendre polynomials} $P_n(x)$.

Conversely, if $\lambda$ is not of the form $n(n+1)$, the recurrence never forces a coefficient to vanish, so the series does not truncate. The corresponding solution is not a polynomial and, upon more detailed analysis, does not stay bounded at both endpoints. Thus the admissible eigenvalues are precisely
\[
\lambda_n = n(n+1),\qquad n=0,1,2,\dots.
\]

\emph{Step 4: Computing the first few $P_n$.}
We can find low-degree polynomials explicitly.

\underline{$n = 0$:} Take $\lambda = 0$. The recurrence becomes
\[
a_{k+2} = \frac{k(k+1)}{(k+2)(k+1)} a_k = \frac{k}{k+2}\,a_k.
\]
If we choose $a_0 = 1$ and $a_1 = 0$, the even sequence is
\[
a_0 = 1,\quad a_2 = 0,\quad a_4 = 0,\dots,
\]
so $y(x) = 1$. This is the degree–$0$ polynomial
\[
P_0(x) = 1.
\]

\underline{$n = 1$:} Take $\lambda = 1\cdot 2 = 2$. The recurrence is
\[
a_{k+2} = \frac{k(k+1) - 2}{(k+2)(k+1)} a_k.
\]
If we choose $a_1 = 1$ and $a_0 = 0$, then
\[
a_3 = \frac{1\cdot 2 - 2}{3\cdot 2}a_1 = 0,\quad
a_5 = 0,\dots,
\]
so only $a_1$ is nonzero in the odd sequence, and we obtain
\[
y(x) = x.
\]
This is the degree–$1$ polynomial
\[
P_1(x) = x.
\]

\underline{$n = 2$:} Take $\lambda = 2\cdot 3 = 6$. Then
\[
a_{k+2} = \frac{k(k+1) - 6}{(k+2)(k+1)} a_k.
\]
Choosing $a_0 = 1$ and $a_1 = 0$ gives for the even sequence
\[
a_2 = \frac{0\cdot 1 - 6}{2\cdot 1} a_0 = -3,\qquad
a_4 = \frac{2\cdot 3 - 6}{4\cdot 3} a_2 = 0,
\]
and higher even coefficients vanish. Hence
\[
y(x) = a_0 + a_2 x^2 = 1 - 3x^2.
\]
Up to normalization, this is $P_2$. The standard choice is to normalize so that $P_n(1)=1$, which is true here, and in fact one usually writes
\[
P_2(x) = \frac{1}{2}(3x^2 - 1).
\]
This differs from $1 - 3x^2$ by an overall factor of $-\tfrac{1}{2}$, which does not affect the property of being an eigenfunction or an orthogonal polynomial. Thus
\[
P_0(x) = 1,\qquad
P_1(x) = x,\qquad
P_2(x) = \frac{1}{2}(3x^2 - 1).
\]

\medskip

\textbf{(c) Orthogonality of $P_m$ and $P_n$ for $m \ne n$.}

The key idea from Sturm–Liouville theory is that eigenfunctions associated with distinct eigenvalues are orthogonal with respect to the weight $w(x)$. Here $w(x) = 1$.

Let $P_n$ and $P_m$ be two bounded solutions of the Legendre equation with parameters $\lambda_n = n(n+1)$ and $\lambda_m = m(m+1)$, where $n \ne m$. They satisfy
\[
\frac{d}{dx}\bigl((1 - x^
2)P_n'(x)\bigr) + \lambda_n P_n(x) = 0,\qquad
\frac{d}{dx}\bigl((1 - x^2)P_m'(x)\bigr) + \lambda_m P_m(x) = 0.
\]
Multiply the first by $P_m$ and the second by $P_n$, then subtract:
\[
P_m\,\frac{d}{dx}\bigl((1 - x^2)P_n'\bigr)
- P_n\,\frac{d}{dx}\bigl((1 - x^2)P_m'\bigr)
+ (\lambda_n - \lambda_m)P_n P_m = 0.
\]
The first two terms combine into a single derivative:
\[
\frac{d}{dx}\Bigl[(1 - x^2)\bigl(P_m P_n' - P_n P_m'\bigr)\Bigr]
+ (\lambda_n - \lambda_m)P_n P_m = 0.
\]
Integrate from $-1$ to $1$:
\[
\int_{-1}^{1}\frac{d}{dx}\Bigl[(1 - x^2)\bigl(P_m P_n' - P_n P_m'\bigr)\Bigr]\,dx
+ (\lambda_n - \lambda_m)\int_{-1}^{1}P_n P_m\,dx = 0.
\]
Thus
\[
(\lambda_n - \lambda_m)\int_{-1}^{1}P_n(x)P_m(x)\,dx
= -\Bigl[(1 - x^2)\bigl(P_m P_n' - P_n P_m'\bigr)\Bigr]_{x=-1}^{x=1}.
\]
But $P_n$ and $P_m$ are polynomials, so $P_n$, $P_m$, $P_n'$, and $P_m'$ are all bounded at $\pm1$, and
\[
1 - x^2 = 0\quad\text{at}\quad x = \pm1,
\]
so the boundary term vanishes:
\[
\Bigl[(1 - x^2)\bigl(P_m P_n' - P_n P_m'\bigr)\Bigr]_{x=-1}^{x=1} = 0.
\]
Hence
\[
(\lambda_n - \lambda_m)\int_{-1}^{1}P_n(x)P_m(x)\,dx = 0.
\]
For $m \ne n$ we have $\lambda_n \ne \lambda_m$, so
\[
\int_{-1}^{1}P_n(x)P_m(x)\,dx = 0.
\]
Thus the Legendre polynomials corresponding to distinct eigenvalues are orthogonal in $L^2(-1,1)$ with respect to the inner product
\[
\langle f,g\rangle = \int_{-1}^{1} f(x)g(x)\,dx.
\]

\medskip

\textbf{Connection with Sturm–Liouville spectral theory.}

In summary:

- The Legendre equation has been written as a Sturm–Liouville eigenvalue problem
  \[
  L[y] = \lambda y,\qquad
  L[y] = -\frac{d}{dx}\bigl((1 - x^2)y'(x)\bigr),
  \]
  on $(-1,1)$, with weight $w(x)=1$ and boundary condition that $y$ remain bounded at $x=\pm1$.

- The boundary condition, together with the self-adjoint structure of $L$, forces a \emph{discrete} spectrum of eigenvalues
  \[
  \lambda_n = n(n+1),\quad n=0,1,2,\dots,
  \]
  obtained by requiring the power series to terminate, yielding polynomial eigenfunctions.

- The corresponding eigenfunctions $P_n$ (Legendre polynomials) are orthogonal with respect to the weight $w(x)$, exactly as predicted by Sturm–Liouville theory for eigenfunctions of a self-adjoint operator.

This example thus exhibits the typical Sturm–Liouville features: a self-adjoint second-order differential operator with variable coefficients, a discrete set of real eigenvalues, and an associated family of orthogonal eigenfunctions (here, the Legendre polynomials), which form the natural basis for expanding solutions of related boundary value problems (such as Laplace’s equation in spherical coordinates).

\end{solution}

% ===== Example 4: Bessel’s equation and radial modes in a circular drum (inquiry-based) =====
\begin{problem}[Bessel’s equation and radial modes in a circular drum]
Consider a thin, circular drumhead of radius $a$ that is tightly clamped along its boundary. Small vertical vibrations of the membrane are governed (to a good approximation) by the two-dimensional wave equation with fixed boundary conditions. In polar coordinates, the geometry suggests that it may be natural to separate variables into a radial part and an angular part. In this problem you will discover that the radial equation is a Sturm–Liouville problem whose eigenfunctions are Bessel functions, and that the allowed frequencies of vibration are determined by the zeros of these Bessel functions.

We take the vertical displacement of the membrane to be $u(r,\theta,t)$, where $(r,\theta)$ are polar coordinates in the disk $0 \le r < a$, $0 \le \theta < 2\pi$, and $t$ is time. The membrane is clamped along the boundary $r=a$, so $u(a,\theta,t)=0$ for all $\theta$ and $t$. Let $c>0$ be the wave speed. The governing equation is
\[
u_{tt} = c^2 \Delta u, \qquad 0\le r<a,\ 0\le\theta<2\pi,\ t>0,
\]
with
\[
u(a,\theta,t)=0, \qquad u \text{ bounded as } r\to 0.
\]

\medskip

(a) Write the Laplacian $\Delta$ in polar coordinates $(r,\theta)$ and rewrite the wave equation in these coordinates. Then seek separated solutions of the form
\[
u(r,\theta,t) = R(r)\,\Theta(\theta)\,T(t).
\]
Carry out the standard separation-of-variables procedure and show that the time factor $T$ satisfies a familiar ordinary differential equation involving a separation constant $\lambda>0$. What equation does $T$ satisfy, and what is the physical interpretation of $\sqrt{\lambda}$?

% Hint: Aim for an equation of the form $T'' + \lambda c^2 T=0$, where $\sqrt{\lambda}c$ plays the role of an angular frequency.

\medskip

(b) Continuing from part (a), separate the spatial variables by writing $w(r,\theta) = R(r)\Theta(\theta)$ and considering the Helmholtz eigenvalue problem
\[
\Delta w + \lambda w = 0.
\]
Show that, after dividing appropriately by $R(r)\Theta(\theta)$, you can separate the angular and radial dependence to obtain an equation of the form
\[
\frac{1}{\Theta}\,\Theta''(\theta) = -m^2
\]
for some constant $m^2$, and a corresponding radial equation. What conditions must $\Theta(\theta)$ satisfy in order for $u$ to be single-valued and $2\pi$-periodic in $\theta$? What values of $m$ are therefore allowed?

% Hint: Enforce $\Theta(\theta+2\pi)=\Theta(\theta)$ and show that this periodicity forces $m$ to be an integer.

\medskip

(c) Show that, for each integer $m\ge 0$, the radial function $R(r)$ satisfies an equation of the form
\[
r^2 R''(r) + r R'(r) + \bigl(k^2 r^2 - m^2\bigr) R(r) = 0, \qquad 0<r<a,
\]
for an appropriate constant $k>0$ depending on $\lambda$. Recognize this as Bessel's differential equation of order $m$, and write the general solution in terms of standard Bessel functions. Which two linearly independent Bessel-type functions appear, and what is their behavior as $r\to 0$?

% Hint: Recall that Bessel's equation of order $m$ is $x^2 y'' + x y' + (x^2 - m^2)y=0$, with independent solutions $J_m(x)$ and $Y_m(x)$. Compare $x=kr$ with the equation above.

\medskip

(d) Impose the physical and boundary conditions on the radial function $R(r)$.

\quad(i) Use the requirement that $u$ be finite (and physically reasonable) at $r=0$ to decide which combination of the two Bessel solutions from part (c) is admissible.

\quad(ii) Impose the clamped boundary condition $u(a,\theta,t)=0$ to obtain a condition on $R(a)$. Show that this condition forces $k a$ to be a zero of the Bessel function $J_m$. Denote by $j_{m,n}$ the $n$-th positive zero of $J_m$, and deduce that the allowed values of $k$ are $k_{m,n} = j_{m,n}/a$. 

\quad(iii) Conclude that the corresponding spatial eigenfunctions for the vibrating drum can be written (up to normalization) as
\[
w_{m,n}(r,\theta) = J_m\!\bigl(j_{m,n} r/a\bigr)\,\bigl(A\cos(m\theta)+B\sin(m\theta)\bigr),
\]
and that the associated temporal frequencies are
\[
\omega_{m,n} = c\,\frac{j_{m,n}}{a}.
\]

% Hint: For (i), recall that $Y_m(x)$ blows up as $x\to 0$, whereas $J_m(x)$ remains finite. For (ii), $R(a)=0$ gives $J_m(ka)=0$.

\medskip

(e) Sturm–Liouville structure and orthogonality.

\quad(i) Rewrite the radial equation from part (c) in Sturm–Liouville form
\[
\frac{d}{dr}\!\left(p(r)\,\frac{dR}{dr}\right) + \bigl(\lambda\,w(r) - q(r)\bigr)R(r) = 0,
\]
for appropriate functions $p(r)$, $w(r)$, and $q(r)$. Identify these three functions explicitly.

\quad(ii) Using the general Sturm–Liouville theory, argue (without detailed proof of the general theorem) that the radial eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to a weight. What is the weight function here, and how does this lead to an orthogonality relation of the form
\[
\int_0^a r\, J_m\!\bigl(j_{m,n} r/a\bigr)\,J_m\!\bigl(j_{m,\ell} r/a\bigr)\,dr = 0 \quad\text{when } n\neq \ell?
\]

\medskip

(f) What if / extensions.

\quad(i) Suppose instead that the membrane were \emph{free} at the boundary $r=a$, so that the radial derivative of $u$ vanishes there. How would the boundary condition on $R$ change, and what condition on $J_m$ (or its derivative) would now select the allowed eigenvalues?

\quad(ii) Consider an annular membrane with inner radius $r=b>0$ and outer radius $r=a>b$, both clamped. How would the radial solution change qualitatively? In particular, would you still discard one of the two Bessel-type solutions from part (c)? Briefly explain.

% Hint: On an annulus, the origin is no longer part of the domain, so singular behavior at $r=0$ may not be relevant. Both $J_m$ and $Y_m$ can occur in the general solution, with boundary conditions at $r=b$ and $r=a$ determining the allowed eigenvalues.
\end{problem}

% ===== Example 4: Bessel’s equation and radial modes in a circular drum (full solution) =====
\begin{problem}[Bessel’s equation and radial modes in a circular drum]
Consider a circular membrane of radius $a$, clamped along its boundary, whose small transverse vibrations satisfy
\[
u_{tt} = c^2 \Delta u \quad \text{in } \{(r,\theta): 0\le r<a,\ 0\le\theta<2\pi\},
\]
with boundary condition $u(a,\theta,t)=0$ and $u$ bounded at $r=0$. 

(a) Write the Laplacian in polar coordinates and seek separated solutions $u(r,\theta,t)=R(r)\Theta(\theta)T(t)$. Show that the spatial part $w(r,\theta)=R(r)\Theta(\theta)$ satisfies the Helmholtz equation
\[
\Delta w + \lambda w = 0,
\]
and that the angular factor satisfies $\Theta'' + m^2 \Theta=0$ with $m\in\mathbb{Z}$.

(b) Show that the corresponding radial factor satisfies
\[
r^2 R''(r) + r R'(r) + \bigl(k^2 r^2 - m^2\bigr) R(r) = 0,\qquad 0<r<a,
\]
where $k^2=\lambda>0$. Identify this as Bessel’s equation of order $m$ and write the general solution in terms of $J_m$ and $Y_m$. Use the boundedness condition at $r=0$ to select the admissible solution.

(c) Impose the boundary condition $u(a,\theta,t)=0$ to show that $k a$ must be a zero of $J_m$, say $j_{m,n}$. Deduce that the allowed wave numbers are $k_{m,n}=j_{m,n}/a$ and that, up to normalization,
\[
w_{m,n}(r,\theta) = J_m\!\bigl(j_{m,n} r/a\bigr)\bigl(A\cos(m\theta)+B\sin(m\theta)\bigr).
\]
Find the corresponding temporal frequencies $\omega_{m,n}$.

(d) Rewrite the radial equation in Sturm–Liouville form and identify $p(r)$, $w(r)$, and $q(r)$. Using Sturm–Liouville theory, state the orthogonality relation for the set $\{J_m(j_{m,n} r/a)\}_{n=1}^\infty$ on $[0,a]$, including the appropriate weight function.

\end{problem}

\begin{solution}
We analyse the membrane vibrations by separation of variables in polar coordinates and then interpret the resulting radial equation as a Sturm–Liouville problem.

\medskip

\textbf{(a) Separation and angular dependence.}
In polar coordinates $(r,\theta)$, the Laplacian is
\[
\Delta u \;=\; u_{rr} + \frac{1}{r}u_r + \frac{1}{r^2}u_{\theta\theta}.
\]
The wave equation therefore becomes
\[
u_{tt} \;=\; c^2\left(u_{rr} + \frac{1}{r}u_r + \frac{1}{r^2}u_{\theta\theta}\right).
\]

We look for separated solutions of the form
\[
u(r,\theta,t) = R(r)\,\Theta(\theta)\,T(t).
\]
Substituting into the wave equation and dividing by $R\Theta T$ gives
\[
\frac{T''(t)}{c^2 T(t)} = \frac{R''(r)}{R(r)} + \frac{1}{r}\frac{R'(r)}{R(r)} + \frac{1}{r^2}\frac{\Theta''(\theta)}{\Theta(\theta)}.
\]
The left-hand side depends only on $t$, while the right-hand side depends only on $r$ and $\theta$. Hence both sides must equal a constant, say $-\lambda$, with $\lambda>0$ chosen for convenience:
\[
\frac{T''}{c^2 T} = -\lambda.
\]
Thus $T$ satisfies
\[
T''(t) + \lambda c^2 T(t) = 0.
\]
This is the harmonic oscillator equation. Its solutions are sinusoidal with angular frequency $\omega = c\sqrt{\lambda}$, so $\sqrt{\lambda}$ represents the spatial wave number (up to the factor $c$).

The spatial factor $w(r,\theta)=R(r)\Theta(\theta)$ then satisfies
\[
\Delta w + \lambda w = 0,
\]
the Helmholtz eigenvalue problem for the Laplacian on the disk.

To separate $r$ and $\theta$ in the spatial equation, we write
\[
\Delta w + \lambda w = 0 \quad\Rightarrow\quad
R''\Theta + \frac{1}{r}R'\Theta + \frac{1}{r^2}R\Theta'' + \lambda R\Theta = 0.
\]
Dividing by $R\Theta$ gives
\[
\frac{R''}{R} + \frac{1}{r}\frac{R'}{R} + \lambda
+ \frac{1}{r^2}\frac{\Theta''}{\Theta} = 0.
\]
Multiplying by $r^2$,
\[
r^2\left(\frac{R''}{R} + \frac{1}{r}\frac{R'}{R} + \lambda\right) + \frac{\Theta''}{\Theta} = 0.
\]
The first term depends only on $r$, and the second only on $\theta$, so each must equal a constant. Denote this constant by $-m^2$:
\[
\frac{\Theta''(\theta)}{\Theta(\theta)} = -m^2,
\]
and correspondingly
\[
r^2\left(\frac{R''}{R} + \frac{1}{r}\frac{R'}{R} + \lambda\right) = m^2.
\]

The angular equation is then
\[
\Theta''(\theta) + m^2 \Theta(\theta) = 0.
\]
Because the physical displacement must be single-valued and periodic in $\theta$ with period $2\pi$, we require $\Theta(\theta+2\pi)=\Theta(\theta)$ for all $\theta$. The general solution is
\[
\Theta(\theta) = A\cos(m\theta) + B\sin(m\theta).
\]
This is $2\pi$-periodic if and only if $m$ is an integer. Hence $m\in\mathbb{Z}$.

\medskip

\textbf{(b) The radial equation and Bessel’s equation.}
From the separated form above,
\[
r^2\left(\frac{R''}{R} + \frac{1}{r}\frac{R'}{R} + \lambda\right) = m^2.
\]
Multiplying through by $R$ and rearranging yields
\[
r^2 R''(r) + r R'(r) + \bigl(\lambda r^2 - m^2\bigr) R(r) = 0.
\]
It is customary to write $\lambda = k^2$ with $k>0$, so the equation becomes
\[
r^2 R''(r) + r R'(r) + \bigl(k^2 r^2 - m^2\bigr) R(r) = 0,\qquad 0<r<a.
\]

This is exactly Bessel’s differential equation of order $m$, with independent variable $kr$ in place of the usual $x$. Indeed, the standard Bessel equation of order $\nu$ is
\[
x^2 y''(x) + x y'(x) + \bigl(x^2 - \nu^2\bigr)y(x) = 0.
\]
By setting $x = kr$ and $\nu = m$, we see that $R(r)$ as a function of $kr$ satisfies this equation. The two linearly independent standard solutions are the Bessel functions of the first and second kind, $J_m(x)$ and $Y_m(x)$. Thus the general radial solution is
\[
R(r) = A J_m(kr) + B Y_m(kr),
\]
for constants $A$ and $B$.

\medskip

\textbf{(c) Regularity at the origin and boundary condition at $r=a$.}
We next apply the physical and boundary conditions.

\emph{Behavior at $r=0$.} The displacement must remain finite at the center of the drum, so $R(r)$ should be bounded as $r\to 0$. The known asymptotics of Bessel functions near the origin are:
\[
J_m(x) \sim \frac{1}{m!}\left(\frac{x}{2}\right)^m \quad\text{as } x\to 0,
\]
so $J_m(x)$ is bounded (and in fact analytic) at $x=0$. On the other hand, for $m\ge 0$,
\[
Y_m(x) \sim 
\begin{cases}
\frac{2}{\pi}\left(\log x + \gamma\right), & m=0,\\[4pt]
-\dfrac{(m-1)!}{\pi}\left(\dfrac{2}{x}\right)^m, & m\ge 1,
\end{cases}
\quad\text{as } x\to 0,
\]
so $Y_m(x)$ either diverges logarithmically (for $m=0$) or like a negative power of $x$ (for $m\ge 1$). In either case, $Y_m(kr)$ is unbounded near $r=0$.

To have a physically reasonable (bounded) solution at the center, we must set $B=0$. Thus
\[
R(r) = A J_m(kr).
\]

\emph{Boundary condition at $r=a$.} The membrane is clamped along the boundary, so $u(a,\theta,t)=0$ for all $\theta$ and $t$. For separated solutions $u=R\Theta T$ with $T$ not identically zero, this requires
\[
R(a)\,\Theta(\theta)\,T(t) = 0 \quad \text{for all }\theta,t.
\]
To avoid trivial angular and temporal factors, we must have
\[
R(a) = 0.
\]
With $R(r)=A J_m(kr)$, this becomes
\[
J_m(ka) = 0.
\]
Thus the allowed values of $k$ are precisely those for which $ka$ is a zero of $J_m$. Let $j_{m,n}$ denote the $n$-th positive zero of $J_m$. Then
\[
k_{m,n}a = j_{m,n} \quad\Longrightarrow\quad k_{m,n} = \frac{j_{m,n}}{a}.
\]

Therefore the admissible radial eigenfunctions are, up to normalization,
\[
R_{m,n}(r) = J_m\!\bigl(j_{m,n} r/a\bigr).
\]
Combining the radial and angular factors, the spatial eigenfunctions are
\[
w_{m,n}(r,\theta) = J_m\!\bigl(j_{m,n} r/a\bigr)\,\bigl(A\cos(m\theta)+B\sin(m\theta)\bigr),
\]
for integers $m\ge 0$ and $n\ge 1$.

\emph{Temporal frequencies.} Returning to the time equation, we had
\[
T''(t) + \lambda c^2 T(t) = 0,\qquad \lambda = k^2.
\]
For the mode with wave number $k_{m,n}=j_{m,n}/a$, the equation is
\[
T''(t) + c^2 k_{m,n}^2 T(t) = 0,
\]
so the temporal angular frequency is
\[
\omega_{m,n} = c\,k_{m,n} = c\,\frac{j_{m,n}}{a}.
\]
Each pair $(m,n)$ labels a distinct normal mode of vibration with frequency $\omega_{m,n}$.

\medskip

\textbf{(d) Sturm–Liouville form and orthogonality.}
We now recast the radial equation as a Sturm–Liouville problem and note the corresponding orthogonality properties.

The radial equation with eigenvalue parameter $\lambda=k^2$ is
\[
r^2 R''(r) + r R'(r) + \bigl(\lambda r^2 - m^2\bigr) R(r) = 0.
\]
Divide through by $r$:
\[
r R''(r) + R'(r) + \bigl(\lambda r - \tfrac{m^2}{r}\bigr) R(r) = 0.
\]
This may be written as
\[
\frac{d}{dr}\!\bigl(r R'(r)\bigr) + \bigl(\lambda r - \tfrac{m^2}{r}\bigr)R(r) = 0.
\]
Thus the equation has Sturm–Liouville form
\[
\frac{d}{dr}\!\left(p(r)\,\frac{dR}{dr}\right) + \bigl(\lambda\,w(r) - q(r)\bigr)R(r) = 0
\]
with
\[
p(r) = r,\qquad w(r) = r,\qquad q(r) = \frac{m^2}{r}.
\]
The interval is $(0,a)$, with boundary conditions
\[
R(0) \text{ bounded}, \qquad R(a)=0.
\]
These conditions define a regular (after a suitable interpretation at $r=0$) self-adjoint Sturm–Liouville problem. Sturm–Liouville theory then asserts, among other things, that:

\begin{itemize}
\item All eigenvalues $\lambda_{m,n}=k_{m,n}^2=(j_{m,n}/a)^2$ are real and form an increasing discrete sequence with no finite accumulation point.
\item Eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to the weight $w(r)=r$ on $(0,a)$.
\end{itemize}

For fixed $m$, the radial eigenfunctions are $R_{m,n}(r) = J_m(j_{m,n} r/a)$ (up to scaling). The orthogonality statement takes the explicit form
\[
\int_0^a r\, J_m\!\bigl(j_{m,n} r/a\bigr)\, J_m\!\bigl(j_{m,\ell} r/a\bigr)\, dr = 0,
\quad\text{whenever } n\neq \ell.
\]
This weighted orthogonality is exactly the radial version of the standard Sturm–Liouville inner product
\[
\langle f,g\rangle = \int_0^a f(r)\,\overline{g(r)}\,w(r)\,dr,
\]
with weight $w(r)=r$.

\medskip

\textbf{Conceptual summary.}
This example illustrates the core ideas of Sturm–Liouville (spectral) theory in a physically motivated setting:

\begin{itemize}
\item Separation of variables for the wave equation on a disk leads to an eigenvalue problem for the Laplacian, $\Delta w + \lambda w=0$, on a bounded domain with Dirichlet boundary conditions.
\item The geometry in polar coordinates transforms the radial part of the Laplacian into a Sturm–Liouville operator with weight $w(r)=r$.
\item Regularity at the origin and boundary conditions at $r=a$ single out a discrete set of eigenvalues $\lambda_{m,n}=(j_{m,n}/a)^2$ corresponding to zeros of Bessel functions $J_m$.
\item The associated eigenfunctions $J_m(j_{m,n} r/a)$ are orthogonal with respect to the weight $r$, which allows expansions of general initial data into series of normal modes.
\end{itemize}

Thus, the vibration modes of a circular drum provide a canonical example of how Sturm–Liouville theory, special functions (Bessel functions), and physical boundary conditions combine to produce a discrete spectrum of eigenvalues and an orthogonal basis of eigenfunctions.
\end{solution}

% ===== Example 5: Transforming a general second-order ODE into Sturm–Liouville form (inquiry-based) =====
\begin{problem}[Transforming a general second-order ODE into Sturm–Liouville form]
Many boundary value problems in applications lead to second-order equations of the form
\[
a(x) y''(x) + b(x) y'(x) + c(x) y(x) = \lambda\, r(x) y(x)
\]
on some interval $[\,\alpha,\beta\,]$, where $a,b,c,r$ are given real-valued coefficient functions and $\lambda$ is an eigenvalue parameter. At first sight this operator is not symmetric with respect to the standard $L^2$ inner product, mainly because of the first-derivative term $b(x) y'$. However, in many cases one can reveal a hidden self-adjoint (Sturm–Liouville) structure by multiplying the equation by a carefully chosen integrating factor. This leads to a weighted inner product and clarifies how the boundary conditions must be chosen to obtain a self-adjoint eigenvalue problem with real eigenvalues and orthogonal eigenfunctions.

In this problem you will discover, step by step, how to transform a general second-order linear ODE into Sturm–Liouville form, and how the weight function and boundary terms emerge from this transformation.

\smallskip

(a) Recall that a (regular) Sturm–Liouville problem on $[\alpha,\beta]$ typically has the form
\[
\frac{d}{dx}\!\bigl(p(x) y'(x)\bigr) + \bigl[\lambda\, w(x) - q(x)\bigr] y(x) = 0,
\]
together with separated boundary conditions, for example
\[
\alpha_1 y(\alpha) + \alpha_2 y'(\alpha) = 0, 
\qquad 
\beta_1 y(\beta) + \beta_2 y'(\beta) = 0,
\]
where $p,w,q$ are given real-valued functions with $p(x)>0$ and $w(x)>0$ on $[\alpha,\beta]$.

\begin{enumerate}
\item[(i)] Rewrite this Sturm–Liouville equation in the form
\[
L[y] = \lambda\, w(x) y,
\]
and identify explicitly the differential operator $L$.
\item[(ii)] Using integration by parts (formal calculations are fine), show that for sufficiently smooth functions $u$ and $v$ one has a Green-type identity of the form
\[
\int_\alpha^\beta \bigl(u\,L[v] - v\,L[u]\bigr)\,dx 
= \Bigl[\,p(x)\,\bigl(u(x)v'(x) - u'(x)v(x)\bigr)\Bigr]_{\alpha}^{\beta}.
\]
(Hint: Start from $u\,(p v')' - v\,(p u')'$ and integrate over $[\alpha,\beta]$.)
\item[(iii)] Explain briefly why, if the boundary conditions ensure that the boundary term on the right-hand side always vanishes for admissible $u$ and $v$, then the operator $L$ is formally self-adjoint with respect to the standard inner product
\[
\langle u, v\rangle_{L^2_w} \;=\; \int_{\alpha}^{\beta} u(x)\,\overline{v(x)}\,w(x)\,dx
\]
up to the weight $w(x)$.
\end{enumerate}

\medskip

(b) Now consider the more general eigenvalue problem
\[
a(x) y''(x) + b(x) y'(x) + c(x) y(x) = \lambda\, r(x) y(x), \qquad \alpha < x < \beta,
\]
with real-valued continuous functions $a,b,c,r$ on $[\alpha,\beta]$, and suppose that
\[
a(x) > 0, \qquad r(x) > 0 \quad \text{for all } x\in[\alpha,\beta].
\]
We would like to transform this equation into Sturm–Liouville form by multiplying by a suitable integrating factor.

\begin{enumerate}
\item[(i)] Suppose we multiply the equation by an unknown, positive function $\mu(x)$ and set
\[
p(x) := \mu(x)\,a(x).
\]
Show that
\[
\mu(x)\,a(x)\,y''(x) + \mu(x)\,b(x)\,y'(x)
\]
can be written as $\dfrac{d}{dx}\bigl(p(x) y'(x)\bigr)$ if and only if $p$ satisfies a certain first-order differential equation. Write down this differential equation in the form
\[
\bigl(\mu(x)\,a(x)\bigr)' = \mu(x)\,b(x),
\]
and then expand this derivative to obtain a first-order linear ODE for $\mu(x)$ alone.
\item[(ii)] Rearrange the resulting equation into the form
\[
\frac{\mu'(x)}{\mu(x)} 
= \frac{b(x) - a'(x)}{a(x)},
\]
and explain why this is analogous to finding an integrating factor for a first-order linear ODE.
\end{enumerate}

\medskip

(c) Continue with part (b). 

\begin{enumerate}
\item[(i)] Solve the first-order ODE
\[
\frac{\mu'(x)}{\mu(x)} 
= \frac{b(x) - a'(x)}{a(x)}
\]
to obtain an explicit formula for $\mu(x)$ in terms of $a$ and $b$. You may leave your answer as an exponential of an integral.
% Hint: Treat this as a separable ODE for \mu, or equivalently integrate d(\ln \mu).

\item[(ii)] Using your expression for $\mu(x)$, define
\[
p(x) := \mu(x) a(x),
\qquad
q(x) := -\,\mu(x)c(x),
\qquad
w(x) := \mu(x) r(x).
\]
Show that the original eigenvalue equation can now be written in the Sturm–Liouville form
\[
\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + \bigl[\lambda\, w(x) - q(x)\bigr] y(x) = 0.
\]
(Hint: Multiply the original equation by $\mu$ and use your defining relation for $p$.)
\end{enumerate}

\medskip

(d) In this step, connect the transformation to self-adjointness and orthogonality.

\begin{enumerate}
\item[(i)] Using the functions $p,q,w$ from part (c), write down the associated Sturm–Liouville operator
\[
L_{\text{SL}}[y] := -\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + q(x) y(x),
\]
so that the eigenvalue problem becomes
\[
L_{\text{SL}}[y] = \lambda\, w(x)\,y.
\]
Explain briefly why this operator is formally self-adjoint with respect to the weighted inner product
\[
\langle u, v\rangle_{L^2_w} 
= \int_{\alpha}^{\beta} u(x)\,\overline{v(x)}\,w(x)\,dx,
\]
provided that the boundary terms from the Green-type identity vanish for admissible functions $u$ and $v$.

\item[(ii)] Suppose we impose separated boundary conditions of the form
\[
\alpha_1 y(\alpha) + \alpha_2 y'(\alpha) = 0, 
\qquad 
\beta_1 y(\beta) + \beta_2 y'(\beta) = 0,
\]
with real constants $\alpha_1,\alpha_2,\beta_1,\beta_2$ not both zero at each endpoint. Show that the boundary term
\[
p(x)\,\bigl(u(x)v'(x)-u'(x)v(x)\bigr)\Big|_{x=\alpha}^{x=\beta}
\]
vanishes for all pairs of functions $u$ and $v$ satisfying the same boundary conditions, and hence the problem is self-adjoint. 
% Hint: Express u'(\alpha) and v'(\alpha) in terms of u(\alpha), v(\alpha) using the boundary conditions (when \alpha_2\neq 0); treat cases \alpha_2=0 or \beta_2=0 separately.

\item[(iii)] Explain, at a high level, how self-adjointness of $L_{\text{SL}}$ with respect to $\langle\cdot,\cdot\rangle_{L^2_w}$ leads to the reality of eigenvalues and orthogonality of eigenfunctions corresponding to different eigenvalues.
\end{enumerate}

\medskip

(e) Explore a few variations and extensions.

\begin{enumerate}
\item[(i)] What can go wrong in the transformation if $a(x)$ vanishes somewhere in $[\alpha,\beta]$? What if $r(x)$ changes sign? Discuss how these issues affect the definition of $p(x)$ and the interpretation of $w(x)$ as a weight function.
\item[(ii)] Apply the transformation you derived to the concrete example
\[
y''(x) + \frac{2}{x} y'(x) + \lambda\, y(x) = 0, \qquad 0 < x < 1,
\]
with the understanding that $y$ remains bounded as $x\to 0^+$ and satisfies $y(1)=0$. Find explicit formulas for $p(x)$ and $w(x)$, and write the equation in Sturm–Liouville form. How do the boundary conditions look in this new formulation?
% Hint: Here a(x)=1, b(x)=2/x, c(x)=0, r(x)=1; compute \mu(x) using your general formula, then p(x)=\mu a and w(x)=\mu r.
\end{enumerate}

\end{problem}

% ===== Example 5: Transforming a general second-order ODE into Sturm–Liouville form (full solution) =====
\begin{problem}[Transforming a general second-order ODE into Sturm–Liouville form]
Let $a,b,c,r$ be real-valued continuous functions on $[\alpha,\beta]$ with
\[
a(x) > 0, \qquad r(x) > 0 \quad \text{for all } x\in[\alpha,\beta].
\]
Consider the eigenvalue problem
\[
a(x) y''(x) + b(x) y'(x) + c(x) y(x) = \lambda\, r(x) y(x), \qquad \alpha < x < \beta,
\]
together with separated boundary conditions
\[
\alpha_1 y(\alpha) + \alpha_2 y'(\alpha) = 0, 
\qquad 
\beta_1 y(\beta) + \beta_2 y'(\beta) = 0,
\]
where $\alpha_1,\alpha_2$ are not both zero, and likewise for $\beta_1,\beta_2$.

\begin{enumerate}
\item[(a)] Show that there exists a positive integrating factor $\mu(x)$ such that, with
\[
p(x) := \mu(x)a(x), 
\qquad 
q(x) := -\,\mu(x)c(x),
\qquad
w(x) := \mu(x) r(x),
\]
the equation can be rewritten in Sturm–Liouville form
\[
\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + \bigl[\lambda\, w(x) - q(x)\bigr] y(x) = 0.
\]
Derive an explicit formula for $\mu(x)$ in terms of $a$ and $b$.

\item[(b)] Write the corresponding Sturm–Liouville operator
\[
L_{\text{\rm SL}}[y] := -\frac{d}{dx}\bigl(p(x)y'(x)\bigr) + q(x) y(x)
\]
and verify the Green-type identity
\[
\int_\alpha^\beta \bigl(u\,L_{\text{\rm SL}}[v] - v\,L_{\text{\rm SL}}[u]\bigr)\,dx
= \Bigl[\,p(x)\,\bigl(u(x)v'(x) - u'(x)v(x)\bigr)\Bigr]_{\alpha}^{\beta}
\]
for sufficiently smooth $u$ and $v$.

Explain why, under the given separated boundary conditions, the boundary term vanishes for all admissible $u$ and $v$, so that $L_{\text{\rm SL}}$ is self-adjoint with respect to the weighted inner product
\[
\langle u, v\rangle_{L^2_w} = \int_{\alpha}^{\beta} u(x)\,\overline{v(x)}\,w(x)\,dx.
\]

\item[(c)] Briefly discuss how this Sturm–Liouville formulation implies that all eigenvalues $\lambda$ are real and that eigenfunctions corresponding to distinct eigenvalues are orthogonal with respect to $\langle\cdot,\cdot\rangle_{L^2_w}$.
\end{enumerate}
\end{problem}

\begin{solution}
We begin with the general eigenvalue problem
\[
a(x) y''(x) + b(x) y'(x) + c(x) y(x) = \lambda\, r(x) y(x),
\qquad \alpha < x < \beta,
\]
where $a(x)>0$ and $r(x)>0$ for all $x\in[\alpha,\beta]$. The central idea is to multiply the equation by an integrating factor $\mu(x)$ so that the left-hand side becomes the derivative of a flux $p(x)y'(x)$ plus a zeroth-order term. This is precisely the structure of a Sturm–Liouville operator.

\medskip

\emph{(a) Construction of the integrating factor and Sturm–Liouville form.}

Multiply the differential equation by an unknown function $\mu(x)$:
\[
\mu a\,y'' + \mu b\,y' + \mu c\,y = \lambda\,\mu r\,y.
\]
We would like the second- and first-derivative terms to combine into a single derivative:
\[
\mu a\,y'' + \mu b\,y' = \frac{d}{dx}\bigl(p(x) y'(x)\bigr)
\]
for some function $p$. It is natural to set
\[
p(x) := \mu(x)a(x).
\]
Then
\[
\frac{d}{dx}\bigl(p y'\bigr) = p\,y'' + p'\,y' = \mu a\,y'' + p'\,y'.
\]
For this to match $\mu a\,y'' + \mu b\,y'$, we must have
\[
p'(x) = \mu(x)\,b(x).
\]
Substituting $p=\mu a$, this condition becomes
\[
(\mu a)' = \mu b.
\]
Expanding the derivative gives
\[
\mu' a + \mu a' = \mu b.
\]
Because $a>0$, we can divide by $a$ and by $\mu$ (we will seek $\mu>0$) to obtain a first-order linear ODE for $\mu$:
\[
\frac{\mu'}{\mu} = \frac{b - a'}{a}.
\]
This is a separable equation. Integrating, we find
\[
\ln \mu(x) 
= \int^x \frac{b(s) - a'(s)}{a(s)}\,ds + C_0,
\]
for some constant $C_0$. Exponentiating,
\[
\mu(x) = C \exp\!\left(\int^x \frac{b(s) - a'(s)}{a(s)}\,ds\right),
\]
where $C = e^{C_0} > 0$ can be chosen arbitrarily. Thus we have an explicit formula for the integrating factor $\mu(x)$ in terms of $a$ and $b$.

We now define
\[
p(x) := \mu(x) a(x), 
\qquad 
q(x) := -\,\mu(x) c(x),
\qquad
w(x) := \mu(x) r(x).
\]
Note that $p(x)>0$ and $w(x)>0$ because $a,r>0$ and we choose $\mu>0$.

With these definitions, the multiplied equation reads
\[
\mu a\,y'' + \mu b\,y' + \mu c\,y = \lambda\, \mu r\,y
\;\;\Longleftrightarrow\;\;
\mu a\,y'' + \mu b\,y' - q\,y = \lambda\, w\,y.
\]
By construction, $\mu a\,y'' + \mu b\,y' = (p y')'$, so the left-hand side becomes
\[
(p y')' - q y.
\]
Thus the eigenvalue equation can be written as
\[
(p y')' - q(x) y(x) = \lambda\, w(x)y(x),
\]
or equivalently,
\[
\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + \bigl[\lambda\, w(x) - q(x)\bigr]\,y(x) = 0.
\]
This is precisely a Sturm–Liouville form with coefficients $p,w,q$ as above and $p,w>0$ on $[\alpha,\beta]$.

\medskip

\emph{(b) Green’s identity and self-adjointness.}

We now introduce the associated Sturm–Liouville operator
\[
L_{\text{SL}}[y] := -\frac{d}{dx}\bigl(p(x) y'(x)\bigr) + q(x) y(x),
\]
so that the eigenvalue problem is
\[
L_{\text{SL}}[y] = \lambda\, w(x)\,y(x).
\]

To obtain the Green-type identity, take sufficiently smooth functions $u$ and $v$ on $[\alpha,\beta]$. Compute
\[
u\,L_{\text{SL}}[v] - v\,L_{\text{SL}}[u]
= u\Bigl(- (p v')' + q v\Bigr)
- v\Bigl(- (p u')' + q u\Bigr).
\]
The zeroth-order terms cancel:
\[
u q v - v q u = 0.
\]
Thus
\[
u\,L_{\text{SL}}[v] - v\,L_{\text{SL}}[u]
= -u (p v')' + v (p u')'.
\]
Integrate over $[\alpha,\beta]$:
\[
\int_{\alpha}^{\beta} \bigl(u\,L_{\text{SL}}[v] - v\,L_{\text{SL}}[u]\bigr)\,dx
= \int_{\alpha}^{\beta} \bigl( -u (p v')' + v (p u')'\bigr)\,dx.
\]

Now integrate by parts each term. For the first term,
\[
\int_{\alpha}^{\beta} -u (p v')'\,dx
= \Bigl[-u\,p\,v'\Bigr]_{\alpha}^{\beta}
+ \int_{\alpha}^{\beta} u'\,p\,v'\,dx.
\]
For the second term,
\[
\int_{\alpha}^{\beta} v (p u')'\,dx
= \Bigl[v\,p\,u'\Bigr]_{\alpha}^{\beta}
- \int_{\alpha}^{\beta} v'\,p\,u'\,dx.
\]
Adding these,
\[
\int_{\alpha}^{\beta} \bigl( -u (p v')' + v (p u')'\bigr)\,dx
= \Bigl[-u\,p\,v' + v\,p\,u'\Bigr]_{\alpha}^{\beta}
+ \int_{\alpha}^{\beta} \bigl(u' p v' - v' p u'\bigr)\,dx.
\]
The integral term cancels identically:
\[
u' p v' - v' p u' = 0.
\]
Therefore
\[
\int_{\alpha}^{\beta} \bigl(u\,L_{\text{SL}}[v] - v\,L_{\text{SL}}[u]\bigr)\,dx
= \Bigl[\,p(x)\,\bigl(v(x)u'(x) - u(x)v'(x)\bigr)\Bigr]_{\alpha}^{\beta},
\]
or, rearranging signs,
\[
\int_{\alpha}^{\beta} \bigl(u\,L_{\text{SL}}[v] - v\,L_{\text{SL}}[u]\bigr)\,dx
= \Bigl[\,p(x)\,\bigl(u(x)v'(x) - u'(x)v(x)\bigr)\Bigr]_{\alpha}^{\beta}.
\]
This is the Green-type identity.

Next, we incorporate the weight $w(x)$. The eigenvalue problem is
\[
L_{\text{SL}}[y] = \lambda\, w y.
\]
The natural inner product is
\[
\langle u, v\rangle_{L^2_w} 
= \int_{\alpha}^{\beta} u(x)\,\overline{v(x)}\,w(x)\,dx,
\]
with
\(w(x)>0\). For the operator that appears naturally in the eigenvalue problem
\[
\frac{1}{w(x)}L_{\text{SL}}[y] = \lambda\,y,
\]
the appropriate notion of (formal) self-adjointness is with respect to the weighted inner product
\[
\langle u, v\rangle_{L^2_w} 
= \int_{\alpha}^{\beta} u(x)\,\overline{v(x)}\,w(x)\,dx.
\]

Define
\[
A[y] := \frac{1}{w(x)}\,L_{\text{SL}}[y].
\]
Then
\[
\langle u, A[v]\rangle_{L^2_w}
= \int_{\alpha}^{\beta} u\,\overline{A[v]}\,w\,dx
= \int_{\alpha}^{\beta} u\,\overline{L_{\text{SL}}[v]}\,dx,
\]
and similarly
\[
\langle A[u], v\rangle_{L^2_w}
= \int_{\alpha}^{\beta} v\,\overline{L_{\text{SL}}[u]}\,dx.
\]
If we restrict to real-valued functions (or take real parts), the Green identity gives
\[
\langle u, A[v]\rangle_{L^2_w} - \langle A[u], v\rangle_{L^2_w}
= \Bigl[\,p(x)\,\bigl(u(x)v'(x) - u'(x)v(x)\bigr)\Bigr]_{\alpha}^{\beta}.
\]
Thus, whenever the boundary term on the right-hand side vanishes for admissible \(u,v\), the operator \(A=(1/w)L_{\text{SL}}\) is formally self-adjoint with respect to \(\langle\cdot,\cdot\rangle_{L^2_w}\).

\medskip

\emph{Boundary conditions and vanishing of the boundary term.}

Now impose separated boundary conditions
\[
\alpha_1 y(\alpha) + \alpha_2 y'(\alpha) = 0, 
\qquad 
\beta_1 y(\beta) + \beta_2 y'(\beta) = 0,
\]
with \((\alpha_1,\alpha_2)\neq(0,0)\) and \((\beta_1,\beta_2)\neq(0,0)\). Let \(u\) and \(v\) be any two functions satisfying these same conditions.

At \(x=\alpha\), we have
\[
\alpha_1 u(\alpha) + \alpha_2 u'(\alpha) = 0,
\qquad
\alpha_1 v(\alpha) + \alpha_2 v'(\alpha) = 0.
\]
There are two cases:

\smallskip

\emph{Case 1:} \(\alpha_2\neq 0\). Then
\[
u'(\alpha) = -\frac{\alpha_1}{\alpha_2} u(\alpha),
\qquad
v'(\alpha) = -\frac{\alpha_1}{\alpha_2} v(\alpha).
\]
Hence
\[
u(\alpha)v'(\alpha) - u'(\alpha)v(\alpha)
= u(\alpha)\left(-\frac{\alpha_1}{\alpha_2}v(\alpha)\right)
   -\left(-\frac{\alpha_1}{\alpha_2}u(\alpha)\right)v(\alpha)
= 0.
\]

\emph{Case 2:} \(\alpha_2=0\). Then \(\alpha_1\neq 0\), and the boundary condition reduces to
\[
u(\alpha) = v(\alpha) = 0.
\]
Consequently,
\[
u(\alpha)v'(\alpha) - u'(\alpha)v(\alpha) = 0.
\]

In either case the contribution at \(x=\alpha\) vanishes. The same argument applies verbatim at \(x=\beta\) (interchanging \(\alpha_1,\alpha_2\) with \(\beta_1,\beta_2\)). Therefore,
\[
p(x)\,\bigl(u(x)v'(x) - u'(x)v(x)\bigr)\Big|_{x=\alpha}^{x=\beta} = 0
\]
for all admissible \(u\) and \(v\). It follows that
\[
\langle u, A[v]\rangle_{L^2_w} = \langle A[u], v\rangle_{L^2_w}
\]
on the domain of functions satisfying the given separated boundary conditions. Thus the Sturm–Liouville operator \(A=(1/w)L_{\text{SL}}\) is self-adjoint (more precisely: symmetric, and under mild additional conditions, self-adjoint) in the Hilbert space \(L^2_w([\alpha,\beta])\).

\medskip

\emph{(c) Reality of eigenvalues and orthogonality of eigenfunctions.}

Consider the eigenvalue problem
\[
A[y] = \lambda\,y,
\]
with \(A\) self-adjoint in \(L^2_w\). Let \(y\) be a (nonzero) eigenfunction with eigenvalue \(\lambda\).

First, take the inner product with \(y\):
\[
\langle A[y], y\rangle_{L^2_w} = \lambda\,\langle y, y\rangle_{L^2_w}.
\]
By self-adjointness,
\[
\langle A[y], y\rangle_{L^2_w} = \langle y, A[y]\rangle_{L^2_w}
= \langle y, \lambda y\rangle_{L^2_w}
= \overline{\lambda}\,\langle y, y\rangle_{L^2_w},
\]
where the bar denotes complex conjugation. Thus
\[
\lambda\,\langle y, y\rangle_{L^2_w}
= \overline{\lambda}\,\langle y, y\rangle_{L^2_w}.
\]
Since \(w(x)>0\) and \(y\not\equiv 0\), we have \(\langle y,y\rangle_{L^2_w} > 0\), so
\[
\lambda = \overline{\lambda},
\]
i.e.\ every eigenvalue \(\lambda\) is real.

Next, let \(y_1\) and \(y_2\) be eigenfunctions corresponding to eigenvalues \(\lambda_1\) and \(\lambda_2\), with \(\lambda_1\neq\lambda_2\). Then
\[
A[y_1] = \lambda_1 y_1,
\qquad
A[y_2] = \lambda_2 y_2.
\]
Using self-adjointness,
\[
\langle A[y_1], y_2\rangle_{L^2_w}
= \langle y_1, A[y_2]\rangle_{L^2_w}.
\]
Substituting the eigenvalue equations,
\[
\lambda_1 \langle y_1, y_2\rangle_{L^2_w}
= \lambda_2 \langle y_1, y_2\rangle_{L^2_w}.
\]
Hence
\[
(\lambda_1 - \lambda_2)\,\langle y_1, y_2\rangle_{L^2_w} = 0.
\]
If \(\lambda_1\neq\lambda_2\), we must have
\[
\langle y_1, y_2\rangle_{L^2_w} = 0,
\]
i.e.\ eigenfunctions corresponding to distinct eigenvalues are orthogonal in \(L^2_w\).

Thus, by recasting the original second-order equation into Sturm–Liouville form via the integrating factor \(\mu(x)\), we obtain a self-adjoint eigenvalue problem with respect to the weighted inner product \(\langle\cdot,\cdot\rangle_{L^2_w}\), guaranteeing real eigenvalues and orthogonal eigenfunctions. 
\end{solution}

% ===== Example 6: Completeness and expanding functions in eigenfunction bases (inquiry-based) =====
\begin{problem}[Completeness and expanding functions in eigenfunction bases]
In many physical models, such as the vibrating string or the one-dimensional heat equation, one first finds a family of normal modes by solving a Sturm--Liouville eigenvalue problem. A central question is whether arbitrary initial data can be expressed as a convergent series of these eigenfunctions. This property is called \emph{completeness} of the eigenfunctions and is the key to turning difficult partial differential equations into simpler ordinary differential equations for the mode amplitudes. In this problem, you will explore completeness concretely for the simplest Sturm--Liouville operator, and use it to solve an initial value problem.

Consider the Sturm--Liouville eigenvalue problem on the interval $(0,\pi)$
\[
-y''(x) = \lambda\,y(x),\qquad 0<x<\pi,\qquad y(0)=0,\quad y(\pi)=0.
\]

\smallskip
(a) Solve the eigenvalue problem.

\quad(i) By considering the cases $\lambda<0$, $\lambda=0$, and $\lambda>0$ separately, determine all eigenvalues $\lambda$ for which there exists a nontrivial solution satisfying the given boundary conditions.

\quad(ii) For each eigenvalue $\lambda_n$ that you find, write down a corresponding eigenfunction $\phi_n(x)$ and simplify it as much as possible.

\quad(iii) Show that the eigenfunctions you found are orthogonal with respect to the standard $L^2$ inner product on $(0,\pi)$, that is,
\[
\int_0^\pi \phi_m(x)\,\phi_n(x)\,dx = 0 \quad \text{for } m\neq n.
\]
Hint: Reduce the orthogonality integral to a simple trigonometric identity.

\smallskip
(b) Let $f(x)=x(\pi-x)$ on $[0,\pi]$.

\quad(i) Check that $f$ satisfies the same boundary conditions as the eigenfunctions. Why is this relevant for trying to expand $f$ as a series in $\{\phi_n\}$?

\quad(ii) Assume that $\{\phi_n\}$ is complete in an appropriate sense (for instance, in $L^2(0,\pi)$). Write down the formal eigenfunction expansion of $f$,
\[
f(x) \sim \sum_{n=1}^{\infty} a_n \,\phi_n(x),
\]
and use orthogonality to derive a formula for the coefficients $a_n$ in terms of an integral involving $f$ and $\phi_n$.

\quad(iii) Specialize your formula from part (ii) to the specific eigenfunctions you found in part (a), and write $a_n$ explicitly as a definite integral over $(0,\pi)$ (do not evaluate it yet).

\smallskip
(c) Now compute the coefficients $a_n$ for $f(x)=x(\pi-x)$.

\quad(i) Evaluate the integral expression for $a_n$ using integration by parts as needed, and simplify your answer as far as possible.

\quad(ii) Show that $a_n=0$ for all even $n$, and find a closed form for $a_{2k+1}$, $k\in\mathbb{N}_0$. What does this tell you about which normal modes are present in the initial shape $f$?

Hint: Look carefully at factors of $(-1)^n$ that appear in your computations.

\smallskip
(d) Use this eigenfunction expansion to solve an initial-boundary value problem for the heat equation.

Consider the heat equation
\[
u_t = u_{xx},\qquad 0<x<\pi,\ t>0,
\]
with boundary conditions
\[
u(0,t)=0,\quad u(\pi,t)=0\quad\text{for all } t>0,
\]
and initial condition
\[
u(x,0)=f(x)=x(\pi-x).
\]

\quad(i) Make a separated-variables ansatz of the form
\[
u(x,t) = \sum_{n=1}^{\infty} b_n(t)\,\phi_n(x)
\]
using the eigenfunctions from part (a). Substitute this into the heat equation and use orthogonality to derive an ordinary differential equation for each $b_n(t)$.

\quad(ii) Solve the resulting ODEs for $b_n(t)$, and use the initial condition at $t=0$ to relate $b_n(0)$ to the coefficients $a_n$ you computed in part (c).

\quad(iii) Write down the final series representation for $u(x,t)$, and explain briefly (in words) where completeness of $\{\phi_n\}$ is used in this construction.

Hint: Separating variables should give decay factors of the form $e^{-\lambda_n t}$ multiplying the spatial eigenfunctions.

\smallskip
(e) Explorations and extensions.

\quad(i) Suppose instead that the boundary conditions were insulating (Neumann) conditions: $y'(0)=0$ and $y'(\pi)=0$. Sketch how the analysis above would change: What eigenfunctions would you expect? What sort of Fourier series would appear?

\quad(ii) In this problem we assumed without proof that $\{\phi_n\}$ is complete. Briefly discuss (qualitatively) what could go wrong in solving the heat equation by eigenfunction expansion if the eigenfunctions of the underlying Sturm--Liouville problem were not complete. How would this manifest itself in terms of the initial condition?

Hint: Think about whether you could match the initial data by a series in the eigenfunctions, and what that would imply for the solution at later times.
\end{problem}

% ===== Example 6: Completeness and expanding functions in eigenfunction bases (full solution) =====
\begin{problem}[Completeness and expanding functions in eigenfunction bases]
Consider the Sturm--Liouville problem
\[
-y''(x) = \lambda\,y(x),\qquad 0<x<\pi,\qquad y(0)=0,\quad y(\pi)=0.
\]
\begin{enumerate}
\item Find all eigenvalues and (unnormalized) eigenfunctions, and show that the eigenfunctions are orthogonal in $L^2(0,\pi)$.
\item For $f(x)=x(\pi-x)$ on $[0,\pi]$, find the Fourier sine series
\[
f(x) \sim \sum_{n=1}^{\infty} a_n \sin(nx),
\]
by computing the coefficients $a_n$ explicitly and simplifying them.
\item Using the completeness of $\{\sin(nx)\}_{n\ge1}$ in $L^2(0,\pi)$, solve the heat equation
\[
u_t = u_{xx},\quad 0<x<\pi,\ t>0,\qquad u(0,t)=u(\pi,t)=0,
\]
with initial condition $u(x,0)=x(\pi-x)$, and write $u(x,t)$ as a convergent eigenfunction series.
\item Briefly explain how this example illustrates the use of Sturm--Liouville (spectral) theory to represent functions and solve initial-boundary value problems.
\end{enumerate}
\end{problem}

\begin{solution}
We proceed step by step, highlighting the spectral ideas as they arise.

\medskip
\noindent\textbf{1. Eigenvalues, eigenfunctions, and orthogonality.}

We solve
\[
-y'' = \lambda y,\qquad y(0)=0,\quad y(\pi)=0.
\]
We consider the three standard cases.

\smallskip
\emph{Case $\lambda<0$.} Write $\lambda=-\mu^2$ with $\mu>0$. Then the ODE becomes
\[
-y'' = -\mu^2 y \quad\Longleftrightarrow\quad y'' = \mu^2 y,
\]
whose general solution is
\[
y(x)=C_1 e^{\mu x} + C_2 e^{-\mu x}.
\]
Imposing $y(0)=0$ gives $C_1 + C_2 = 0$, so $C_2=-C_1$ and $y(x)=C_1(e^{\mu x}-e^{-\mu x})=2C_1\sinh(\mu x)$. Then
\[
y(\pi)=0\quad\Rightarrow\quad \sinh(\mu\pi)=0,
\]
which is impossible for $\mu>0$. Thus there are no nontrivial eigenfunctions for $\lambda<0$.

\smallskip
\emph{Case $\lambda=0$.} The ODE becomes $y''=0$ with general solution $y(x)=Ax+B$. The boundary condition $y(0)=0$ forces $B=0$, and $y(\pi)=0$ then gives $A\pi=0$, so $A=0$. Hence $y\equiv0$ is the only solution, so $\lambda=0$ is not an eigenvalue.

\smallskip
\emph{Case $\lambda>0$.} Write $\lambda=\alpha^2$ with $\alpha>0$. The ODE becomes
\[
-y'' = \alpha^2 y \quad\Longleftrightarrow\quad y'' + \alpha^2 y = 0,
\]
whose general solution is
\[
y(x)=A\cos(\alpha x) + B\sin(\alpha x).
\]
The boundary condition $y(0)=0$ forces $A\cos 0 + B\sin 0 = A = 0$, so $y(x)=B\sin(\alpha x)$. Then
\[
y(\pi)=0 \quad\Rightarrow\quad B\sin(\alpha\pi) = 0.
\]
For a nontrivial solution we require $B\neq 0$, so $\sin(\alpha\pi)=0$, that is $\alpha\pi = n\pi$ for some integer $n$. Since $\alpha>0$ and $\alpha$ appears in $\sin(\alpha x)$, we take $n\in\mathbb{N}$. Thus $\alpha=n$ and
\[
\lambda_n = n^2,\qquad \phi_n(x) = \sin(nx),\qquad n=1,2,3,\dots
\]
are all eigenpairs (up to a multiplicative constant in $\phi_n$).

\smallskip
\emph{Orthogonality.} The standard $L^2(0,\pi)$ inner product is
\[
\langle f,g\rangle = \int_0^\pi f(x)\,g(x)\,dx.
\]
For $m\neq n$,
\[
\int_0^\pi \sin(mx)\sin(nx)\,dx
= \frac{1}{2}\int_0^\pi\bigl(\cos((m-n)x)-\cos((m+n)x)\bigr)\,dx.
\]
Integrating, we obtain
\[
\int_0^\pi \sin(mx)\sin(nx)\,dx
= \frac{1}{2}\left[\frac{\sin((m-n)x)}{m-n} - \frac{\sin((m+n)x)}{m+n}\right]_{0}^{\pi}.
\]
Since $m\pm n$ are nonzero integers, $\sin((m\pm n)\pi)=0$ and also $\sin(0)=0$, so the bracketed expression vanishes. Hence
\[
\int_0^\pi \sin(mx)\sin(nx)\,dx = 0,\quad m\neq n.
\]
Thus the eigenfunctions $\{\sin(nx)\}$ are pairwise orthogonal in $L^2(0,\pi)$.

\medskip
\noindent\textbf{2. Expanding $f(x)=x(\pi-x)$ in the eigenfunction basis.}

We are given $f(x)=x(\pi-x)$ on $[0,\pi]$. First note that
\[
f(0) = 0\cdot \pi = 0,\qquad f(\pi) = \pi(\pi-\pi) = 0,
\]
so $f$ satisfies the same homogeneous Dirichlet boundary conditions as the eigenfunctions. This is compatible with trying to represent $f$ as a sine series, since the sine functions also vanish at $x=0$ and $x=\pi$.

The general completeness theorem (which we invoke without proof) states that the family $\{\sin(nx)\}_{n\ge1}$ is complete in $L^2(0,\pi)$. Therefore any square-integrable function on $(0,\pi)$, and in particular any piecewise $C^1$ function with $f(0)=f(\pi)=0$, can be expanded in an $L^2$-convergent sine series
\[
f(x) \sim \sum_{n=1}^{\infty} a_n \sin(nx).
\]
Because the eigenfunctions are orthogonal, we can compute $a_n$ by projecting $f$ onto each $\sin(nx)$. We write
\[
a_n = \frac{\langle f,\sin(nx)\rangle}{\langle \sin(nx),\sin(nx)\rangle}
= \frac{\displaystyle\int_0^\pi f(x)\sin(nx)\,dx}{\displaystyle\int_0^\pi\sin^2(nx)\,dx}.
\]
We use the standard identity
\[
\int_0^\pi \sin^2(nx)\,dx = \frac{\pi}{2},\quad n\in\mathbb{N}.
\]
Therefore
\[
a_n = \frac{2}{\pi}\int_0^\pi f(x)\,\sin(nx)\,dx
= \frac{2}{\pi}\int_0^\pi x(\pi-x)\sin(nx)\,dx.
\]

\medskip
\noindent\textbf{3. Computing the coefficients $a_n$.}

Let
\[
I_n := \int_0^\pi x(\pi-x)\sin(nx)\,dx.
\]
Then $a_n = \dfrac{2}{\pi} I_n$. We compute $I_n$ explicitly.

First expand $x(\pi-x) = \pi x - x^2$:
\[
I_n = \pi\int_0^\pi x\sin(nx)\,dx - \int_0^\pi x^2\sin(nx)\,dx
=: \pi A_n - B_n.
\]

\smallskip
\emph{Step 1: Compute $A_n=\displaystyle\int_0^\pi x\sin(nx)\,dx$.}

An antiderivative of $x\sin(nx)$ is
\[
\int x\sin(nx)\,dx = -\frac{x\cos(nx)}{n} + \frac{\sin(nx)}{n^2}.
\]
Thus
\[
A_n = \left[-\frac{x\cos(nx)}{n} + \frac{\sin(nx)}{n^2}\right]_0^\pi
= \left(-\frac{\pi\cos(n\pi)}{n} + 0\right) - (0+0)
= -\frac{\pi(-1)^n}{n}.
\]
Therefore
\[
\pi A_n = -\frac{\pi^2(-1)^n}{n}.
\]

\smallskip
\emph{Step 2: Compute $B_n=\displaystyle\int_0^\pi x^2\sin(nx)\,dx$.}

We integrate by parts once, with $u=x^2$ and $dv=\sin(nx)\,dx$. Then $du=2x\,dx$ and $v=-\cos(nx)/n$, so
\[
B_n = -\frac{x^2\cos(nx)}{n}\Big|_{0}^{\pi} + \frac{2}{n}\int_0^\pi x\cos(nx)\,dx.
\]
At the endpoints we get
\[
-\frac{x^2\cos(nx)}{n}\Big|_{0}^{\pi}
= -\frac{\pi^2\cos(n\pi)}{n} - 0
= -\frac{\pi^2(-1)^n}{n}.
\]
Let
\[
C_n := \int_0^\pi x\cos(nx)\,dx.
\]
An antiderivative is
\[
\int x\cos(nx)\,dx = \frac{x\sin(nx)}{n} + \frac{\cos(nx)}{n^2},
\]
so
\[
C_n = \left[\frac{x\sin(nx)}{n} + \frac{\cos(nx)}{n^2}\right]_0^\pi
= \left(\frac{\pi\sin(n\pi)}{n} + \frac{\cos(n\pi)}{n^2}\right) - \left(0 + \frac{1}{n^2}\right).
\]
Since $\sin(n\pi)=0$ and $\cos(n\pi)=(-1)^n$, this simplifies to
\[
C_n = \frac{(-1)^n}{n^2} - \frac{1}{n^2}
= \frac{(-1)^n - 1}{n^2}.
\]
Thus
\[
B_n = -\frac{\pi^2(-1)^n}{n} + \frac{2}{n}C_n
= -\frac{\pi^2(-1)^n}{n} + \frac{2}{n}\cdot \frac{(-1)^n - 1}{n^2}
= -\frac{\pi^2(-1)^n}{n} + \frac{2\bigl((-1)^n - 1\bigr)}{n^3}.
\]

\smallskip
\emph{Step 3: Combine to get $I_n$ and $a_n$.}

Recall that $I_n = \pi A_n - B_n$, so
\[
I_n = \left(-\frac{\pi^2(-1)^n}{n}\right)
- \left(-\frac{\pi^2(-1)^n}{n} + \frac{2\bigl((-1)^n - 1\bigr)}{n^3}\right)
= -\frac{2\bigl((-1)^n - 1\bigr)}{n^3}.
\]
We can rewrite this as
\[
I_n = \frac{2(1-(-1)^n)}{n^3}.
\]
Therefore
\[
a_n = \frac{2}{\pi}I_n
= \frac{4}{\pi}\,\frac{1-(-1)^n}{n^3}.
\]

Observe that for even $n$, say $n=2k$, we have $(-1)^n = 1$, so $1-(-1)^n=0$ and thus $a_{2k}=0$. For odd $n$, say $n=2k+1$, we have $(-1)^n=-1$, hence $1-(-1)^n = 2$, and
\[
a_{2k+1} = \frac{4}{\pi}\cdot \frac{2}{(2k+1)^3}
= \frac{8}{\pi(2k+1)^3}.
\]
We have therefore obtained the expansion
\[
f(x) = x(\pi-x) \sim \sum_{k=0}^{\infty} \frac{8}{\pi(2k+1)^3}\,\sin\bigl((2k+1)x\bigr),
\]
with convergence in the $L^2(0,\pi)$ sense, and in fact pointwise and uniformly on $[0,\pi]$ since $f$ is smooth.

Physically, the vanishing of $a_{2k}$ tells us that only odd normal modes are excited by the initial shape $f(x)=x(\pi-x)$.

\medskip
\noindent\textbf{4. Solving the heat equation via eigenfunction expansion.}

We now solve
\[
u_t = u_{xx},\quad 0<x<\pi,\ t>0,\qquad
u(0,t)=u(\pi,t)=0,\qquad
u(x,0)=f(x)=x(\pi-x).
\]
The spectral idea is to expand $u(x,t)$ in the complete eigenfunction basis $\{\sin(nx)\}$ of the spatial operator $-d^2/dx^2$ with Dirichlet boundary conditions. We make the ansatz
\[
u(x,t) = \sum_{n=1}^{\infty} b_n(t)\,\sin(nx),
\]
where the time-dependent coefficients $b_n(t)$ are to be determined. This form automatically satisfies the boundary conditions $u(0,t)=u(\pi,t)=0$ for all $t$, because each $\sin(nx)$ vanishes at $x=0$ and $x=\pi$.

We differentiate term-by-term:
\[
u_t(x,t) = \sum_{n=1}^{\infty} b_n'(t)\,\sin(nx),\qquad
u_{xx}(x,t) = \sum_{n=1}^{\infty} b_n(t)\,\frac{d^2}{dx^2}\sin(nx).
\]
Since $\dfrac{d^2}{dx^2}\sin(nx) = -n^2\sin(nx)$, we have
\[
u_{xx}(x,t) = \sum_{n=1}^{\infty} -n^2 b_n(t)\,\sin(nx).
\]
Substituting $u_t=u_{xx}$ gives
\[
\sum_{n=1}^{\infty} b_n'(t)\,\sin(nx)
= \sum_{n=1}^{\infty} -n^2 b_n(t)\,\sin(nx).
\]
By orthogonality and completeness of the sine functions, the only way two such series are equal in $L^2(0,\pi)$ is for the coefficients of each $\sin(nx)$ to agree. Thus, for each $n\ge1$,
\[
b_n'(t) = -n^2 b_n(t).
\]
This is a first-order linear ODE with solution
\[
b_n(t) = C_n e^{-n^2 t},
\]
where $C_n$ is a constant determined by the initial condition.

At $t=0$, the initial condition $u(x,0)=f(x)$ gives
\[
f(x) = u(x,0) = \sum_{n=1}^{\infty} b_n(0)\,\sin(nx)
= \sum_{n=1}^{\infty} C_n\,\sin(nx).
\]
By the uniqueness of the sine series expansion (again using orthogonality and completeness), the constants $C_n$ must coincide with the Fourier sine coefficients $a_n$ of $f$ computed earlier. Therefore
\[
b_n(t) = a_n e^{-n^2 t},
\]
and hence
\[
u(x,t) = \sum_{n=1}^{\infty} a_n e^{-n^2 t}\,\sin(nx).
\]
Substituting the explicit coefficients, we obtain
\[
u(x,t) = \sum_{n=1}^{\infty} \frac{4}{\pi}\,\frac{1-(-1)^n}{n^3}\,e^{-n^2 t}\,\sin(nx)
= \sum_{k=0}^{\infty} \frac{8}{\pi(2k+1)^3}\,e^{-(2k+1)^2 t}\,\sin\bigl((2k+1)x\bigr).
\]

This series converges in $L^2(0,\pi)$ for each $t>0$, and in fact uniformly on $[0,\pi]$ for each fixed $t>0$ due to the exponential decay factor $e^{-n^2 t}$.

\medskip
\noindent\textbf{5. Conceptual summary and connection to Sturm--Liouville theory.}

The key ideas from Sturm--Liouville (spectral) theory in this example are:
\begin{itemize}
\item The spatial operator $L[y] = -y''$ with Dirichlet boundary conditions is a self-adjoint Sturm--Liouville operator on $(0,\pi)$.
\item Its eigenvalue problem $L\phi_n = \lambda_n\phi_n$ yields a real, discrete spectrum $\lambda_n=n^2$ and orthogonal eigenfunctions $\phi_n(x)=\sin(nx)$.
\item The eigenfunctions form a complete orthogonal basis of $L^2(0,\pi)$, so any reasonable function (such as $f(x)=x(\pi-x)$) can be expanded in an $L^2$-convergent series in these eigenfunctions.
\item For the heat equation, representing the solution as a sum of separated solutions $e^{-\lambda_n t}\phi_n(x)$ reduces the PDE to a family of decoupled ODEs for the mode amplitudes, which are easy to solve.
\end{itemize}
Thus, this example illustrates how Sturm--Liouville spectral theory provides a systematic way to (i) represent functions in terms of eigenfunctions of a differential operator, and (ii) solve initial-boundary value problems by decomposing arbitrary initial data into normal modes and evolving each mode independently in time.
\end{solution}

\section{Phase Space Dynamics for Conservative and Perturbed Systems}
% --- Narrative plan (auto-generated) ---
% This section develops the phase space viewpoint for ordinary differential equations, with special attention to conservative systems and what happens when we introduce small perturbations such as damping, forcing, or noise. Instead of focusing only on explicit formulas for solutions, we learn to visualize trajectories in the plane or higher-dimensional phase space, identify invariant sets and conserved quantities, and understand qualitative features such as equilibria, periodic orbits, and separatrices. Conservative systems, which preserve an energy-like quantity, provide a clean starting point where orbits lie on level sets, while perturbed systems show how these structures deform, break, or give rise to more complex behavior.
%
% This perspective is central in applied mathematics, dynamical systems, and the study of partial differential equations viewed as infinite-dimensional dynamical systems. Many PDEs have conserved energies and can be approximated by finite-dimensional conservative systems that capture essential behavior, while perturbations model dissipation, driving, or external influences. The ideas here connect to Hamiltonian mechanics from classical physics, to complex analysis through analytic continuation of vector fields and normal forms, and to Fourier analysis via modal decompositions that turn PDEs into coupled ODEs in phase space. By learning to read phase portraits and track how they change under perturbations, you gain a durable toolkit for understanding models far beyond those that can be solved in closed form.

% ===== Example 1: Simple Harmonic Oscillator in Phase Space (inquiry-based) =====
\begin{problem}[Simple Harmonic Oscillator in Phase Space]
The simple harmonic oscillator is one of the most fundamental models in mechanics.  It describes, for instance, the motion of a mass attached to an ideal spring with no friction.  In this problem, you will reformulate the usual second-order equation as a first-order system, discover a conserved ``energy,'' and see how this leads to circular orbits in the phase plane.  This example will serve as a model for more general conservative and perturbed systems.

Consider the dimensionless simple harmonic oscillator
\[
x''(t) + x(t) = 0,
\]
where $x(t)$ is the displacement of the mass from equilibrium.

\smallskip

\noindent
(a) Introduce a new variable $v(t)$ representing the velocity of the mass and rewrite the second-order equation as a first-order system in the variables $(x,v)$.

\emph{Hint:} Let $v(t) = x'(t)$ and express both $x'(t)$ and $v'(t)$ in terms of $x(t)$ and $v(t)$.

\smallskip

\noindent
(b) Solutions of this system can be drawn in the \emph{phase plane}, whose axes are $x$ (horizontal) and $v$ (vertical).  To understand the geometry of these trajectories, we look for a quantity that stays constant along solutions.

Define the \emph{energy function}
\[
E(x,v) = \frac{1}{2}v^2 + \frac{1}{2}x^2.
\]
Compute the derivative $\dfrac{d}{dt}E\bigl(x(t),v(t)\bigr)$ along a solution $(x(t),v(t))$ of your first-order system from part (a).  Simplify your expression as far as possible.

\emph{Hint:} Use the chain rule
\[
\frac{d}{dt}E(x(t),v(t)) = E_x(x,v)\,x'(t) + E_v(x,v)\,v'(t),
\]
and then substitute $x'(t)$ and $v'(t)$ from your system.

\smallskip

\noindent
(c) Show that the derivative you found in part (b) is actually \emph{identically zero} along any solution of the system.  Conclude that the energy $E(x(t),v(t))$ is constant in time for every solution.

\emph{Hint:} You should obtain an exact cancellation between the terms that involve $x$ and those that involve $v$.

\smallskip

\noindent
(d) Fix a constant $E_0 > 0$.  Describe the set of all points $(x,v)$ in the phase plane that satisfy $E(x,v) = E_0$.  What kind of curve is this?  Sketch or visualize these sets for several values of $E_0$.

Then explain why every solution $(x(t),v(t))$ of the system must remain on exactly one of these curves for all time.  What does this say about the shape of the trajectories in the phase plane, and about the qualitative behavior of $x(t)$ (for example, periodicity)?

\emph{Hint:} The equation $E(x,v) = E_0$ can be written in a familiar form involving $x^2$ and $v^2$.

\smallskip

\noindent
(e) (Explorations.)

\begin{enumerate}
  \item Suppose we add a small damping term to the original equation and consider
  \[
  x''(t) + 2\gamma x'(t) + x(t) = 0, \qquad \gamma > 0.
  \]
  Rewrite this damped equation as a first-order system in $(x,v)$, and compute $\dfrac{d}{dt}E(x(t),v(t))$ along its solutions, where $E$ is the same energy function as before.  Is the energy still conserved?  How does this change the qualitative picture of the trajectories in the phase plane?

  \emph{Hint:} Compare the sign of $\dfrac{d}{dt}E$ now to the undamped case.

  \item Instead of using $v = x'$ as the vertical variable, one might use the \emph{momentum} $p = v$ (which in our dimensionless setting is the same as $v$) or rescale the axes differently.  How would rescaling $x$ and $v$ (for instance, considering $u = ax$ and $w = bv$ for constants $a,b>0$) change the appearance of the phase portrait, and what features of the dynamics would remain the same?

  \emph{Hint:} Think about how circles transform under scaling of the axes.
\end{enumerate}

\end{problem}

% ===== Example 1: Simple Harmonic Oscillator in Phase Space (full solution) =====
\begin{problem}[Simple Harmonic Oscillator in Phase Space]
Consider the simple harmonic oscillator
\[
x''(t) + x(t) = 0.
\]
\begin{enumerate}
  \item Rewrite this second-order equation as a first-order system in the variables $x$ (position) and $v$ (velocity).
  \item Define the energy function
  \[
  E(x,v) = \frac{1}{2}v^2 + \frac{1}{2}x^2.
  \]
  Show that $E(x(t),v(t))$ is constant along any solution $(x(t),v(t))$ of the first-order system.
  \item Describe the level sets $\{(x,v) : E(x,v) = E_0\}$ in the phase plane and explain what this implies about the trajectories of the system and the qualitative behavior of $x(t)$.
\end{enumerate}
\end{problem}

\begin{solution}
We study the ordinary differential equation
\[
x''(t) + x(t) = 0
\]
from the point of view of phase space dynamics.  The main ideas are to rewrite the second-order scalar equation as a first-order system, to identify a conserved quantity (the mechanical energy), and to interpret its level sets as trajectories in the phase plane.

\medskip

\noindent\textbf{(1) First-order system formulation.}
We introduce the velocity variable
\[
v(t) = x'(t).
\]
Then we can express the second-order equation as a system of two first-order equations.  By definition,
\[
x'(t) = v(t).
\]
Differentiating $v(t)$ and using the original equation, we have
\[
v'(t) = x''(t) = -x(t).
\]
Thus the phase space system is
\[
\begin{cases}
x'(t) = v(t),\\[4pt]
v'(t) = -x(t).
\end{cases}
\]
This is a linear system $\mathbf{y}' = A\mathbf{y}$ with state vector $\mathbf{y} = (x,v)^{\mathsf{T}}$ and coefficient matrix
\[
A = \begin{pmatrix}
0 & 1\\
-1 & 0
\end{pmatrix},
\]
whose dynamics we will interpret geometrically in the $(x,v)$-plane, called the phase plane.

\medskip

\noindent\textbf{(2) Conservation of energy.}
We define the energy function
\[
E(x,v) = \frac{1}{2}v^2 + \frac{1}{2}x^2.
\]
Physically, this represents the sum of kinetic energy $\frac{1}{2}v^2$ and potential energy $\frac{1}{2}x^2$ (in nondimensional units).  We now show that this energy is constant along solutions of the system.

Let $(x(t),v(t))$ be any solution of the system.  We compute the time derivative of $E$ along this solution using the chain rule:
\[
\frac{d}{dt}E(x(t),v(t))
= E_x(x(t),v(t))\,x'(t) + E_v(x(t),v(t))\,v'(t),
\]
where $E_x$ and $E_v$ denote the partial derivatives of $E$ with respect to $x$ and $v$, respectively.

We first compute these partial derivatives:
\[
E_x(x,v) = \frac{\partial}{\partial x}\left(\frac{1}{2}v^2 + \frac{1}{2}x^2\right) = x,
\quad
E_v(x,v) = \frac{\partial}{\partial v}\left(\frac{1}{2}v^2 + \frac{1}{2}x^2\right) = v.
\]
Therefore,
\[
\frac{d}{dt}E(x(t),v(t))
= x(t)\,x'(t) + v(t)\,v'(t).
\]
Now we substitute the expressions for $x'(t)$ and $v'(t)$ from the first-order system,
\[
x'(t) = v(t), \qquad v'(t) = -x(t),
\]
to obtain
\[
\frac{d}{dt}E(x(t),v(t))
= x(t)\,v(t) + v(t)\,(-x(t))
= x(t)\,v(t) - v(t)\,x(t) = 0.
\]
Thus $\dfrac{d}{dt}E(x(t),v(t)) = 0$ for all $t$ where the solution is defined.  This shows that
\[
E(x(t),v(t)) = \text{constant in } t.
\]
In other words, the energy is conserved along every trajectory.  This is the hallmark of a \emph{conservative} system.

\medskip

\noindent\textbf{(3) Level sets and phase plane trajectories.}
We next interpret the conservation of energy geometrically.  Fix any constant $E_0 > 0$.  The level set of the energy function corresponding to $E_0$ is
\[
\{(x,v) \in \mathbb{R}^2 : E(x,v) = E_0\}
= \left\{(x,v) : \frac{1}{2}v^2 + \frac{1}{2}x^2 = E_0\right\}.
\]
Multiplying both sides of the defining equation by $2$, we obtain
\[
x^2 + v^2 = 2E_0.
\]
This is the equation of a circle in the $(x,v)$-plane, centered at the origin, with radius $\sqrt{2E_0}$.  Thus each level set of $E$ with $E_0>0$ is a circle around the origin.  The special level set $E_0 = 0$ consists only of the single point $(0,0)$.

Because $E(x(t),v(t))$ is constant along any solution $(x(t),v(t))$, each trajectory must lie entirely within a single level set of $E$, determined by the initial condition.  More precisely, if the initial condition is $(x(0),v(0)) = (x_0,v_0)$, then for all times $t$ we have
\[
E(x(t),v(t)) = E(x_0,v_0),
\]
so the trajectory $(x(t),v(t))$ remains on the circle
\[
x^2 + v^2 = x_0^2 + v_0^2.
\]
Consequently, every nontrivial trajectory in the phase plane is a closed orbit, specifically a circle centered at the origin.  The origin itself is an equilibrium solution (corresponding to the mass at rest at equilibrium) and is represented by the single point $(0,0)$.

From the point of view of the original scalar equation, the fact that phase trajectories are closed circles implies that the motion is periodic.  Indeed, solving $x'' + x = 0$ in the usual way yields
\[
x(t) = A\cos t + B\sin t,
\]
and the velocity is
\[
v(t) = x'(t) = -A\sin t + B\cos t.
\]
The pair $(x(t),v(t))$ traces out a circle of radius $\sqrt{A^2+B^2}$ in the phase plane as $t$ increases.  The motion repeats itself with period $2\pi$, which is reflected in the fact that the trajectory is a closed curve.

\medskip

\noindent\textbf{Context within phase space dynamics.}
This example illustrates several central ideas of the section on \emph{Phase Space Dynamics for Conservative and Perturbed Systems}.  First, we saw how a scalar second-order equation can be reformulated as a first-order system, which allows us to analyze trajectories in the phase plane.  Second, we identified a conserved quantity, the energy, whose level sets organize the dynamics: each initial condition leads to motion along a fixed energy contour.  For this conservative system, all nontrivial trajectories are closed orbits corresponding to periodic motion.

In later examples, when damping or external forcing is added, the system will no longer conserve energy, the level sets will no longer be invariant curves, and phase trajectories will tend to spiral toward or away from equilibria rather than forming closed loops.  The simple harmonic oscillator thus serves as a canonical model of conservative dynamics against which perturbed systems can be compared.

\end{solution}

% ===== Example 2: Nonlinear Conservative Oscillator and Energy Level Sets (inquiry-based) =====
\begin{problem}[Nonlinear Conservative Oscillator and Energy Level Sets]
A mass attached to an ideal linear spring executes simple harmonic motion, and its trajectories in phase space are perfect circles. Real springs, however, are not perfectly linear. In this problem we study a simple model of a nonlinear spring, where the restoring force contains both linear and cubic terms. The system remains conservative, so there is an energy function, but the corresponding level sets in the phase plane are no longer circles. Instead, they are distorted closed curves, and the period of oscillation depends on the amplitude.

Consider the nonlinear oscillator
\[
x'' + x + x^3 = 0,
\]
where $x(t)$ is the displacement of the mass from equilibrium.

\smallskip

(a) Show that this equation is conservative by finding a quantity that is constant along every solution.  
More precisely, show that there exists a function $E(x,x')$ such that $\dfrac{d}{dt}E(x(t),x'(t)) = 0$ whenever $x$ satisfies the differential equation.

\emph{Hint:} A common trick for finding such an ``energy'' is to multiply the equation $x'' + x + x^3 = 0$ by $x'(t)$ and then integrate with respect to time.

\smallskip

(b) Interpret the energy function $E(x,x')$ mechanically.  

(i) Identify which part of $E$ should be thought of as kinetic energy and which part as potential energy.  
(ii) From your expression for $E$, read off the potential energy function $V(x)$ and sketch the graph of $V(x)$ as a function of $x$.

\emph{Hint:} For a one–dimensional mechanical system, the kinetic energy has the form $\tfrac12 (x')^2$ (up to a constant factor that we may take to be $1$ here), and the potential $V(x)$ satisfies $-V'(x)$ equal to the restoring force.

\smallskip

(c) Rewrite the second–order equation as a planar first–order system and describe the phase portrait in terms of the energy.  

(i) Introduce the velocity variable $y = x'$ and write the system in $(x,y)$–coordinates.  
(ii) Using your expression for $E(x,y)$, show that
\[
\frac{d}{dt} E(x(t),y(t)) = 0
\]
for any solution $(x(t),y(t))$ of the system. Conclude that each trajectory of the planar system lies entirely on a level set
\[
E(x,y) = \text{constant}.
\]

\emph{Hint:} Compute $\dfrac{d}{dt}E(x(t),y(t))$ using the chain rule:
\[
\frac{dE}{dt} = E_x(x,y)\,x' + E_y(x,y)\,y',
\]
and then substitute $x' = y$ and $y'$ from your first–order system.

\smallskip

(d) Now examine the geometry of the level sets and what they tell you about the motion.

(i) For a fixed energy level $E>0$, solve the relation $E(x,y) = E$ for $y^2$ in terms of $x$. What is the maximum displacement $|x|$ that a trajectory with energy $E$ can reach?  

(ii) Compare these energy curves to the circles that arise from the linear oscillator $x''+x=0$. In what way are they similar, and in what way are they different? Make a qualitative sketch of several level sets $E(x,y)=E$ in the $(x,y)$–plane and explain why they correspond to periodic orbits.

(iii) Let $A>0$ denote the amplitude of an oscillation, that is, the maximum value of $|x(t)|$ along the trajectory. Use energy conservation to derive an integral formula for the period $T(A)$ of the oscillation as a function of $A$.

\emph{Hint:} On a given trajectory, the energy $E$ is fixed and can be expressed in terms of $A$. Solve the equation $E = \tfrac12 (x')^2 + V(x)$ for $x' = \dfrac{dx}{dt}$ as a function of $x$ and $A$, and separate variables to write $dt$ in terms of $dx$. One quarter of the period corresponds to $x$ moving from $0$ to $A$ (or from $A$ back to $0$).

\smallskip

(e) Explore how the phase portrait changes when the model is modified.

(i) Suppose instead that the restoring force has the form $-x + x^3$, leading to the equation
\[
x'' + x - x^3 = 0.
\]
Find the corresponding potential energy $V(x)$ and sketch it. How does the shape of $V(x)$ suggest a different qualitative phase portrait (for example, the possibility of multiple equilibrium positions or trajectories that escape to infinity)?

(ii) Suppose we add a small linear damping term and consider
\[
x'' + \varepsilon x' + x + x^3 = 0,
\qquad 0<\varepsilon\ll 1.
\]
Explain qualitatively (without detailed computation) what happens to the conserved energy function $E(x,x')$ and how the phase portrait is altered. In particular, what happens to the closed level curves you found in part (d) when damping is present?

\end{problem}

% ===== Example 2: Nonlinear Conservative Oscillator and Energy Level Sets (full solution) =====
\begin{problem}[Nonlinear Conservative Oscillator and Energy Level Sets]
Consider the nonlinear oscillator
\[
x'' + x + x^3 = 0.
\]
\begin{enumerate}
\item Show that this equation is conservative by finding a first integral (energy)
\[
H(x,x') = \frac12 (x')^2 + V(x)
\]
that is constant along solutions, and determine $V(x)$ explicitly.
\item Rewrite the equation as a planar system in variables $(x,y)$, $y=x'$, and show that $H(x,y)$ is constant along trajectories of this system. Conclude that trajectories in the phase plane lie on level sets $H(x,y)=\text{constant}$.
\item Sketch qualitatively the phase portrait in the $(x,y)$–plane, and compare it to the phase portrait of the linear oscillator $x''+x=0$. Explain why the nonlinear system still has periodic orbits and why their period depends on the amplitude.
\item Let $A>0$ denote the maximum displacement of a given periodic solution. Use the conserved energy to derive an integral formula for the period $T(A)$ of oscillation as a function of $A$.
\end{enumerate}
\end{problem}

\begin{solution}
We study the equation
\[
x'' + x + x^3 = 0,
\]
which models a frictionless mass attached to a spring with both linear and cubic restoring forces. The central concepts are conservation of energy, the representation of the dynamics in the phase plane, and the description of trajectories as level sets of an energy (Hamiltonian) function.

\medskip

\textbf{1. Energy integral and potential.}
We first seek a conserved quantity of the form
\[
H(x,x') = \frac12 (x')^2 + V(x),
\]
where $V(x)$ plays the role of a potential energy. A standard way to find such a quantity is to multiply the differential equation by $x'(t)$ and integrate.

Starting from
\[
x'' + x + x^3 = 0,
\]
multiply by $x'$:
\[
x''x' + x x' + x^3 x' = 0.
\]
Each term is an exact time derivative:
\[
x''x' = \frac{d}{dt}\!\left(\frac12 (x')^2\right),\qquad
x x' = \frac{d}{dt}\!\left(\frac12 x^2\right),\qquad
x^3 x' = \frac{d}{dt}\!\left(\frac14 x^4\right).
\]
Therefore
\[
\frac{d}{dt}\!\left(\frac12 (x')^2 + \frac12 x^2 + \frac14 x^4\right) = 0.
\]
This shows that
\[
H(x,x') = \frac12 (x')^2 + \frac12 x^2 + \frac14 x^4
\]
is constant along any solution. Thus the system is conservative with potential energy
\[
V(x) = \frac12 x^2 + \frac14 x^4.
\]

Mechanically, $\tfrac12 (x')^2$ is the kinetic energy (for a unit mass), and $V(x)$ is the potential energy whose derivative gives the restoring force:
\[
F(x) = -V'(x) = -\bigl(x + x^3\bigr),
\]
which matches the right–hand side of the original equation.

The graph of $V(x)$ is even, strictly increasing in $|x|$, and behaves like $\tfrac14 x^4$ for large $|x|$. It has a single minimum at $x=0$ with $V(0)=0$. Because $V(x)\to +\infty$ as $|x|\to\infty$, the potential well confines the motion for any finite energy.

\medskip

\textbf{2. First–order system and conserved Hamiltonian.}
To view the system in phase space, we introduce the velocity variable
\[
y = x'.
\]
Then the second–order equation becomes the planar first–order system
\[
\begin{cases}
x' = y,\\[4pt]
y' = -x - x^3.
\end{cases}
\]
In these variables, the energy function becomes
\[
H(x,y) = \frac12 y^2 + \frac12 x^2 + \frac14 x^4.
\]
We verify directly that $H$ is conserved along trajectories. Using the chain rule,
\[
\frac{d}{dt} H(x(t),y(t))
= H_x(x,y)\,x' + H_y(x,y)\,y'.
\]
We compute the partial derivatives
\[
H_x(x,y) = x + x^3,\qquad H_y(x,y) = y,
\]
and substitute $x' = y$, $y' = -x - x^3$ from the system. This yields
\[
\frac{dH}{dt}
= (x + x^3) \cdot y + y \cdot (-x - x^3)
= (x + x^3)y - (x + x^3)y
= 0.
\]
Therefore $H(x(t),y(t))$ is constant in time along any solution $(x(t),y(t))$, so each trajectory is contained in a level set
\[
H(x,y) = C,\qquad C\ge 0.
\]
This is the typical structure of a conservative (Hamiltonian) system in the plane: the flow preserves the value of a Hamiltonian $H$, and trajectories lie on its level curves.

\medskip

\textbf{3. Geometry of level sets and comparison to the linear oscillator.}
We now analyze the level sets of $H$ and what they imply for the motion.

Fix an energy level $C>0$. The equation
\[
H(x,y) = C
\]
reads
\[
\frac12 y^2 + \frac12 x^2 + \frac14 x^4 = C.
\]
Solving for $y^2$ gives
\[
y^2 = 2C - x^2 - \frac12 x^4.
\]
Thus the level set consists of those $(x,y)$ for which the right–hand side is nonnegative:
\[
2C - x^2 - \frac12 x^4 \ge 0.
\]
This inequality determines a bounded interval of accessible $x$–values. The turning points in $x$ occur where $y=0$, that is, where
\[
\frac12 x^2 + \frac14 x^4 = C,
\]
or equivalently,
\[
x^4 + 2x^2 - 4C = 0.
\]
For each $C>0$, this quartic equation has exactly two real roots, symmetric about zero, say at $x = \pm A$, where $A>0$ is the amplitude of the motion. Since $V(x)$ is even and strictly increasing for $x>0$, there is a unique $A>0$ with
\[
C = V(A) = \frac12 A^2 + \frac14 A^4.
\]
Along a trajectory with energy $C$, the motion in $x$ is confined to the interval $-A \le x \le A$, with $x=\pm A$ occurring when the velocity $y=x'$ is zero.

The phase curves $H(x,y)=C$ are therefore closed, smooth, simple curves encircling the origin. They are symmetric with respect to both axes because $H$ is even in both $x$ and $y$.

For comparison, the linear oscillator $x''+x=0$ has energy
\[
H_{\text{lin}}(x,y) = \frac12 y^2 + \frac12 x^2,
\]
and its level sets $H_{\text{lin}}(x,y)=C$ are circles
\[
x^2 + y^2 = 2C.
\]
In the nonlinear case, the extra quartic term $\tfrac14 x^4$ deforms these circles into rounded rectangles: for small $|x|$, the dynamics is close to that of the linear system, so the curves are nearly circular near the origin; for larger $|x|$, the quartic term dominates, so the curves flatten out in the $y$–direction because the same energy $C$ must now accommodate a larger potential contribution.

Because the vector field is smooth, and the phase curves are closed and surround the equilibrium at the bottom of the potential well, the motion on each such energy curve is periodic in time. Thus each closed level set $H(x,y)=C>0$ corresponds to a periodic orbit in the plane.

\medskip

\textbf{4. Period as a function of amplitude.}
We now derive an integral formula expressing the period of oscillation in terms of the amplitude. Let $A>0$ be the maximum displacement of a given periodic solution. Then the corresponding energy level is
\[
C = H(A,0) = \frac12 A^2 + \frac14 A^4.
\]
On this energy level, the velocity $y=x'$ satisfies
\[
\frac12 (x')^2 + \frac12 x^2 + \frac14 x^4 = \frac12 A^2 + \frac14 A^4.
\]
Solving for $(x')^2$ gives
\[
(x')^2 = A^2 + \frac12 A^4 - x^2 - \frac12 x^4.
\]
We may write
\[
\frac{dx}{dt} = \pm \sqrt{A^2 + \frac12 A^4 - x^2 - \frac12 x^4}.
\]
To obtain the period, we separate variables:
\[
dt = \frac{dx}{\sqrt{A^2 + \tfrac12 A^4 - x^2 - \tfrac12 x^4}}
\]
for motion in one direction (say, as $x$ increases from $0$ to $A$). Because the system is symmetric and the motion is periodic, one quarter of the period $T(A)$ corresponds to $x$ moving from $0$ to $A$. Thus
\[
\frac{T(A)}{4} = \int_0^{A} \frac{dx}{\sqrt{A^2 + \tfrac12 A^4 - x^2 - \tfrac12 x^4}},
\]
and therefore
\[
T(A) = 4 \int_0^{A} \frac{dx}{\sqrt{A^2 + \tfrac12 A^4 - x^2 - \tfrac12 x^4}}.
\]
This integral cannot be expressed in elementary functions; it is a special case of an elliptic integral. Nevertheless, the formula makes clear that $T(A)$ depends on $A$: unlike the linear oscillator, whose period is independent of amplitude, the nonlinear oscillator exhibits amplitude–dependent frequency.

For small amplitudes $A$, one can expand the integrand in a Taylor series and show that $T(A)$ is close to the linear period $2\pi$, with corrections of order $A^2$. This reflects the fact that for small $x$ the cubic term $x^3$ is small and the system behaves almost linearly, while for larger amplitudes the nonlinearity significantly affects the timing of the oscillation.

\medskip

\textbf{5. Relation to phase space dynamics for conservative and perturbed systems.}
This example illustrates several key ideas about phase space dynamics for conservative systems. The existence of a conserved energy $H$ lets us reduce the analysis of trajectories to the study of level sets $H(x,y)=C$ in the phase plane. For the linear oscillator, these level sets are circles; for the nonlinear oscillator, they are distorted but still closed curves, reflecting the confining potential. Each closed level curve corresponds to a periodic orbit, but the period now depends on the energy or amplitude because the restoring force is nonlinear.

If one were to add a small damping term, the system would no longer be conservative: $dH/dt$ would become negative, and trajectories in the phase plane would spiral slowly inward across level sets rather than staying on a single curve. Thus the comparison between the undamped (conservative) nonlinear oscillator treated here and its damped (perturbed) counterpart provides a concrete example of how conservation laws shape phase portraits and how small perturbations qualitatively change the long–term dynamics.

\end{solution}

% ===== Example 3: Introducing Damping: From Closed Orbits to Spirals (inquiry-based) =====
\begin{problem}[Introducing Damping: From Closed Orbits to Spirals]
In the undamped harmonic oscillator, every trajectory in phase space is a closed orbit: the system oscillates forever with constant energy. In reality, friction or resistance gradually removes energy, and the oscillations decay. One simple way to model this is to add a linear damping term to the equation. In this problem you will see how this small change turns closed orbits into spirals, and how different strengths of damping lead to different qualitative behaviors in the phase plane.

Consider the \emph{damped} harmonic oscillator
\[
x''(t) + 2\alpha\,x'(t) + x(t) = 0, \qquad \alpha \ge 0,
\]
where $x(t)$ denotes the displacement and $\alpha$ is a (dimensionless) damping parameter.

\smallskip

(a) Rewrite this second-order equation as a first-order system in the phase variables
\[
x(t), \qquad v(t) = x'(t).
\]
What is the $2\times 2$ coefficient matrix $A$ of the resulting linear system
\[
\begin{pmatrix} x' \\[4pt] v' \end{pmatrix} = A \begin{pmatrix} x \\[4pt] v \end{pmatrix}?
\]
Locate and classify the equilibrium point(s) of this system at the level of ``name only'' (for example, ``center,'' ``node,'' ``saddle,'' etc.) in the special case $\alpha = 0$. How does this relate to the familiar picture of the undamped harmonic oscillator in phase space?

% Hint: For $\alpha = 0$ you should recognize a linear Hamiltonian system with circular or elliptic trajectories.

\smallskip

(b) Now suppose $\alpha > 0$. Compute the eigenvalues of $A$ as functions of $\alpha$ by solving the characteristic equation
\[
\det(\lambda I - A) = 0.
\]
For which values of $\alpha$ are the eigenvalues:
\begin{itemize}
    \item a pair of complex conjugates with nonzero imaginary part?
    \item a repeated real eigenvalue?
    \item two distinct real eigenvalues?
\end{itemize}
Give the corresponding qualitative names of the equilibrium at the origin in each case.

Hint: Pay attention to the discriminant of the quadratic equation for the eigenvalues, and recall what types of phase portraits arise from complex versus real eigenvalues.

\smallskip

(c) Focus on the \emph{underdamped} case $0 < \alpha < 1$. Solve the scalar differential equation explicitly in this case, and express your answer in the form
\[
x(t) = e^{-\alpha t}\bigl(C_1 \cos(\omega t) + C_2 \sin(\omega t)\bigr),
\]
for some frequency $\omega$ depending on $\alpha$. Then find $v(t) = x'(t)$.

Using this explicit solution, describe the behavior of the trajectory $(x(t), v(t))$ in the phase plane as $t \to +\infty$. Why does this correspond to a \emph{spiral} rather than a closed orbit?

% Hint: Think about what the factor $e^{-\alpha t}$ does to the amplitude as $t$ increases, and remember what $(\cos(\omega t), \sin(\omega t))$ does in the plane.

\smallskip

(d) Now consider the \emph{critically damped} case $\alpha = 1$ and the \emph{overdamped} case $\alpha > 1$.

\begin{itemize}
    \item For $\alpha = 1$, solve the scalar equation and write $x(t)$ in the form
    \[
    x(t) = (C_1 + C_2 t)\, e^{-t}.
    \]
    Use this to describe the qualitative shape of trajectories in the phase plane. Do solutions oscillate? Do they approach the origin tangentially to a single direction, or in many directions?
    
    \item For $\alpha > 1$, express the eigenvalues $\lambda_1, \lambda_2$ of $A$ explicitly, and write the general solution of the form
    \[
    x(t) = C_1 e^{\lambda_1 t} + C_2 e^{\lambda_2 t}.
    \]
    Based on the signs of $\lambda_1$ and $\lambda_2$, what does this tell you about the approach to the origin in the phase plane? Is the origin still stable? Are there oscillations?
\end{itemize}

Summarize how the phase portrait changes as $\alpha$ increases from $0$ to values larger than $1$, using the qualitative terms \emph{center}, \emph{stable spiral} (or \emph{stable focus}), and \emph{stable node}.

% Hint: Connect what you know about real versus complex eigenvalues to whether or not the trajectories wind around the origin.

\smallskip

(e) The mechanical energy of the undamped oscillator is
\[
E(t) = \frac{1}{2}\bigl(x'(t)^2 + x(t)^2\bigr).
\]
\begin{itemize}
    \item For the damped system with $\alpha > 0$, compute $\dfrac{dE}{dt}$ along solutions. Show that $\dfrac{dE}{dt} \le 0$ for all $t$ and interpret this physically.
    \item (\emph{What if} extension.) What would happen to the eigenvalues of $A$ and to $\dfrac{dE}{dt}$ if we took $\alpha < 0$ (``negative damping'')? What sort of phase portrait would you expect, and what physical behavior would this correspond to?
\end{itemize}

% Hint: Differentiate $E(t)$ using the product and chain rules, then substitute the differential equation to simplify. For $\alpha < 0$, think about whether the real parts of the eigenvalues are positive or negative.
\end{problem}

% ===== Example 3: Introducing Damping: From Closed Orbits to Spirals (full solution) =====
\begin{problem}[Introducing Damping: From Closed Orbits to Spirals]
Consider the damped harmonic oscillator
\[
x''(t) + 2\alpha\,x'(t) + x(t) = 0, \qquad \alpha \in \mathbb{R}.
\]
\begin{enumerate}
    \item Rewrite this equation as a first-order linear system in the phase variables $x$ and $v = x'$, and find the equilibrium point(s).
    \item For $\alpha \ge 0$, compute the eigenvalues of the system matrix and classify the equilibrium at the origin for the cases $0<\alpha<1$, $\alpha=1$, and $\alpha>1$.
    \item In the underdamped case $0<\alpha<1$, solve the scalar equation explicitly and show that all nontrivial trajectories in the phase plane are spirals that wind into the origin as $t \to +\infty$.
    \item Define the energy
    \[
    E(t) = \frac{1}{2}\bigl(x'(t)^2 + x(t)^2\bigr).
    \]
    For $\alpha>0$, compute $\dfrac{dE}{dt}$ along solutions and prove that $E(t)$ is strictly decreasing unless the solution is identically zero.
    \item Briefly describe how the phase portrait of the damped system ($\alpha>0$) differs qualitatively from the undamped case $\alpha=0$, and explain how this illustrates the effect of a nonconservative perturbation on a conservative system.
\end{enumerate}
\end{problem}

\begin{solution}
We begin by rewriting the second-order equation as a first-order system. Introduce the velocity variable
\[
v(t) = x'(t).
\]
Then $x'(t) = v(t)$ and
\[
v'(t) = x''(t) = -2\alpha x'(t) - x(t) = -2\alpha v(t) - x(t).
\]
Thus the system can be written in vector form as
\[
\begin{pmatrix} x' \\[4pt] v' \end{pmatrix}
=
A \begin{pmatrix} x \\[4pt] v \end{pmatrix}
\quad\text{with}\quad
A = \begin{pmatrix} 0 & 1 \\[4pt] -1 & -2\alpha \end{pmatrix}.
\]
The equilibrium points satisfy $x' = 0$ and $v' = 0$, which are equivalent to $v = 0$ and $-x - 2\alpha v = 0$. These equations imply $x = 0$ and $v = 0$, so the origin $(x,v) = (0,0)$ is the unique equilibrium.

When $\alpha = 0$, the matrix
\[
A = \begin{pmatrix} 0 & 1 \\[4pt] -1 & 0 \end{pmatrix}
\]
has eigenvalues $\lambda = \pm i$. The origin is therefore a \emph{center}, and the phase portrait consists of closed curves around the origin. This matches the usual picture of the undamped harmonic oscillator: solutions oscillate forever with constant amplitude, and their trajectories in the $(x,v)$-plane are ellipses (which are circles in appropriately scaled coordinates).

\medskip

We now study the dependence on $\alpha$ of the eigenvalues of $A$ when $\alpha \ge 0$. The characteristic polynomial is
\[
\det(\lambda I - A)
= \det\begin{pmatrix} \lambda & -1 \\[4pt] 1 & \lambda + 2\alpha \end{pmatrix}
= \lambda(\lambda + 2\alpha) + 1
= \lambda^2 + 2\alpha \lambda + 1.
\]
Thus the eigenvalues satisfy
\[
\lambda^2 + 2\alpha \lambda + 1 = 0,
\]
so
\[
\lambda = -\alpha \pm \sqrt{\alpha^2 - 1}.
\]
The discriminant is $\Delta = (2\alpha)^2 - 4 = 4(\alpha^2 - 1)$, and its sign determines the type of eigenvalues.

\begin{itemize}
    \item If $0 < \alpha < 1$, then $\alpha^2 - 1 < 0$, and we get a complex conjugate pair
    \[
    \lambda_{1,2} = -\alpha \pm i\omega,
    \quad \text{where} \quad
    \omega = \sqrt{1 - \alpha^2} > 0.
    \]
    The real part is $-\alpha < 0$, so the origin is a \emph{stable spiral} (or \emph{stable focus}).

    \item If $\alpha = 1$, then $\alpha^2 - 1 = 0$, and both eigenvalues are equal to $-1$. There is a repeated real eigenvalue with negative sign. One can check that $A$ has only one linearly independent eigenvector, so the origin is a \emph{degenerate stable node} (the critically damped case).

    \item If $\alpha > 1$, then $\alpha^2 - 1 > 0$, and both eigenvalues
    \[
    \lambda_{1,2} = -\alpha \pm \sqrt{\alpha^2 - 1}
    \]
    are real and negative. Since $\sqrt{\alpha^2 - 1} < \alpha$, we have
    \[
    \lambda_1 = -\alpha + \sqrt{\alpha^2 - 1} < 0,
    \qquad
    \lambda_2 = -\alpha - \sqrt{\alpha^2 - 1} < 0.
    \]
    The origin is then a \emph{stable node}.
\end{itemize}

Thus for all $\alpha > 0$ the origin is asymptotically stable, but the qualitative nature of the approach (spiraling versus straight-in) depends on whether the eigenvalues are complex or real.

\medskip

In the underdamped case $0 < \alpha < 1$, the scalar equation can be solved explicitly by the standard method for constant coefficient linear ordinary differential equations. Since the characteristic equation
\[
r^2 + 2\alpha r + 1 = 0
\]
has roots $r = -\alpha \pm i\omega$ with $\omega = \sqrt{1-\alpha^2}$, the general solution is
\[
x(t) = e^{-\alpha t}\bigl(C_1 \cos(\omega t) + C_2 \sin(\omega t)\bigr),
\]
for arbitrary real constants $C_1$ and $C_2$. Differentiating, we obtain
\[
v(t) = x'(t) = e^{-\alpha t}\bigl(\tilde{C}_1 \cos(\omega t) + \tilde{C}_2 \sin(\omega t)\bigr),
\]
for suitable constants $\tilde{C}_1$ and $\tilde{C}_2$ that depend linearly on $C_1$ and $C_2$ and on $\alpha$ and $\omega$. The precise formulas are not important for the qualitative picture; what matters is that $v(t)$ has the same exponential factor $e^{-\alpha t}$ multiplying a sinusoidal term.

The factor $e^{-\alpha t}$ tends to $0$ as $t \to +\infty$. Meanwhile, the vector
\[
\bigl(\cos(\omega t), \sin(\omega t)\bigr)
\]
rotates around the origin with angular frequency $\omega$. Thus the pair $(x(t), v(t))$ traces out a curve that winds around the origin, with its radius decaying like $e^{-\alpha t}$ as $t$ increases. Consequently, each nontrivial trajectory is a spiral that winds inward and converges to the origin. Unlike the undamped case $\alpha=0$, the trajectory no longer closes on itself, since the amplitude is no longer constant in time.

\medskip

We now turn to the critically damped case $\alpha = 1$ and the overdamped case $\alpha > 1$.

When $\alpha=1$, the characteristic equation
\[
r^2 + 2r + 1 = 0
\]
has a double root $r=-1$. The general solution of the scalar ordinary differential equation is
\[
x(t) = (C_1 + C_2 t)\, e^{-t}.
\]
Here the factor $e^{-t}$ forces the solution to decay to zero, while the factor $(C_1 + C_2 t)$ allows for an initial transient that may grow linearly before the exponential decay dominates. There is no sinusoidal factor, so there are no oscillations: the solution crosses zero at most once. In the phase plane, all nontrivial trajectories approach the origin tangentially to the single eigen-direction associated with the eigenvalue $-1$. The origin is a degenerate stable node, and the approach is monotone in the sense that there is no winding around the origin.

When $\alpha>1$, the characteristic equation has two distinct real roots
\[
\lambda_{1,2} = -\alpha \pm \sqrt{\alpha^2 - 1},
\]
both negative. The general solution is
\[
x(t) = C_1 e^{\lambda_1 t} + C_2 e^{\lambda_2 t},
\]
with $C_1, C_2$ real. Since both $\lambda_1$ and $\lambda_2$ are strictly negative, both exponentials decay to zero as $t \to +\infty$, and the origin is again asymptotically stable. There is still no sinusoidal term, so there is no oscillation. In the phase plane, each nontrivial trajectory approaches the origin along a curve that becomes tangent to one of the two eigendirections as $t \to +\infty$. This produces the familiar picture of a stable node: nearby trajectories are drawn straight in, with no spiraling.

As $\alpha$ increases from $0$ to values larger than $1$, the qualitative classification evolves as follows:
\[
\text{center at } \alpha = 0
\;\longrightarrow\;
\text{stable spiral for } 0<\alpha<1
\;\longrightarrow\;
\text{degenerate stable node at } \alpha=1
\;\longrightarrow\;
\text{stable node for } \alpha>1.
\]
The transition from purely imaginary eigenvalues ($\alpha=0$) to complex eigenvalues with negative real part ($0<\alpha<1$) reflects the transition from closed orbits to inward spirals, while the disappearance of the imaginary part at $\alpha=1$ marks the end of oscillatory behavior.

\medskip

We now analyze the energy. Define
\[
E(t) = \frac{1}{2}\bigl(x'(t)^2 + x(t)^2\bigr)
= \frac{1}{2}\bigl(v(t)^2 + x(t)^2\bigr).
\]
Differentiating with respect to time, we obtain
\[
\frac{dE}{dt}
= v v' + x x'
= v v' + x v.
\]
Using the system equations, we substitute $v' = -2\alpha v - x$ to get
\[
\frac{dE}{dt}
= v(-2\alpha v - x) + x v
= -2\alpha v^2 - xv + xv
= -2\alpha v^2.
\]
Thus
\[
\frac{dE}{dt} = -2\alpha v^2.
\]
If $\alpha>0$, this derivative is nonpositive for all $t$, and it is equal to zero only when $v(t)=0$. Along any nontrivial solution, $v(t)$ is not identically zero, so $v(t)^2$ is positive on a set of times with nonzero measure, and hence $E(t)$ is strictly decreasing along such a solution. Physically, this expresses the fact that the damping term removes mechanical energy from the oscillator, converting it into heat or some other form of dissipated energy.

\medskip

Finally, we compare the damped system to the undamped one. When $\alpha=0$, the system is conservative: the energy $E(t)$ is exactly constant, all trajectories are closed curves encircling the origin, and the origin is a center. When $\alpha>0$, the damping term is a nonconservative perturbation. It changes the eigenvalues from purely imaginary to having negative real part, so the center becomes a stable spiral for small damping and then a node for stronger damping. At the same time, the energy becomes a strict Lyapunov function that decreases along nontrivial trajectories.

This example illustrates a central idea of phase space dynamics for conservative and perturbed systems: a seemingly small nonconservative perturbation (here, the linear damping term $2\alpha x'$) can drastically change the qualitative structure of trajectories in the phase plane, turning closed orbits into spirals and converting neutral stability into asymptotic stability.
\end{solution}

% ===== Example 4: Driven, Damped Oscillator and Resonance in Phase Space (inquiry-based) =====
\begin{problem}[Driven, Damped Oscillator and Resonance in Phase Space]
We study a mass--spring system with damping and a time-periodic forcing term. This system is a basic model for many physical and engineering situations, from car suspensions to driven circuits. Our goal is to understand, from the viewpoint of phase space dynamics, how solutions spiral toward a steady periodic motion and how this motion changes as the driving frequency approaches resonance. Along the way we will compare the conservative, damped, and driven cases.

Consider the differential equation
\[
m\ddot{x} + c\dot{x} + kx \;=\; F\cos(\omega t),
\]
where $m>0$, $c\ge 0$, $k>0$, $F\in\mathbb{R}$, and $\omega>0$ are constants.

\smallskip

(a) \textbf{Warm-up: conservative oscillator in phase space.}  

Assume first that there is no damping and no forcing, so that $c = 0$ and $F=0$, and divide the equation by $m$ to write
\[
\ddot{x} + \omega_0^2 x = 0, \qquad \text{where } \omega_0^2 = \frac{k}{m}.
\]
Introduce the velocity variable $v = \dot{x}$ and write this as a first-order system in the $(x,v)$-plane.

\begin{enumerate}
\item[(i)] Write the system in matrix form $\dot{\mathbf{y}} = A\mathbf{y}$ with $\mathbf{y} = (x,v)^{\mathsf{T}}$ and determine the eigenvalues of $A$.
\item[(ii)] Show that the quantity
\[
E(x,v) = \frac{1}{2}m v^2 + \frac{1}{2}k x^2
\]
is constant along solutions. What do the level sets of $E$ look like in the $(x,v)$-plane?
\item[(iii)] Sketch the phase portrait and describe in words how trajectories move in phase space for this conservative system.
\end{enumerate}
% Hint: For (ii), differentiate $E(x(t),v(t))$ with respect to time and use the ODE.

\smallskip

(b) \textbf{Adding damping: spirals to the origin.}  

Now include linear damping but no forcing, so that $F=0$ and $c>0$:
\[
m\ddot{x} + c\dot{x} + kx = 0.
\]
Again write the system in first-order form in the $(x,v)$-plane.

\begin{enumerate}
\item[(i)] Write down the corresponding matrix $A$ and compute its eigenvalues. Assume the \emph{underdamped} case $c^2 < 4mk$, so that the eigenvalues are complex with negative real part.
\item[(ii)] Describe the qualitative phase portrait in the $(x,v)$-plane in this underdamped case. How does it differ from the conservative case? In particular, what happens to the origin, and what happens to nearby trajectories as $t\to\infty$?
\item[(iii)] What happens to the \emph{energy} $E(x,v)$ from part (a) along solutions now?
\end{enumerate}
Hint: For (iii), compute $\frac{d}{dt}E(x(t),v(t))$ again, but now with the damping term present.

\smallskip

(c) \textbf{Adding periodic forcing: solving the ODE and finding the steady state.}  

Now consider the full driven, damped equation
\[
m\ddot{x} + c\dot{x} + kx = F\cos(\omega t),
\]
with $m>0$, $c>0$, $k>0$, and fixed driving frequency $\omega>0$.

\begin{enumerate}
\item[(i)] Solve the homogeneous equation $m\ddot{x} + c\dot{x} + kx = 0$ in the underdamped case $c^2 < 4mk$. Express the general homogeneous solution $x_{\mathrm{h}}(t)$ in terms of exponentials or sines and cosines multiplied by a decaying exponential.
\item[(ii)] Find a particular solution $x_{\mathrm{p}}(t)$ to the full inhomogeneous equation. Look for a solution of the form
\[
x_{\mathrm{p}}(t) = A\cos(\omega t) + B\sin(\omega t)
\]
for suitable constants $A$ and $B$ depending on $m,c,k,F,\omega$.  
Hint: Substitute this ansatz into the ODE and equate coefficients of $\cos(\omega t)$ and $\sin(\omega t)$.
\item[(iii)] Show that the general solution can be written as
\[
x(t) = x_{\mathrm{h}}(t) + x_{\mathrm{p}}(t),
\]
and explain why $x_{\mathrm{h}}(t)\to 0$ as $t\to\infty$ when $c>0$. Conclude that every solution approaches the same time-periodic motion $x_{\mathrm{p}}(t)$, regardless of initial conditions.
\end{enumerate}

\smallskip

(d) \textbf{Resonance and the attracting periodic orbit in phase space.}  

Still working with $c>0$, view the system in the phase plane with coordinates $(x,v)$, where $v=\dot{x}$.

\begin{enumerate}
\item[(i)] Write explicit formulas for $x_{\mathrm{p}}(t)$ and $v_{\mathrm{p}}(t)=\dot{x}_{\mathrm{p}}(t)$ in terms of an \emph{amplitude} $R(\omega)$ and a \emph{phase shift} $\phi(\omega)$, so that
\[
x_{\mathrm{p}}(t) = R(\omega)\cos\bigl(\omega t - \phi(\omega)\bigr).
\]
Determine $R(\omega)$ explicitly.  
Hint: Use your expressions for $A$ and $B$ from part (c)(ii) and apply a trigonometric identity like $a\cos(\theta) + b\sin(\theta) = \sqrt{a^2+b^2}\,\cos(\theta-\phi)$ for a suitable $\phi$.
\item[(ii)] Show that
\[
R(\omega) = \frac{F}{\sqrt{(k - m\omega^2)^2 + (c\omega)^2}}.
\]
For fixed $m,c,k$, analyze how $R(\omega)$ depends on $\omega$. At approximately which driving frequency $\omega$ is $R(\omega)$ largest? (You may use calculus or a qualitative argument.)
\item[(iii)] Describe the limiting motion in the phase plane. What curve is traced by $t\mapsto\bigl(x_{\mathrm{p}}(t),v_{\mathrm{p}}(t)\bigr)$ as $t$ varies over one forcing period $T = \tfrac{2\pi}{\omega}$? How do other trajectories $(x(t),\dot{x}(t))$ behave relative to this curve as $t\to\infty$?
\item[(iv)] As $\omega$ approaches the frequency that maximizes $R(\omega)$ (the resonance frequency), how does the size of the attracting periodic orbit in the $(x,v)$-plane change? How would you see this change if you sketched several phase portraits for increasing $\omega$?
\end{enumerate}

\smallskip

(e) \textbf{Poincaré (stroboscopic) map and extended phase space.}  

One way to study the long-time behavior of a periodically forced system is to sample the solution once every forcing period. Let $T = \tfrac{2\pi}{\omega}$ and define the \emph{Poincaré map} $P$ by
\[
P:\ (x_0,v_0)\mapsto\bigl(x(T),v(T)\bigr),
\]
where $x(t)$ is the solution of the ODE with initial data $x(0)=x_0$, $\dot{x}(0)=v_0$.

\begin{enumerate}
\item[(i)] Explain why the Poincaré map $P$ is an \emph{affine} map of the form
\[
P(\mathbf{y}) = M\mathbf{y} + \mathbf{b}
\]
for some $2\times 2$ matrix $M$ and vector $\mathbf{b}\in\mathbb{R}^2$. (You do not need to find $M$ and $\mathbf{b}$ explicitly, but explain the reasoning.)
\item[(ii)] Argue that all eigenvalues of $M$ have modulus strictly less than $1$ when $c>0$. (Hint: Think about what happens in the unforced damped case $F=0$ over one period.) Conclude that $P$ has a unique fixed point, and that every orbit under repeated application of $P$ converges to this fixed point.
\item[(iii)] Interpret this fixed point in terms of the original differential equation. What trajectory in the continuous-time phase portrait does it correspond to?
\item[(iv)] (Open-ended) How might you view the driven damped oscillator as an \emph{autonomous} system in a three-dimensional \emph{extended phase space}? What additional variable would you introduce, and what would its evolution equation be?
\end{enumerate}

In your answers, emphasize the geometric picture: closed curves versus spirals in phase space, the effect of damping and forcing on energy, and how resonance appears both in the time series $x(t)$ and in the phase portrait.
\end{problem}

% ===== Example 4: Driven, Damped Oscillator and Resonance in Phase Space (full solution) =====
\begin{problem}[Driven, Damped Oscillator and Resonance in Phase Space]
Consider the driven, damped harmonic oscillator
\[
m\ddot{x} + c\dot{x} + kx = F\cos(\omega t),
\]
with $m>0$, $c>0$, $k>0$, forcing amplitude $F\in\mathbb{R}$, and driving frequency $\omega>0$.

\begin{enumerate}
\item[(i)] Rewrite the equation as a first-order system in the phase plane $(x,v)$ with $v=\dot{x)$. Describe qualitatively the phase portrait when $F=0$ in the cases $c=0$ (conservative) and $c>0$ with $c^2<4mk$ (underdamped).
\item[(ii)] For $F\neq 0$, solve the inhomogeneous equation by finding the general homogeneous solution and a particular solution of the form $x_{\mathrm{p}}(t) = A\cos(\omega t)+B\sin(\omega t)$. Show that all solutions converge, as $t\to\infty$, to a unique time-periodic steady state $x_{\mathrm{p}}(t)$ of period $T = \tfrac{2\pi}{\omega}$.
\item[(iii)] Express the steady state as
\[
x_{\mathrm{p}}(t) = R(\omega)\cos\bigl(\omega t - \phi(\omega)\bigr)
\]
for suitable $R(\omega)>0$ and phase shift $\phi(\omega)$, and show that
\[
R(\omega) = \frac{F}{\sqrt{(k - m\omega^2)^2 + (c\omega)^2}}.
\]
Determine the driving frequency $\omega$ at which $R(\omega)$ is maximal (resonance) and describe how this affects the size of the attracting periodic orbit in the $(x,v)$ phase plane.
\item[(iv)] Define the Poincaré map $P:\mathbb{R}^2\to\mathbb{R}^2$ by
\[
P(x_0,v_0) = \bigl(x(T),v(T)\bigr),
\]
where $x(t)$ solves the ODE with initial data $x(0)=x_0$, $\dot{x}(0)=v_0$, and $T=\tfrac{2\pi}{\omega}$. Explain why $P$ is an affine map $P(\mathbf{y}) = M\mathbf{y} + \mathbf{b}$ with all eigenvalues of $M$ of modulus less than $1$, and conclude that $P$ has a unique attracting fixed point corresponding to the periodic orbit. Briefly state how this example illustrates the role of phase space and Poincaré maps in understanding conservative versus perturbed systems.
\end{enumerate}
\end{problem}

\begin{solution}
We analyze the system
\[
m\ddot{x} + c\dot{x} + kx = F\cos(\omega t)
\]
step by step, emphasizing both explicit solutions and their phase space interpretation.

\medskip

\noindent\textbf{(i) First-order formulation and unforced phase portraits.}

Introduce the velocity variable $v = \dot{x}$ and let $\mathbf{y} = (x,v)^{\mathsf{T}}$. Then
\[
\dot{x} = v,\qquad
m\dot{v} = -c v - k x + F\cos(\omega t).
\]
Dividing the second equation by $m$ gives
\[
\dot{x} = v,\qquad
\dot{v} = -\frac{k}{m}x - \frac{c}{m}v + \frac{F}{m}\cos(\omega t).
\]
This is a linear, time-periodic, non-autonomous system in the $(x,v)$-plane.

When $F=0$, the system becomes autonomous:
\[
\dot{x} = v,\qquad
\dot{v} = -\frac{k}{m}x - \frac{c}{m}v.
\]
In matrix form,
\[
\dot{\mathbf{y}} = A\mathbf{y},\qquad
A = \begin{pmatrix}
0 & 1 \\
-\dfrac{k}{m} & -\dfrac{c}{m}
\end{pmatrix}.
\]

\emph{Case $c=0$ (conservative oscillator).} Then
\[
A = \begin{pmatrix}
0 & 1 \\
-\dfrac{k}{m} & 0
\end{pmatrix},
\]
with characteristic polynomial $\lambda^2 + \dfrac{k}{m} = 0$. The eigenvalues are
\[
\lambda = \pm i\omega_0,\qquad \omega_0 = \sqrt{\frac{k}{m}}.
\]
These are purely imaginary, so the origin is a linear center. One can verify that the “energy”
\[
E(x,v) = \frac{1}{2}m v^2 + \frac{1}{2}k x^2
\]
is constant along trajectories. Indeed,
\[
\frac{d}{dt}E(x(t),v(t))
= m v\dot{v} + k x\dot{x}
= m v\Bigl(-\frac{k}{m}x\Bigr) + k x v
= -k xv + k xv = 0.
\]
Thus $E$ is conserved, and level sets $E=\text{constant}>0$ are ellipses in the $(x,v)$-plane. The phase portrait consists of closed, periodic orbits encircling the origin, which is a nonlinear center.

\emph{Case $c>0$ with $c^2<4mk$ (underdamped).} The characteristic polynomial is
\[
\lambda^2 + \frac{c}{m}\lambda + \frac{k}{m} = 0,
\]
with discriminant $\Delta = \frac{c^2}{m^2} - 4\frac{k}{m} = \frac{c^2-4mk}{m^2}<0$. Hence the eigenvalues are complex with negative real part:
\[
\lambda = -\frac{c}{2m} \pm i\sqrt{\frac{k}{m} - \frac{c^2}{4m^2}}.
\]
The origin is a stable spiral (or spiral sink). Every nontrivial trajectory spirals inward toward the origin as $t\to\infty$.

The energy $E(x,v)$ is no longer conserved. A direct computation gives
\[
\frac{d}{dt}E(x(t),v(t))
= m v\dot{v} + k x\dot{x}
= m v\Bigl(-\frac{c}{m}v - \frac{k}{m}x\Bigr) + k x v
= -c v^2 \le 0.
\]
Thus the energy decreases monotonically due to dissipation, and the ellipses of constant $E$ shrink over time to the origin, explaining the spiral trajectories.

\medskip

\noindent\textbf{(ii) General solution with forcing and convergence to a periodic state.}

We now consider
\[
m\ddot{x} + c\dot{x} + kx = F\cos(\omega t),\qquad m>0,\ c>0,\ k>0.
\]
The associated homogeneous equation
\[
m\ddot{x} + c\dot{x} + kx = 0
\]
has, in the underdamped case $c^2<4mk$, a general solution of the form
\[
x_{\mathrm{h}}(t) = e^{-\alpha t}\bigl(C_1\cos(\beta t) + C_2\sin(\beta t)\bigr),
\]
where
\[
\alpha = \frac{c}{2m}>0,\qquad
\beta = \sqrt{\frac{k}{m} - \frac{c^2}{4m^2}}>0,
\]
and $C_1,C_2$ are determined by initial conditions. The exponential factor $e^{-\alpha t}$ implies $x_{\mathrm{h}}(t)\to 0$ and $\dot{x}_{\mathrm{h}}(t)\to 0$ as $t\to\infty$.

To find a particular solution to the full forced equation, we seek a steady-state response with the same frequency as the forcing:
\[
x_{\mathrm{p}}(t) = A\cos(\omega t) + B\sin(\omega t).
\]
Differentiating,
\[
\dot{x}_{\mathrm{p}}(t) = -A\omega\sin(\omega t) + B\omega\cos(\omega t),
\]
\[
\ddot{x}_{\mathrm{p}}(t) = -A\omega^2\cos(\omega t) - B\omega^2\sin(\omega t).
\]
Substituting into the ODE gives
\[
m(-A\omega^2\cos\omega t - B\omega^2\sin\omega t)
+ c(-A\omega\sin\omega t + B\omega\cos\omega t)
+ k(A\cos\omega t + B\sin\omega t)
= F\cos(\omega t).
\]
Grouping the coefficients of $\cos(\omega t)$ and $\sin(\omega t)$, we obtain
\[
\text{cosine:}\quad (-m\omega^2 + k)A + c\omega B = F,
\]
\[
\text{sine:}\quad (-m\omega^2 + k)B - c\omega A = 0.
\]
This is a linear system for $A$ and $B$. Solving the second equation for $B$ gives
\[
B = \frac{c\omega}{k - m\omega^2}\,A
\]
(provided $k-m\omega^2\neq 0$; that special case can be treated by continuity). Substituting into the cosine equation yields
\[
(k - m\omega^2)A + c\omega\cdot\frac{c\omega}{k - m\omega^2}A = F,
\]
so
\[
\left[(k - m\omega^2) + \frac{c^2\omega^2}{k - m\omega^2}\right]A = F.
\]
Thus
\[
A = \frac{F(k - m\omega^2)}{(k - m\omega^2)^2 + (c\omega)^2},\qquad
B = \frac{Fc\omega}{(k - m\omega^2)^2 + (c\omega)^2}.
\]

The general solution of the forced equation is therefore
\[
x(t) = x_{\mathrm{h}}(t) + x_{\mathrm{p}}(t),
\]
with $x_{\mathrm{h}}(t)$ decaying exponentially in time and $x_{\mathrm{p}}(t)$ bounded and periodic of period $T = 2\pi/\omega$. Since $x_{\mathrm{h}}(t)\to 0$ and $\dot{x}_{\mathrm{h}}(t)\to 0$ as $t\to\infty$, every solution $(x(t),\dot{x}(t))$ converges to the same $T$-periodic orbit defined by $x_{\mathrm{p}}(t)$, independent of initial conditions.

\medskip

\noindent\textbf{(iii) Amplitude, resonance, and phase-plane interpretation.}

The steady-state solution can be written more compactly in amplitude-phase form. Recall that any linear combination $A\cos(\omega t)+B\sin(\omega t)$ can be expressed as
\[
A\cos(\omega t)+B\sin(\omega t)
= R(\omega)\cos\bigl(\omega t - \phi(\omega)\bigr),
\]
where
\[
R(\omega) = \sqrt{A^2 + B^2},\qquad
\phi(\omega) = \arctan\left(\frac{B}{A}\right)
\]
(up to the appropriate quadrant choice for $\phi$). Therefore the amplitude of the steady-state oscillation is
\[
R(\omega) = \sqrt{A^2 + B^2}
= \sqrt{\frac{F^2\bigl[(k - m\omega^2)^2 + (c\omega)^2\bigr]}{\bigl[(k - m\omega^2)^2 + (c\omega)^2\bigr]^2}}
= \frac{F}{\sqrt{(k - m\omega^2)^2 + (c\omega)^2}}.
\]
This is the desired expression.

To find the driving frequency $\omega$ that maximizes $R(\omega)$, it is convenient to minimize the denominator
\[
D(\omega) = (k - m\omega^2)^2 + (c\omega)^2.
\]
Differentiate with respect to $\omega$:
\[
\frac{dD}{d\omega} = 2(k - m\omega^2)(-2m\omega) + 2c^2\omega
= -4m\omega(k - m\omega^2) + 2c^2\omega.
\]
Setting $\frac{dD}{d\omega}=0$ and assuming $\omega>0$ gives
\[
-4m(k - m\omega^2) + 2c^2 = 0
\quad\Longrightarrow\quad
4m^2\omega^2 = 4mk - 2c^2.
\]
Hence
\[
\omega_{\mathrm{res}}^2 = \frac{4mk - 2c^2}{4m^2} 
= \frac{k}{m} - \frac{c^2}{2m^2}.
\]
If damping is weak ($c^2 < 2mk$), then $\omega_{\mathrm{res}}>0$ and satisfies
\[
\omega_{\mathrm{res}} < \sqrt{\frac{k}{m}} = \omega_0.
\]
Thus the resonant frequency is slightly lower than the natural frequency of the undamped oscillator. At this frequency the amplitude $R(\omega)$ is maximal, and the corresponding periodic orbit in the $(x,v)$ phase plane has the largest radius (in an appropriate sense).

In phase space, the steady-state trajectory is
\[
t\mapsto \bigl(x_{\mathrm{p}}(t),v_{\mathrm{p}}(t)\bigr),
\quad
x_{\mathrm{p}}(t) = R(\omega)\cos(\omega t - \phi),\quad
v_{\mathrm{p}}(t) = \dot{x}_{\mathrm{p}}(t) = -R(\omega)\omega\sin(\omega t - \phi).
\]
Over one period $T=2\pi/\omega$, this traces a closed curve which, after an appropriate linear rescaling, is an ellipse centered at the origin. Because all homogeneous contributions decay, any solution $(x(t),\dot{x}(t))$ spirals toward this closed curve as $t\to\infty$. As the driving frequency approaches $\omega_{\mathrm{res}}$, the amplitude $R(\omega)$ grows and this limiting closed curve expands outward in the phase plane. Near resonance, the spiral transient can also take many cycles to settle, since the decay rate is governed by the damping while the amplitude is large.

\medskip

\noindent\textbf{(iv) Poincaré map, contraction, and periodic orbit.}

The forcing has period $T = 2\pi/\omega$, so it is natural to sample solutions stroboscopically at times $t=nT$. Fix an initial condition $(x_0,v_0)$ at $t=0$, and let $x(t)$ be the corresponding solution. The Poincaré map $P$ is defined by
\[
P(x_0,v_0) = \bigl(x(T),v(T)\bigr).
\]

Because the differential equation is linear in $(x,\dot{x})$ and the forcing term $F\cos(\omega t)$ does not depend on $(x,v)$, the solution at time $T$ depends \emph{affinely} on the initial data. More precisely, if we write $\mathbf{y}(t)=(x(t),v(t))^{\mathsf{T}}$, then for each fixed $t$ there exists a $2\times 2$ matrix $\Phi(t)$ (the state-transition matrix of the homogeneous system) and a vector $\mathbf{w}(t)$ such that
\[
\mathbf{y}(t) = \Phi(t)\mathbf{y}(0) + \mathbf{w}(t).
\]
At $t=T$ this gives
\[
P(\mathbf{y}_0) = \mathbf{y}(T) = M\mathbf{y}_0 + \mathbf{b},
\]
where $M = \Phi(T)$ and $\mathbf{b} = \mathbf{w}(T)$ are fixed. Thus $P$ is an affine map.

To understand the eigenvalues of $M$, note that in the unforced damped case $F=0$ we have
\[
\dot{\mathbf{y}} = A\mathbf{y},\qquad
\mathbf{y}(t) = e^{At}\mathbf{y}(0).
\]
The eigenvalues of $A$ are $\lambda_{1,2} = -\alpha \pm i\beta$, so the eigenvalues of $e^{AT}$ are $e^{\lambda_{1}T}$ and $e^{\lambda_{2}T}$, which have modulus $e^{-\alpha T}<1$. Thus the homogeneous flow over one period is a linear contraction toward $0$ in the $(x,v)$-plane.

For the forced system, the matrix $M$ is precisely $e^{AT}$, since it is the linear part of the map taking initial data to the homogeneous contribution at time $T$. Hence the eigenvalues of $M$ still satisfy $|\mu_i|<1$. It follows that the linear part of $P$ is a contraction. In particular, the affine map $P(\mathbf{y}) = M\mathbf{y} + \mathbf{b}$ has a unique fixed point $\mathbf{y}^*$ solving
\[
\mathbf{y}^* = M\mathbf{y}^* + \mathbf{b},
\quad\text{that is,}\quad
(I - M)\mathbf{y}^* = \mathbf{b},
\]
and for any initial condition $\mathbf{y}_0$,
\[
P^n(\mathbf{y}_0) \to \mathbf{y}^*\quad\text{as }n\to\infty.
\]

The fixed point $\mathbf{y}^*$ corresponds exactly to the steady-state periodic solution. Indeed, if we start at $(x(0),v(0))=\mathbf{y}^*$, then sampling after one period gives
\[
P(\mathbf{y}^*) = \mathbf{y}^*,
\]
so the state after time $T$ returns to the same point in phase space. By uniqueness of solutions, the trajectory must then be $T$-periodic. Conversely, any $T$-periodic solution yields a fixed point of $P$.

\medskip

\noindent\textbf{Conceptual summary and connection to phase space dynamics.}

In the unforced, undamped case ($c=F=0$), the system is conservative: energy is conserved, and phase space is foliated by closed orbits encircling a center. Adding damping ($c>0$, $F=0$) perturbs this picture: energy monotonically decreases, and each closed orbit is replaced by a spiral trajectory toward an attracting equilibrium at the origin.

Finally, adding periodic forcing ($c>0$, $F\neq 0$) creates a balance between input and dissipation. The origin is no longer an equilibrium, but the system develops a unique attracting periodic orbit in the $(x,v)$-plane. The phase portrait shows trajectories spiraling toward this closed curve, whose size depends on the driving frequency and becomes largest near resonance. The Poincaré map provides a discrete-time description of this attraction: it is an affine contraction with a unique attracting fixed point corresponding to the periodic orbit.

This example illustrates the main ideas of the section: how phase
space geometry changes under damping and forcing, and how Poincaré maps give a compact, discrete-time description of the long-time behavior of periodically forced systems by encoding convergence to attracting periodic orbits.
\end{solution}

% ===== Example 5: Double-Well Potential: Separatrices and Heteroclinic Orbits (inquiry-based) =====
\begin{problem}[Double-Well Potential: Separatrices and Heteroclinic Orbits]
A simple model for a particle moving in a one-dimensional double-well potential is the second-order equation
\[
x'' + V'(x)=0,\qquad V(x)=\frac14(x^2-1)^2.
\]
The potential \(V\) has two minima (the “wells”) and one maximum (the “barrier” between the wells). In phase space, some trajectories are trapped near one well, while others have enough energy to cross over the barrier and explore both wells. The delicate boundary between these types of trajectories is formed by separatrix curves; in this particular example those separatrices are homoclinic loops to a saddle equilibrium, and in related systems one can also see heteroclinic connections between distinct saddle points.

We rewrite the equation as a first-order system by introducing the velocity \(y=x'\):
\[
x' = y,\qquad y' = -V'(x)=x - x^3.
\]

\smallskip
\noindent
(a) \emph{Equilibria and the potential landscape.}  
Find all equilibrium points \((x,y)\) of the planar system and classify them as critical points (minima, maxima, or saddle-type points) of the potential \(V(x)\) in the one-dimensional sense. Sketch the graph of \(V(x)\) and indicate the three equilibrium positions on this graph.

\medskip
\noindent
(b) \emph{Energy as a first integral.}  
Show that the function
\[
H(x,y)=\frac12 y^2 + V(x)
\]
is conserved along solutions, that is, \(\frac{d}{dt}H(x(t),y(t))=0\) whenever \((x(t),y(t))\) solves the system. Conclude that solution curves in the phase plane lie on the level sets \(H(x,y)=E\), where \(E\) is a constant interpreted as the total mechanical energy.

\emph{Hint:} Differentiate \(H(x(t),y(t))\) with respect to time and use the system \(x'=y\), \(y'=x-x^3\).

\medskip
\noindent
(c) \emph{Equilibria in phase space and their linearization.}  
Determine all equilibrium points \((x,y)\) of the planar system directly, and classify their type in the phase plane (center, saddle, node, etc.) using the Jacobian matrix of the vector field
\[
F(x,y)=(y,\;x-x^3).
\]
Compute the eigenvalues at each equilibrium. How does this phase-plane classification relate to the shape of the potential \(V(x)\) at the corresponding positions?

\emph{Hint:} At a point \(x_0\) where \(V'(x_0)=0\), the second derivative \(V''(x_0)\) determines whether \(V\) has a local minimum or maximum there, and the sign of \(V''(x_0)\) also determines whether the corresponding equilibrium in phase space is a center or a saddle.

\medskip
\noindent
(d) \emph{Separatrices and homoclinic orbits.}  

(i) Compute the energy values \(H\) at each equilibrium. Show that the two “well” equilibria \((x,y)=(\pm 1,0)\) lie at energy level \(E_{\min}\) (a minimum of \(H\)), while the “barrier” equilibrium \((0,0)\) lies at a higher energy level \(E_{\mathrm{bar}}\).

(ii) For a fixed energy \(E\in(E_{\min},E_{\mathrm{bar}})\), describe qualitatively the corresponding level curve \(H(x,y)=E\) in the phase plane. Explain why such a trajectory remains trapped in one of the wells.

(iii) For energies \(E>E_{\mathrm{bar}}\), describe how the level curves change and why such trajectories can cross over the barrier between the wells.

(iv) Show that the critical energy level \(E=E_{\mathrm{bar}}\) is special: the level set \(H(x,y)=E_{\mathrm{bar}}\) consists of two symmetric separatrix curves passing through the saddle point \((0,0)\). Show that these curves have the explicit equation
\[
H(x,y)=0 \quad\Longleftrightarrow\quad y^2 = x^2\Bigl(1-\frac{x^2}{2}\Bigr),
\]
and sketch them in the phase plane.

\emph{Hint:} Set \(H(x,y)=0\) and solve for \(y\) in terms of \(x\). Observe that the separatrices intersect the \(x\)-axis only at the origin.

(v) We can go further and parametrize one branch of the separatrix as a solution \((x(t),y(t))\) of the system. Use the energy relation for the level \(H=0\),
\[
\frac12 (x')^2 + \frac14 (x^2-1)^2 = 0,
\]
to obtain a separable first-order equation for \(x(t)\). Solve this equation to show that there are homoclinic orbits of the form
\[
x(t) = \pm \sqrt{2}\,\operatorname{sech}(t-t_0), 
\qquad y(t)=x'(t),
\]
which leave the saddle \((0,0)\) as \(t\to -\infty\), loop around one of the wells, and return to \((0,0)\) as \(t\to +\infty\).

\emph{Hint:} After setting \(H=0\), you should find an equation of the form \(x' = \pm x\sqrt{1-\frac{x^2}{2}}\), which can be separated and integrated. A substitution based on \(x=\sqrt{2}\,\sech t\) is convenient for inverting the implicit solution.

\medskip
\noindent
(e) \emph{Extensions and perturbations.}

(i) Suppose we add a small linear damping term to the equation of motion,
\[
x'' + \varepsilon x' + V'(x) = 0,\quad \varepsilon>0 \text{ small}.
\]
Qualitatively, what happens to the conserved energy \(H\) and to the separatrix as time evolves? What kinds of trajectories would you expect to see for initial conditions that are just inside or just outside the original separatrix?

(ii) In higher-dimensional Hamiltonian systems with multiple saddle points in phase space, one often encounters \emph{heteroclinic} orbits that connect \emph{different} saddles. Based on your understanding of this double-well example, explain in words how separatrices and heteroclinic connections might organize the possible transitions between different “wells” in a more complicated potential landscape.
\end{problem}

% ===== Example 5: Double-Well Potential: Separatrices and Heteroclinic Orbits (full solution) =====
\begin{problem}[Double-Well Potential: Separatrices and Heteroclinic Orbits]
Consider the planar system
\[
x' = y,\qquad y' = x - x^3.
\]
\begin{enumerate}
\item Show that the function
\[
H(x,y)=\frac12 y^2 + \frac14(x^2-1)^2
\]
is constant along trajectories, and interpret \(H\) as the total energy of a particle moving in the double-well potential \(V(x)=\frac14(x^2-1)^2\).

\item Find all equilibria and classify them in the phase plane by computing the eigenvalues of the Jacobian of the vector field. Relate your classification to the shape of the potential \(V\).

\item Compute the energy levels \(H\) at the equilibria and describe qualitatively the phase portraits:
  \begin{enumerate}
  \item for energies between the minimum and the barrier height,
  \item for energies above the barrier height.
  \end{enumerate}

\item Show that the critical energy level through the saddle at the origin is given by \(H(x,y)=0\), and that on this level the separatrix curves satisfy
\[
y^2 = x^2\Bigl(1-\frac{x^2}{2}\Bigr).
\]
Sketch these separatrices and explain why they form the boundary between trajectories trapped in a single well and trajectories that can cross over the barrier.

\item Using the energy relation at level \(H=0\), derive a separable first-order equation for \(x(t)\) and solve it to obtain an explicit homoclinic orbit. Show that
\[
x(t) = \sqrt{2}\,\operatorname{sech}(t-t_0),\qquad y(t)=x'(t),
\]
parametrizes one branch of the separatrix and that \((x(t),y(t))\to(0,0)\) as \(t\to\pm\infty\).
\end{enumerate}
\end{problem}

\begin{solution}
We study a one-degree-of-freedom conservative mechanical system written as a first-order planar system. The central ideas are the conservation of an energy-like Hamiltonian, the classification of equilibria via linearization and the potential shape, and the role of separatrices (here, homoclinic loops) as boundaries between qualitatively different regions of phase space. This is a prototypical example of phase space dynamics for conservative systems, and it also serves as a starting point for understanding how perturbations (such as damping) deform such structures.

\medskip
\noindent\textbf{1. Energy and the double-well potential.}

The given system is
\[
x' = y,\qquad y' = x - x^3.
\]
We are told to consider
\[
H(x,y)=\frac12 y^2 + \frac14(x^2-1)^2.
\]
To show that \(H\) is conserved, we compute its derivative along a trajectory \((x(t),y(t))\):
\[
\frac{d}{dt}H(x(t),y(t))
= \frac{\partial H}{\partial x} x' + \frac{\partial H}{\partial y} y'.
\]
We first compute the partial derivatives:
\[
\frac{\partial H}{\partial x}
= \frac14\cdot 2
\[
\frac{\partial H}{\partial x}
= \frac14 \cdot 2(x^2-1)\cdot 2x
= x(x^2-1)=x^3-x,
\qquad
\frac{\partial H}{\partial y}=y.
\]
Therefore
\[
\frac{d}{dt}H(x(t),y(t))
= (x^3-x)x' + y\,y'
= (x^3-x)y + y(x-x^3) = 0.
\]
So \(H\) is constant along trajectories.

Interpreting the terms,
\[
H(x,y) = \frac12 y^2 + V(x),
\qquad V(x) = \frac14(x^2-1)^2,
\]
we see that \(\tfrac12 y^2\) is the kinetic energy and \(V(x)\) is the potential energy. Thus \(H\) is the total mechanical energy of a particle moving in the double-well potential \(V\).

\medskip
\noindent\textbf{2. Equilibria, Jacobian, and relation to the potential.}

Equilibria satisfy
\[
x'=0,\qquad y'=0
\quad\Longrightarrow\quad
y=0,\; x-x^3=0.
\]
Hence
\[
x(x^2-1)=0 \quad\Longrightarrow\quad x\in\{0,1,-1\},
\]
so the equilibria are
\[
(x,y)=(0,0),\quad (1,0),\quad (-1,0).
\]

The Jacobian of the vector field \(F(x,y)=(y,\;x-x^3)\) is
\[
DF(x,y)
=
\begin{pmatrix}
\frac{\partial}{\partial x}(y) & \frac{\partial}{\partial y}(y)\\[4pt]
\frac{\partial}{\partial x}(x-x^3) & \frac{\partial}{\partial y}(x-x^3)
\end{pmatrix}
=
\begin{pmatrix}
0 & 1\\[4pt]
1-3x^2 & 0
\end{pmatrix}.
\]
At an equilibrium \((x_0,0)\), the Jacobian is
\[
J(x_0)=
\begin{pmatrix}
0 & 1\\
1-3x_0^2 & 0
\end{pmatrix}.
\]
The characteristic polynomial is
\[
\lambda^2 - \mathrm{tr}(J)\lambda + \det(J)
= \lambda^2 + \det(J),
\]
since \(\mathrm{tr}(J)=0\). Moreover
\[
\det(J)=0\cdot 0 - 1(1-3x_0^2) = -(1-3x_0^2),
\]
so the eigenvalues satisfy
\[
\lambda^2 - (1-3x_0^2)=0
\quad\Longrightarrow\quad
\lambda=\pm\sqrt{1-3x_0^2}.
\]

\emph{At \((0,0)\):}  
Here \(x_0=0\), so
\[
J(0,0)=
\begin{pmatrix}
0 & 1\\
1 & 0
\end{pmatrix},
\qquad
\lambda=\pm1.
\]
The eigenvalues are real with opposite signs, so \((0,0)\) is a \emph{saddle} in the phase plane.

\emph{At \((\pm1,0)\):}  
Here \(x_0=\pm1\), so \(1-3x_0^2=1-3=-2\), and
\[
J(\pm1,0)=
\begin{pmatrix}
0 & 1\\
-2 & 0
\end{pmatrix},
\qquad
\lambda^2+2=0 \;\Longrightarrow\; \lambda=\pm i\sqrt{2}.
\]
The eigenvalues are purely imaginary, so the linearization has a center. Because the system is Hamiltonian (it has a nondegenerate conserved quantity \(H\)), these equilibria are in fact \emph{nonlinear centers}: nearby trajectories are closed curves (small oscillations in each well).

\smallskip
\noindent\emph{Relation to the potential.}

The equilibria occur at critical points of \(V\), that is, at \(V'(x_0)=0\). Here
\[
V'(x)=x(x^2-1),\qquad
V''(x)=3x^2-1.
\]
Then
\[
V''(0)=-1<0 \;\Rightarrow\; V \text{ has a local maximum at } x=0,
\]
\[
V''(\pm1)=2>0 \;\Rightarrow\; V \text{ has local minima at } x=\pm1.
\]
Thus the particle sits at the top of the barrier at \(x=0\) (unstable equilibrium, saddle in phase space) and at the bottoms of the wells at \(x=\pm1\) (stable in the 1D mechanical sense, centers in phase space). This is the standard correspondence in 1D Hamiltonian systems: local maxima of \(V\) give saddles, local minima of \(V\) give centers.

\medskip
\noindent\textbf{3. Energy levels and qualitative phase portraits.}

We first compute the energy at the equilibria:
\[
H(x,y)=\frac12 y^2 + \frac14(x^2-1)^2.
\]

\emph{At the well minima \((\pm1,0)\):}
\[
H(\pm1,0)
= \frac12\cdot 0^2 + \frac14\,(1-1)^2
=0.
\]
This is the minimum possible energy, so \(E_{\min}=0\).

\emph{At the barrier top \((0,0)\):}
\[
H(0,0)=\frac12\cdot0^2 + \frac14\,(0^2-1)^2=\frac14.
\]
Thus the barrier energy is \(E_{\mathrm{bar}}=\frac14\).

\smallskip
\noindent\emph{(a) Energies between the minimum and the barrier.}

Take \(E\) with
\[
0<E<\frac14.
\]
The level set in the phase plane is defined by
\[
\frac12 y^2 + \frac14(x^2-1)^2 = E.
\]
For such \(E\), the condition \(V(x)\le E\) restricts \(x\) to two disjoint intervals, one around \(x=-1\) and one around \(x=+1\), because the barrier at \(x=0\) has higher potential \(V(0)=\frac14>E\). Consequently, the level set \(H(x,y)=E\) consists of \emph{two disjoint closed curves}: one surrounding the center at \((-1,0)\), the other surrounding the center at \((1,0)\). These correspond to periodic oscillations trapped in a single well: the particle oscillates back and forth around one minimum and cannot reach or cross the barrier.

\smallskip
\noindent\emph{(b) Energies above the barrier.}

Now take \(E>\frac14\). Then
\[
V(x)\le E
\]
for all \(x\) in a single connected interval \([-x_{\max},x_{\max}]\) containing the origin, because \(V(x)\to\infty\) as \(|x|\to\infty\) but its maximum between the wells is only \(\tfrac14<E\). Hence the level set \(H(x,y)=E\) is a \emph{single} closed curve that passes over the barrier region near \((0,0)\). 

In terms of dynamics, the particle moves in a single large oscillation that explores both wells: it repeatedly passes over the barrier, moves to one side, turns around at a turning point, crosses back over the barrier, and so on. In the phase plane, this is a large closed orbit enclosing all three equilibria.

\medskip
\noindent\textbf{4. The critical energy level and the separatrices.}

The critical energy is the barrier energy
\[
E_{\mathrm{bar}} = H(0,0) = \frac14.
\]
The corresponding level set
\[
H(x,y)=\frac14
\]
passes through the saddle \((0,0)\) and separates the “trapped” oscillations from the “over-the-barrier” oscillations.

Starting from
\[
\frac12 y^2 + \frac14(x^2-1)^2 = \frac14,
\]
we simplify:
\[
\frac12 y^2 + \frac14(x^2-1)^2 - \frac14 = 0
\;\Longleftrightarrow\;
\frac12 y^2 + \frac14(x^4 - 2x^2 + 1) -\frac14 =0
\]
\[
\Longleftrightarrow\;
\frac12 y^2 + \frac14 x^4 - \frac12 x^2 =0
\;\Longleftrightarrow\;
y^2 + \frac12 x^4 - x^2 =0
\]
\[
\Longleftrightarrow\;
y^2 = x^2 - \frac12 x^4
= x^2\Bigl(1-\frac{x^2}{2}\B
igr).
\]
Thus the critical level through the saddle is given implicitly by
\[
y^2 = x^2\Bigl(1-\frac{x^2}{2}\Bigr),
\]
or, equivalently,
\[
y = \pm\, x\,\sqrt{1-\frac{x^2}{2}},\qquad |x|\le \sqrt{2}.
\]
These curves are symmetric with respect to both coordinate axes and pass through the saddle \((0,0)\). They form two closed “figure-eight”–like loops (one around each well), and they separate initial conditions leading to trajectories trapped in a single well (inside the loops) from those that can cross the barrier (outside the loops).

\medskip
\noindent\textbf{5. Explicit homoclinic orbit on the separatrix.}

We now parametrize one branch of the separatrix as a solution \((x(t),y(t))\). On the critical energy level through the saddle we have
\[
H(x,y)=\frac14 \quad\Longleftrightarrow\quad
\frac12 (x')^2 + \frac14(x^2-1)^2 = \frac14.
\]
Rewriting,
\[
\frac12 (x')^2 = \frac14 - \frac14(x^2-1)^2
= \frac14\bigl[1-(x^2-1)^2\bigr]
= \frac14\bigl(2x^2 - x^4\bigr)
= \frac14 x^2(2-x^2),
\]
so
\[
(x')^2 = x^2\Bigl(1-\frac{x^2}{2}\Bigr)
\quad\Longrightarrow\quad
x' = \pm x\sqrt{1-\frac{x^2}{2}}.
\]
This is a separable first-order equation. Instead of performing the full integration, we can propose the explicit form suggested in the problem statement and verify that it satisfies the second-order equation
\[
x'' = x - x^3.
\]

Take
\[
x(t) = \sqrt{2}\,\sech\bigl(t-t_0\bigr),
\]
with \(t_0\in\mathbb{R}\) arbitrary (time-translation invariance). Let \(s=t-t_0\); then
\[
x(t) = \sqrt{2}\,\sech s.
\]
Using the identity
\[
\frac{d^2}{ds^2}\sech s = \sech s - 2\sech^3 s,
\]
we compute
\[
x''(t) = \sqrt{2}\,\bigl(\sech s - 2\sech^3 s\bigr).
\]
Moreover,
\[
x^3(t) = (\sqrt{2}\,\sech s)^3 = 2\sqrt{2}\,\sech^3 s,
\]
so
\[
x(t) - x^3(t)
= \sqrt{2}\,\sech s - 2\sqrt{2}\,\sech^3 s
= \sqrt{2}\,\bigl(\sech s - 2\sech^3 s\bigr)
= x''(t).
\]
Hence \(x(t)\) satisfies
\[
x'' = x - x^3,
\]
which is equivalent to the original planar system \(x'=y,\ y'=x-x^3\) when we set
\[
y(t) = x'(t) = -\sqrt{2}\,\sech s\,\tanh s.
\]

As \(t\to\pm\infty\), we have \(s\to\pm\infty\) and \(\sech s\to 0\), \(\tanh s\to\pm1\), so
\[
x(t)\to 0,\qquad y(t)=x'(t)\to 0.
\]
Thus \((x(t),y(t))\) is a \emph{homoclinic orbit} to the saddle \((0,0)\): it leaves the saddle as \(t\to-\infty\), loops around one well, and returns to the saddle as \(t\to+\infty\).

By symmetry, the function
\[
x(t) = -\sqrt{2}\,\sech(t-t_0),\qquad y(t)=x'(t),
\]
gives the symmetric homoclinic loop around the other well. Together, these two homoclinic trajectories form the pair of separatrix loops that bound the trapped oscillations in each well and separate them from the over-the-barrier motions.

\end{solution}

% ===== Example 6: Small Perturbations of a Hamiltonian System (inquiry-based) =====
\begin{problem}[Small Perturbations of a Hamiltonian System]
Consider a unit mass attached to a linear spring, moving without friction on a horizontal surface. In the idealized, frictionless case, the motion is perfectly periodic and the total mechanical energy is conserved. In phase space, this gives rise to closed orbits corresponding to constant energy levels. In reality, small non-conservative effects such as weak damping cause the energy to change slowly in time, and trajectories drift between these energy levels in a way that can often be captured by averaging or energy-balance methods.

We study the scalar equation
\[
x'' + x = -\varepsilon x', \qquad 0 < \varepsilon \ll 1,
\]
where $x(t)$ is the displacement, the term $x$ arises from the restoring spring force, and the term $-\varepsilon x'$ models weak linear damping.

\medskip

(a) First consider the unperturbed system with no damping,
\[
x'' + x = 0.
\]
Introduce the velocity variable $y = x'$ and write the system in first-order form. Define the \emph{Hamiltonian}
\[
H(x,y) = \frac{1}{2}x^2 + \frac{1}{2}y^2.
\]
Show that along any solution of the unperturbed system we have $\dfrac{dH}{dt} = 0$. Describe the level sets $\{H = E\}$ in the $(x,y)$-plane and explain why each of them (for $E>0$) is a periodic orbit. What is the period of these orbits?

% Hint: Compute $\dot H = H_x \dot x + H_y \dot y$ and substitute the equations of motion. For the period, solve $x''+x=0$ explicitly, or note that the system is a harmonic oscillator.

\medskip

(b) Now consider the weakly damped system
\[
x'' + x = -\varepsilon x', \qquad 0<\varepsilon \ll 1.
\]
Again set $y = x'$ and use the same Hamiltonian $H(x,y)$ as in part (a). Compute $\dfrac{dH}{dt}$ along solutions of this \emph{perturbed} system and show that
\[
\frac{dH}{dt} = -\varepsilon\, y^2 \le 0.
\]
Interpret this inequality in terms of the mechanical energy of the oscillator and the shape of trajectories in the phase plane. What happens to the invariant curves $\{H = \text{constant}\}$ from the Hamiltonian system when $\varepsilon>0$?

% Hint: Only the $y'$ equation has changed compared to part (a). Think about whether a trajectory can stay on a fixed level set of $H$ when $dH/dt<0$ unless $y$ vanishes.

\medskip

(c) In the conservative case $\varepsilon = 0$, the general nontrivial solution can be written in the form
\[
x(t) = r \cos(t+\phi), \qquad y(t) = x'(t) = -r \sin(t+\phi),
\]
where $r>0$ and $\phi$ are constants determined by initial data, and the energy is
\[
H = \frac{1}{2}r^2.
\]
For fixed energy $H=E>0$, compute the \emph{time average} over one period $T = 2\pi$ of the quantity $y^2(t)$:
\[
\langle y^2 \rangle := \frac{1}{T} \int_0^T y^2(t)\,dt.
\]
Show that $\langle y^2 \rangle = E$.

% Hint: Use the trigonometric identity $\sin^2(\theta) = \frac{1}{2}(1-\cos(2\theta))$ and remember that the average of $\cos(2\theta)$ over a full period is zero.

\medskip

(d) We now use the idea that weak damping ($0<\varepsilon\ll 1$) causes the energy to change slowly compared to the fast oscillation. Suppose that for small $\varepsilon>0$ the motion is still approximately sinusoidal with a slowly varying amplitude $r(t)$, so that
\[
x(t) \approx r(t) \cos(t+\phi), \qquad y(t) \approx -r(t) \sin(t+\phi),
\]
and hence $H(t) \approx \tfrac{1}{2}r(t)^2$.

Using your expression for $\dfrac{dH}{dt}$ from part (b) and your average $\langle y^2\rangle$ from part (c), argue (by averaging over one oscillation period) that for small $\varepsilon$
\[
\frac{dH}{dt} \approx -\varepsilon H.
\]
Solve this approximate differential equation for $H(t)$ and express the result in terms of the initial energy $H(0) = H_0$. Compare the decay timescale of $H(t)$ to the oscillation period. How does this manifest in the phase portrait: what do typical trajectories look like in the $(x,y)$-plane when $\varepsilon > 0$ is small?

% Hint: You are replacing $y^2(t)$ by its average value over one period. The resulting ODE for $H$ is linear and separable.

\medskip

(e) Explorations and extensions.

\begin{enumerate}
  \item Suppose instead that the equation is
  \[
  x'' + x = +\varepsilon x'.
  \]
  How does the sign change in $\varepsilon$ affect the sign of $\dfrac{dH}{dt}$ and the qualitative phase portrait? What do typical orbits do now?

  % Hint: Repeat the computation in part (b) with the new sign and think about spirals in the phase plane.

  \item Consider adding a small periodic forcing term:
  \[
  x'' + x = -\varepsilon x' + \varepsilon a \cos(\omega t),
  \]
  with constants $a,\omega>0$ and $0<\varepsilon\ll 1$. Without doing detailed calculations, discuss qualitatively how you might extend the energy-balance or averaging ideas from parts (b)–(d) to study the long-term behavior. In particular, what role might the forcing frequency $\omega$ play in whether the energy tends to zero, stays bounded away from zero, or grows?

  % Hint: Think about whether the forcing is in resonance with the natural frequency (here equal to 1) of the unperturbed oscillator, and how this might affect the average energy input per cycle versus the energy lost to damping.
\end{enumerate}

\end{problem}

% ===== Example 6: Small Perturbations of a Hamiltonian System (full solution) =====
\begin{problem}[Small Perturbations of a Hamiltonian System]
Consider the weakly damped harmonic oscillator
\[
x'' + x = -\varepsilon x', \qquad 0<\varepsilon\ll 1,
\]
and define $y = x'$ and the Hamiltonian
\[
H(x,y) = \frac{1}{2}x^2 + \frac{1}{2}y^2.
\]
\begin{enumerate}
  \item For the unperturbed system $x''+x=0$, write the corresponding first-order system, show that $H$ is conserved, and describe the phase portrait, including the period of the closed orbits.
  \item For the damped system, compute $\dfrac{dH}{dt}$ along solutions and show that $\dfrac{dH}{dt} = -\varepsilon y^2 \le 0$. Interpret this in terms of the phase portrait and the fate of the invariant curves $\{H = \text{constant}\}$.
  \item Using the fact that in the conservative case solutions can be written as $x(t) = r\cos(t+\phi)$, $y(t) = -r\sin(t+\phi)$ with $H = \tfrac{1}{2}r^2$, compute the average of $y^2(t)$ over one period and show that it equals $H$.
  \item Assuming that for $0<\varepsilon\ll 1$ the motion remains approximately sinusoidal with slowly varying amplitude, use an averaging argument to derive the approximate evolution equation
  \[
  \frac{dH}{dt} \approx -\varepsilon H,
  \]
  solve it for $H(t)$, and describe qualitatively the resulting trajectories in the phase plane.
\end{enumerate}
Explain briefly how this example illustrates the effect of small non-conservative perturbations on a Hamiltonian system with closed orbits.
\end{problem}

\begin{solution}
We begin by viewing the equation as a planar system in phase space, with position $x$ and velocity $y=x'$, and by using the function $H(x,y)$ as the energy.

\medskip

\textbf{(1) Unperturbed system: Hamiltonian structure and phase portrait.}

For the unperturbed system,
\[
x'' + x = 0,
\]
we introduce $y = x'$ to obtain the first-order system
\[
\begin{cases}
x' = y,\\[0.4em]
y' = -x.
\end{cases}
\]
We define the Hamiltonian
\[
H(x,y) = \frac{1}{2}x^2 + \frac{1}{2}y^2.
\]
To check conservation of $H$, we differentiate $H$ along a solution:
\[
\frac{dH}{dt} = H_x x' + H_y y' = x\cdot x' + y\cdot y' = x y + y(-x) = 0.
\]
Thus $H$ is constant along trajectories. Geometrically, the level sets
\[
\{(x,y)\colon H(x,y) = E\} = \left\{(x,y)\colon \frac{1}{2}x^2 + \frac{1}{2}y^2 = E\right\}
\]
are circles of radius $r = \sqrt{2E}$ centered at the origin in the $(x,y)$-plane. Since $H$ is constant along solutions, each trajectory with $E>0$ lies on one such circle, giving a closed orbit. The origin corresponds to $E=0$ and is an equilibrium.

The scalar equation $x''+x=0$ has general solution
\[
x(t) = r\cos(t+\phi), \qquad y(t) = x'(t) = -r\sin(t+\phi),
\]
for constants $r\ge0$ and phase $\phi\in\mathbb{R}$. From this representation, each nontrivial solution is periodic with period
\[
T = 2\pi,
\]
independent of $r$ (or $E$). In phase space, the motion is uniform counterclockwise rotation along the circle $x^2 + y^2 = r^2$.

\medskip

\textbf{(2) Damped system: energy decay and loss of invariant curves.}

For the weakly damped system
\[
x'' + x = -\varepsilon x', \qquad 0<\varepsilon\ll 1,
\]
we again set $y = x'$ and obtain
\[
\begin{cases}
x' = y,\\[0.3em]
y' = -x - \varepsilon y.
\end{cases}
\]
We keep the same function $H(x,y) = \frac{1}{2}x^2 + \frac{1}{2}y^2$ and compute its derivative along solutions:
\[
\frac{dH}{dt} = H_x x' + H_y y' = x y + y(-x - \varepsilon y) = x y - x y - \varepsilon y^2 = -\varepsilon\, y^2.
\]
Therefore
\[
\frac{dH}{dt} = -\varepsilon\, y^2 \le 0,
\]
with equality if and only if $y=0$. 

Physically, $H$ is the total mechanical energy (kinetic plus potential) of the oscillator. The formula shows that the energy decreases monotonically along any trajectory except on the $x$-axis, where the velocity $y$ vanishes. This expresses the dissipative effect of the damping term $-\varepsilon x'$: energy is lost at a rate proportional to the square of the velocity.

In the unperturbed Hamiltonian system, each level set $\{H=E\}$ was an invariant closed orbit. In the damped system, these curves are no longer invariant because $H$ is no longer constant; instead, trajectories cross these circles in the direction of decreasing $H$. In the phase plane, typical orbits spiral inward toward the origin, crossing successive circles $H=E$ as the energy decays. Only the equilibrium at the origin, with $x=y=0$, remains as a stationary solution; all nonzero periodic orbits are destroyed by the damping.

\medskip

\textbf{(3) Time average of $y^2$ in the conservative system.}

In the conservative case $\varepsilon = 0$, we have
\[
x(t) = r\cos(t+\phi), \qquad y(t) = -r\sin(t+\phi)
\]
for some amplitude $r>0$ and phase $\phi$. The energy is
\[
H = \frac{1}{2}x^2 + \frac{1}{2}y^2 = \frac{1}{2}r^2.
\]
For a fixed energy level $H=E>0$, we thus have $r^2 = 2E$. Then
\[
y^2(t) = r^2 \sin^2(t+\phi) = 2E \sin^2(t+\phi).
\]
The period of motion is $T=2\pi$. The time average of $y^2$ over one period is
\[
\langle y^2 \rangle 
= \frac{1}{T}\int_0^T y^2(t)\,dt
= \frac{1}{2\pi}\int_0^{2\pi} 2E \sin^2(t+\phi)\,dt.
\]
Using $\sin^2\theta = \tfrac{1}{2}(1-\cos 2\theta)$, we find
\[
\sin^2(t+\phi) = \frac{1}{2}\bigl(1 - \cos(2t+2\phi)\bigr),
\]
so
\[
\langle y^2 \rangle 
= \frac{1}{2\pi}\int_0^{2\pi} 2E \cdot \frac{1}{2}(1 - \cos(2t+2\phi))\,dt
= \frac{E}{\pi}\int_0^{2\pi} \bigl(1 - \cos(2t+2\phi)\bigr)\,dt.
\]
The integral of $1$ over $[0,2\pi]$ is $2\pi$, and the integral of $\cos(2t+2\phi)$ over a full period is zero. Thus
\[
\langle y^2 \rangle = \frac{E}{\pi} \cdot 2\pi = E.
\]
In other words, for the harmonic oscillator the average kinetic energy per unit mass (which is $\tfrac{1}{2}\langle y^2\rangle$) is exactly half of the total energy, and the average of $y^2$ is equal to $H$ itself.

\medskip

\textbf{(4) Averaged evolution of energy under weak damping.}

In the damped system we found
\[
\frac{dH}{dt} = -\varepsilon y^2(t).
\]
For $0<\varepsilon\ll 1$, the damping is weak, so the energy changes slowly compared to the fast oscillation with period $2\pi$. On the timescale of a single oscillation, the trajectory is still approximately sinusoidal; only over many periods does the amplitude noticeably change. This separation of timescales is at the heart of averaging methods.

To make this precise at a heuristic level, we assume that for small $\varepsilon$ the solution can be approximated by
\[
x(t) \approx r(t)\cos(t+\phi), \qquad y(t) \approx -r(t)\sin(t+\phi),
\]
where the amplitude $r(t)$ (and hence the energy $H(t)=\tfrac{1}{2}r^2(t)$) varies slowly. Substituting into the energy derivative gives
\[
\frac{dH}{dt} = -\varepsilon y^2(t) \approx -\varepsilon r^2(t)\sin^2(t+\phi).
\]
We now average this expression over one period $T=2\pi$ of the fast oscillation, keeping $H(t)$ (and thus $r(t)$) essentially constant over that short interval. Using the time average from part (3), we obtain
\[
\left\langle \frac{dH}{dt} \right\rangle 
\approx -\varepsilon \langle y^2 \rangle 
= -\varepsilon H.
\]
Thus the \emph{averaged} evolution of the slowly varying energy satisfies the approximate differential equation
\[
\frac{dH}{dt} \approx -\varepsilon H.
\]

This linear, separable equation has solution
\[
H(t) \approx H_0 e^{-\varepsilon t},
\]
where $H_0 = H(0)$ is the initial energy. The decay timescale is of order $1/\varepsilon$, which is much larger than the oscillation period $2\pi$ when $\varepsilon\ll 1$. Hence the system performs many nearly Hamiltonian oscillations before its energy changes significantly.

In the phase plane, this means that trajectories still look like closed circles for short times, but on a longer timescale they slowly spiral inward toward the origin, crossing the former invariant energy curves. Locally, the motion is tangent to the Hamiltonian circles (fast rotation), but there is a slow radial drift inward due to the damping. This is a typical picture of how a small non-conservative perturbation deforms invariant curves of a conservative system and turns periodic orbits into slow spirals.

\medskip

\textbf{Connection to the chapter theme.}

This example begins with a two-dimensional Hamiltonian system whose phase portrait consists of closed orbits corresponding to energy level sets. The Hamiltonian $H$ is conserved, and the phase flow is area-preserving. Introducing a small non-conservative perturbation in the form of weak damping breaks exact energy conservation: the invariant curves $\{H=\text{constant}\}$ are no longer invariant, and trajectories drift monotonically across them. By exploiting the separation between the fast oscillations and the slow energy decay, we use an averaging (or energy-balance) argument to derive an effective one-dimensional evolution law for the energy. This illustrates a central theme of \emph{phase space dynamics for conservative and perturbed systems}: small perturbations of Hamiltonian systems often produce slow drift along or across invariant manifolds, which can be captured by reduced, averaged equations that describe the long-term behavior.
\end{solution}

